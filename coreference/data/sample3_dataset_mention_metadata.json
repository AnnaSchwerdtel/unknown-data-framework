[{"mentioned_in_paper": "0", "context_id": "2", "dataset_context": "Despite the simplicity of the proposed method, our experiments with different state-of-the-art deep learning architectures on PASCAL VOC and MS COCO datasets demonstrate the effectiveness and generality of our Mutual Guidance strategy.", "mention_start": 125, "mention_end": 157, "dataset_mention": "PASCAL VOC and MS COCO datasets"}, {"mentioned_in_paper": "0", "context_id": "20", "dataset_context": "Despite the simplicity of the proposed strategy, Mutual Guidance brings consistent Average Precision (AP) gains over the traditional static strategy with different deep learn-ing architectures on PASCAL VOC [13] and MS COCO [14] datasets, especially on strict metrics such as AP75.", "mention_start": 195, "mention_end": 237, "dataset_mention": "PASCAL VOC [13] and MS COCO [14] datasets"}, {"mentioned_in_paper": "0", "context_id": "84", "dataset_context": "The backbone networks are pre-trained on ImageNet-1k classification dataset [28].", "mention_start": 41, "mention_end": 75, "dataset_mention": "ImageNet-1k classification dataset"}, {"mentioned_in_paper": "0", "context_id": "93", "dataset_context": "PASCAL VOC dataset has 20 object categories.", "mention_start": 0, "mention_end": 18, "dataset_mention": "PASCAL VOC dataset"}, {"mentioned_in_paper": "0", "context_id": "95", "dataset_context": "MS COCO dataset contains 80 classes.", "mention_start": 0, "mention_end": 15, "dataset_mention": "MS COCO dataset"}, {"mentioned_in_paper": "0", "context_id": "100", "dataset_context": "Since the size of the objects greatly varies between MS COCO and PASCAL VOC, these size-dependent measures are ignored when experimenting with PASCAL VOC dataset.", "mention_start": 142, "mention_end": 161, "dataset_mention": "PASCAL VOC dataset"}, {"mentioned_in_paper": "0", "context_id": "103", "dataset_context": "The results obtained on the PASCAL VOC dataset are given in Table 1.", "mention_start": 24, "mention_end": 46, "dataset_mention": "the PASCAL VOC dataset"}, {"mentioned_in_paper": "0", "context_id": "113", "dataset_context": "We then conduct experiments on the more difficult MS COCO [14] dataset and report our results in Table 2.", "mention_start": 31, "mention_end": 70, "dataset_mention": "the more difficult MS COCO [14] dataset"}, {"mentioned_in_paper": "0", "context_id": "134", "dataset_context": "We assess our method on different architectures and different public datasets and compare it with the traditional static anchor matching strategy.", "mention_start": 24, "mention_end": 77, "dataset_mention": "different architectures and different public datasets"}, {"mentioned_in_paper": "0", "context_id": "143", "dataset_context": "Experiments are performed on the PASCAL VOC dataset.", "mention_start": 29, "mention_end": 51, "dataset_mention": "the PASCAL VOC dataset"}, {"mentioned_in_paper": "1", "context_id": "180", "dataset_context": "\u2022 MSRC-v1 [29] : This is a scene recognition dataset containing 240 images in 8 categories.", "mention_start": 24, "mention_end": 52, "dataset_mention": "a scene recognition dataset"}, {"mentioned_in_paper": "1", "context_id": "203", "dataset_context": "To simulate a multi-view dataset with low-quality views, we added a fake view to the Caltech-101-7 dataset and Gaussian noise to two original views on the Caltech-101-20 dataset.", "mention_start": 80, "mention_end": 106, "dataset_mention": "the Caltech-101-7 dataset"}, {"mentioned_in_paper": "1", "context_id": "203", "dataset_context": "To simulate a multi-view dataset with low-quality views, we added a fake view to the Caltech-101-7 dataset and Gaussian noise to two original views on the Caltech-101-20 dataset.", "mention_start": 150, "mention_end": 177, "dataset_mention": "the Caltech-101-20 dataset"}, {"mentioned_in_paper": "1", "context_id": "207", "dataset_context": "We report the clustering performance of K-means for each view of the Caltech-101-7 dataset in Fig. 6.", "mention_start": 65, "mention_end": 90, "dataset_mention": "the Caltech-101-7 dataset"}, {"mentioned_in_paper": "1", "context_id": "210", "dataset_context": "We report the weight change curves of weight for DACK and LACK on the Caltech-101-20 dataset in Fig. 2. The results show that view 4 is secondary emphasized or emphasized in LACK while its importance is ignored in DACK when \u03c4 = {0.01,", "mention_start": 66, "mention_end": 92, "dataset_mention": "the Caltech-101-20 dataset"}, {"mentioned_in_paper": "1", "context_id": "213", "dataset_context": "The matrix X 7 is called the fake view and is incorporated into the Caltech-101-7 dataset as 7th view to simulate the lowquality dataset with the fake view.", "mention_start": 64, "mention_end": 89, "dataset_mention": "the Caltech-101-7 dataset"}, {"mentioned_in_paper": "1", "context_id": "213", "dataset_context": "The matrix X 7 is called the fake view and is incorporated into the Caltech-101-7 dataset as 7th view to simulate the lowquality dataset with the fake view.", "mention_start": 114, "mention_end": 136, "dataset_mention": "the lowquality dataset"}, {"mentioned_in_paper": "1", "context_id": "219", "dataset_context": "We report the clustering performance of K-means for each view of the Caltech-101-20 dataset in Table 7.", "mention_start": 65, "mention_end": 91, "dataset_mention": "the Caltech-101-20 dataset"}, {"mentioned_in_paper": "1", "context_id": "221", "dataset_context": "We report the weight change curves of weight for DACK and LACK on the Caltech-101-20 dataset in Fig. 3.", "mention_start": 66, "mention_end": 92, "dataset_mention": "the Caltech-101-20 dataset"}, {"mentioned_in_paper": "1", "context_id": "229", "dataset_context": "Firstly, on the Caltech-101-7 and Caltech-101-20 datasets, we randomly selected five different sets of training data and test data.", "mention_start": 11, "mention_end": 57, "dataset_mention": "the Caltech-101-7 and Caltech-101-20 datasets"}, {"mentioned_in_paper": "2", "context_id": "386", "dataset_context": "Initially we consider a semantically-rich hypothetical dataset with four-attribute records (gross domestic product, population, per capita income and country name).", "mention_start": 22, "mention_end": 62, "dataset_mention": "a semantically-rich hypothetical dataset"}, {"mentioned_in_paper": "3", "context_id": "207", "dataset_context": "The Yelp dataset is provided by the Yelp Dataset Challenge, 5 which has been used in previous research in recommendation [5], [18], [30].", "mention_start": 0, "mention_end": 16, "dataset_mention": "The Yelp dataset"}, {"mentioned_in_paper": "3", "context_id": "207", "dataset_context": "The Yelp dataset is provided by the Yelp Dataset Challenge, 5 which has been used in previous research in recommendation [5], [18], [30].", "mention_start": 32, "mention_end": 48, "dataset_mention": "the Yelp Dataset"}, {"mentioned_in_paper": "3", "context_id": "209", "dataset_context": "The Douban Dataset is obtained from [15].", "mention_start": 0, "mention_end": 18, "dataset_mention": "The Douban Dataset"}, {"mentioned_in_paper": "4", "context_id": "121", "dataset_context": "We assume the data is subjected to a 2D Gaussian distribution and two datasets was randomly sampled with different mean centers and stand deviations in order to simulate the Non-IID scenario.", "mention_start": 35, "mention_end": 78, "dataset_mention": "a 2D Gaussian distribution and two datasets"}, {"mentioned_in_paper": "4", "context_id": "146", "dataset_context": "We split it into training dataset of 4.8 million images and validation dataset of 0.5 million images for FedBoosting.", "mention_start": 37, "mention_end": 78, "dataset_mention": "4.8 million images and validation dataset"}, {"mentioned_in_paper": "4", "context_id": "161", "dataset_context": "We deploy Synth90K and SynthText datasets on two separate clients.", "mention_start": 10, "mention_end": 41, "dataset_mention": "Synth90K and SynthText datasets"}, {"mentioned_in_paper": "4", "context_id": "169", "dataset_context": "For example, the FedAvg model with batch size of 800 and epoch of 1 reports 86.67% on IIIT5k dataset, where an improvement of 1.19% is achieved compared to CRNN that is of 85.48% using the same setting.", "mention_start": 85, "mention_end": 100, "dataset_mention": "IIIT5k dataset"}, {"mentioned_in_paper": "4", "context_id": "170", "dataset_context": "An improvement of 1.65% of FedAvg with the batch size of 512 and epoch of 1 can be observed on IC15 dataset.", "mention_start": 95, "mention_end": 107, "dataset_mention": "IC15 dataset"}, {"mentioned_in_paper": "4", "context_id": "179", "dataset_context": "\"90K\" and \"ST\" stand for Synth90K and SynthText datasets respectively.", "mention_start": 25, "mention_end": 56, "dataset_mention": "Synth90K and SynthText datasets"}, {"mentioned_in_paper": "4", "context_id": "183", "dataset_context": "Even it has accuracy raising on IC15 dataset from 72.37% to 73.00%.", "mention_start": 32, "mention_end": 44, "dataset_mention": "IC15 dataset"}, {"mentioned_in_paper": "4", "context_id": "187", "dataset_context": "As DP encryption is only used to encrypt local gradients between clients for evaluation and get the results on all clients' validation datasets, so DP has little impact on global gradients generating.", "mention_start": 111, "mention_end": 143, "dataset_mention": "all clients' validation datasets"}, {"mentioned_in_paper": "5", "context_id": "18", "dataset_context": "The example images are taken from the recently released NRSfM Challenge Dataset [14].", "mention_start": 47, "mention_end": 79, "dataset_mention": "released NRSfM Challenge Dataset"}, {"mentioned_in_paper": "5", "context_id": "184", "dataset_context": "We performed extensive experiments on both new and old benchmark datasets [1, 31, 14].", "mention_start": 38, "mention_end": 73, "dataset_mention": "both new and old benchmark datasets"}, {"mentioned_in_paper": "5", "context_id": "187", "dataset_context": "To keep our statistics consistent with the newly proposed NRSfM dataset, we used their error evaluation code to compute the robust root mean square error (RMSE) metric as proposed in Taylor et al. work [29].", "mention_start": 39, "mention_end": 71, "dataset_mention": "the newly proposed NRSfM dataset"}, {"mentioned_in_paper": "5", "context_id": "188", "dataset_context": "( Figure 5 : Reconstruction results of our method on the NRSfM synthetic benchmark dataset [1, 2].", "mention_start": 52, "mention_end": 90, "dataset_mention": "the NRSfM synthetic benchmark dataset"}, {"mentioned_in_paper": "5", "context_id": "191", "dataset_context": " [19] on NRSfM challenge dataset [14].", "mention_start": 8, "mention_end": 32, "dataset_mention": "NRSfM challenge dataset"}, {"mentioned_in_paper": "5", "context_id": "194", "dataset_context": "leased this dataset as a part of NRSfM competition held at CVPR 2017 [14].", "mention_start": 0, "mention_end": 19, "dataset_mention": "leased this dataset"}, {"mentioned_in_paper": "6", "context_id": "203", "dataset_context": "Note that for the Market1501 dataset, since there are on average 14.8 cross-camera ground truth matches for each query, we additionally use mean average precision(mAP) as in [42] to evaluate the performance.", "mention_start": 14, "mention_end": 36, "dataset_mention": "the Market1501 dataset"}, {"mentioned_in_paper": "6", "context_id": "226", "dataset_context": " (5) The deep learning based method [1] does not fare well on this small dataset despite the fact that the model has been pretrained on the far-larger CUHK01+CUHK03 datasets.", "mention_start": 135, "mention_end": 173, "dataset_mention": "the far-larger CUHK01+CUHK03 datasets"}, {"mentioned_in_paper": "6", "context_id": "238", "dataset_context": "It can be seen from Table 4 that, as expected, on this much larger dataset, the deep learning based model [1] with its millions of parameters becomes much more competitive -with manually cropped images, our result with single feature type is higher on Rank 1 but lower on other ranks.", "mention_start": 49, "mention_end": 74, "dataset_mention": "this much larger dataset"}, {"mentioned_in_paper": "6", "context_id": "247", "dataset_context": "For semi-supervised setting, we use the VIPeR and PRID2011 datasets.", "mention_start": 35, "mention_end": 67, "dataset_mention": "the VIPeR and PRID2011 datasets"}, {"mentioned_in_paper": "8", "context_id": "20", "dataset_context": "MS-COCO dataset [15]) is that our system relies on background information on art history and artistic styles.", "mention_start": 0, "mention_end": 15, "dataset_mention": "MS-COCO dataset"}, {"mentioned_in_paper": "8", "context_id": "32", "dataset_context": "Mensink and Van Gemert introduce in [19] the large-scale Rijksmuseum dataset for multi-class prediction, consisting on 112,039 images from artistic objects, although only 3,593 are from fine-art paintings.", "mention_start": 36, "mention_end": 76, "dataset_mention": "[19] the large-scale Rijksmuseum dataset"}, {"mentioned_in_paper": "8", "context_id": "43", "dataset_context": "3 SemArt Dataset", "mention_start": 0, "mention_end": 16, "dataset_mention": "3 SemArt Dataset"}, {"mentioned_in_paper": "8", "context_id": "44", "dataset_context": "To create the SemArt dataset, we collect artistic data from the Web Gallery of Art (WGA), a website with more than 44,809 images of European fine-art reproductions between the 8th and the 19th century.", "mention_start": 10, "mention_end": 28, "dataset_mention": "the SemArt dataset"}, {"mentioned_in_paper": "8", "context_id": "185", "dataset_context": "We presented the SemArt dataset, the first collection of fine-art images with attributes and artistic comments for semantic art understanding.", "mention_start": 0, "mention_end": 31, "dataset_mention": "We presented the SemArt dataset"}, {"mentioned_in_paper": "9", "context_id": "171", "dataset_context": "For all the datasets, we split the categories into seen and unseen sets in the same way as [10] : (1) There are two attribute datasets in aP&Y: aPascal and aYahoo.", "mention_start": 111, "mention_end": 134, "dataset_mention": "two attribute datasets"}, {"mentioned_in_paper": "9", "context_id": "173", "dataset_context": "The categories in aPascal dataset are used as seen classes and those in aYahoo as the unseen ones.", "mention_start": 18, "mention_end": 33, "dataset_mention": "aPascal dataset"}, {"mentioned_in_paper": "9", "context_id": "175", "dataset_context": "(3) CUB-200 is a bird dataset for fine-grained recognition.", "mention_start": 15, "mention_end": 29, "dataset_mention": "a bird dataset"}, {"mentioned_in_paper": "10", "context_id": "12", "dataset_context": "HG2-LB HR F-Clip L-CNN [37] HAWP [29] TP-LSD [10] HT [14] Figure 1 : Speed (ms) versus accuracy (sAP 5 ) trade-off of state-ofthe-art algorithms on the ShanghaiTech wireframe dataset.", "mention_start": 147, "mention_end": 182, "dataset_mention": "the ShanghaiTech wireframe dataset"}, {"mentioned_in_paper": "10", "context_id": "107", "dataset_context": "The (c), (d), (e) figures are the histograms of line angles in the Shang-haiTech dataset [9], the YorkUrban dataset [3], and the SceneCity dataset [39].", "mention_start": 62, "mention_end": 88, "dataset_mention": "the Shang-haiTech dataset"}, {"mentioned_in_paper": "10", "context_id": "107", "dataset_context": "The (c), (d), (e) figures are the histograms of line angles in the Shang-haiTech dataset [9], the YorkUrban dataset [3], and the SceneCity dataset [39].", "mention_start": 93, "mention_end": 115, "dataset_mention": " the YorkUrban dataset"}, {"mentioned_in_paper": "10", "context_id": "107", "dataset_context": "The (c), (d), (e) figures are the histograms of line angles in the Shang-haiTech dataset [9], the YorkUrban dataset [3], and the SceneCity dataset [39].", "mention_start": 120, "mention_end": 146, "dataset_mention": " and the SceneCity dataset"}, {"mentioned_in_paper": "10", "context_id": "198", "dataset_context": "We train and test F-Clip on the ShanghaiTech wireframe dataset [9], which contains 5,000 training images and 462 testing images of man-made scenes.", "mention_start": 28, "mention_end": 62, "dataset_mention": "the ShanghaiTech wireframe dataset"}, {"mentioned_in_paper": "10", "context_id": "199", "dataset_context": "We also include York Urban dataset [3], a small dataset containing 102 images, as the testing dataset to evaluate the generalizability of different methods.", "mention_start": 16, "mention_end": 34, "dataset_mention": "York Urban dataset"}, {"mentioned_in_paper": "10", "context_id": "204", "dataset_context": "We use the pre-trained models provided by the authors of each paper for evaluation, which are also trained on the ShanghaiTech wireframe dataset.", "mention_start": 109, "mention_end": 144, "dataset_mention": "the ShanghaiTech wireframe dataset"}, {"mentioned_in_paper": "10", "context_id": "211", "dataset_context": "All of the experiments are performed on the ShanghaiTech dataset [9] and structural AP are reported.", "mention_start": 40, "mention_end": 64, "dataset_mention": "the ShanghaiTech dataset"}, {"mentioned_in_paper": "10", "context_id": "250", "dataset_context": "Finally, F-Clip also achieves the state-of-the-art results on YorkUrban dataset which shows its generalizability.", "mention_start": 61, "mention_end": 79, "dataset_mention": "YorkUrban dataset"}, {"mentioned_in_paper": "10", "context_id": "252", "dataset_context": "For the ShanghaiTech dataset, our method achieves higher recall and performs better in the higher recall regime, similarly for the YorkUrban Dataset.", "mention_start": 4, "mention_end": 28, "dataset_mention": "the ShanghaiTech dataset"}, {"mentioned_in_paper": "10", "context_id": "252", "dataset_context": "For the ShanghaiTech dataset, our method achieves higher recall and performs better in the higher recall regime, similarly for the YorkUrban Dataset.", "mention_start": 126, "mention_end": 148, "dataset_mention": "the YorkUrban Dataset"}, {"mentioned_in_paper": "11", "context_id": "119", "dataset_context": "We evaluate our proposed method on two common WSOL datasets: ImageNet-1k [15] and CUB-200 [25].", "mention_start": 35, "mention_end": 59, "dataset_mention": "two common WSOL datasets"}, {"mentioned_in_paper": "11", "context_id": "120", "dataset_context": "The ImageNet-1k dataset is a large dataset with 1000 classes, containing 1,281,197 training images and 50,000 validation images.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The ImageNet-1k dataset"}, {"mentioned_in_paper": "11", "context_id": "126", "dataset_context": "For the CUB-200 dataset, it contains 200 categories of birds with 5,994 training images and 5,794 testing images.", "mention_start": 4, "mention_end": 23, "dataset_mention": "the CUB-200 dataset"}, {"mentioned_in_paper": "11", "context_id": "162", "dataset_context": "However, our SCR model can only provide one bounding box output for Table 1 : The GT-Known Loc accuracy on the ImageNet-1k validation dataset of various weakly and co-supervised localization (DDT) methods.", "mention_start": 106, "mention_end": 141, "dataset_mention": "the ImageNet-1k validation dataset"}, {"mentioned_in_paper": "11", "context_id": "235", "dataset_context": "Table 5 : Compare our method with state-of-the-art fully supervised methods on ImageNet-1k validation datasets.", "mention_start": 78, "mention_end": 110, "dataset_mention": "ImageNet-1k validation datasets"}, {"mentioned_in_paper": "14", "context_id": "26", "dataset_context": "(c) Beauty Fig. 2. Sales behavior in three Amazon datasets.", "mention_start": 37, "mention_end": 58, "dataset_mention": "three Amazon datasets"}, {"mentioned_in_paper": "14", "context_id": "31", "dataset_context": "Figure 2 is based on the Amazon datasets [15], which demonstrate user purchasing behaviors, as well as the corresponding timestamps over a 500-day period.", "mention_start": 21, "mention_end": 40, "dataset_mention": "the Amazon datasets"}, {"mentioned_in_paper": "14", "context_id": "199", "dataset_context": "Our experiments are conducted on real-world e-commerce datasets from Amazon [15, 35], where each dataset contains user interactions (i.e., reviews) and product metadata (e.g., descriptions, brand, price, features, etc) on one specific product category.", "mention_start": 33, "mention_end": 63, "dataset_mention": "real-world e-commerce datasets"}, {"mentioned_in_paper": "14", "context_id": "256", "dataset_context": "From Figure 5b, we observe that the number of invalid users on the Beauty dataset generated by PGPR is 3, while the number increases to 24 and 29 on other datasets.", "mention_start": 62, "mention_end": 81, "dataset_mention": "the Beauty dataset"}, {"mentioned_in_paper": "14", "context_id": "268", "dataset_context": "Our TPRec achieves the best performance on the Clothing and the Cell Phones datasets, whereas, slightly worse than TimelyRec and SASRec on the Beauty dataset.", "mention_start": 43, "mention_end": 84, "dataset_mention": "the Clothing and the Cell Phones datasets"}, {"mentioned_in_paper": "14", "context_id": "268", "dataset_context": "Our TPRec achieves the best performance on the Clothing and the Cell Phones datasets, whereas, slightly worse than TimelyRec and SASRec on the Beauty dataset.", "mention_start": 138, "mention_end": 157, "dataset_mention": "the Beauty dataset"}, {"mentioned_in_paper": "14", "context_id": "272", "dataset_context": "In the Clothing dataset, even PGPR is much better than other sequence-based methods, which indicates that the KG side information is really useful in the Clothing dataset.", "mention_start": 3, "mention_end": 23, "dataset_mention": "the Clothing dataset"}, {"mentioned_in_paper": "14", "context_id": "272", "dataset_context": "In the Clothing dataset, even PGPR is much better than other sequence-based methods, which indicates that the KG side information is really useful in the Clothing dataset.", "mention_start": 149, "mention_end": 170, "dataset_mention": "the Clothing dataset"}, {"mentioned_in_paper": "14", "context_id": "286", "dataset_context": "To gain more insights of this phenomenon, we visualize the results of the Cell Phones dataset clustering in Figure 6, and the other two datasets have similar performance.", "mention_start": 69, "mention_end": 93, "dataset_mention": "the Cell Phones dataset"}, {"mentioned_in_paper": "14", "context_id": "295", "dataset_context": "It can be seen from the better performance of TPRec over w/    on dataset Clothing and all three datasets in sequence-based test setting.", "mention_start": 63, "mention_end": 105, "dataset_mention": "dataset Clothing and all three datasets"}, {"mentioned_in_paper": "14", "context_id": "306", "dataset_context": "As discussed in Section 4.2, Beauty is a relatively simple dataset and does not need such complex structure.", "mention_start": 38, "mention_end": 66, "dataset_mention": "a relatively simple dataset"}, {"mentioned_in_paper": "14", "context_id": "312", "dataset_context": "(2) In the Cell Phones and Beauty dataset, all three versions of TPRec perform better when the number of time cluster  = 12 or 16.", "mention_start": 7, "mention_end": 41, "dataset_mention": "the Cell Phones and Beauty dataset"}, {"mentioned_in_paper": "14", "context_id": "314", "dataset_context": "(3) In the Cloth dataset, when  increases, the performance of our methods is relatively stable.", "mention_start": 7, "mention_end": 24, "dataset_mention": "the Cloth dataset"}, {"mentioned_in_paper": "14", "context_id": "349", "dataset_context": "The first example (Case 1) comes from the Clothing dataset, where we make a recommendation for a specific user on 24-11-2013.", "mention_start": 38, "mention_end": 58, "dataset_mention": "the Clothing dataset"}, {"mentioned_in_paper": "15", "context_id": "131", "dataset_context": "The SynthText dataset [6] is composed of 800,000 natural images, on which text in random colors, fonts, scales, and orientations is rendered carefully to have a realistic look.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The SynthText dataset"}, {"mentioned_in_paper": "15", "context_id": "134", "dataset_context": "The ICDAR 2015 dataset [15] is collected for the ICDAR 2015 Robust Reading Competition, with 1,000 natural images for training and 500 for testing.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The ICDAR 2015 dataset"}, {"mentioned_in_paper": "15", "context_id": "156", "dataset_context": "In the warming-up step, we apply Adam optimizer to train our model with learning rate 1e-4, and the learning rate decay factor is 0.94 on the SynthText dataset.", "mention_start": 137, "mention_end": 159, "dataset_mention": "the SynthText dataset"}, {"mentioned_in_paper": "15", "context_id": "162", "dataset_context": "Specially for curved polygon labeled datasets, we crop images without crossing text instances to avoid the destruction of polygon annotations.", "mention_start": 14, "mention_end": 45, "dataset_mention": "curved polygon labeled datasets"}, {"mentioned_in_paper": "15", "context_id": "163", "dataset_context": "The cropped image regions will be rotated randomly in 4 directions (0 \u2022 , 90 \u2022 , 180 \u2022 , and 270 \u2022 ) and standardized by subtracting the RGB mean value of ImageNet dataset.", "mention_start": 154, "mention_end": 171, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "15", "context_id": "193", "dataset_context": "In order to verify the validity for detecting oriented text, we compare SAST with the state-of-the-art methods on ICDAR 2015 dataset, a standard oriented text dataset.", "mention_start": 113, "mention_end": 132, "dataset_mention": "ICDAR 2015 dataset"}, {"mentioned_in_paper": "15", "context_id": "193", "dataset_context": "In order to verify the validity for detecting oriented text, we compare SAST with the state-of-the-art methods on ICDAR 2015 dataset, a standard oriented text dataset.", "mention_start": 133, "mention_end": 166, "dataset_mention": " a standard oriented text dataset"}, {"mentioned_in_paper": "15", "context_id": "204", "dataset_context": "Several experiments demonstrate that the proposed SAST is effective in detecting arbitrarily-shaped text, and is also robust in generalizing to multilingual scene text datasets.", "mention_start": 143, "mention_end": 176, "dataset_mention": "multilingual scene text datasets"}, {"mentioned_in_paper": "16", "context_id": "5", "dataset_context": "Doing so, we use di erent real-world mobility datasets with data about the presence of mobile entities in a variety of spaces.", "mention_start": 16, "mention_end": 54, "dataset_mention": "di erent real-world mobility datasets"}, {"mentioned_in_paper": "16", "context_id": "26", "dataset_context": " (3) We evaluate the robustness of this ngerprinting scheme in the presence of common sources of uncertainty in ubiquitously collected mobility data sets.", "mention_start": 134, "mention_end": 153, "dataset_mention": "mobility data sets"}, {"mentioned_in_paper": "16", "context_id": "27", "dataset_context": " (4) We validate our method by showing its classi cation performance using a WiFi-based detection data set and a Foursquare check-in dataset.", "mention_start": 74, "mention_end": 106, "dataset_mention": "a WiFi-based detection data set"}, {"mentioned_in_paper": "16", "context_id": "27", "dataset_context": " (4) We validate our method by showing its classi cation performance using a WiFi-based detection data set and a Foursquare check-in dataset.", "mention_start": 74, "mention_end": 140, "dataset_mention": "a WiFi-based detection data set and a Foursquare check-in dataset"}, {"mentioned_in_paper": "16", "context_id": "152", "dataset_context": "By looking at two consecutive datasets of duration F D, a resolution FR that maximizes the mutual distance of their vectorized versions e ectively captures all di erences that would have also been captured by a smaller resolution.", "mention_start": 14, "mention_end": 38, "dataset_mention": "two consecutive datasets"}, {"mentioned_in_paper": "16", "context_id": "190", "dataset_context": "Generating the mobility dataset: Note that each pa ern implicitly de nes a set of detections.", "mention_start": 11, "mention_end": 31, "dataset_mention": "the mobility dataset"}, {"mentioned_in_paper": "16", "context_id": "226", "dataset_context": "e rst dataset is a set of WiFi detections very rich in terms of the number of detections collected per space but contains data from a limited number of spaces.", "mention_start": 0, "mention_end": 13, "dataset_mention": "e rst dataset"}, {"mentioned_in_paper": "16", "context_id": "230", "dataset_context": "In what follows, we demonstrate the procedure of extracting ngerprinting parameters and feature vectors using the WiFi dataset.", "mention_start": 109, "mention_end": 126, "dataset_mention": "the WiFi dataset"}, {"mentioned_in_paper": "16", "context_id": "272", "dataset_context": "e rst one, similar to evaluations on the WiFi dataset, is to classify feature vectors to know from which space they were extracted.", "mention_start": 36, "mention_end": 53, "dataset_mention": "the WiFi dataset"}, {"mentioned_in_paper": "16", "context_id": "277", "dataset_context": "e accuracy of clustering algorithm is Spaceprint: a Mobility-based Fingerprinting Scheme for Public Spaces Conference'17, July 2017, Washington, DC, USA q q q q q q q q q 0.2 q q q q q q q q q 0.6 0.7 0.8 q q q q q q q q q 0.2 (c) q q q q q q q q q 0.1 Figure 7 : Tests with Foursquare dataset.", "mention_start": 274, "mention_end": 293, "dataset_mention": "Foursquare dataset"}, {"mentioned_in_paper": "17", "context_id": "239", "dataset_context": "The experiments are performed on the MNIST (d = 1024) and CIFAR (d = 512) datasets.", "mention_start": 33, "mention_end": 82, "dataset_mention": "the MNIST (d = 1024) and CIFAR (d = 512) datasets"}, {"mentioned_in_paper": "18", "context_id": "5", "dataset_context": "These combined data are captured from similar sensors in order to bootstrap the training and transfer learning task, especially valuable because visible-thermal face datasets are limited.", "mention_start": 144, "mention_end": 174, "dataset_mention": "visible-thermal face datasets"}, {"mentioned_in_paper": "18", "context_id": "10", "dataset_context": "The ability to generate a thermal face image from a visible face may be challenging due to the fact that there are limited, paired thermal-visible face datasets [8].", "mention_start": 123, "mention_end": 160, "dataset_mention": " paired thermal-visible face datasets"}, {"mentioned_in_paper": "18", "context_id": "13", "dataset_context": "We train on combined visible-thermal datasets in our method, called favtGAN (facial-visible-thermal-GAN or \"favorite GAN\").", "mention_start": 21, "mention_end": 45, "dataset_mention": "visible-thermal datasets"}, {"mentioned_in_paper": "18", "context_id": "60", "dataset_context": "We use four paired thermal-visible image datasets: Eurecom [24], FLIR ADAS [25], Iris [26]", "mention_start": 7, "mention_end": 49, "dataset_mention": "four paired thermal-visible image datasets"}, {"mentioned_in_paper": "18", "context_id": "78", "dataset_context": "In Fig. 2 we show samples comparing pix2pix trained on a single dataset and the best performing favtGAN architecture trained on the combined face and cityscape dataset.", "mention_start": 141, "mention_end": 167, "dataset_mention": "face and cityscape dataset"}, {"mentioned_in_paper": "21", "context_id": "12", "dataset_context": "For example, the well-known public benchmark HAR dataset PAMAP2 [4] uses a heart rate sensor and three wireless inertial measurement units (IMUs).", "mention_start": 12, "mention_end": 56, "dataset_mention": " the well-known public benchmark HAR dataset"}, {"mentioned_in_paper": "21", "context_id": "150", "dataset_context": "We performed experiments to evaluate the effectiveness of our proposed method -EE-using public benchmark datasets for sensor-based HAR.", "mention_start": 58, "mention_end": 113, "dataset_mention": "our proposed method -EE-using public benchmark datasets"}, {"mentioned_in_paper": "21", "context_id": "164", "dataset_context": "Fig. 3 shows the results of 100 trials of the HASC dataset for each ensemble model, where BL is the aggregate results of 100 trials of the four models generated during PE training (total 400 trials).", "mention_start": 42, "mention_end": 58, "dataset_mention": "the HASC dataset"}, {"mentioned_in_paper": "21", "context_id": "185", "dataset_context": "The additional datasets used were the UCI Smartphone Dataset [31], WISDM [32], UniMiB SHAR [33], and PAMAP2 [4].", "mention_start": 29, "mention_end": 60, "dataset_mention": "were the UCI Smartphone Dataset"}, {"mentioned_in_paper": "21", "context_id": "219", "dataset_context": "Notably, the UCI Smartphone Dataset [31] combines total acceleration (3ch), estimated body acceleration (3ch), and angular velocity (3ch) data in the channel direction into nine channels  of data.", "mention_start": 8, "mention_end": 35, "dataset_mention": " the UCI Smartphone Dataset"}, {"mentioned_in_paper": "21", "context_id": "228", "dataset_context": "First, we evaluated the effectiveness of the input variationer on data measured using a single sensor modality, such as the HASC dataset.", "mention_start": 119, "mention_end": 136, "dataset_mention": "the HASC dataset"}, {"mentioned_in_paper": "22", "context_id": "6", "dataset_context": "With the proposed modules, our recognition system surpasses previous state-of-the-art scores on irregular and perspective text datasets, including, ICDAR 2015, CUTE, and Total-Text, while paralleling state-of-theart performance on regular text datasets.", "mention_start": 230, "mention_end": 252, "dataset_mention": "regular text datasets"}, {"mentioned_in_paper": "22", "context_id": "121", "dataset_context": "\"90K\" and \"ST\" are the Synth90k and the SynthText datasets, respectively.", "mention_start": 19, "mention_end": 58, "dataset_mention": "the Synth90k and the SynthText datasets"}, {"mentioned_in_paper": "23", "context_id": "26", "dataset_context": "Little-known entities: interesting journalistic datasets feature some extremely well-known entities (e.g., world leaders in the pharmaceutical industry) next to others of much smaller notoriety (e.g., an expert consulted by EU institutions, or a little-known trade association).", "mention_start": 22, "mention_end": 56, "dataset_mention": " interesting journalistic datasets"}, {"mentioned_in_paper": "23", "context_id": "259", "dataset_context": "Further, solutions span over several datasets, demonstrating the interest of multi-dataset search enabled, and that P-GAM exploits this possibility.", "mention_start": 76, "mention_end": 90, "dataset_mention": "multi-dataset"}, {"mentioned_in_paper": "23", "context_id": "266", "dataset_context": "However, we found that: () their datasets are changing, text-rich and schema-less, () running a set of data stores (plus a mediator) was not feasible for them, () knowledge of a schema or the capacity to devise integration plan was lacking.", "mention_start": 23, "mention_end": 41, "dataset_mention": " () their datasets"}, {"mentioned_in_paper": "24", "context_id": "37", "dataset_context": "After that, a reconstruction network pretrained on a large-scale 3D human dataset [79] is used as a strong prior for producing a high-fidelity 3D human with full-body details from the fused normal maps.", "mention_start": 50, "mention_end": 81, "dataset_mention": "a large-scale 3D human dataset"}, {"mentioned_in_paper": "24", "context_id": "151", "dataset_context": "To reconstruct 3D geometry from the fused canonical normal maps F fused & B fused , we pretrain a reconstruction network on a large-scale 3D human dataset [79].", "mention_start": 123, "mention_end": 154, "dataset_mention": "a large-scale 3D human dataset"}, {"mentioned_in_paper": "24", "context_id": "264", "dataset_context": "Model Reconstruction We introduce a reconstruction network pretrained on a large-scale human dataset (THuman 2.0 [79]) to leverage the data prior to infer the 3D model from the fused normal maps.", "mention_start": 73, "mention_end": 100, "dataset_mention": "a large-scale human dataset"}, {"mentioned_in_paper": "25", "context_id": "118", "dataset_context": "It is noted that the real datasets may have large volume of data points and involve multiple semantic labels, and the proposed \u03b5-dragging operation can be well utilized in these datasets as well.", "mention_start": 6, "mention_end": 34, "dataset_mention": "noted that the real datasets"}, {"mentioned_in_paper": "25", "context_id": "213", "dataset_context": "Specifically, 5000 instances are randomly selected from the MIRFlickr dataset to compute e i and e i by Eq. (33).", "mention_start": 55, "mention_end": 77, "dataset_mention": "the MIRFlickr dataset"}, {"mentioned_in_paper": "25", "context_id": "246", "dataset_context": "A. Experimental Settings 1) Datasets: The popular multi-modal PASCAL-VOC-2007 [45], MIRFlickr [46] and NUS-WIDE [47] datasets are selected for evaluation.", "mention_start": 0, "mention_end": 36, "dataset_mention": "A. Experimental Settings 1) Datasets"}, {"mentioned_in_paper": "25", "context_id": "246", "dataset_context": "A. Experimental Settings 1) Datasets: The popular multi-modal PASCAL-VOC-2007 [45], MIRFlickr [46] and NUS-WIDE [47] datasets are selected for evaluation.", "mention_start": 83, "mention_end": 125, "dataset_mention": " MIRFlickr [46] and NUS-WIDE [47] datasets"}, {"mentioned_in_paper": "25", "context_id": "247", "dataset_context": "Similar to [16], PASCAL-VOC-2007 dataset (abbreviated as PASCAL-VOC) is divided into train, val, and test subsets.", "mention_start": 16, "mention_end": 40, "dataset_mention": " PASCAL-VOC-2007 dataset"}, {"mentioned_in_paper": "25", "context_id": "250", "dataset_context": "For NUS-WIDE dataset, we select 186,577 annotated instances from the top 10 most frequent concepts to guarantee that each concept has abundant training samples, and randomly select 1866 instances as a query set.", "mention_start": 4, "mention_end": 20, "dataset_mention": "NUS-WIDE dataset"}, {"mentioned_in_paper": "25", "context_id": "260", "dataset_context": "As CMFH [28], SePH [11] and GSePH [9] method are computationally expensive in training process, it is difficult to learn their corresponding hash functions on the whole NUS-WIDE dataset.", "mention_start": 158, "mention_end": 185, "dataset_mention": "the whole NUS-WIDE dataset"}, {"mentioned_in_paper": "25", "context_id": "268", "dataset_context": "In the experiments, parameters \u00b5, \u03b8, \u03b4 are empirically set at {10 0 , 10 \u22123 , 10 3 }, {10 \u22122 , 10 \u22123 , 10 3 }, {10 \u22123 , 10 \u22123 , 10 3 }, respectively, for PASCAL-VOC, MIR-Flickr and NUS-WIDE multi-modal datasets.", "mention_start": 165, "mention_end": 210, "dataset_mention": " MIR-Flickr and NUS-WIDE multi-modal datasets"}, {"mentioned_in_paper": "25", "context_id": "275", "dataset_context": "For instance, the mAP scores obtained by FDDH (32 bits on T\u2192I) reach up to 0.9048, 0.8022 and 0.8133, respectively, evaluated on PASCAL-VOC, MIRFlickr and NUS-WIDE datasets.", "mention_start": 140, "mention_end": 172, "dataset_mention": " MIRFlickr and NUS-WIDE datasets"}, {"mentioned_in_paper": "25", "context_id": "276", "dataset_context": "Specifically, DLFH [30] proposes a novel discrete latent factor model to learn the binary hash codes without continuous relaxation, which performs well on some T\u2192I retrieval task, mainly tested on MIRFlickr and NUS-WIDE datasets.", "mention_start": 196, "mention_end": 228, "dataset_mention": "MIRFlickr and NUS-WIDE datasets"}, {"mentioned_in_paper": "25", "context_id": "282", "dataset_context": "For instance, if the hash length is set at 64 bits, the mAP scores obtained by FDDH and tested on I\u2192T task are higher than 0.79, 0.87, and 0.85, respectively, evaluated on PASCAL-VOC, MIRFlickr and NUS-WIDE datasets.", "mention_start": 183, "mention_end": 215, "dataset_mention": " MIRFlickr and NUS-WIDE datasets"}, {"mentioned_in_paper": "25", "context_id": "304", "dataset_context": "For fair comparison, we evaluate the competing algorithms on NUS-WIDE dataset with handcrafted features, and record the execution time of different training sizes with 128 hash bits.", "mention_start": 60, "mention_end": 77, "dataset_mention": "NUS-WIDE dataset"}, {"mentioned_in_paper": "25", "context_id": "327", "dataset_context": "Further, we vary the training size of MIRFlickr dataset from 10% to 90% and select the handcrated features of MIRFlickr dataset for illustration.", "mention_start": 37, "mention_end": 55, "dataset_mention": "MIRFlickr dataset"}, {"mentioned_in_paper": "25", "context_id": "327", "dataset_context": "Further, we vary the training size of MIRFlickr dataset from 10% to 90% and select the handcrated features of MIRFlickr dataset for illustration.", "mention_start": 109, "mention_end": 127, "dataset_mention": "MIRFlickr dataset"}, {"mentioned_in_paper": "25", "context_id": "345", "dataset_context": "Further, we vary the training size of MIRFlickr dataset from 10% to 90% and select the handcrated features of MIRFlickr dataset for illustration.", "mention_start": 37, "mention_end": 55, "dataset_mention": "MIRFlickr dataset"}, {"mentioned_in_paper": "25", "context_id": "345", "dataset_context": "Further, we vary the training size of MIRFlickr dataset from 10% to 90% and select the handcrated features of MIRFlickr dataset for illustration.", "mention_start": 109, "mention_end": 127, "dataset_mention": "MIRFlickr dataset"}, {"mentioned_in_paper": "25", "context_id": "367", "dataset_context": "By fixing the code length to be 32 bits, we record the objective value and mAP score at each iteration and select NUS-WIDE dataset for illustration.", "mention_start": 87, "mention_end": 130, "dataset_mention": "each iteration and select NUS-WIDE dataset"}, {"mentioned_in_paper": "25", "context_id": "374", "dataset_context": "Specifically, the large NUS-WIDE dataset associated with handcrafted features is selected for illustration, and we also utilize the optimization scheme within DPLM to update H in Eq. ( 11) (abbreviated as FDDH-DPLM).", "mention_start": 13, "mention_end": 40, "dataset_mention": " the large NUS-WIDE dataset"}, {"mentioned_in_paper": "25", "context_id": "379", "dataset_context": "Therefore, the proposed FDDH algorithm runs sufficiently fast, and it is particularly suitable for processing the large-scale multimedia datasets.", "mention_start": 98, "mention_end": 145, "dataset_mention": "processing the large-scale multimedia datasets"}, {"mentioned_in_paper": "26", "context_id": "127", "dataset_context": "Real-world Datasets: We use two real-word datasets to analyze the performance of all methods.", "mention_start": 27, "mention_end": 50, "dataset_mention": "two real-word datasets"}, {"mentioned_in_paper": "26", "context_id": "130", "dataset_context": "The learning rate is set to 5 \u00d7 10 \u22124 and we train the model for 300 epochs for Rain200L/H, Rain800, and Rain1200 datasets.", "mention_start": 100, "mention_end": 122, "dataset_mention": " and Rain1200 datasets"}, {"mentioned_in_paper": "26", "context_id": "141", "dataset_context": "In order to demonstrate that our method is also applicable in real-world scenarios, we compare with other methods on SPA-Data and a real-world dataset.", "mention_start": 116, "mention_end": 150, "dataset_mention": "SPA-Data and a real-world dataset"}, {"mentioned_in_paper": "26", "context_id": "154", "dataset_context": "To demonstrate that our proposed can benefit visionbased applications, we employ Google Vision API to evaluate the deraining results, as shown in Fig. 9. Fig. 9 (a Table 6 : The detection performance comparison on COCO350 dataset [16].", "mention_start": 213, "mention_end": 229, "dataset_mention": "COCO350 dataset"}, {"mentioned_in_paper": "26", "context_id": "170", "dataset_context": "Experimental results on several synthetic deraining datasets and real-world scenarios have shown the great superiority of our proposed SPDNet over other top-performing methods.", "mention_start": 24, "mention_end": 60, "dataset_mention": "several synthetic deraining datasets"}, {"mentioned_in_paper": "27", "context_id": "1", "dataset_context": "Although TSC algorithms are designed for balanced datasets, most real-life time series datasets are imbalanced.", "mention_start": 59, "mention_end": 95, "dataset_mention": " most real-life time series datasets"}, {"mentioned_in_paper": "27", "context_id": "5", "dataset_context": "Despite having a high imbalance ratio, the result showed that F score could be as high as 97.6% for the simulated TwoPatterns Dataset.", "mention_start": 99, "mention_end": 133, "dataset_mention": "the simulated TwoPatterns Dataset"}, {"mentioned_in_paper": "27", "context_id": "22", "dataset_context": "In Sec 4 seven different time series datasets from the UCR archive are described.", "mention_start": 3, "mention_end": 45, "dataset_mention": "Sec 4 seven different time series datasets"}, {"mentioned_in_paper": "27", "context_id": "25", "dataset_context": "This section discusses the papers related to addressing imbalanced dataset.", "mention_start": 45, "mention_end": 74, "dataset_mention": "addressing imbalanced dataset"}, {"mentioned_in_paper": "27", "context_id": "50", "dataset_context": "Fig 1a shows a histogram representation of a step imbalance dataset where all the majority and minority classes have 500 and 5000 instances respectively.", "mention_start": 43, "mention_end": 67, "dataset_mention": "a step imbalance dataset"}, {"mentioned_in_paper": "27", "context_id": "61", "dataset_context": "Out of the seven datasets, four are EEG dataset, one is ECG dataset, one is a simulated dataset, and one is human activity recognition dataset.", "mention_start": 35, "mention_end": 47, "dataset_mention": "EEG dataset"}, {"mentioned_in_paper": "27", "context_id": "61", "dataset_context": "Out of the seven datasets, four are EEG dataset, one is ECG dataset, one is a simulated dataset, and one is human activity recognition dataset.", "mention_start": 55, "mention_end": 67, "dataset_mention": "ECG dataset"}, {"mentioned_in_paper": "27", "context_id": "61", "dataset_context": "Out of the seven datasets, four are EEG dataset, one is ECG dataset, one is a simulated dataset, and one is human activity recognition dataset.", "mention_start": 107, "mention_end": 142, "dataset_mention": "human activity recognition dataset"}, {"mentioned_in_paper": "27", "context_id": "63", "dataset_context": "This section has discussed different methods to address imbalanced time series dataset.", "mention_start": 56, "mention_end": 86, "dataset_mention": "imbalanced time series dataset"}, {"mentioned_in_paper": "27", "context_id": "83", "dataset_context": "(b) Bootstrapping approach is an algorithmic approach that addresses imbalanced dataset by creating a balanced mini-batch from the majority and minority classes [Yan+15].", "mention_start": 30, "mention_end": 87, "dataset_mention": "an algorithmic approach that addresses imbalanced dataset"}, {"mentioned_in_paper": "27", "context_id": "139", "dataset_context": "For example, GMSE punishes the network highly for miss-classifying the two patterns dataset because class separability score for this dataset is high (.54).", "mention_start": 49, "mention_end": 91, "dataset_mention": "miss-classifying the two patterns dataset"}, {"mentioned_in_paper": "27", "context_id": "140", "dataset_context": "On the other hand, it would not punish the network highly for the SelfRegulationSCP2 dataset", "mention_start": 61, "mention_end": 92, "dataset_mention": "the SelfRegulationSCP2 dataset"}, {"mentioned_in_paper": "27", "context_id": "144", "dataset_context": "For instance, the class separability score for SelfRegula-tionSCP2 dataset is only -.00123, and our F3 and AUC are 55.65% and 60% respectively, which is higher than other methods.", "mention_start": 46, "mention_end": 74, "dataset_mention": "SelfRegula-tionSCP2 dataset"}, {"mentioned_in_paper": "27", "context_id": "147", "dataset_context": "For instance, in SelfRegulationSCP2 dataset, the training time for GMSE method is 127.13 seconds, whereas training time for unweighted loss, weighted loss and bootstrapping is 109,110 and 119 seconds respectively.", "mention_start": 16, "mention_end": 43, "dataset_mention": "SelfRegulationSCP2 dataset"}, {"mentioned_in_paper": "28", "context_id": "556", "dataset_context": "Additionally, we train two classifiers, MNIST 1 and MNIST 2 , on the MNIST dataset [LeCun and Cortes 2010].", "mention_start": 64, "mention_end": 82, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "29", "context_id": "22", "dataset_context": "Encoding local feature of the input image makes 3D object coordinates inherently robust to partial occlusion and achieves top-level results on the Occluded LineMOD dataset [6].", "mention_start": 143, "mention_end": 171, "dataset_mention": "the Occluded LineMOD dataset"}, {"mentioned_in_paper": "29", "context_id": "52", "dataset_context": "3. We show the effectiveness of projection grouping and corresponding evaluation, and the corresponding two-stage pipeline achieves state-of-the-art performance on public benchmarks including LineMod dataset [18], Occluded LineMOD dataset [6], and YCB-Video dataset [19].", "mention_start": 191, "mention_end": 207, "dataset_mention": "LineMod dataset"}, {"mentioned_in_paper": "29", "context_id": "52", "dataset_context": "3. We show the effectiveness of projection grouping and corresponding evaluation, and the corresponding two-stage pipeline achieves state-of-the-art performance on public benchmarks including LineMod dataset [18], Occluded LineMOD dataset [6], and YCB-Video dataset [19].", "mention_start": 213, "mention_end": 238, "dataset_mention": " Occluded LineMOD dataset"}, {"mentioned_in_paper": "29", "context_id": "52", "dataset_context": "3. We show the effectiveness of projection grouping and corresponding evaluation, and the corresponding two-stage pipeline achieves state-of-the-art performance on public benchmarks including LineMod dataset [18], Occluded LineMOD dataset [6], and YCB-Video dataset [19].", "mention_start": 243, "mention_end": 265, "dataset_mention": " and YCB-Video dataset"}, {"mentioned_in_paper": "29", "context_id": "205", "dataset_context": "In this section, three public datasets: LineMOD dataset [18], Occluded LineMOD dataset [6] and YCB-Video dataset [19] are employed to evaluate the proposed backend and integrated two-stage pipeline.", "mention_start": 39, "mention_end": 55, "dataset_mention": " LineMOD dataset"}, {"mentioned_in_paper": "29", "context_id": "205", "dataset_context": "In this section, three public datasets: LineMOD dataset [18], Occluded LineMOD dataset [6] and YCB-Video dataset [19] are employed to evaluate the proposed backend and integrated two-stage pipeline.", "mention_start": 61, "mention_end": 86, "dataset_mention": " Occluded LineMOD dataset"}, {"mentioned_in_paper": "29", "context_id": "205", "dataset_context": "In this section, three public datasets: LineMOD dataset [18], Occluded LineMOD dataset [6] and YCB-Video dataset [19] are employed to evaluate the proposed backend and integrated two-stage pipeline.", "mention_start": 61, "mention_end": 112, "dataset_mention": " Occluded LineMOD dataset [6] and YCB-Video dataset"}, {"mentioned_in_paper": "29", "context_id": "206", "dataset_context": "The LineMOD dataset consists of 15 different object sequences and corresponding ground truth pose.", "mention_start": 0, "mention_end": 19, "dataset_mention": "The LineMOD dataset"}, {"mentioned_in_paper": "29", "context_id": "207", "dataset_context": "The occluded version [6] is generated by selecting images from LineMOD dataset, and these objects occlude each other to a large extent under different viewing directions.", "mention_start": 63, "mention_end": 78, "dataset_mention": "LineMOD dataset"}, {"mentioned_in_paper": "29", "context_id": "208", "dataset_context": "The YCB-Video dataset contains 21 different object sequences with significant image noise, illumination changes, background clutter and severe occlusion.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The YCB-Video dataset"}, {"mentioned_in_paper": "29", "context_id": "222", "dataset_context": "As shown in Figures 4 and 5, we observe the following results: (1) the projection grouping module with residual architecture and dropout layer achieves best results in FPS metric on both datasets; (2) Compared with the results corresponding to Occluded LineMOD dataset, all test methods give a lower number of FPS.", "mention_start": 243, "mention_end": 268, "dataset_mention": "Occluded LineMOD dataset"}, {"mentioned_in_paper": "29", "context_id": "242", "dataset_context": "It should be noted that the evaluation of DeepHMap on the LineMOD dataset hasn't been given, and thus we list the corresponding results from BB8 [8] as a substitute.", "mention_start": 54, "mention_end": 73, "dataset_mention": "the LineMOD dataset"}, {"mentioned_in_paper": "29", "context_id": "243", "dataset_context": "Table 1 shows that both of the two versions significantly outperform BB8 [11] on the LineMOD dataset [18].", "mention_start": 81, "mention_end": 100, "dataset_mention": "the LineMOD dataset"}, {"mentioned_in_paper": "29", "context_id": "248", "dataset_context": "Similar test results from Occluded LineMOD dataset [6] can be found in Table 2. Margins between RANSAC based strategy in DeepHMap and two versions of CorrNet reach an average of 1.6% and 1.0% in ADD|I metric, 5.2% and 3.0% in the 2D reprojection error metric.", "mention_start": 26, "mention_end": 50, "dataset_mention": "Occluded LineMOD dataset"}, {"mentioned_in_paper": "29", "context_id": "251", "dataset_context": "We now evaluate our full pipeline on two datasets with serve occlusion, namely Occluded LineMOD dataset [6], and YCB-Video dataset [19].", "mention_start": 71, "mention_end": 103, "dataset_mention": " namely Occluded LineMOD dataset"}, {"mentioned_in_paper": "29", "context_id": "251", "dataset_context": "We now evaluate our full pipeline on two datasets with serve occlusion, namely Occluded LineMOD dataset [6], and YCB-Video dataset [19].", "mention_start": 108, "mention_end": 130, "dataset_mention": " and YCB-Video dataset"}, {"mentioned_in_paper": "29", "context_id": "254", "dataset_context": "Especially for the YCB-Video dataset, the area under the accuracy-threshold curve (AUC) [19] is utilized as an additional metric.", "mention_start": 15, "mention_end": 36, "dataset_mention": "the YCB-Video dataset"}, {"mentioned_in_paper": "29", "context_id": "256", "dataset_context": "For all eight sequences from the Occluded LineMOD dataset [6], DeepHMap++ steadily achieves better results than DeepHMap under different pixel thresholds.", "mention_start": 29, "mention_end": 57, "dataset_mention": "the Occluded LineMOD dataset"}, {"mentioned_in_paper": "29", "context_id": "263", "dataset_context": "For all object sequences from YCB-Video dataset, DeepHMap++ consistently improves DeepHMap, which utilizes RANSAC based correspondence sampling and max function based projection grouping in three different metrics.", "mention_start": 30, "mention_end": 47, "dataset_mention": "YCB-Video dataset"}, {"mentioned_in_paper": "34", "context_id": "5", "dataset_context": "With weak supervision from two large scale CMU-Panoptic and AVA-LAEO activity datasets, we show significant improvements in (a) the accuracy of semisupervised gaze estimation and (b) cross-domain generalization on the state-of-the-art physically unconstrained in-the-wild Gaze360 gaze estimation benchmark.", "mention_start": 27, "mention_end": 86, "dataset_mention": "two large scale CMU-Panoptic and AVA-LAEO activity datasets"}, {"mentioned_in_paper": "34", "context_id": "11", "dataset_context": "Fortunately, several 3D gaze datasets with large camera-to-subject dis- tances and variability in head pose have been collected recently in indoor laboratory environments using specialized multi-cameras setups [43, 8, 44, 28].", "mention_start": 12, "mention_end": 37, "dataset_mention": " several 3D gaze datasets"}, {"mentioned_in_paper": "34", "context_id": "12", "dataset_context": "In contrast, the recent Gaze360 dataset [17] was collected both indoors and outdoors, at greater distances to subjects.", "mention_start": 12, "mention_end": 39, "dataset_mention": " the recent Gaze360 dataset"}, {"mentioned_in_paper": "34", "context_id": "32", "dataset_context": "We conduct various within-and cross-dataset experiments and obtain LAEO labels from two large-scale datasets: (a) the CMU Panoptic [16] with known 3D scene geometry and (b) the in-the-wild AVA-LAEO activity dataset [25] containing Internet videos.", "mention_start": 11, "mention_end": 43, "dataset_mention": "various within-and cross-dataset"}, {"mentioned_in_paper": "34", "context_id": "32", "dataset_context": "We conduct various within-and cross-dataset experiments and obtain LAEO labels from two large-scale datasets: (a) the CMU Panoptic [16] with known 3D scene geometry and (b) the in-the-wild AVA-LAEO activity dataset [25] containing Internet videos.", "mention_start": 140, "mention_end": 214, "dataset_mention": "known 3D scene geometry and (b) the in-the-wild AVA-LAEO activity dataset"}, {"mentioned_in_paper": "34", "context_id": "47", "dataset_context": "Yet, such collection methods are still difficult to scale compared to data sourced from the web, or via crowd-sourced participation such as done for the GazeCapture dataset [20].", "mention_start": 148, "mention_end": 172, "dataset_mention": "the GazeCapture dataset"}, {"mentioned_in_paper": "34", "context_id": "52", "dataset_context": "Chong et al. [3] improve performance on the static gaze following task further by jointly training to predict 3D gaze direction using the EYEDIAP dataset [9], and by explicitly predicting whether the target is in frame.", "mention_start": 134, "mention_end": 153, "dataset_mention": "the EYEDIAP dataset"}, {"mentioned_in_paper": "34", "context_id": "55", "dataset_context": "However, gaze following datasets are complex to annotate, and do not lend themselves well to the task of learning to predict 3D gaze due to the lack of scene and object geometry information.", "mention_start": 8, "mention_end": 32, "dataset_mention": " gaze following datasets"}, {"mentioned_in_paper": "34", "context_id": "59", "dataset_context": "The recently published AVA-LAEO dataset [25] is an extension of the AVA dataset [10] and demonstrates the ease of acquiring such annotations for existing videos.", "mention_start": 13, "mention_end": 39, "dataset_mention": "published AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "59", "dataset_context": "The recently published AVA-LAEO dataset [25] is an extension of the AVA dataset [10] and demonstrates the ease of acquiring such annotations for existing videos.", "mention_start": 64, "mention_end": 79, "dataset_mention": "the AVA dataset"}, {"mentioned_in_paper": "34", "context_id": "61", "dataset_context": "Furthermore, adding LAEO-based constraints and objectives consistently improves performance in cross-dataset and semi-supervised gaze estimation, further validating the real-world efficacy of our approach.", "mention_start": 94, "mention_end": 108, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "34", "context_id": "174", "dataset_context": "We employ two LAEO datasets -CMU Panoptic [16] and AVA [10, 25].", "mention_start": 10, "mention_end": 27, "dataset_mention": "two LAEO datasets"}, {"mentioned_in_paper": "34", "context_id": "181", "dataset_context": "To acquire LAEO data from in-the-wild Internet videos, we leverage the large scale AVA human activity dataset [10] with LAEO annotations [25] provided by Marin-Jimenez et al. (called \"AVA-LAEO\").", "mention_start": 54, "mention_end": 109, "dataset_mention": " we leverage the large scale AVA human activity dataset"}, {"mentioned_in_paper": "34", "context_id": "188", "dataset_context": "We validate the efficacy of our weaklysupervised approach on the large-scale physically unconstrained in-the-wild Gaze360 [17] dataset.", "mention_start": 61, "mention_end": 134, "dataset_mention": "the large-scale physically unconstrained in-the-wild Gaze360 [17] dataset"}, {"mentioned_in_paper": "34", "context_id": "192", "dataset_context": "For semi-supervised training, we additionally use two largescale gaze datasets with known 3D gaze ground-truth -GazeCapture [20] and ETH-XGaze [44].", "mention_start": 49, "mention_end": 78, "dataset_mention": "two largescale gaze datasets"}, {"mentioned_in_paper": "34", "context_id": "200", "dataset_context": ", GazeCapture [20] and ETH-XGaze [44] datasets.", "mention_start": 1, "mention_end": 46, "dataset_mention": " GazeCapture [20] and ETH-XGaze [44] datasets"}, {"mentioned_in_paper": "34", "context_id": "203", "dataset_context": "To verify the contributions of our individual losses, we conduct a purely weakly-supervised cross-dataset ablation study.", "mention_start": 64, "mention_end": 105, "dataset_mention": "a purely weakly-supervised cross-dataset"}, {"mentioned_in_paper": "34", "context_id": "204", "dataset_context": "We train our method with the CMU Panoptic or AVA-LAEO datasets and evaluate performance on the Gaze360 dataset's test partition.", "mention_start": 45, "mention_end": 62, "dataset_mention": "AVA-LAEO datasets"}, {"mentioned_in_paper": "34", "context_id": "204", "dataset_context": "We train our method with the CMU Panoptic or AVA-LAEO datasets and evaluate performance on the Gaze360 dataset's test partition.", "mention_start": 91, "mention_end": 110, "dataset_mention": "the Gaze360 dataset"}, {"mentioned_in_paper": "34", "context_id": "205", "dataset_context": "Table 1 highlights the effect of the various weakly-supervised LAEO losses in this crossdataset setting.", "mention_start": 78, "mention_end": 95, "dataset_mention": "this crossdataset"}, {"mentioned_in_paper": "34", "context_id": "212", "dataset_context": "We observe that the best performance is achieved by utilizing a combination of L 3D geom , L 2D geom and the L pseudo G losses, especially with the real-world AVA-LAEO dataset where the scene geometry is not known.", "mention_start": 143, "mention_end": 175, "dataset_mention": "the real-world AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "216", "dataset_context": "Despite successfully learning to estimate gaze, the performance of our purely weakly-supervised model (trained on the AVA-LAEO dataset and tested on the Gaze360 dataset) lags behind the fully-supervised model on Gaze360's training set [17] as shown in LAEO data (as discussed in Sec.", "mention_start": 113, "mention_end": 134, "dataset_mention": "the AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "216", "dataset_context": "Despite successfully learning to estimate gaze, the performance of our purely weakly-supervised model (trained on the AVA-LAEO dataset and tested on the Gaze360 dataset) lags behind the fully-supervised model on Gaze360's training set [17] as shown in LAEO data (as discussed in Sec.", "mention_start": 148, "mention_end": 168, "dataset_mention": "the Gaze360 dataset"}, {"mentioned_in_paper": "34", "context_id": "217", "dataset_context": "D.3 of the supplementary) and the other is the the existence of domain gap between the AVA-LAEO and Gaze360 datasets.", "mention_start": 83, "mention_end": 116, "dataset_mention": "the AVA-LAEO and Gaze360 datasets"}, {"mentioned_in_paper": "34", "context_id": "221", "dataset_context": "We conduct both cross-dataset and within-dataset experiments.", "mention_start": 11, "mention_end": 29, "dataset_mention": "both cross-dataset"}, {"mentioned_in_paper": "34", "context_id": "221", "dataset_context": "We conduct both cross-dataset and within-dataset experiments.", "mention_start": 11, "mention_end": 48, "dataset_mention": "both cross-dataset and within-dataset"}, {"mentioned_in_paper": "34", "context_id": "222", "dataset_context": "For the crossdataset experiment, we train our model with several existing datasets other than Gaze360 and test on Gaze360's test partition.", "mention_start": 4, "mention_end": 20, "dataset_mention": "the crossdataset"}, {"mentioned_in_paper": "34", "context_id": "223", "dataset_context": "For the within-dataset experiment, we train on various subsets of Gaze360's training partition along with LAEO data and evaluate performance on Gaze360's test set.", "mention_start": 4, "mention_end": 22, "dataset_mention": "the within-dataset"}, {"mentioned_in_paper": "34", "context_id": "226", "dataset_context": "Both these supervised gaze datasets, although large, are limited in some respect for the task of physically unconstrained gaze estimation in-the-wild.", "mention_start": 0, "mention_end": 35, "dataset_mention": "Both these supervised gaze datasets"}, {"mentioned_in_paper": "34", "context_id": "227", "dataset_context": "The GazeCapture dataset contains images acquired indoors and outdoors, but of mostly frontal faces with a narrow distribution of gaze angles (Fig. 3).", "mention_start": 0, "mention_end": 23, "dataset_mention": "The GazeCapture dataset"}, {"mentioned_in_paper": "34", "context_id": "228", "dataset_context": "The ETH-XGaze dataset, on the other hand, has a broad distribution of gaze angles from 80 subjects (Fig. 3), but is captured indoors only.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The ETH-XGaze dataset"}, {"mentioned_in_paper": "34", "context_id": "232", "dataset_context": "Fig. 3 shows that the AVA-LAEO dataset complements both the Gaze-Capture and ETH-XGaze datasets by expanding their underlying distributions via weak gaze labels (see more details in Sec.", "mention_start": 13, "mention_end": 38, "dataset_mention": "that the AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "232", "dataset_context": "Fig. 3 shows that the AVA-LAEO dataset complements both the Gaze-Capture and ETH-XGaze datasets by expanding their underlying distributions via weak gaze labels (see more details in Sec.", "mention_start": 13, "mention_end": 95, "dataset_mention": "that the AVA-LAEO dataset complements both the Gaze-Capture and ETH-XGaze datasets"}, {"mentioned_in_paper": "34", "context_id": "234", "dataset_context": "In Table 2, we also show the cross-dataset performance of jointly training with CMU Panoptic and AVA-LAEO with their weak gaze labels only.", "mention_start": 24, "mention_end": 42, "dataset_mention": "the cross-dataset"}, {"mentioned_in_paper": "34", "context_id": "246", "dataset_context": "Through many experiments we demonstrate that our approach is successful in augmenting gaze datasets limited in gaze distributions, subjects, or environmental conditions with unconstrained images of people under LAEO, resulting in improved physically unconstrained gaze estimation in the wild.", "mention_start": 86, "mention_end": 99, "dataset_mention": "gaze datasets"}, {"mentioned_in_paper": "34", "context_id": "250", "dataset_context": "This provides additional weak gaze annotations from the noisy, in-the-wild AVA-LAEO dataset.", "mention_start": 62, "mention_end": 91, "dataset_mention": " in-the-wild AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "252", "dataset_context": "VATnet's head conditioning branch is a ResNet-50 module initialized with weights from a gaze estimation network trained on the EYEDIAP dataset [9].", "mention_start": 123, "mention_end": 142, "dataset_mention": "the EYEDIAP dataset"}, {"mentioned_in_paper": "34", "context_id": "254", "dataset_context": "In our experiments we jointly train this VATNet architecture with both the training set of the original fully-supervised VAT dataset and with the AVA-LAEO dataset.", "mention_start": 91, "mention_end": 132, "dataset_mention": "the original fully-supervised VAT dataset"}, {"mentioned_in_paper": "34", "context_id": "254", "dataset_context": "In our experiments we jointly train this VATNet architecture with both the training set of the original fully-supervised VAT dataset and with the AVA-LAEO dataset.", "mention_start": 142, "mention_end": 162, "dataset_mention": "the AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "255", "dataset_context": "To do so, we modify the VATNet architecture and add two fully connected layers to the output of the head conditioning branch, and train it to additionally predict weak 3D gaze vectors derived from the AVA-LAEO dataset (see Fig. 5).", "mention_start": 196, "mention_end": 217, "dataset_mention": "the AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "263", "dataset_context": "Results Following Chong et al. [4], we evaluate the area under the curve (AUC) for correct target location prediction (within a pre-specified distance threshold on the image 3. Improvements to the VATnet baseline [4] by adding weak supervision from the AVA-LAEO dataset using the best configuration of LAEO loss functions described in Table 1 of the main paper.", "mention_start": 248, "mention_end": 269, "dataset_mention": "the AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "265", "dataset_context": "We report the scores on the VAT test dataset, averaged across training epochs 2-30, both for the author's original method [4] and our proposed modification.", "mention_start": 24, "mention_end": 44, "dataset_mention": "the VAT test dataset"}, {"mentioned_in_paper": "34", "context_id": "266", "dataset_context": "Table 3 shows the benefits of jointly training with the AVA-LAEO and VAT datasets.", "mention_start": 52, "mention_end": 81, "dataset_mention": "the AVA-LAEO and VAT datasets"}, {"mentioned_in_paper": "34", "context_id": "269", "dataset_context": "We also note a reduction in the out-of-frame AP, which is not surprising as all target locations for a given subject in the AVA-LAEO dataset lie within image bounds and hence it provides labels for only one (i.e., the in-frame) class.", "mention_start": 119, "mention_end": 140, "dataset_mention": "the AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "274", "dataset_context": "Table 4 shows a detailed comparison of the effects of adding the symmetry constraint to the pinball (from [17]) and aleatoric (ours) loss functions for a within-dataset fully-supervised experiment on Gaze360.", "mention_start": 152, "mention_end": 168, "dataset_mention": "a within-dataset"}, {"mentioned_in_paper": "34", "context_id": "277", "dataset_context": "We also observe that for this within-dataset experiment, the aleatoric loss consistently outperforms the pinball loss and that the combination of the aleatoric and symmetry losses results in the best overall performance (Table 4).", "mention_start": 25, "mention_end": 44, "dataset_mention": "this within-dataset"}, {"mentioned_in_paper": "34", "context_id": "285", "dataset_context": "In either experiment, we replace the L pseudo G loss in our best (temporal) purely weakly-supervised cross-dataset configuration that is trained with all the LAEO losses L sym +L pseudo G +L 2D geom + L 3D geom (corresponding to the last row in Table 1 of the main paper) with one of these losses.", "mention_start": 55, "mention_end": 114, "dataset_mention": "our best (temporal) purely weakly-supervised cross-dataset"}, {"mentioned_in_paper": "34", "context_id": "290", "dataset_context": "Our experiments show a reduction in cross-dataset performance on the entire Gaze360 test set (CMU Panoptic: 25.9 \u2022 \u2192 28.2 \u2022 and AVA-LAEO: 26.3 \u2022 \u2192 26.9 \u2022 ) with this naive variant of the LAEO loss versus the one described in Sec.", "mention_start": 36, "mention_end": 49, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "34", "context_id": "294", "dataset_context": "That is, g 3D pseudo = \u011d3D A if W A \u2265 W B (from Eq. 1 in the main paper) and vice versa for subject B. Our experiments show a reduction in cross-dataset performance with this variant of the LAEO pseudo ground truth label as well versus the one used in Sec.", "mention_start": 138, "mention_end": 152, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "34", "context_id": "300", "dataset_context": "To quantify the contribution of L 2D geom to the overall performance of our system, we add increasing noise (z-only) as a ratio of the absolute ground truth depth of the 3D eye positions to subjects under LAEO in the CMU Panoptic dataset, train various purely weakly-supervised configurations (as described in Sec.", "mention_start": 212, "mention_end": 237, "dataset_mention": "the CMU Panoptic dataset"}, {"mentioned_in_paper": "34", "context_id": "304", "dataset_context": "For the cross-dataset experiment described in Sec.", "mention_start": 4, "mention_end": 21, "dataset_mention": "the cross-dataset"}, {"mentioned_in_paper": "34", "context_id": "309", "dataset_context": "The AVA-LAEO dataset exhibits a large distribution of extreme gaze angles as the LAEO activity largely consists of people with side profiles fixating at each other (see Fig. 1 and Fig. 2 in main paper and Fig. 11 in the supplementary for examples).", "mention_start": 0, "mention_end": 20, "dataset_mention": "The AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "310", "dataset_context": "This conveniently augments datasets with narrow gaze distributions, e.g., GazeCapture (dashed versus solid curves in Fig. 8), which is largely concentrated about gaze pitch and yaw values of zero (from Fig. 3 of the main paper) and helps them generalize better to Gaze360.", "mention_start": 0, "mention_end": 35, "dataset_mention": "This conveniently augments datasets"}, {"mentioned_in_paper": "34", "context_id": "311", "dataset_context": "The AVA-LAEO dataset also contains a large appearance variability because of being collected from in-the-wild videos, which positively augments datasets collected indoors only, e.g., ETH-XGaze (dashed versus solid curves Fig. 9) and helps it generalize better to Gaze360 as well.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "311", "dataset_context": "The AVA-LAEO dataset also contains a large appearance variability because of being collected from in-the-wild videos, which positively augments datasets collected indoors only, e.g., ETH-XGaze (dashed versus solid curves Fig. 9) and helps it generalize better to Gaze360 as well.", "mention_start": 123, "mention_end": 152, "dataset_mention": "positively augments datasets"}, {"mentioned_in_paper": "34", "context_id": "312", "dataset_context": "On jointly training either the GazeCapture or ETH-XGaze dataset with AVA-LAEO, we see a significant boost in their performance on all head crops from Gaze360, including faces with large profile views (blue curves in Fig. 8 and Fig. 9).", "mention_start": 46, "mention_end": 63, "dataset_mention": "ETH-XGaze dataset"}, {"mentioned_in_paper": "34", "context_id": "313", "dataset_context": "Interestingly, adding the AVA-LAEO dataset improves crossdomain performance of GazeCapture and ETH-XGaze on Gaze360's frontal face crops as well (red curves in Fig. 8 and Fig. 9).", "mention_start": 14, "mention_end": 42, "dataset_mention": " adding the AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "314", "dataset_context": "We first describe in detail how we pre-process the CMU Panoptic (haggling activity subset) and the AVA-LAEO datasets.", "mention_start": 47, "mention_end": 116, "dataset_mention": "the CMU Panoptic (haggling activity subset) and the AVA-LAEO datasets"}, {"mentioned_in_paper": "34", "context_id": "317", "dataset_context": "The CMU Panoptic dataset contains 31 views captured from high-definition cameras within a dome with available accurate body/facial 3D landmark locations and camera intrinsic and extrinsic parameters.", "mention_start": 0, "mention_end": 24, "dataset_mention": "The CMU Panoptic dataset"}, {"mentioned_in_paper": "34", "context_id": "319", "dataset_context": "Such a convenient setup allows us to quickly gather our own large-scale gaze dataset by leveraging the LAEO constraint.", "mention_start": 37, "mention_end": 84, "dataset_mention": "quickly gather our own large-scale gaze dataset"}, {"mentioned_in_paper": "34", "context_id": "333", "dataset_context": "These annotation, however are not available in the AVA-LAEO dataset.", "mention_start": 46, "mention_end": 67, "dataset_mention": "the AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "338", "dataset_context": "See Fig. 11 for a positive and a negative example of head pose fitting, where the latter results in noisy gaze labels for the AVA-LAEO dataset.", "mention_start": 121, "mention_end": 142, "dataset_mention": "the AVA-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "339", "dataset_context": "When scene geometry is unknown (e.g., in real-world LAEO datasets), 3D gaze labels derived from LAEO are indeed noisy.", "mention_start": 40, "mention_end": 65, "dataset_mention": "real-world LAEO datasets"}, {"mentioned_in_paper": "34", "context_id": "340", "dataset_context": "We introduce various constraints while training our system to counter this issue, and show results on both controlled (CMU Panoptic) and in-the-wild (AVA-LAEO) datasets.", "mention_start": 101, "mention_end": 168, "dataset_mention": "both controlled (CMU Panoptic) and in-the-wild (AVA-LAEO) datasets"}, {"mentioned_in_paper": "34", "context_id": "342", "dataset_context": "3.3 of the main paper) and its ground truth values using a subset of 3495 images from the CMU Panoptic dataset.", "mention_start": 86, "mention_end": 110, "dataset_mention": "the CMU Panoptic dataset"}, {"mentioned_in_paper": "34", "context_id": "345", "dataset_context": "Additionally, the assumption that people look at each others' 3D eye centers introduces < 4.3 \u2022 gaze error for sub-Figure 11. (Top) a positive and (bottom) a negative example of scene geometry reconstruction from the AVE-LAEO dataset.", "mention_start": 212, "mention_end": 233, "dataset_mention": "the AVE-LAEO dataset"}, {"mentioned_in_paper": "34", "context_id": "350", "dataset_context": "These label errors are significantly smaller than those encountered in cross-dataset (\u223c 30 \u2022 from [44]) and semi-supervised (> 25 \u2022 from Fig. 4 of the main paper) training for Gaze360 making LAEO data a reliable source of supervision for 3D gaze learning in physically unconstrained settings.", "mention_start": 71, "mention_end": 84, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "37", "context_id": "6", "dataset_context": "We validate our method with experiments on two feed-forward networks trained on MNIST and CELEB data sets, and one recurrent network trained on PenDigits.", "mention_start": 80, "mention_end": 105, "dataset_mention": "MNIST and CELEB data sets"}, {"mentioned_in_paper": "37", "context_id": "41", "dataset_context": "The MNIST data set consists of 70,000 28\u00d728-pixel handwritten digit images belonging to ten classes, which are split into training and test sets as 60,000 and 10,000.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The MNIST data set"}, {"mentioned_in_paper": "37", "context_id": "81", "dataset_context": "To show that, we have also done experiments using the Pendigits data set, available at the UCI repository, which consists of 5,621 training sequences of 2D coordinates of a stylus on a touch-sensitive tablet while handwriting the ten digits.", "mention_start": 49, "mention_end": 72, "dataset_mention": "the Pendigits data set"}, {"mentioned_in_paper": "37", "context_id": "84", "dataset_context": "Let {x i,t } Ti t denote a single instance i as a sequence of length T i and {{x i,t } Ti t } N i the entire data set of N sequences.", "mention_start": 93, "mention_end": 117, "dataset_mention": " N i the entire data set"}, {"mentioned_in_paper": "37", "context_id": "104", "dataset_context": "We validate our method with experiments on two feed-forward networks trained on MNIST and CELEB data sets and one recurrent network trained on PenDigits.", "mention_start": 80, "mention_end": 105, "dataset_mention": "MNIST and CELEB data sets"}, {"mentioned_in_paper": "39", "context_id": "63", "dataset_context": "Figure 2 shows an example of a two-dimensional dataset.", "mention_start": 29, "mention_end": 54, "dataset_mention": "a two-dimensional dataset"}, {"mentioned_in_paper": "39", "context_id": "91", "dataset_context": "It is possible to compute the ND set in several different ways, and the main distinction between the available algorithms is made on whether the technique is divided in phases, i.e., the computation of ND set comes after the computation of skyline set otherwise contemporarily, or the algorithm contemplates presorting the dataset or not.", "mention_start": 280, "mention_end": 330, "dataset_mention": "the algorithm contemplates presorting the dataset"}, {"mentioned_in_paper": "39", "context_id": "94", "dataset_context": "Being the dataset sorted, no tuples will be removed from ND.", "mention_start": 0, "mention_end": 17, "dataset_mention": "Being the dataset"}, {"mentioned_in_paper": "39", "context_id": "109", "dataset_context": "We have a 2-dimensional dataset D and, if we assume preference weights  !", "mention_start": 8, "mention_end": 31, "dataset_mention": "a 2-dimensional dataset"}, {"mentioned_in_paper": "41", "context_id": "36", "dataset_context": "Experiments are carried out on the UCSD Ped2 dataset and the Avenue dataset.", "mention_start": 31, "mention_end": 52, "dataset_mention": "the UCSD Ped2 dataset"}, {"mentioned_in_paper": "41", "context_id": "36", "dataset_context": "Experiments are carried out on the UCSD Ped2 dataset and the Avenue dataset.", "mention_start": 31, "mention_end": 75, "dataset_mention": "the UCSD Ped2 dataset and the Avenue dataset"}, {"mentioned_in_paper": "41", "context_id": "115", "dataset_context": "This section evaluates and analyzes MAAS on two publicly available benchmark datasets: the CUHK Avenue dataset [30] and the UCSD Pedestrian dataset [18].", "mention_start": 86, "mention_end": 110, "dataset_mention": " the CUHK Avenue dataset"}, {"mentioned_in_paper": "41", "context_id": "115", "dataset_context": "This section evaluates and analyzes MAAS on two publicly available benchmark datasets: the CUHK Avenue dataset [30] and the UCSD Pedestrian dataset [18].", "mention_start": 86, "mention_end": 147, "dataset_mention": " the CUHK Avenue dataset [30] and the UCSD Pedestrian dataset"}, {"mentioned_in_paper": "41", "context_id": "116", "dataset_context": "The UCSD Pedestrian dataset [18] and CUHK Avenue dataset [30] are two commonly used datasets in video anomaly detection task.", "mention_start": 0, "mention_end": 27, "dataset_mention": "The UCSD Pedestrian dataset"}, {"mentioned_in_paper": "41", "context_id": "116", "dataset_context": "The UCSD Pedestrian dataset [18] and CUHK Avenue dataset [30] are two commonly used datasets in video anomaly detection task.", "mention_start": 0, "mention_end": 56, "dataset_mention": "The UCSD Pedestrian dataset [18] and CUHK Avenue dataset"}, {"mentioned_in_paper": "41", "context_id": "118", "dataset_context": "The UCSD Pedestrian dataset is composed of two subsets: Ped1 and Ped2.", "mention_start": 0, "mention_end": 27, "dataset_mention": "The UCSD Pedestrian dataset"}, {"mentioned_in_paper": "41", "context_id": "129", "dataset_context": "In the training process, the Adam [32] is adopted as optimizer and the mini-batch size is set to 4. For gray scale datasets, the learning rate of  and  are set as 0.0001 and 0.00001, respectively, while for color scale datasets, they are set as 0.0002 and 0.00002 respectively.", "mention_start": 103, "mention_end": 123, "dataset_mention": "gray scale datasets"}, {"mentioned_in_paper": "41", "context_id": "129", "dataset_context": "In the training process, the Adam [32] is adopted as optimizer and the mini-batch size is set to 4. For gray scale datasets, the learning rate of  and  are set as 0.0001 and 0.00001, respectively, while for color scale datasets, they are set as 0.0002 and 0.00002 respectively.", "mention_start": 206, "mention_end": 227, "dataset_mention": "color scale datasets"}, {"mentioned_in_paper": "41", "context_id": "153", "dataset_context": "(1)  \u0303 performs well in Ped2 dataset, but perform poorly in  Avenue dataset;  \u0303 perform well in Avenue dataset, but perform poorly in Ped2 dataset.", "mention_start": 23, "mention_end": 36, "dataset_mention": "Ped2 dataset"}, {"mentioned_in_paper": "41", "context_id": "153", "dataset_context": "(1)  \u0303 performs well in Ped2 dataset, but perform poorly in  Avenue dataset;  \u0303 perform well in Avenue dataset, but perform poorly in Ped2 dataset.", "mention_start": 58, "mention_end": 75, "dataset_mention": " Avenue dataset"}, {"mentioned_in_paper": "41", "context_id": "153", "dataset_context": "(1)  \u0303 performs well in Ped2 dataset, but perform poorly in  Avenue dataset;  \u0303 perform well in Avenue dataset, but perform poorly in Ped2 dataset.", "mention_start": 94, "mention_end": 110, "dataset_mention": "Avenue dataset"}, {"mentioned_in_paper": "41", "context_id": "153", "dataset_context": "(1)  \u0303 performs well in Ped2 dataset, but perform poorly in  Avenue dataset;  \u0303 perform well in Avenue dataset, but perform poorly in Ped2 dataset.", "mention_start": 133, "mention_end": 146, "dataset_mention": "Ped2 dataset"}, {"mentioned_in_paper": "41", "context_id": "156", "dataset_context": "(2) Neither of  \u0303 and  \u0303 perform well in Avenue and Ped2 datasets.", "mention_start": 39, "mention_end": 65, "dataset_mention": "Avenue and Ped2 datasets"}, {"mentioned_in_paper": "42", "context_id": "7", "dataset_context": "Extensive experiments are conducted using real-world subway and ride-hailing datasets from New York City, and the results verify the improved performance of our proposed approach over existing methods across modes.", "mention_start": 42, "mention_end": 85, "dataset_mention": "real-world subway and ride-hailing datasets"}, {"mentioned_in_paper": "42", "context_id": "45", "dataset_context": "\u2022 Extensive experiments are conducted based on real-world subway and ride-hailing datasets from NYC.", "mention_start": 47, "mention_end": 90, "dataset_mention": "real-world subway and ride-hailing datasets"}, {"mentioned_in_paper": "42", "context_id": "235", "dataset_context": "To align different datasets, we aggregate the multimodal demand data into 4-hour intervals and apply min-max normalization to each mode.", "mention_start": 3, "mention_end": 27, "dataset_mention": "align different datasets"}, {"mentioned_in_paper": "42", "context_id": "333", "dataset_context": "To test the model performance, extensive experiments are conducted on real-world subway and ride-hailing datasets from NYC.", "mention_start": 69, "mention_end": 113, "dataset_mention": "real-world subway and ride-hailing datasets"}, {"mentioned_in_paper": "43", "context_id": "36", "dataset_context": "To limit training time we extracted a new dataset -\u03b1-whales from the labeled data (fig.", "mention_start": 18, "mention_end": 49, "dataset_mention": "time we extracted a new dataset"}, {"mentioned_in_paper": "43", "context_id": "91", "dataset_context": "Even though we have failed to demonstrate better performance classifying our \u03b1-whales dataset using a CNN vs the kNN we can still produce statistically significant results of about 0.15 classification accuracy on the validation set.", "mention_start": 73, "mention_end": 93, "dataset_mention": "our \u03b1-whales dataset"}, {"mentioned_in_paper": "43", "context_id": "114", "dataset_context": "Ideally this larger dataset should be more trainable without overfitting the data then our \u03b1-whales dataset.", "mention_start": 0, "mention_end": 27, "dataset_mention": "Ideally this larger dataset"}, {"mentioned_in_paper": "43", "context_id": "114", "dataset_context": "Ideally this larger dataset should be more trainable without overfitting the data then our \u03b1-whales dataset.", "mention_start": 38, "mention_end": 107, "dataset_mention": "more trainable without overfitting the data then our \u03b1-whales dataset"}, {"mentioned_in_paper": "45", "context_id": "4", "dataset_context": "We evaluate our method on three driving datasets and show that our model clearly improves depth estimation while decomposing the scene into separately moving components.", "mention_start": 26, "mention_end": 48, "dataset_mention": "three driving datasets"}, {"mentioned_in_paper": "45", "context_id": "35", "dataset_context": "We build on top of the recent self-supervised learning approaches for monocular depth estimation and show the effectiveness of learning to decompose the scene on KITTI [14, 13], Cityscapes [8], and DDAD [17] datasets.", "mention_start": 193, "mention_end": 216, "dataset_mention": " and DDAD [17] datasets"}, {"mentioned_in_paper": "45", "context_id": "151", "dataset_context": "We evaluate MonoDepthSeg on KITTI [13], Cityscapes [8], and DDAD [17] datasets.", "mention_start": 55, "mention_end": 78, "dataset_mention": " and DDAD [17] datasets"}, {"mentioned_in_paper": "45", "context_id": "155", "dataset_context": "We train our model on the Eigen split [10] of KITTI dataset [14, 13].", "mention_start": 46, "mention_end": 59, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "45", "context_id": "159", "dataset_context": "We train only on KITTI dataset, without pre-training on Cityscapes as done by some previous work, and compare to the other approaches under the same training conditions.", "mention_start": 17, "mention_end": 30, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "45", "context_id": "160", "dataset_context": "We also train and evaluate on the Cityscapes dataset [8].", "mention_start": 30, "mention_end": 52, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "45", "context_id": "166", "dataset_context": "We also conduct experiments on the recent DDAD dataset [17] longer ranges up to 250 m, making it a more challenging and realistic benchmark for the task of depth estimation.", "mention_start": 31, "mention_end": 54, "dataset_mention": "the recent DDAD dataset"}, {"mentioned_in_paper": "45", "context_id": "196", "dataset_context": "segmentation model, DeepLabv3+ [6], with the ResNet50 backbone and first pre-train it on Cityscapes dataset [8].", "mention_start": 88, "mention_end": 107, "dataset_mention": "Cityscapes dataset"}, {"mentioned_in_paper": "45", "context_id": "217", "dataset_context": "Finally, we evaluate the performance of our model on the more complex DDAD dataset.", "mention_start": 52, "mention_end": 82, "dataset_mention": "the more complex DDAD dataset"}, {"mentioned_in_paper": "45", "context_id": "261", "dataset_context": "Table 10 compares our model to Monodepth2 [16] on the moving regions of KITTI and Cityscapes datasets.", "mention_start": 72, "mention_end": 101, "dataset_mention": "KITTI and Cityscapes datasets"}, {"mentioned_in_paper": "45", "context_id": "262", "dataset_context": "Our method outperforms Monodepth2 [16] on both datasets in all metrics, except for one metric on the KITTI dataset.", "mention_start": 96, "mention_end": 114, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "45", "context_id": "266", "dataset_context": "Table 12 shows the full quantitative results on the DDAD dataset, as reported by the online evaluation server [9] after the challenge ended on June 19, 2021.", "mention_start": 48, "mention_end": 64, "dataset_mention": "the DDAD dataset"}, {"mentioned_in_paper": "45", "context_id": "269", "dataset_context": "In Figures 6, 7, and 8, we provide additional qualitative results of our best model on KITTI, Cityscapes and DDAD datasets, respectively.", "mention_start": 93, "mention_end": 122, "dataset_mention": " Cityscapes and DDAD datasets"}, {"mentioned_in_paper": "45", "context_id": "279", "dataset_context": "Abs Rel Sq Rel RMSE RMSE log \u03b4 < 1.25 \u03b4 < 1.25 4 in the main paper for the moving regions, comparing our approach to Monodepth2 [16] for moving regions on KITTI and Cityscapes datasets for all metrics.", "mention_start": 154, "mention_end": 184, "dataset_mention": "KITTI and Cityscapes datasets"}, {"mentioned_in_paper": "45", "context_id": "280", "dataset_context": "Our method achieves better results than Monodepth2 [16] on both datasets in every metric except for one metric on KITTI dataset.", "mention_start": 114, "mention_end": 127, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "49", "context_id": "11", "dataset_context": "For example, the JIF is imprecise because sets of citation counts are highly skewed and its calculation uses the arithmetic mean, which is inappropriate for skewed data setsthe geometric is a better option (Thelwall & Fairclough, 2015; Zitt, 2012).", "mention_start": 156, "mention_end": 173, "dataset_mention": "skewed data sets"}, {"mentioned_in_paper": "49", "context_id": "118", "dataset_context": "Given the scarcity of large journals in these areas, it may be difficult to test this hypothesis with pure data sets.", "mention_start": 101, "mention_end": 116, "dataset_mention": "pure data sets"}, {"mentioned_in_paper": "49", "context_id": "124", "dataset_context": "This paper also introduces, and makes available online (see Appendix B), a new more powerful method for fitting the hooked power law to citation-like data sets.", "mention_start": 135, "mention_end": 159, "dataset_mention": "citation-like data sets"}, {"mentioned_in_paper": "51", "context_id": "39", "dataset_context": "The polarization dataset presented here offers clues in understanding the radiation mechanisms of repeating FRBs.", "mention_start": 0, "mention_end": 24, "dataset_mention": "The polarization dataset"}, {"mentioned_in_paper": "53", "context_id": "145", "dataset_context": "We train our model for 200 epochs with the base learning rate of 0.1, batch size of 128 and weight decay of 1e \u2212 4 on the ImageNet-100 dataset.", "mention_start": 117, "mention_end": 142, "dataset_mention": "the ImageNet-100 dataset"}, {"mentioned_in_paper": "53", "context_id": "146", "dataset_context": "For the CIFAR-10, STL-10 and CIFAR-100 datasets, the models are trained for 1000 epochs with the base learning rate of 0.03, batch size of 64 and weight decay of 5e \u2212 4. Additionally, each learning rate is linearly scaled with the batch size (i.e., LearningRate = BaseLearningRate \u00d7 BatchSize/256).", "mention_start": 17, "mention_end": 47, "dataset_mention": " STL-10 and CIFAR-100 datasets"}, {"mentioned_in_paper": "53", "context_id": "163", "dataset_context": "Following the literature [22, 3, 4], we adopt a ResNet-50 backbone on STL-10 and ImageNet-100 datasets, and adopt a ResNet-18 backbone on CIFAR-10 and CIFAR-100 datasets for all comparison methods.", "mention_start": 69, "mention_end": 102, "dataset_mention": "STL-10 and ImageNet-100 datasets"}, {"mentioned_in_paper": "53", "context_id": "163", "dataset_context": "Following the literature [22, 3, 4], we adopt a ResNet-50 backbone on STL-10 and ImageNet-100 datasets, and adopt a ResNet-18 backbone on CIFAR-10 and CIFAR-100 datasets for all comparison methods.", "mention_start": 137, "mention_end": 169, "dataset_mention": "CIFAR-10 and CIFAR-100 datasets"}, {"mentioned_in_paper": "53", "context_id": "169", "dataset_context": "For example, on the CIFAR-10 dataset, ROMA w/o Random can obtain 0.56%, 2.06% and 0.59% improvements over SimCLR, MoCo v2 and SimSiam, respectively.", "mention_start": 15, "mention_end": 36, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "53", "context_id": "185", "dataset_context": "Specifically, on the more challenging ImageNet-100 dataset, ROMA can still gain significantly improvements over SimCLR, MoCo v2 and SimSiam by 3.42%, 9.14% and 5.91%, respectively, even with a smaller batch size of 128.", "mention_start": 37, "mention_end": 58, "dataset_mention": "ImageNet-100 dataset"}, {"mentioned_in_paper": "53", "context_id": "226", "dataset_context": "Table 6 and Table 7 present the results on the CIFAR-10 and ImageNet-100 datasets, respectively.", "mention_start": 43, "mention_end": 81, "dataset_mention": "the CIFAR-10 and ImageNet-100 datasets"}, {"mentioned_in_paper": "53", "context_id": "230", "dataset_context": "To demonstrate the effectiveness of the proposed ROMA in a more intuitive way and show that the learned features are conducive to the classification, we visualize the feature spaces learnt by different methods in Figure 5. First, three models are trained on the CIFAR-10 dataset with a CIFAR variant of ResNet-18 by using SimCLR, SimSiam and ROMA, respectively.", "mention_start": 257, "mention_end": 278, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "54", "context_id": "109", "dataset_context": "MNIST We first compare the softmax loss, the center loss (with the softmax loss) [32], the large-margin softmax loss (L-Softmax loss for short) [25] and the L-GM loss by visualizing their learned 2D feature spaces for the MNIST Handwritten Digit dataset [20].", "mention_start": 217, "mention_end": 253, "dataset_mention": "the MNIST Handwritten Digit dataset"}, {"mentioned_in_paper": "54", "context_id": "134", "dataset_context": "Second, for the augmented data set C100+, increasing the margin parameter \u03b1 consistently benefits the recognition performance.", "mention_start": 11, "mention_end": 34, "dataset_mention": "the augmented data set"}, {"mentioned_in_paper": "54", "context_id": "138", "dataset_context": "Loss Functions C100 C100+ Center [32] 24.85 \u00b1 0.06 21.05 \u00b1 0.03 L-Softmax [22] ImageNet We investigate the performance on large-scale image classification using the ImageNet dataset [4].", "mention_start": 161, "mention_end": 181, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "54", "context_id": "147", "dataset_context": "We conduct the face verification experiments on the Labeled Face in the Wild (LFW) dataset [13], which contains 13,233 face images from 5749 different identities with large variations in pose, expression and illumination.", "mention_start": 68, "mention_end": 90, "dataset_mention": "the Wild (LFW) dataset"}, {"mentioned_in_paper": "54", "context_id": "149", "dataset_context": "We follow the standard unrestricted, labeled outside data protocol of LFW and use only the CASIA-WebFace dataset [34] for training.", "mention_start": 86, "mention_end": 112, "dataset_mention": "the CASIA-WebFace dataset"}, {"mentioned_in_paper": "54", "context_id": "150", "dataset_context": "The CASIA-WebFace dataset consists of 494,414 face images from 10,575 subjects.", "mention_start": 0, "mention_end": 25, "dataset_mention": "The CASIA-WebFace dataset"}, {"mentioned_in_paper": "54", "context_id": "163", "dataset_context": "In [32], Y. Wen et al. reported a higher accuracy of 99.28% for the center loss by using both the CASIA-Webface and the Celebrity+ [23] dataset for training, with 0.7M training images in total.", "mention_start": 88, "mention_end": 143, "dataset_mention": "both the CASIA-Webface and the Celebrity+ [23] dataset"}, {"mentioned_in_paper": "56", "context_id": "1", "dataset_context": "While there are a variety of different methods for STR actively being researched, it is difficult to evaluate superiority because previously proposed methods do not use the same standardized training/evaluation dataset.", "mention_start": 168, "mention_end": 218, "dataset_mention": "the same standardized training/evaluation dataset"}, {"mentioned_in_paper": "56", "context_id": "2", "dataset_context": "We use the same standardized training/testing dataset to evaluate the performance of several previous methods after standardized re-implementation.", "mention_start": 7, "mention_end": 53, "dataset_mention": "the same standardized training/testing dataset"}, {"mentioned_in_paper": "56", "context_id": "55", "dataset_context": "We also show the results of evaluating previous methods with one standardized dataset.", "mention_start": 61, "mention_end": 85, "dataset_mention": "one standardized dataset"}, {"mentioned_in_paper": "56", "context_id": "151", "dataset_context": "The Oxford Synthetic text dataset [6] is adopted for training and evaluation.", "mention_start": 0, "mention_end": 33, "dataset_mention": "The Oxford Synthetic text dataset"}, {"mentioned_in_paper": "56", "context_id": "212", "dataset_context": "In this paper, we reimplemented prominent previously proposed methods, trained and evaluated on respective standardized datasets, and evaluated their accuracies, model size, and inference time in an objectively fair manner.", "mention_start": 95, "mention_end": 128, "dataset_mention": "respective standardized datasets"}, {"mentioned_in_paper": "57", "context_id": "363", "dataset_context": "Thus, in order to capture the time-varying period, or the IF, of our analyzed dataset, we apply the PT in the frequency domain.", "mention_start": 64, "mention_end": 85, "dataset_mention": "our analyzed dataset"}, {"mentioned_in_paper": "58", "context_id": "6", "dataset_context": "We train our method on over 1000 nodules from the LIDC dataset.", "mention_start": 46, "mention_end": 62, "dataset_mention": "the LIDC dataset"}, {"mentioned_in_paper": "58", "context_id": "15", "dataset_context": "efforts have been made towards constructing large medical image datasets, options are limited beyond using simple automatic methods [8], huge amounts of radiologist labor [1], or mining from radiologist reports [14].", "mention_start": 44, "mention_end": 72, "dataset_mention": "large medical image datasets"}, {"mentioned_in_paper": "58", "context_id": "71", "dataset_context": "We first validate our CGAN using the LIDC dataset [1].", "mention_start": 33, "mention_end": 49, "dataset_mention": "the LIDC dataset"}, {"mentioned_in_paper": "58", "context_id": "73", "dataset_context": "The LIDC dataset contains 1018 chest CT scans of patients with observed lung nodules, totaling roughly 2000 nodules.", "mention_start": 0, "mention_end": 16, "dataset_mention": "The LIDC dataset"}, {"mentioned_in_paper": "58", "context_id": "94", "dataset_context": "Prior to any performed experiments, we selected 34 images from the LIDC dataset exhibiting such peripheral nodules.", "mention_start": 62, "mention_end": 79, "dataset_mention": "the LIDC dataset"}, {"mentioned_in_paper": "58", "context_id": "111", "dataset_context": "Tests demonstrate the superiority of our approach over three competitor CGANs on the LIDC dataset, including Isola et al.'s state-of-the-art method [6].", "mention_start": 81, "mention_end": 97, "dataset_mention": "the LIDC dataset"}, {"mentioned_in_paper": "58", "context_id": "112", "dataset_context": "We further use our proposed CGAN to generate a fine-tuning dataset for the published P-HNN model [5], which can struggle when encountering lung nodules adjoining the lung boundary.", "mention_start": 45, "mention_end": 66, "dataset_mention": "a fine-tuning dataset"}, {"mentioned_in_paper": "58", "context_id": "114", "dataset_context": "As such, our CGAN approach can provide an effective and generic means to help overcome the dataset bottleneck commonly encountered within medical imaging.", "mention_start": 72, "mention_end": 98, "dataset_mention": "help overcome the dataset"}, {"mentioned_in_paper": "59", "context_id": "150", "dataset_context": "We used the MNIST [26] dataset as a toy example to give a simple comparison of different embedding spaces with different loss functions.", "mention_start": 8, "mention_end": 30, "dataset_mention": "the MNIST [26] dataset"}, {"mentioned_in_paper": "59", "context_id": "166", "dataset_context": "Many types of FGVC datasets have been built, including identification different species of animals and plants, classifying galaxies [37] and categorizing different air crafts [38].", "mention_start": 14, "mention_end": 27, "dataset_mention": "FGVC datasets"}, {"mentioned_in_paper": "59", "context_id": "171", "dataset_context": "Taking Oxford Flowers data set [28] as an example, it contains 102 categories of flowers and only allows to use ten images for training and 10 for validation for each class.", "mention_start": 0, "mention_end": 30, "dataset_mention": "Taking Oxford Flowers data set"}, {"mentioned_in_paper": "59", "context_id": "173", "dataset_context": "2) Oxford-IIIT Pet: The Oxford-IIIT Pet [29] data set provides image data from 37 different breeds of dogs and cats, among which 25 categories are dogs and 12 classes are cats.", "mention_start": 19, "mention_end": 53, "dataset_mention": " The Oxford-IIIT Pet [29] data set"}, {"mentioned_in_paper": "59", "context_id": "176", "dataset_context": "Compared with previous flower-102 dataset, this pet data set has a larger variance in shapes, colors and scales.", "mention_start": 14, "mention_end": 41, "dataset_mention": "previous flower-102 dataset"}, {"mentioned_in_paper": "59", "context_id": "176", "dataset_context": "Compared with previous flower-102 dataset, this pet data set has a larger variance in shapes, colors and scales.", "mention_start": 42, "mention_end": 60, "dataset_mention": " this pet data set"}, {"mentioned_in_paper": "59", "context_id": "178", "dataset_context": "3) Stanford Dogs: The Stanford Dogs data set [30] contains 120 categories of dog breads, and around 150 images per class and 20, 580 in total.", "mention_start": 17, "mention_end": 44, "dataset_mention": " The Stanford Dogs data set"}, {"mentioned_in_paper": "59", "context_id": "183", "dataset_context": "It can be found from above classification results that our vMF model achieves state-of-the-art performance on the tree fine-grained datasets.", "mention_start": 110, "mention_end": 140, "dataset_mention": "the tree fine-grained datasets"}, {"mentioned_in_paper": "59", "context_id": "203", "dataset_context": "1) Cars196: Cars196 [32] is a large car dataset that includes 16, 185 images from 196 classes of cars.", "mention_start": 27, "mention_end": 47, "dataset_mention": "a large car dataset"}, {"mentioned_in_paper": "59", "context_id": "218", "dataset_context": "To demonstrate this property, we evaluate our model with different depths of ResNet [47] on the CIFAR-100 dataset [39].", "mention_start": 91, "mention_end": 113, "dataset_mention": "the CIFAR-100 dataset"}, {"mentioned_in_paper": "59", "context_id": "243", "dataset_context": "The Cars196 [32] is used as an exemplar dataset.", "mention_start": 28, "mention_end": 47, "dataset_mention": "an exemplar dataset"}, {"mentioned_in_paper": "60", "context_id": "139", "dataset_context": "Nakajima and Ngoc Bui proposed a new coverage metric called dataset coverage for the testing of machine learning programs [34].", "mention_start": 0, "mention_end": 67, "dataset_mention": "Nakajima and Ngoc Bui proposed a new coverage metric called dataset"}, {"mentioned_in_paper": "60", "context_id": "142", "dataset_context": "However, this metric has some drawbacks: 1. complete dataset coverage is not possible because an infinite number of data points can exist in between any pair of adjacent data points.", "mention_start": 40, "mention_end": 60, "dataset_mention": " 1. complete dataset"}, {"mentioned_in_paper": "62", "context_id": "20", "dataset_context": "For example, for the Ima-geNet classification dataset (Deng et al., 2009) with around one million samples, the dimension of parameters would be in the same order.", "mention_start": 16, "mention_end": 53, "dataset_mention": "the Ima-geNet classification dataset"}, {"mentioned_in_paper": "62", "context_id": "34", "dataset_context": "Comprehensive experiments on CIFAR-10/100 and Ima-geNet datasets (Krizhevsky, 2009; Deng et al., 2009) with different networks show that the Autosampling can increase the top-1 accuracy by up to 2.85% on CIFAR-10, 2.19% on CIFAR-100, and 2.83% on ImageNet.", "mention_start": 29, "mention_end": 64, "dataset_mention": "CIFAR-10/100 and Ima-geNet datasets"}, {"mentioned_in_paper": "62", "context_id": "39", "dataset_context": "However, they are often restricted to certain tasks and datasets based on which they are proposed, and their ability to generalize to a broader range of tasks with different data distribution may be limited.", "mention_start": 37, "mention_end": 64, "dataset_mention": "certain tasks and datasets"}, {"mentioned_in_paper": "62", "context_id": "56", "dataset_context": "In particular for interval t, to exploit child models are evaluated on a held-out validation dataset:", "mention_start": 70, "mention_end": 100, "dataset_mention": "a held-out validation dataset"}, {"mentioned_in_paper": "62", "context_id": "115", "dataset_context": "Experiments on CIFAR We use the same training configuration for both CIFAR-100 and CIFAR-10 datasets, which both consist of 50000 training images.", "mention_start": 64, "mention_end": 100, "dataset_mention": "both CIFAR-100 and CIFAR-10 datasets"}, {"mentioned_in_paper": "62", "context_id": "127", "dataset_context": "For this part, we gradually build up and test components of AutoSampling on CIFAR-100, and then examine their performances on CIFAR-10 and ImageNet datasets.", "mention_start": 125, "mention_end": 156, "dataset_mention": "CIFAR-10 and ImageNet datasets"}, {"mentioned_in_paper": "62", "context_id": "201", "dataset_context": "We test our method on CIFAR-10/100 and ImageNet datasets (Krizhevsky, 2009; Deng et al., 2009) with different networks show that it consistently outperforms the baseline methods across different benchmarks.", "mention_start": 22, "mention_end": 56, "dataset_mention": "CIFAR-10/100 and ImageNet datasets"}, {"mentioned_in_paper": "63", "context_id": "9", "dataset_context": "Moreover, it takes advantage of temporal features contained in videos to perform better than Yolov5 in our temporal dataset, making TYolov5 suitable for real-world applications.", "mention_start": 102, "mention_end": 123, "dataset_mention": "our temporal dataset"}, {"mentioned_in_paper": "63", "context_id": "41", "dataset_context": "All the aforementioned characteristics concede TYolov5 an improvement in accuracy over Yolov5 in our temporal dataset, whereas keeping the required performance of a real-time detector.", "mention_start": 97, "mention_end": 117, "dataset_mention": "our temporal dataset"}, {"mentioned_in_paper": "63", "context_id": "43", "dataset_context": "\u2022 Two datasets are publicly available.", "mention_start": 0, "mention_end": 14, "dataset_mention": "\u2022 Two datasets"}, {"mentioned_in_paper": "63", "context_id": "82", "dataset_context": "Yolov5 claims to have comparable results to Yolov4 in COCO dataset [14], whereas reducing the training time by 52 times for custom datasets [15].", "mention_start": 54, "mention_end": 66, "dataset_mention": "COCO dataset"}, {"mentioned_in_paper": "63", "context_id": "82", "dataset_context": "Yolov5 claims to have comparable results to Yolov4 in COCO dataset [14], whereas reducing the training time by 52 times for custom datasets [15].", "mention_start": 123, "mention_end": 139, "dataset_mention": "custom datasets"}, {"mentioned_in_paper": "63", "context_id": "83", "dataset_context": "Since training temporal object detectors is slower than training static object detectors, and Yolov5 and Yolov4 achieve better results in COCO dataset, than all the architectures used by the state-of-the-art for handgun detection (Table 1), we chose Yolov5 as base for our temporal architecture.", "mention_start": 137, "mention_end": 150, "dataset_mention": "COCO dataset"}, {"mentioned_in_paper": "63", "context_id": "97", "dataset_context": "Finally, TYolov5 large, aims to achieve the highest mAP in our temporal dataset.", "mention_start": 58, "mention_end": 79, "dataset_mention": "our temporal dataset"}, {"mentioned_in_paper": "63", "context_id": "122", "dataset_context": "Additionally, to train TYolov5 with our temporal dataset, we implemented five temporal data augmentation techniques.", "mention_start": 35, "mention_end": 56, "dataset_mention": "our temporal dataset"}, {"mentioned_in_paper": "63", "context_id": "126", "dataset_context": "Mosaic augmentation improved the AP 50 of Yolov4 by 1.8% in COCO Dataset, proving that this data augmentation technique is worth exploring for temporal object detectors [14].", "mention_start": 60, "mention_end": 72, "dataset_mention": "COCO Dataset"}, {"mentioned_in_paper": "63", "context_id": "151", "dataset_context": "According to Olmos et al. [2] the current datasets for handgun detection contain two possible enhancements: handgun datasets do not contain contain multiple categories to let the network learn the differences between similar features; The dataset contain images of handguns which are not used in context.", "mention_start": 13, "mention_end": 50, "dataset_mention": "Olmos et al. [2] the current datasets"}, {"mentioned_in_paper": "63", "context_id": "151", "dataset_context": "According to Olmos et al. [2] the current datasets for handgun detection contain two possible enhancements: handgun datasets do not contain contain multiple categories to let the network learn the differences between similar features; The dataset contain images of handguns which are not used in context.", "mention_start": 107, "mention_end": 124, "dataset_mention": " handgun datasets"}, {"mentioned_in_paper": "63", "context_id": "152", "dataset_context": "Following those recommendations, we compiled the Hands Guns and Phones (HGP) dataset.", "mention_start": 32, "mention_end": 84, "dataset_mention": " we compiled the Hands Guns and Phones (HGP) dataset"}, {"mentioned_in_paper": "63", "context_id": "157", "dataset_context": "To build a temporal architecture, the first step is to implement and train an image object detector with a static dataset.", "mention_start": 104, "mention_end": 121, "dataset_mention": "a static dataset"}, {"mentioned_in_paper": "63", "context_id": "159", "dataset_context": "Finally, train only the RNN modules and the head with a temporal dataset.", "mention_start": 53, "mention_end": 72, "dataset_mention": "a temporal dataset"}, {"mentioned_in_paper": "63", "context_id": "164", "dataset_context": "To train Yolov5 with our custom dataset, we applied transfer learning from the weights trained for COCO Dataset.", "mention_start": 21, "mention_end": 39, "dataset_mention": "our custom dataset"}, {"mentioned_in_paper": "63", "context_id": "164", "dataset_context": "To train Yolov5 with our custom dataset, we applied transfer learning from the weights trained for COCO Dataset.", "mention_start": 98, "mention_end": 111, "dataset_mention": "COCO Dataset"}, {"mentioned_in_paper": "63", "context_id": "168", "dataset_context": "For training our model we used our HGP dataset.", "mention_start": 31, "mention_end": 46, "dataset_mention": "our HGP dataset"}, {"mentioned_in_paper": "63", "context_id": "169", "dataset_context": "Since we are attempting to build a static object detector, we mixed HGP with THGP dataset to increase the number of images and help the network to generalize better.", "mention_start": 76, "mention_end": 89, "dataset_mention": "THGP dataset"}, {"mentioned_in_paper": "63", "context_id": "179", "dataset_context": "For training our temporal architectures, we combined THGP and HGP dataset.", "mention_start": 52, "mention_end": 73, "dataset_mention": "THGP and HGP dataset"}, {"mentioned_in_paper": "63", "context_id": "214", "dataset_context": "Temporal data augmentation improved the accuracy of TYolov5 in our temporal dataset, with the benefit of not increasing the complexity of the model, or the number of parameters.", "mention_start": 63, "mention_end": 83, "dataset_mention": "our temporal dataset"}, {"mentioned_in_paper": "63", "context_id": "218", "dataset_context": "QRNNs showed comparable results to Convolutional LSTMs in our temporal dataset.", "mention_start": 58, "mention_end": 78, "dataset_mention": "our temporal dataset"}, {"mentioned_in_paper": "67", "context_id": "54", "dataset_context": "In common datasets, there is usually a tradeoff between the two metrics.", "mention_start": 3, "mention_end": 18, "dataset_mention": "common datasets"}, {"mentioned_in_paper": "67", "context_id": "92", "dataset_context": "Ground truth segmentation from the original Crack Forest dataset is also provided.", "mention_start": 31, "mention_end": 64, "dataset_mention": "the original Crack Forest dataset"}, {"mentioned_in_paper": "67", "context_id": "107", "dataset_context": "This paper proposes a fully automated semantic segmentation model to effectively detect cracks and overcome the challenge of the highly imbalanced dataset.", "mention_start": 125, "mention_end": 154, "dataset_mention": "the highly imbalanced dataset"}, {"mentioned_in_paper": "68", "context_id": "4", "dataset_context": "Experiments of CompL on monocular depth estimation, semantic segmentation, and boundary detection show consistent performance improvements in fully and partially labeled datasets.", "mention_start": 141, "mention_end": 178, "dataset_mention": "fully and partially labeled datasets"}, {"mentioned_in_paper": "68", "context_id": "43", "dataset_context": "This enables the easy switching of different supervised target tasks or auxiliary self-supervised tasks, without requiring any architectural changes, enabling the wider reach of joint training across tasks and datasets.", "mention_start": 199, "mention_end": 218, "dataset_mention": "tasks and datasets"}, {"mentioned_in_paper": "68", "context_id": "82", "dataset_context": "However, the above is not a necessary condition for CompL, meaning the selfsupervised task could be trained on an independent dataset.", "mention_start": 110, "mention_end": 133, "dataset_mention": "an independent dataset"}, {"mentioned_in_paper": "68", "context_id": "181", "dataset_context": "Robustness to zero-shot dataset transfer So far we have only evaluated on the same distribution as that used for training, however, distribution shifts during deployment are common.", "mention_start": 14, "mention_end": 31, "dataset_mention": "zero-shot dataset"}, {"mentioned_in_paper": "68", "context_id": "183", "dataset_context": "We evaluate the zero-shot capabilities of the models on the challenging BDD100K [68] dataset in Fig. 6, a diverse driving dataset.", "mention_start": 72, "mention_end": 92, "dataset_mention": "BDD100K [68] dataset"}, {"mentioned_in_paper": "68", "context_id": "183", "dataset_context": "We evaluate the zero-shot capabilities of the models on the challenging BDD100K [68] dataset in Fig. 6, a diverse driving dataset.", "mention_start": 103, "mention_end": 129, "dataset_mention": " a diverse driving dataset"}, {"mentioned_in_paper": "68", "context_id": "197", "dataset_context": "Experimental protocol We study boundary detection on the BSDS500 [1] dataset, consisting of 300 train and 200 test images.", "mention_start": 53, "mention_end": 76, "dataset_mention": "the BSDS500 [1] dataset"}, {"mentioned_in_paper": "68", "context_id": "199", "dataset_context": "Performance is evaluated using the Optimal-Dataset-Scale F-measure (ODS Fscore) [50].", "mention_start": 31, "mention_end": 50, "dataset_mention": "the Optimal-Dataset"}, {"mentioned_in_paper": "68", "context_id": "211", "dataset_context": "Robustness to zero-shot dataset transfer We evaluate the zero-shot dataset transfer capabilities of the BSDS500 [1] models from Table 3 on NYUD-v2 [57].", "mention_start": 14, "mention_end": 31, "dataset_mention": "zero-shot dataset"}, {"mentioned_in_paper": "68", "context_id": "211", "dataset_context": "Robustness to zero-shot dataset transfer We evaluate the zero-shot dataset transfer capabilities of the BSDS500 [1] models from Table 3 on NYUD-v2 [57].", "mention_start": 53, "mention_end": 74, "dataset_mention": "the zero-shot dataset"}, {"mentioned_in_paper": "68", "context_id": "233", "dataset_context": "We show consistent performance improvements in fully and partially labeled datasets for both semantic segmentation and monocular depth estimation.", "mention_start": 47, "mention_end": 83, "dataset_mention": "fully and partially labeled datasets"}, {"mentioned_in_paper": "68", "context_id": "235", "dataset_context": "Additionally, the semantic segmentation models trained under CompL yield better robustness on zero-shot cross dataset transfer.", "mention_start": 93, "mention_end": 117, "dataset_mention": "zero-shot cross dataset"}, {"mentioned_in_paper": "68", "context_id": "237", "dataset_context": "Joint optimization on identical dataset subsets Table S.5 presents the performance of the monocular depth estimation single-task baseline and the best performing self-supervised task, DenseCL.", "mention_start": 22, "mention_end": 39, "dataset_mention": "identical dataset"}, {"mentioned_in_paper": "69", "context_id": "3", "dataset_context": "We propose a scalable novel framework for reducing multiple biases in high-dimensional data sets in order to train more reliable predictors.", "mention_start": 70, "mention_end": 96, "dataset_mention": "high-dimensional data sets"}, {"mentioned_in_paper": "69", "context_id": "6", "dataset_context": "Biased data sets are of particular issue in this domain.", "mention_start": 0, "mention_end": 16, "dataset_mention": "Biased data sets"}, {"mentioned_in_paper": "69", "context_id": "26", "dataset_context": "\u2022 We propose a scalable novel methodology for reducing multiple biases in high-dimensional data sets at the same time.", "mention_start": 74, "mention_end": 100, "dataset_mention": "high-dimensional data sets"}, {"mentioned_in_paper": "70", "context_id": "7", "dataset_context": "Experimental results on the DeepLesion dataset demonstrate that the SGAN enhancements alone can push GrabCut performance over HNN trained on original images.", "mention_start": 24, "mention_end": 46, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "70", "context_id": "35", "dataset_context": "Experimental results on the large scale DeepLesion dataset [10] demonstrate the effectiveness of our SGAN approach.", "mention_start": 24, "mention_end": 58, "dataset_mention": "the large scale DeepLesion dataset"}, {"mentioned_in_paper": "70", "context_id": "56", "dataset_context": "However, in the DeepLesion dataset lesion sizes vary considerably, meaning they have to be enlarged with correspondingly different zooming factors.", "mention_start": 11, "mention_end": 34, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "70", "context_id": "63", "dataset_context": "For the sake of SGAN training, we leverage transfer learning using a large-scale synthesized natural image dataset: DIV2K [14] where all images are converted into gray scale and down-sampled to produce training pairs.", "mention_start": 66, "mention_end": 114, "dataset_mention": "a large-scale synthesized natural image dataset"}, {"mentioned_in_paper": "70", "context_id": "88", "dataset_context": "For this reason, we construct a high quality trimap T using the RECIST diameter marks within the DeepLesion dataset [10].", "mention_start": 92, "mention_end": 115, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "70", "context_id": "92", "dataset_context": "Since the DeepLesion dataset does not provide the ground truth lesion masks, the GrabCut segmentation results are used as supervision to train the HNN segmentation model until convergence.", "mention_start": 6, "mention_end": 28, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "70", "context_id": "93", "dataset_context": "The DeepLesion dataset [10] is composed of 32, 735 PACS CT lesion images annotated with RECIST long and short diameters.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The DeepLesion dataset"}, {"mentioned_in_paper": "70", "context_id": "99", "dataset_context": "Although we do not possess corresponding high quality images in the DeepLesion dataset, we can implicitly evaluate the performance of the proposed SGAN model for CT image enhancement by comparing the segmentation performance with or without enhanced images.", "mention_start": 64, "mention_end": 86, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "70", "context_id": "122", "dataset_context": "Experimental results on the DeepLesion dataset test segmentation performance when GrabCut and HNN are applied on OG and enhanced images.", "mention_start": 24, "mention_end": 46, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "71", "context_id": "5", "dataset_context": "Extensive experiments on different user mobility datasets across the U.S. and Japan show that our model significantly outperforms stateof-the-art methods for modeling continuous-time sequences.", "mention_start": 25, "mention_end": 57, "dataset_mention": "different user mobility datasets"}, {"mentioned_in_paper": "71", "context_id": "23", "dataset_context": "In summary, the key contributions we make in this paper via Reformd are three-fold: (i) We propose Reformd, a transfer-learning model for predicting mobility dynamics in checkin-scarce datasets by incorporating mobility parameters trained on a checkin-rich region.", "mention_start": 169, "mention_end": 193, "dataset_mention": "checkin-scarce datasets"}, {"mentioned_in_paper": "71", "context_id": "26", "dataset_context": "(iii) Finally, we empirically show that Reformd outperforms the state-of-the-art models by up to 20% and 23% for checkin-category and time prediction and can easily be extended to product recommendation datasets.", "mention_start": 179, "mention_end": 211, "dataset_mention": "product recommendation datasets"}, {"mentioned_in_paper": "71", "context_id": "87", "dataset_context": "RQ3 Can we extend Reformd for non-spatial datasets?", "mention_start": 30, "mention_end": 50, "dataset_mention": "non-spatial datasets"}, {"mentioned_in_paper": "71", "context_id": "88", "dataset_context": "For evaluating mobility prediction, we consider six POI datasets from the U.S. and Japan.", "mention_start": 47, "mention_end": 64, "dataset_mention": "six POI datasets"}, {"mentioned_in_paper": "71", "context_id": "113", "dataset_context": "For this, we plot the actual inter-checkin time differences and the difference time predicted by Reformd in Figure 2 for Virginia and Aichi datasets.", "mention_start": 120, "mention_end": 148, "dataset_mention": "Virginia and Aichi datasets"}, {"mentioned_in_paper": "72", "context_id": "165", "dataset_context": "Each sub-dataset can be viewed as a different domain.", "mention_start": 0, "mention_end": 16, "dataset_mention": "Each sub-dataset"}, {"mentioned_in_paper": "73", "context_id": "4", "dataset_context": "We evaluate our proposed tracker on the MOT17 test dataset, showing that our proposed method can reduce identity switches significantly by 22.6% and obtain a notable improvement of 1.5% in IDF1 compared to the original CenterTrack's under the same tracklet lifetime.", "mention_start": 36, "mention_end": 58, "dataset_mention": "the MOT17 test dataset"}, {"mentioned_in_paper": "73", "context_id": "29", "dataset_context": "\u2022 Evaluate the proposed method on MOT17 dataset to obtain a significant reduction in identity switches (IDs)", "mention_start": 34, "mention_end": 47, "dataset_mention": "MOT17 dataset"}, {"mentioned_in_paper": "73", "context_id": "99", "dataset_context": "We use MOT17 [18] dataset to train and evaluate the proposed method in our paper.", "mention_start": 7, "mention_end": 25, "dataset_mention": "MOT17 [18] dataset"}, {"mentioned_in_paper": "73", "context_id": "112", "dataset_context": "The network is pre-trained on CrowdHuman dataset [23] before training on MOT17 dataset.", "mention_start": 30, "mention_end": 48, "dataset_mention": "CrowdHuman dataset"}, {"mentioned_in_paper": "73", "context_id": "112", "dataset_context": "The network is pre-trained on CrowdHuman dataset [23] before training on MOT17 dataset.", "mention_start": 73, "mention_end": 86, "dataset_mention": "MOT17 dataset"}, {"mentioned_in_paper": "73", "context_id": "130", "dataset_context": "Experiments on MOT17 test dataset under private protocol show that our proposed method achieves the best performance among the trackers only using spatial features in the association.", "mention_start": 15, "mention_end": 33, "dataset_mention": "MOT17 test dataset"}, {"mentioned_in_paper": "75", "context_id": "77", "dataset_context": "We test the performance of ASTRAGEM on the Zinc250k dataset [18].", "mention_start": 39, "mention_end": 59, "dataset_mention": "the Zinc250k dataset"}, {"mentioned_in_paper": "77", "context_id": "30", "dataset_context": "We also provide a theoretical convergence analysis of FedLAMA for smooth and non-convex problems under non-IID data settings.", "mention_start": 66, "mention_end": 119, "dataset_mention": "smooth and non-convex problems under non-IID data set"}, {"mentioned_in_paper": "77", "context_id": "31", "dataset_context": "We evaluate the performance of FedLAMA across three representative image classification benchmark datasets: CIFAR-10 ( Krizhevsky et al. [2009]), CIFAR-100, and Federated Extended MNIST (Cohen et al. [2017]).", "mention_start": 46, "mention_end": 106, "dataset_mention": "three representative image classification benchmark datasets"}, {"mentioned_in_paper": "77", "context_id": "143", "dataset_context": "Experimental Settings -We evaluate FedLAMA using three representative benchmark datasets: CIFAR-10 (ResNet20 (He et al. [2016])), CIFAR-100 (WideResNet28-10 (Zagoruyko and Komodakis [2016])), and Federated Extended MNIST (CNN (Caldas et al. [2018])).", "mention_start": 49, "mention_end": 88, "dataset_mention": "three representative benchmark datasets"}, {"mentioned_in_paper": "77", "context_id": "162", "dataset_context": "Experimental Results with IID Data -We first present CIFAR-10 and CIFAR-100 classification results under IID data settings.", "mention_start": 53, "mention_end": 117, "dataset_mention": "CIFAR-10 and CIFAR-100 classification results under IID data set"}, {"mentioned_in_paper": "78", "context_id": "60", "dataset_context": "FitNets [21] imitates the soft output of a large teacher network using a thin and deep student network, and in turn yields 10.4\u00d7 fewer parameters and similar accuracy to a large teacher network on the CIFAR-10 dataset.", "mention_start": 196, "mention_end": 217, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "78", "context_id": "65", "dataset_context": "In Incremental Network Quantization, Zhou et al. [31] quantize the parameter incrementally and show that it is even possible to further reduce the weight precision to 2-5 bits with slightly higher accuracy than a fullprecision network on the ImageNet dataset.", "mention_start": 237, "mention_end": 258, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "78", "context_id": "66", "dataset_context": "In BinaryConnect [1], Courbariaux et al. employ 1-bit precision weights (1 and -1) while maintaining sufficiently high accuracy on the MNIST, CIFAR10 and SVHN datasets.", "mention_start": 141, "mention_end": 167, "dataset_mention": " CIFAR10 and SVHN datasets"}, {"mentioned_in_paper": "78", "context_id": "70", "dataset_context": "Successful attempts include DoReFa-Net [33] and QNN [8], which explore neural networks trained with 1-bit weights and 2-bit activations, and the accuracy drops by 6.1% and 4.9% respectively on the ImageNet dataset compared to the real-valued AlexNet.", "mention_start": 192, "mention_end": 213, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "78", "context_id": "71", "dataset_context": "Additionally, Bina-ryNet [7] uses only 1-bit weights and 1-bit activations in a neural network and achieves comparable accuracy as full-precision neural networks on the MNIST and CIFAR-10 datasets.", "mention_start": 164, "mention_end": 196, "dataset_mention": "the MNIST and CIFAR-10 datasets"}, {"mentioned_in_paper": "78", "context_id": "156", "dataset_context": "The experiments are carried out on the ILSVRC12 ImageNet classification dataset [22].", "mention_start": 35, "mention_end": 79, "dataset_mention": "the ILSVRC12 ImageNet classification dataset"}, {"mentioned_in_paper": "78", "context_id": "161", "dataset_context": "For each image in the ImageNet dataset, the smaller dimension of the image is rescaled to 256 while keeping the aspect ratio intact.", "mention_start": 18, "mention_end": 38, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "78", "context_id": "182", "dataset_context": "To study how these proposals benefit the 1-bit CNNs individually and collectively, we train the 18-layer structure and the 34-layer structure with a combination of these techniques on the ImageNet dataset.", "mention_start": 183, "mention_end": 204, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "80", "context_id": "28", "dataset_context": "For example, in a standard 2-layer CNN network for MNIST dataset, convolutional layers only have less than 20000 parameters, but fully-connected dimension reduction layers have more than 1 million parameters.", "mention_start": 50, "mention_end": 64, "dataset_mention": "MNIST dataset"}, {"mentioned_in_paper": "80", "context_id": "181", "dataset_context": "For the 10-class MNIST dataset, we then have 1000 clusters with cluster centers x i and z i in high and low dimensions.", "mention_start": 4, "mention_end": 30, "dataset_mention": "the 10-class MNIST dataset"}, {"mentioned_in_paper": "80", "context_id": "370", "dataset_context": "\u2022 MNIST: handwritten digit dataset, which consists of 60,000 training images and 10,000 testing images.", "mention_start": 8, "mention_end": 34, "dataset_mention": " handwritten digit dataset"}, {"mentioned_in_paper": "80", "context_id": "372", "dataset_context": "\u2022 CIFAR10: natural image dataset, which contains 50,000 training images and 10,000 testing images in ten different classes.", "mention_start": 10, "mention_end": 32, "dataset_mention": " natural image dataset"}, {"mentioned_in_paper": "80", "context_id": "374", "dataset_context": "For the MNIST dataset, the encoder is setup as a two-layer CNN with kernel size 5\u00d75 and 5/20 output channels in the first and second layer, and a 2\u00d72 pooling following each convolutional layer.", "mention_start": 4, "mention_end": 21, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "80", "context_id": "376", "dataset_context": "For CIFAR10 dataset, we employ VGG16 architecture as the encoder, and it leads to a 512-dimensional high-dimensional embedding.", "mention_start": 4, "mention_end": 19, "dataset_mention": "CIFAR10 dataset"}, {"mentioned_in_paper": "80", "context_id": "388", "dataset_context": "The MNIST dataset is known to be a simpler Our results coincide with both our findings about intrinsic dimension of the two datasets.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The MNIST dataset"}, {"mentioned_in_paper": "80", "context_id": "389", "dataset_context": "(3) In the MNIST dataset, we also consider different choices of reference points.", "mention_start": 7, "mention_end": 24, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "81", "context_id": "77", "dataset_context": "A new attention-based fully gated convolutional RNN was proposed by Abdallah [24], this model was trained and tested on the HKR dataset [4].", "mention_start": 119, "mention_end": 135, "dataset_mention": "the HKR dataset"}, {"mentioned_in_paper": "81", "context_id": "82", "dataset_context": "It also achieved the state-of-the-art for offline Kazakh and Russian handwriting dataset [4].", "mention_start": 42, "mention_end": 88, "dataset_mention": "offline Kazakh and Russian handwriting dataset"}, {"mentioned_in_paper": "81", "context_id": "83", "dataset_context": "Atten-CNN-BGRU architecture achieves 4.5% CER in the first dataset and 19.2% WER in HKR dataset and 6.4% CER and 24.0%", "mention_start": 84, "mention_end": 95, "dataset_mention": "HKR dataset"}, {"mentioned_in_paper": "81", "context_id": "93", "dataset_context": "In the classification of handwritten test dataset images, 46.30% and 16.12% error rates were achieved, respectively.", "mention_start": 25, "mention_end": 49, "dataset_mention": "handwritten test dataset"}, {"mentioned_in_paper": "81", "context_id": "117", "dataset_context": "Some examples of HKR dataset are shown in Figure 2.", "mention_start": 17, "mention_end": 28, "dataset_mention": "HKR dataset"}, {"mentioned_in_paper": "81", "context_id": "118", "dataset_context": "This final dataset was then divided into Training (70%), Validation (15%), and Test (15%) datasets.", "mention_start": 74, "mention_end": 98, "dataset_mention": " and Test (15%) datasets"}, {"mentioned_in_paper": "81", "context_id": "119", "dataset_context": "The test dataset itself was split into two sub-datasets (7.5% each): the first dataset was named TEST1 and it consisted of words that were not included in the Training and Validation datasets; the other sub-dataset was named TEST2 and consisted of words that were included in the Training dataset but had completely different handwriting styles.", "mention_start": 192, "mention_end": 214, "dataset_mention": " the other sub-dataset"}, {"mentioned_in_paper": "81", "context_id": "120", "dataset_context": "The main purpose of splitting the Test dataset into TEST1 and TEST2 datasets was to check the difference in accuracy between recognition of unseen words and words seen in the training stage but with unseen handwriting styles.", "mention_start": 52, "mention_end": 76, "dataset_mention": "TEST1 and TEST2 datasets"}, {"mentioned_in_paper": "81", "context_id": "212", "dataset_context": "\u2022 DataLoader file for reading and pre-possessing the image dataset and reading the annotation file belongs to the images", "mention_start": 22, "mention_end": 66, "dataset_mention": "reading and pre-possessing the image dataset"}, {"mentioned_in_paper": "81", "context_id": "213", "dataset_context": "\u2022 The dataset was divided into two subsets: 90% for training and 10% for validation of the trained model.", "mention_start": 0, "mention_end": 13, "dataset_mention": "\u2022 The dataset"}, {"mentioned_in_paper": "81", "context_id": "227", "dataset_context": "The Bluche and Puigcerver model were trained on the second dataset (HKR dataset).", "mention_start": 48, "mention_end": 79, "dataset_mention": "the second dataset (HKR dataset"}, {"mentioned_in_paper": "82", "context_id": "7", "dataset_context": "To show the effectiveness of BPFNet, we carry out experiments on the large-scale touchless palmprint datasets CUHKSZ-v1 and TongJi and the proposed method achieves state-of-the-art performances.", "mention_start": 64, "mention_end": 109, "dataset_mention": "the large-scale touchless palmprint datasets"}, {"mentioned_in_paper": "82", "context_id": "24", "dataset_context": "However, most public bimodal datasets do not contain adequate camera information for ROI alignment, thus the alignment is restricted at image-level.", "mention_start": 8, "mention_end": 37, "dataset_mention": " most public bimodal datasets"}, {"mentioned_in_paper": "82", "context_id": "34", "dataset_context": "2. A novel ROI localization scheme is applied, which is also compatible with other datasets, achieving 90.65% and % IoU on CUHKSZ and TongJi datasets respectively.", "mention_start": 122, "mention_end": 149, "dataset_mention": "CUHKSZ and TongJi datasets"}, {"mentioned_in_paper": "82", "context_id": "44", "dataset_context": "The earlier touchless palmprint datasets are CASIA and IITD-v1 which are released by the Chinese Academy of Sciences [1] and the Indian Institute of Technology in Dehli [2] respectively.", "mention_start": 0, "mention_end": 40, "dataset_mention": "The earlier touchless palmprint datasets"}, {"mentioned_in_paper": "82", "context_id": "49", "dataset_context": "The above datasets follow [15] to locate the region-of-interest (ROI) region, which is based on finding landmarks on the extracted hand contour.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The above datasets"}, {"mentioned_in_paper": "82", "context_id": "64", "dataset_context": "The details of the touchless datasets are summarized in Table 1.", "mention_start": 15, "mention_end": 37, "dataset_mention": "the touchless datasets"}, {"mentioned_in_paper": "82", "context_id": "80", "dataset_context": "Touchless palmprint datasets may have different annotation systems and the number of keypoints also varies (Table 1).", "mention_start": 0, "mention_end": 28, "dataset_mention": "Touchless palmprint datasets"}, {"mentioned_in_paper": "82", "context_id": "85", "dataset_context": "In CUHKSZ dataset, only RGB images are annotated while the ground truth ROIs for palm vein images are not available.", "mention_start": 3, "mention_end": 17, "dataset_mention": "CUHKSZ dataset"}, {"mentioned_in_paper": "82", "context_id": "154", "dataset_context": "In this section, we conduct experiments on CUHKSZ dataset and evaluate our method in various aspects.", "mention_start": 42, "mention_end": 57, "dataset_mention": "CUHKSZ dataset"}, {"mentioned_in_paper": "82", "context_id": "185", "dataset_context": "Since CUHKSZ dataset is more challenging and the comparison results are more clear, after we will conduct experiments only on CUHKSZ dataset.", "mention_start": 6, "mention_end": 20, "dataset_mention": "CUHKSZ dataset"}, {"mentioned_in_paper": "82", "context_id": "185", "dataset_context": "Since CUHKSZ dataset is more challenging and the comparison results are more clear, after we will conduct experiments only on CUHKSZ dataset.", "mention_start": 125, "mention_end": 140, "dataset_mention": "CUHKSZ dataset"}, {"mentioned_in_paper": "83", "context_id": "7", "dataset_context": "We evaluate our approach on the Fine Grained 3D Car dataset with superior performance in shape and pose errors.", "mention_start": 28, "mention_end": 59, "dataset_mention": "the Fine Grained 3D Car dataset"}, {"mentioned_in_paper": "83", "context_id": "143", "dataset_context": "The experiments are carried out on the Fine Grained 3D Car dataset (FG3DCar) [21], since it is the only dataset with both landmark projection in the image and pose annotation for 3D objects.", "mention_start": 35, "mention_end": 66, "dataset_mention": "the Fine Grained 3D Car dataset"}, {"mentioned_in_paper": "83", "context_id": "155", "dataset_context": "To build the shape models, we learned a dictionary consisting of 10 basis shapes from the 3D models provided in the FG3DCar dataset.", "mention_start": 111, "mention_end": 131, "dataset_mention": "the FG3DCar dataset"}, {"mentioned_in_paper": "83", "context_id": "164", "dataset_context": "Since the selected discriminative landmarks are not identical to the landmarks provided in the FG3DCar dataset, we also compare the meanAPD on the landmarks provided in the dataset.", "mention_start": 91, "mention_end": 110, "dataset_mention": "the FG3DCar dataset"}, {"mentioned_in_paper": "84", "context_id": "38", "dataset_context": "To summarize, with ReforestTree, we contribute the following: 1) the first publicly available dataset of tropical agro-forestry containing both ground truth field data matched with high resolution RGB drone imagery at the individual tree level and 2) a methodology for reducing the current overestimation of forest carbon stock through deep learning and aerial imagery for carbon offsetting projects.", "mention_start": 61, "mention_end": 101, "dataset_mention": " 1) the first publicly available dataset"}, {"mentioned_in_paper": "84", "context_id": "64", "dataset_context": "There are several datasets for tree detection and classification from drone imagery such as the NEON dataset (Weinstein et al. 2020a), or the Swedish Forest Agency mainly from temperate forests from the US or Europe.", "mention_start": 92, "mention_end": 108, "dataset_mention": "the NEON dataset"}, {"mentioned_in_paper": "84", "context_id": "66", "dataset_context": "The ReforesTree dataset consists of six agro-forestry sites in the central coastal region of Ecuador.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The ReforesTree dataset"}, {"mentioned_in_paper": "84", "context_id": "80", "dataset_context": "The goal of this process is to have a machine learning ready dataset that consists of matched drone image of an individual tree with the trees labels, such as AGB value.", "mention_start": 36, "mention_end": 68, "dataset_mention": "a machine learning ready dataset"}, {"mentioned_in_paper": "84", "context_id": "81", "dataset_context": "Figure 3 : The raw data and data processing pipeline for the ReforesTree dataset, resulting in labels matched to bounding boxes per tree.", "mention_start": 56, "mention_end": 80, "dataset_mention": "the ReforesTree dataset"}, {"mentioned_in_paper": "84", "context_id": "99", "dataset_context": "With the emerging new biomass maps and forest stock estimation models, we used the ReforesTree dataset to benchmark these maps and compare with our baseline CNN model for AGB estimation.", "mention_start": 78, "mention_end": 102, "dataset_mention": "the ReforesTree dataset"}, {"mentioned_in_paper": "84", "context_id": "101", "dataset_context": "These are not encouraging results showing that these maps are far from being accurate enough to be used in remote sensing of forest carbon stock at a small scale, as is the case for the ReforesTree dataset.", "mention_start": 181, "mention_end": 205, "dataset_mention": "the ReforesTree dataset"}, {"mentioned_in_paper": "84", "context_id": "104", "dataset_context": "We introduce the ReforesTree dataset in hopes of encouraging the fellow machine learning community to take on the challenge of developing low-cost, scalable, trustworthy and accurate solutions for monitoring, verification and reporting of tropical reforestation inventory.", "mention_start": 13, "mention_end": 36, "dataset_mention": "the ReforesTree dataset"}, {"mentioned_in_paper": "84", "context_id": "105", "dataset_context": "We also present an outlined methodology for creating an annotated machine learning dataset from field data and drone imagery, and train a baseline CNN model for individual tree aboveground biomass estimation.", "mention_start": 53, "mention_end": 90, "dataset_mention": "an annotated machine learning dataset"}, {"mentioned_in_paper": "84", "context_id": "107", "dataset_context": "The ReforesTree dataset of field measurements and lowcost, high-resolution RGB drone imagery represents the trade-off for accuracy and data availability of remote sensing of forest carbon stock in tropical regions.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The ReforesTree dataset"}, {"mentioned_in_paper": "84", "context_id": "111", "dataset_context": "Future work will investigate ways to improve the methodology and reduce error in the machine learning ready dataset, and increase the explainability to have a trustworthy and transparent model.", "mention_start": 81, "mention_end": 115, "dataset_mention": "the machine learning ready dataset"}, {"mentioned_in_paper": "85", "context_id": "2", "dataset_context": "Accumulating this 671 GB dataset utilized 5,448 CPU core-years, 17.8 GPU-years, and 111.2 node-years.", "mention_start": 13, "mention_end": 32, "dataset_mention": "this 671 GB dataset"}, {"mentioned_in_paper": "85", "context_id": "12", "dataset_context": "Accumulating this 671 GB dataset utilized 5,448 CPU core-years, 17.8 GPU-years, and 111.2 node-years across four high-performance computing (HPC) systems and two datacenters.", "mention_start": 13, "mention_end": 32, "dataset_mention": "this 671 GB dataset"}, {"mentioned_in_paper": "85", "context_id": "42", "dataset_context": "1. We present BUTTER, and empirical deep learning dataset covering an average of 20 repetitions of 176 thousand distinct experiments totalling 3.5 million training runs and 13.1 billion epochs.", "mention_start": 21, "mention_end": 57, "dataset_mention": " and empirical deep learning dataset"}, {"mentioned_in_paper": "85", "context_id": "74", "dataset_context": "We hope that other researchers adopt and expand this framework to generate additional large empirical machine learning datasets to inform both theoretical and practical research in the field.", "mention_start": 75, "mention_end": 127, "dataset_mention": "additional large empirical machine learning datasets"}, {"mentioned_in_paper": "85", "context_id": "78", "dataset_context": "The entire 620 GB run dataset is available from the Open Energy Data Initiative (OEDI) data repository as partitioned parquet files [27].", "mention_start": 0, "mention_end": 29, "dataset_mention": "The entire 620 GB run dataset"}, {"mentioned_in_paper": "85", "context_id": "103", "dataset_context": "We presented BUTTER, a empirical deep learning dataset to further our understanding of the deep learning phenomenon, and hopefully to guide theoretical research and inform practical application of neural networks.", "mention_start": 20, "mention_end": 54, "dataset_mention": " a empirical deep learning dataset"}, {"mentioned_in_paper": "85", "context_id": "104", "dataset_context": "It is critical to expand empirical study of machine learning beyond this dataset to different types and aspects of neural network architectures including sparse, convolutional, recurrent, and transformer networks.", "mention_start": 44, "mention_end": 80, "dataset_mention": "machine learning beyond this dataset"}, {"mentioned_in_paper": "85", "context_id": "111", "dataset_context": "However, by providing this dataset as a public and free resource we hope to reduce future energy costs that would have otherwise been incurred by others in recreating these results.", "mention_start": 11, "mention_end": 34, "dataset_mention": "providing this dataset"}, {"mentioned_in_paper": "86", "context_id": "13", "dataset_context": "For example, a single image from the Cityscapes dataset required about 1.5 hours to annotate [9].", "mention_start": 32, "mention_end": 55, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "86", "context_id": "16", "dataset_context": "However, such an assumption may not be practical, where either the source dataset cannot be shared to maintain privacy or the source dataset may take up a lot of storage space.", "mention_start": 55, "mention_end": 81, "dataset_mention": "either the source dataset"}, {"mentioned_in_paper": "86", "context_id": "16", "dataset_context": "However, such an assumption may not be practical, where either the source dataset cannot be shared to maintain privacy or the source dataset may take up a lot of storage space.", "mention_start": 121, "mention_end": 140, "dataset_mention": "the source dataset"}, {"mentioned_in_paper": "86", "context_id": "19", "dataset_context": "For example, storing the GTA5 dataset [40] used as source in most UDA methods, needs 57 GB of storage whereas a model trained using the same data requires only 0.17 GB.", "mention_start": 20, "mention_end": 37, "dataset_mention": "the GTA5 dataset"}, {"mentioned_in_paper": "86", "context_id": "39", "dataset_context": "Further, to demonstrate the generalisability of our consistency based approach, we apply it in the fully test-time adaptation setting [55], where it outperforms loss functions proposed by recent methods dedicated for fully test time adaptation [35, 55] on all the above three dataset settings.", "mention_start": 255, "mention_end": 283, "dataset_mention": "all the above three dataset"}, {"mentioned_in_paper": "86", "context_id": "42", "dataset_context": "\u2022 Empirical analysis shows that our method establishes the new state-of-the-art for source-free adaptation, and is only a few points away from the state-of-the-art which uses the entire labeled source dataset to adapt.", "mention_start": 174, "mention_end": 208, "dataset_mention": "the entire labeled source dataset"}, {"mentioned_in_paper": "86", "context_id": "54", "dataset_context": "Compared to these approaches, we assume no access to the source dataset, which is a more realistic setting but makes the task much more challenging.", "mention_start": 52, "mention_end": 71, "dataset_mention": "the source dataset"}, {"mentioned_in_paper": "86", "context_id": "88", "dataset_context": "Additionally, we impose consistency using certain transforms, which gives the model an opportunity to improve beyond learning from just the pseudo-labeled dataset.", "mention_start": 130, "mention_end": 162, "dataset_mention": "just the pseudo-labeled dataset"}, {"mentioned_in_paper": "86", "context_id": "93", "dataset_context": "We label the unlabeled target dataset D t using the pseudo-labeling function to obtain a new dataset Dt = {X i , \u0176i } n i=1 , where \u0176i = PL(M s (X i )).", "mention_start": 80, "mention_end": 100, "dataset_mention": "obtain a new dataset"}, {"mentioned_in_paper": "86", "context_id": "101", "dataset_context": "This observation motivates us to use this consistency as constraint while learning from the pseudo-labeled dataset Dt , which acts as an additional signal to improve beyond pseudo-labeling.", "mention_start": 88, "mention_end": 114, "dataset_mention": "the pseudo-labeled dataset"}, {"mentioned_in_paper": "86", "context_id": "150", "dataset_context": "The outdoor scene dataset has 19 categories, whereas the indoor has 13 categories.", "mention_start": 0, "mention_end": 25, "dataset_mention": "The outdoor scene dataset"}, {"mentioned_in_paper": "88", "context_id": "149", "dataset_context": "Table 1 presents the datasets characteristics where the unit of snapshot generation is one day.", "mention_start": 0, "mention_end": 29, "dataset_mention": "Table 1 presents the datasets"}, {"mentioned_in_paper": "89", "context_id": "8", "dataset_context": "Here we introduce WHOI-Plankton: a large scale, fine-grained visual recognition dataset for plankton classification, which comprises over 3.4 million expert-labeled images across 70 classes.", "mention_start": 47, "mention_end": 87, "dataset_mention": " fine-grained visual recognition dataset"}, {"mentioned_in_paper": "89", "context_id": "57", "dataset_context": "These results provide a starting point for developing classification methods on this unique data set.", "mention_start": 80, "mention_end": 100, "dataset_mention": "this unique data set"}, {"mentioned_in_paper": "90", "context_id": "92", "dataset_context": "\u2022 D 1,2 : For each patient, images of T2 and T1-weighted are already aligned because both are acquired through one scan and hence, we constructed a dataset which have these two different modalities within each image pair.", "mention_start": 130, "mention_end": 155, "dataset_mention": " we constructed a dataset"}, {"mentioned_in_paper": "90", "context_id": "102", "dataset_context": "We evaluate the trained models on a MR brain image dataset with ground truth segmentation masks which were prepared from our hospital database.", "mention_start": 34, "mention_end": 58, "dataset_mention": "a MR brain image dataset"}, {"mentioned_in_paper": "90", "context_id": "134", "dataset_context": "This self-supervised learning method would be preferable because during the dataset preparation, it only requires one image for each training sample instead of two aligned images of different modalities.", "mention_start": 65, "mention_end": 83, "dataset_mention": "during the dataset"}, {"mentioned_in_paper": "93", "context_id": "71", "dataset_context": "Given that I have set the goal to compress general but simple data sets, the question arises whether the algorithm that performs that task can expected to be complex or rather simple as well.", "mention_start": 55, "mention_end": 71, "dataset_mention": "simple data sets"}, {"mentioned_in_paper": "95", "context_id": "116", "dataset_context": "In this section we describe and evaluate our framework on the KITTI dataset.", "mention_start": 58, "mention_end": 75, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "95", "context_id": "151", "dataset_context": "To show generalisation performance, in Figure 2, we additionally qualitatively evaluate the same depth network on the Cityscapes dataset [? ].", "mention_start": 113, "mention_end": 136, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "96", "context_id": "9", "dataset_context": "Our experiments are targeted on videos of kitchen activities from the Epic-Kitchens dataset.", "mention_start": 66, "mention_end": 91, "dataset_mention": "the Epic-Kitchens dataset"}, {"mentioned_in_paper": "96", "context_id": "27", "dataset_context": "The scope of this work is to assess up to what point existing object detection Fig. 1. Results of a hand detector from two consecutive frames on the Epic-Kitchens dataset [12].", "mention_start": 145, "mention_end": 170, "dataset_mention": "the Epic-Kitchens dataset"}, {"mentioned_in_paper": "96", "context_id": "72", "dataset_context": "The EgoHands dataset together with an egocentric hand detection and segmentation pipeline are developed before inference of activities.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The EgoHands dataset"}, {"mentioned_in_paper": "96", "context_id": "74", "dataset_context": "Inspired by this, we augment the EgoHands hand dataset with additional samples for our hand detector and introduce tracking into the action recognition pipeline to improve the detection output.", "mention_start": 28, "mention_end": 54, "dataset_mention": "the EgoHands hand dataset"}, {"mentioned_in_paper": "96", "context_id": "83", "dataset_context": "In this section we describe the process to produce a hand track dataset from the raw frames of Epic-Kitchens [12].", "mention_start": 51, "mention_end": 71, "dataset_mention": "a hand track dataset"}, {"mentioned_in_paper": "96", "context_id": "86", "dataset_context": "The Epic-Kitchens dataset comprises a set of 432 egocentric videos recorded by 32 participants in their kitchens at 60fps with a head mounted camera.", "mention_start": 0, "mention_end": 25, "dataset_mention": "The Epic-Kitchens dataset"}, {"mentioned_in_paper": "96", "context_id": "96", "dataset_context": "In order to acquire hand regions from Epic-Kitchens we train a hand detector with YOLOv3 [16] on the combination of a collection of egocentric hand datasets.", "mention_start": 132, "mention_end": 156, "dataset_mention": "egocentric hand datasets"}, {"mentioned_in_paper": "96", "context_id": "97", "dataset_context": "1) Dataset collection: We utilize hand annotations from existing egocentric datasets, namely EgoHands [25], EGTEA Gaze+ [18], [27], CMU EDSH [22] and THU-READ [28].", "mention_start": 0, "mention_end": 10, "dataset_mention": "1) Dataset"}, {"mentioned_in_paper": "96", "context_id": "97", "dataset_context": "1) Dataset collection: We utilize hand annotations from existing egocentric datasets, namely EgoHands [25], EGTEA Gaze+ [18], [27], CMU EDSH [22] and THU-READ [28].", "mention_start": 55, "mention_end": 84, "dataset_mention": "existing egocentric datasets"}, {"mentioned_in_paper": "96", "context_id": "100", "dataset_context": "We manually annotate 11,683 such frames from the Intel Egocentric Object Recognition Dataset [29].", "mention_start": 45, "mention_end": 92, "dataset_mention": "the Intel Egocentric Object Recognition Dataset"}, {"mentioned_in_paper": "96", "context_id": "101", "dataset_context": "Information about the amount of hand annotations and the size of each train and test split is detailed in Table I. 2) Training: We perform various experiments to train the hand detectors to determine the optimal dataset combination that supports generalization, since our eventual task is to apply the detector on an unseen dataset for extraction.", "mention_start": 199, "mention_end": 219, "dataset_mention": "the optimal dataset"}, {"mentioned_in_paper": "96", "context_id": "102", "dataset_context": "We train a detector for each available hand dataset (except IEORD) and one for the combined train sets.", "mention_start": 24, "mention_end": 51, "dataset_mention": "each available hand dataset"}, {"mentioned_in_paper": "96", "context_id": "110", "dataset_context": "3) Detection on Epic-Kitchens: We apply the combined hand detector on the Epic-Kitchens dataset to extract hand instances.", "mention_start": 69, "mention_end": 95, "dataset_mention": "the Epic-Kitchens dataset"}, {"mentioned_in_paper": "96", "context_id": "161", "dataset_context": "Our learning scheme tar-gets the 125 verb classes of the Epic-Kitchens dataset.", "mention_start": 53, "mention_end": 78, "dataset_mention": "the Epic-Kitchens dataset"}, {"mentioned_in_paper": "96", "context_id": "224", "dataset_context": "We focus on actions performed in kitchen environments, utilizing the recent Epic-Kitchens [12] egocentric video dataset.", "mention_start": 64, "mention_end": 119, "dataset_mention": "the recent Epic-Kitchens [12] egocentric video dataset"}, {"mentioned_in_paper": "98", "context_id": "120", "dataset_context": "ScanNet We also evaluate our method on a recently proposed indoor dataset, ScanNet [4], which has more than 1600 scenes.", "mention_start": 50, "mention_end": 73, "dataset_mention": "proposed indoor dataset"}, {"mentioned_in_paper": "98", "context_id": "121", "dataset_context": "Its official test split contains 100 scenes, and we uniformly select 2167 images from them for crossdataset evaluation.", "mention_start": 94, "mention_end": 107, "dataset_mention": "crossdataset"}, {"mentioned_in_paper": "98", "context_id": "123", "dataset_context": "Depth estimation accuracy We compare our method with other state-of-the-art methods on NYUD-V2 dataset.", "mention_start": 87, "mention_end": 102, "dataset_mention": "NYUD-V2 dataset"}, {"mentioned_in_paper": "98", "context_id": "125", "dataset_context": "Moreover, to further evaluate the generalization of our method, we compare our method with some strong SO-TAs on unseen ScanNet dataset.", "mention_start": 112, "mention_end": 135, "dataset_mention": "unseen ScanNet dataset"}, {"mentioned_in_paper": "100", "context_id": "45", "dataset_context": "The rest of the paper is structured as follows: In Section II we cover the related work on sign language recognition, sign spotting and sign language datasets.", "mention_start": 117, "mention_end": 158, "dataset_mention": " sign spotting and sign language datasets"}, {"mentioned_in_paper": "100", "context_id": "51", "dataset_context": "Our work is related to several topics in the literature including the curation of sign language datasets as well as sign language recognition and sign spotting.", "mention_start": 82, "mention_end": 104, "dataset_mention": "sign language datasets"}, {"mentioned_in_paper": "100", "context_id": "53", "dataset_context": "Sign language datasets: There is no universal sign language.", "mention_start": 0, "mention_end": 22, "dataset_mention": "Sign language datasets"}, {"mentioned_in_paper": "100", "context_id": "59", "dataset_context": "A large BSL dataset that is available and partially annotated is the BSLCORPUS captured by Schembri et al. [39].", "mention_start": 0, "mention_end": 19, "dataset_mention": "A large BSL dataset"}, {"mentioned_in_paper": "100", "context_id": "130", "dataset_context": "First, we describe the sign video dataset used and then the pre-processing stages.", "mention_start": 18, "mention_end": 41, "dataset_mention": "the sign video dataset"}, {"mentioned_in_paper": "100", "context_id": "153", "dataset_context": "To do so we use the popular German Sign Language dataset: PHOENIX14T [7], which covers unconstrained DGS sign from 9 different signers with a vocabulary of 1066 different signs and translations into spoken German with a vocabulary of 2887 different words.", "mention_start": 16, "mention_end": 56, "dataset_mention": "the popular German Sign Language dataset"}, {"mentioned_in_paper": "100", "context_id": "184", "dataset_context": "Finally, we evaluate our model on other sign language datasets to show the generalization ability of Sign-Lookup amongst different sign languages.", "mention_start": 33, "mention_end": 62, "dataset_mention": "other sign language datasets"}, {"mentioned_in_paper": "100", "context_id": "229", "dataset_context": "Their provided pretrained model is trained jointly on BSL-1K [1] and BSLDict datasets.", "mention_start": 54, "mention_end": 85, "dataset_mention": "BSL-1K [1] and BSLDict datasets"}, {"mentioned_in_paper": "100", "context_id": "230", "dataset_context": "However, neither the BSL-1K dataset nor the training code are accessible publicly.", "mention_start": 16, "mention_end": 35, "dataset_mention": "the BSL-1K dataset"}, {"mentioned_in_paper": "100", "context_id": "231", "dataset_context": "Hence, we apply their pretrained model on the BSLCORPUS using the common dictionary between BSLDict and SignBank, while also noting the possibility of getting improved results if trained on the applied datasets.", "mention_start": 189, "mention_end": 210, "dataset_mention": "the applied datasets"}, {"mentioned_in_paper": "100", "context_id": "235", "dataset_context": "To evaluate the generalization of our model to different datasets, we apply Sign-Lookup to the German Sign Language dataset: PHOENIX14T.", "mention_start": 90, "mention_end": 123, "dataset_mention": "the German Sign Language dataset"}, {"mentioned_in_paper": "100", "context_id": "240", "dataset_context": "The performance of our model applied to the PHOENIX14T dataset is shown in Table VII.", "mention_start": 40, "mention_end": 62, "dataset_mention": "the PHOENIX14T dataset"}, {"mentioned_in_paper": "100", "context_id": "241", "dataset_context": "From the table, we can see that it outperforms the BSLCORPUS dataset tests.", "mention_start": 26, "mention_end": 68, "dataset_mention": "that it outperforms the BSLCORPUS dataset"}, {"mentioned_in_paper": "100", "context_id": "253", "dataset_context": "In the second example, we apply our model to a DGS Pheonix2014T example, demonstrating generalization to sign languages and dataset.", "mention_start": 104, "mention_end": 131, "dataset_mention": "sign languages and dataset"}, {"mentioned_in_paper": "100", "context_id": "257", "dataset_context": "By also applying Sign-Lookup to a popular DGS dataset, we demonstrate its capability to generalise to other sign languages.", "mention_start": 32, "mention_end": 53, "dataset_mention": "a popular DGS dataset"}, {"mentioned_in_paper": "101", "context_id": "2", "dataset_context": "To that end, we carefully reorganize two publicly available action-related datasets with step-procedure-task structure.", "mention_start": 36, "mention_end": 83, "dataset_mention": "two publicly available action-related datasets"}, {"mentioned_in_paper": "101", "context_id": "3", "dataset_context": "To fully investigate the effectiveness of any method, we collect a scripted video dataset enumerating all kinds of step-level transformations in chemical experiments.", "mention_start": 64, "mention_end": 89, "dataset_mention": "a scripted video dataset"}, {"mentioned_in_paper": "101", "context_id": "22", "dataset_context": "Thus, an appropriate dataset is crucial to perform this task well.", "mention_start": 5, "mention_end": 28, "dataset_mention": " an appropriate dataset"}, {"mentioned_in_paper": "101", "context_id": "23", "dataset_context": "However, existing trimmed video datasets such as UCF101 [83], Kinetics [10], and Moments in Time [60] etc. are leveraged to carry out single-label action recognition.", "mention_start": 8, "mention_end": 40, "dataset_mention": " existing trimmed video datasets"}, {"mentioned_in_paper": "101", "context_id": "24", "dataset_context": "On the other hand, untrimmed video datasets like EPIC-KITCHENS [15], Breakfast [?], Hollywood Extended [7], ActivityNet [8] provide videos composed by multiple subactions and the corresponding step annotations, but they do not collect videos that especially performs similar or identical procedures.", "mention_start": 18, "mention_end": 43, "dataset_mention": " untrimmed video datasets"}, {"mentioned_in_paper": "101", "context_id": "30", "dataset_context": "Apart from that, we introduce a scripted filming dataset performing chemical procedures, where it includes all kinds of step-level transformations such as deletions, additions, and order exchanges.", "mention_start": 29, "mention_end": 56, "dataset_mention": "a scripted filming dataset"}, {"mentioned_in_paper": "101", "context_id": "31", "dataset_context": "Thus, the effectiveness of any algorithm can be well justified by this newly proposed dataset.", "mention_start": 65, "mention_end": 93, "dataset_mention": "this newly proposed dataset"}, {"mentioned_in_paper": "101", "context_id": "42", "dataset_context": "ii) Benchmark: We rearrange two unscripted video datasets with significant diversity and propose a new scripted dataset with multiple step-level transformations to support this task.", "mention_start": 27, "mention_end": 57, "dataset_mention": "two unscripted video datasets"}, {"mentioned_in_paper": "101", "context_id": "42", "dataset_context": "ii) Benchmark: We rearrange two unscripted video datasets with significant diversity and propose a new scripted dataset with multiple step-level transformations to support this task.", "mention_start": 96, "mention_end": 119, "dataset_mention": "a new scripted dataset"}, {"mentioned_in_paper": "101", "context_id": "48", "dataset_context": "In addition, ActivityNet [8] and Kinetics [41] collect videos from YouTube and builds large-scale action recognition datasets.", "mention_start": 85, "mention_end": 125, "dataset_mention": "large-scale action recognition datasets"}, {"mentioned_in_paper": "101", "context_id": "50", "dataset_context": "EPIC-KITCHENS dataset [15] collects the human actions such as washing glass or cutting bell pepper in kitchen scenes and targets at the firstperson perspective to reflect people's goals and motivations.", "mention_start": 0, "mention_end": 21, "dataset_mention": "EPIC-KITCHENS dataset"}, {"mentioned_in_paper": "101", "context_id": "57", "dataset_context": "Thus, we collect a novel scripted dataset, Chemical Sequence Verification, enumerating all kinds of procedures in the same task, which will be introduced later.", "mention_start": 16, "mention_end": 41, "dataset_mention": "a novel scripted dataset"}, {"mentioned_in_paper": "101", "context_id": "61", "dataset_context": "As shown in Figure 1 (c), each dataset used in this paper contains videos completing various tasks, e.g., the original COIN dataset contains 180 tasks common in daily life.", "mention_start": 105, "mention_end": 131, "dataset_mention": " the original COIN dataset"}, {"mentioned_in_paper": "101", "context_id": "65", "dataset_context": "COIN [86] is a comprehensive instructional video dataset that contains 180 tasks such as 'Replace the door knob', 'Change the car tire', and 'Install a ceiling fan'.", "mention_start": 13, "mention_end": 56, "dataset_mention": "a comprehensive instructional video dataset"}, {"mentioned_in_paper": "101", "context_id": "71", "dataset_context": "Diving48 [51] dataset records diving competition videos with 48 kinds of diving procedures standardized by the international federation FINA, which consists of around 18,000 trimmed videos.", "mention_start": 0, "mention_end": 21, "dataset_mention": "Diving48 [51] dataset"}, {"mentioned_in_paper": "101", "context_id": "80", "dataset_context": "In a word, the CSV dataset includes 14 tasks, and each consists of 5 procedures.", "mention_start": 10, "mention_end": 26, "dataset_mention": " the CSV dataset"}, {"mentioned_in_paper": "101", "context_id": "88", "dataset_context": "For a certain dataset D = {(V i , S i )} n i=1 , a set of n video clips V are given with corresponding procedure annotations S.", "mention_start": 4, "mention_end": 21, "dataset_mention": "a certain dataset"}, {"mentioned_in_paper": "101", "context_id": "169", "dataset_context": "The experiments are conducted on the testing set of the CSV dataset if not specially stated.", "mention_start": 52, "mention_end": 67, "dataset_mention": "the CSV dataset"}, {"mentioned_in_paper": "101", "context_id": "173", "dataset_context": "We also visualize the 128-d embedding vectors extracted by different models via PCA in Figure 4. Concretely, we select the first three procedures in the first task of the CSV dataset.", "mention_start": 166, "mention_end": 182, "dataset_mention": "the CSV dataset"}, {"mentioned_in_paper": "101", "context_id": "192", "dataset_context": "Though we have introduced two reorganized datasets and collected one scripted dataset for this new task, it still suffers from data insufficiency leading to an unsatisfactory performance on Video Swin Transformer.", "mention_start": 26, "mention_end": 50, "dataset_mention": "two reorganized datasets"}, {"mentioned_in_paper": "101", "context_id": "192", "dataset_context": "Though we have introduced two reorganized datasets and collected one scripted dataset for this new task, it still suffers from data insufficiency leading to an unsatisfactory performance on Video Swin Transformer.", "mention_start": 65, "mention_end": 85, "dataset_mention": "one scripted dataset"}, {"mentioned_in_paper": "101", "context_id": "196", "dataset_context": "To that end, we reorganize two publicly available actionrelated datasets with step-procedure-task structure and collect an egocentric dataset asking volunteers to perform various scripted procedures.", "mention_start": 26, "mention_end": 72, "dataset_mention": "two publicly available actionrelated datasets"}, {"mentioned_in_paper": "101", "context_id": "196", "dataset_context": "To that end, we reorganize two publicly available actionrelated datasets with step-procedure-task structure and collect an egocentric dataset asking volunteers to perform various scripted procedures.", "mention_start": 119, "mention_end": 141, "dataset_mention": "an egocentric dataset"}, {"mentioned_in_paper": "101", "context_id": "201", "dataset_context": "The first section contains some complementary information of our proposed CSV dataset, e.g., data gathering, annotations, and statistics information.", "mention_start": 61, "mention_end": 85, "dataset_mention": "our proposed CSV dataset"}, {"mentioned_in_paper": "101", "context_id": "204", "dataset_context": "The existing action datasets can hardly support our task due to the following reasons: i) some datasets focus on single actions and don't provide procedure videos; ii) some other datasets which contain procedure videos target other tasks such as action segmentation and action localization, i.e., they focus on the understanding of a single video rather than the verification of two videos, which leads to the lack of videos for similar procedures.", "mention_start": 0, "mention_end": 28, "dataset_mention": "The existing action datasets"}, {"mentioned_in_paper": "101", "context_id": "204", "dataset_context": "The existing action datasets can hardly support our task due to the following reasons: i) some datasets focus on single actions and don't provide procedure videos; ii) some other datasets which contain procedure videos target other tasks such as action segmentation and action localization, i.e., they focus on the understanding of a single video rather than the verification of two videos, which leads to the lack of videos for similar procedures.", "mention_start": 86, "mention_end": 103, "dataset_mention": " i) some datasets"}, {"mentioned_in_paper": "101", "context_id": "204", "dataset_context": "The existing action datasets can hardly support our task due to the following reasons: i) some datasets focus on single actions and don't provide procedure videos; ii) some other datasets which contain procedure videos target other tasks such as action segmentation and action localization, i.e., they focus on the understanding of a single video rather than the verification of two videos, which leads to the lack of videos for similar procedures.", "mention_start": 163, "mention_end": 187, "dataset_mention": " ii) some other datasets"}, {"mentioned_in_paper": "101", "context_id": "206", "dataset_context": "For the above reasons, we collect a new action verification dataset to support our proposed task.", "mention_start": 33, "mention_end": 67, "dataset_mention": "a new action verification dataset"}, {"mentioned_in_paper": "101", "context_id": "235", "dataset_context": "We can find Figure 12 has different procedure annotation from Figure 13 and 14, since the original COIN dataset [86] has temporal annotation for each step but Div-ing48 [51] and CSV doesn't.", "mention_start": 85, "mention_end": 111, "dataset_mention": "the original COIN dataset"}, {"mentioned_in_paper": "102", "context_id": "47", "dataset_context": "Finally, we verify the performance of our method on the PASCAL VOC dataset.", "mention_start": 51, "mention_end": 74, "dataset_mention": "the PASCAL VOC dataset"}, {"mentioned_in_paper": "102", "context_id": "121", "dataset_context": "Following previous work [7, 8, 9, 16, 17, 20], we evaluate our instance segmentation performance on the PASCAL VOC dataset [12].", "mention_start": 99, "mention_end": 122, "dataset_mention": "the PASCAL VOC dataset"}, {"mentioned_in_paper": "102", "context_id": "159", "dataset_context": "We achieve the best results, 67.3% and 54.4%, on the PASCAL VOC dataset, which are nearly 2% higher than those of FCIS.", "mention_start": 48, "mention_end": 71, "dataset_mention": "the PASCAL VOC dataset"}, {"mentioned_in_paper": "104", "context_id": "11", "dataset_context": "In the experimental section, thirty-three popular public datasets are used for verification, and over ten representative algorithms are used for comparison.", "mention_start": 28, "mention_end": 65, "dataset_mention": " thirty-three popular public datasets"}, {"mentioned_in_paper": "104", "context_id": "174", "dataset_context": "The total accuracy (Acc) can be used to test the classification performance of a model once the dataset has reached a balanced state.", "mention_start": 79, "mention_end": 103, "dataset_mention": "a model once the dataset"}, {"mentioned_in_paper": "104", "context_id": "202", "dataset_context": "Six datasets are chosen, including DM, Blood-transfusion, Glass6, Yeast-0-2-5-7-9-vs-3-6-8, Ecoli4 and Yeast5; and they represent two types of high-and low-IR datasets.", "mention_start": 142, "mention_end": 167, "dataset_mention": "high-and low-IR datasets"}, {"mentioned_in_paper": "104", "context_id": "206", "dataset_context": "After the datasets reach balanced, the performance of all three metrics has improved significantly.", "mention_start": 0, "mention_end": 18, "dataset_mention": "After the datasets"}, {"mentioned_in_paper": "104", "context_id": "213", "dataset_context": "The results also show that the proposed algorithm works better on datasets with a higher IR such as the Yeast-1-2-8-9-vs-7, Poker-9-vs-7 and Yeast4 datasets compared to other datasets.", "mention_start": 123, "mention_end": 156, "dataset_mention": " Poker-9-vs-7 and Yeast4 datasets"}, {"mentioned_in_paper": "105", "context_id": "105", "dataset_context": "Our model was trained using a subset of the MIMIC dataset [25], which contains the electronic health records of 46,521 patients.", "mention_start": 40, "mention_end": 57, "dataset_mention": "the MIMIC dataset"}, {"mentioned_in_paper": "105", "context_id": "188", "dataset_context": "More specifically, medical records for patients with cardiovascular disease and at least one hospitalization were chosen from the MIMIC dataset, and their medical records were transformed into one or more event sequences based on a 6-month time window.", "mention_start": 125, "mention_end": 143, "dataset_mention": "the MIMIC dataset"}, {"mentioned_in_paper": "105", "context_id": "265", "dataset_context": "Both E 1,2 mentioned that the quality of the electronic health records collected in Chinese hospitals were much worse than that of the MIMIC dataset.", "mention_start": 131, "mention_end": 148, "dataset_mention": "the MIMIC dataset"}, {"mentioned_in_paper": "106", "context_id": "142", "dataset_context": "We present a first-person interaction dataset consisting of 42 two-minute sequences from one-on-one interactions between 10 individuals.", "mention_start": 11, "mention_end": 45, "dataset_mention": "a first-person interaction dataset"}, {"mentioned_in_paper": "106", "context_id": "166", "dataset_context": "Existing pose detection and tracking datasets (e.g., [5, 27]) are captured in the third-person viewpoint.", "mention_start": 0, "mention_end": 45, "dataset_mention": "Existing pose detection and tracking datasets"}, {"mentioned_in_paper": "106", "context_id": "167", "dataset_context": "Existing egocentric datasets are either limited to visible hands and arms [32, 44], contain only single-person sequences [26, 5, 27], consist of synthetic test data [69], or lack body-pose joint labels [68].", "mention_start": 0, "mention_end": 28, "dataset_mention": "Existing egocentric datasets"}, {"mentioned_in_paper": "106", "context_id": "190", "dataset_context": "We use the same network structure presented in the baseline, but retrain it on our egocentric dataset, altering the output space for 3D joints.", "mention_start": 78, "mention_end": 101, "dataset_mention": "our egocentric dataset"}, {"mentioned_in_paper": "108", "context_id": "68", "dataset_context": "CIFAR-10 [10] dataset consists of 60,000 images of 32 \u00d7 32 pixels for 10 categories, of which 50,000 are used for training and 10,000 are used for testing.", "mention_start": 0, "mention_end": 21, "dataset_mention": "CIFAR-10 [10] dataset"}, {"mentioned_in_paper": "108", "context_id": "69", "dataset_context": "The Street View House Numbers (SVHN) [11] dataset is real house numbers captured by Google Street View.", "mention_start": 0, "mention_end": 49, "dataset_mention": "The Street View House Numbers (SVHN) [11] dataset"}, {"mentioned_in_paper": "111", "context_id": "31", "dataset_context": "\u2022 An extensive evaluation of the behaviour of the modelled uncertainty on the training domain as well as on two additional well established datasets.", "mention_start": 108, "mention_end": 148, "dataset_mention": "two additional well established datasets"}, {"mentioned_in_paper": "111", "context_id": "117", "dataset_context": "The Sceneflow dataset contains With increasing number of predictions T used for estimating a disparity map di (c.f.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The Sceneflow dataset"}, {"mentioned_in_paper": "111", "context_id": "122", "dataset_context": "On the other hand, the KITTI dataset contains real image pairs, which were captured using vehicle mounted stereo camera set-ups and provides LIDAR based ground truth disparity maps with disparities for 30 % of the pixels.", "mention_start": 18, "mention_end": 36, "dataset_mention": " the KITTI dataset"}, {"mentioned_in_paper": "111", "context_id": "124", "dataset_context": "Finally, the Middlebury dataset contains 15 image pairs showing various indoor scenes captured with a static stereo set-up and providing dense ground truth disparity maps based on structured light.", "mention_start": 8, "mention_end": 31, "dataset_mention": " the Middlebury dataset"}, {"mentioned_in_paper": "111", "context_id": "125", "dataset_context": "Due to hardware limitations, the images of the Middlebury dataset are processed at one quarter of the original resolution, within this evaluation.", "mention_start": 42, "mention_end": 65, "dataset_mention": "the Middlebury dataset"}, {"mentioned_in_paper": "111", "context_id": "126", "dataset_context": "To train the proposed probabilistic neural network, the stereo image pairs contained in the Sceneflow dataset are used.", "mention_start": 87, "mention_end": 109, "dataset_mention": "the Sceneflow dataset"}, {"mentioned_in_paper": "111", "context_id": "132", "dataset_context": "Analysing the results presented in Table 1, it can be seen that our probabilistic approach estimates disparity maps for the Sceneflow dataset with an accuracy comparable to the one of the original approach.", "mention_start": 119, "mention_end": 141, "dataset_mention": "the Sceneflow dataset"}, {"mentioned_in_paper": "111", "context_id": "134", "dataset_context": "A similar behaviour can be observed on the Middlebury dataset.", "mention_start": 39, "mention_end": 61, "dataset_mention": "the Middlebury dataset"}, {"mentioned_in_paper": "111", "context_id": "136", "dataset_context": "From the Sceneflow and the KITTI dataset, 100 random image pairs and from the Middlebury dataset, all 15 training pairs are used for evaluation.", "mention_start": 5, "mention_end": 40, "dataset_mention": "the Sceneflow and the KITTI dataset"}, {"mentioned_in_paper": "111", "context_id": "136", "dataset_context": "From the Sceneflow and the KITTI dataset, 100 random image pairs and from the Middlebury dataset, all 15 training pairs are used for evaluation.", "mention_start": 73, "mention_end": 96, "dataset_mention": "the Middlebury dataset"}, {"mentioned_in_paper": "111", "context_id": "144", "dataset_context": "On the KITTI dataset, the percentage of bad pixels as well as the MAE and the RMSE increase significantly for all three methods compared to the Sceneflow dataset.", "mention_start": 3, "mention_end": 20, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "111", "context_id": "144", "dataset_context": "On the KITTI dataset, the percentage of bad pixels as well as the MAE and the RMSE increase significantly for all three methods compared to the Sceneflow dataset.", "mention_start": 139, "mention_end": 161, "dataset_mention": "the Sceneflow dataset"}, {"mentioned_in_paper": "111", "context_id": "148", "dataset_context": "Since the original GC-Net is less sensitive to the domain gap between the different analysed datasets, the modifications made on the architecture and the loss function to not only obtain disparity but also aleatoric uncertainty information, seem to result in an over-fitting effect on the training domain.", "mention_start": 70, "mention_end": 101, "dataset_mention": "the different analysed datasets"}, {"mentioned_in_paper": "111", "context_id": "163", "dataset_context": "As can be seen in Table 1, the epistemic uncertainty is relatively small for the Sceneflow dataset, which was also used for training the network.", "mention_start": 76, "mention_end": 98, "dataset_mention": "the Sceneflow dataset"}, {"mentioned_in_paper": "111", "context_id": "164", "dataset_context": "On the Middlebury dataset the epistemic uncertainty is a bit higher, indicating that the contained image pairs show scenes which slightly differ from the training set.", "mention_start": 3, "mention_end": 25, "dataset_mention": "the Middlebury dataset"}, {"mentioned_in_paper": "111", "context_id": "165", "dataset_context": "For the KITTI dataset, on the other hand, the epistemic uncertainty is significantly higher, indicating that the characteristics of this dataset differs greatly from those of the training samples.", "mention_start": 4, "mention_end": 21, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "111", "context_id": "166", "dataset_context": "While the Sceneflow dataset contains images showing synthetic scenes, the Middlebury images show indoor environments with controlled external influences and the KITTI dataset shows road scenes with varying external influences.", "mention_start": 6, "mention_end": 27, "dataset_mention": "the Sceneflow dataset"}, {"mentioned_in_paper": "111", "context_id": "166", "dataset_context": "While the Sceneflow dataset contains images showing synthetic scenes, the Middlebury images show indoor environments with controlled external influences and the KITTI dataset shows road scenes with varying external influences.", "mention_start": 121, "mention_end": 174, "dataset_mention": "controlled external influences and the KITTI dataset"}, {"mentioned_in_paper": "111", "context_id": "189", "dataset_context": "From left to right, one example from the Sceneflow FlyingThings3D, the Middlebury v3 and the KITTI 2015 dataset is shown.", "mention_start": 66, "mention_end": 111, "dataset_mention": " the Middlebury v3 and the KITTI 2015 dataset"}, {"mentioned_in_paper": "111", "context_id": "193", "dataset_context": "The example of the KITTI dataset in particular demonstrates that epistemic uncertainty is crucial to identify samples for which the learned model is unable to make a correct prediction due to a domain gap.", "mention_start": 15, "mention_end": 32, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "112", "context_id": "119", "dataset_context": "Therefore, we can take the advantage of this type of video sequences (as the dataset described in the following) and collect training data for learning both the expression and identity representations in an unsupervised manner.", "mention_start": 52, "mention_end": 84, "dataset_mention": "video sequences (as the dataset"}, {"mentioned_in_paper": "112", "context_id": "121", "dataset_context": "The proposed model is trained on the combination of VoxCeleb1 [29] and VoxCeleb2 [5] datasets, in which both datasets are built upon videos of interviews.", "mention_start": 52, "mention_end": 93, "dataset_mention": "VoxCeleb1 [29] and VoxCeleb2 [5] datasets"}, {"mentioned_in_paper": "112", "context_id": "124", "dataset_context": "We adopted VoxCeleb2 test dataset for visualizing the intermediate results of our disentanglement process.", "mention_start": 11, "mention_end": 33, "dataset_mention": "VoxCeleb2 test dataset"}, {"mentioned_in_paper": "112", "context_id": "139", "dataset_context": "We directly adopt the models officially released by their authors (which are all pretrained on the Voxceleb dataset) for experimenting the downstream tasks of expression classification and head pose regression.", "mention_start": 95, "mention_end": 115, "dataset_mention": "the Voxceleb dataset"}, {"mentioned_in_paper": "112", "context_id": "148", "dataset_context": "FER-2013 dataset [12] consists of 28,709 training and 3,589 testing images, while RAF-DB dataset consists of around 30K diverse facial images downloaded from the Internet.", "mention_start": 0, "mention_end": 16, "dataset_mention": "FER-2013 dataset"}, {"mentioned_in_paper": "112", "context_id": "148", "dataset_context": "FER-2013 dataset [12] consists of 28,709 training and 3,589 testing images, while RAF-DB dataset consists of around 30K diverse facial images downloaded from the Internet.", "mention_start": 81, "mention_end": 96, "dataset_mention": "RAF-DB dataset"}, {"mentioned_in_paper": "112", "context_id": "149", "dataset_context": "Please note that for the RAF-DB dataset, we follow the experimental setup as [23] to particularly use the basic emotion subset of RAF-DB, which includes 12,271 training and 3,068 testing images.", "mention_start": 21, "mention_end": 39, "dataset_mention": "the RAF-DB dataset"}, {"mentioned_in_paper": "112", "context_id": "150", "dataset_context": "For the evaluation scheme of linear-protocol, in order to directly verify the capacity of the expression fea-FER-2013 RAF-DB Method Accuracy (%) Accuracy (%) Fully supervised FSN [50] 67.60 81.10 ALT [10] 69 [12] and RAF-DB dataset [23].", "mention_start": 89, "mention_end": 231, "dataset_mention": "the expression fea-FER-2013 RAF-DB Method Accuracy (%) Accuracy (%) Fully supervised FSN [50] 67.60 81.10 ALT [10] 69 [12] and RAF-DB dataset"}, {"mentioned_in_paper": "112", "context_id": "158", "dataset_context": "We adopt the 300W-LP [33] dataset and the AFLW2000 [52] dataset as the training and testing sets respectively, for experimenting the head pose regression.", "mention_start": 9, "mention_end": 33, "dataset_mention": "the 300W-LP [33] dataset"}, {"mentioned_in_paper": "112", "context_id": "158", "dataset_context": "We adopt the 300W-LP [33] dataset and the AFLW2000 [52] dataset as the training and testing sets respectively, for experimenting the head pose regression.", "mention_start": 9, "mention_end": 63, "dataset_mention": "the 300W-LP [33] dataset and the AFLW2000 [52] dataset"}, {"mentioned_in_paper": "112", "context_id": "163", "dataset_context": "We also investigate the applications of identity representations learned by using the proposed method on the Vox-Celeb dataset.", "mention_start": 105, "mention_end": 126, "dataset_mention": "the Vox-Celeb dataset"}, {"mentioned_in_paper": "112", "context_id": "165", "dataset_context": "In this work we adopt LFW [17] and CPLFW [51] dataset for the evaluation of person recognition, particularly on person verification.", "mention_start": 22, "mention_end": 53, "dataset_mention": "LFW [17] and CPLFW [51] dataset"}, {"mentioned_in_paper": "112", "context_id": "166", "dataset_context": "The LFW dataset comprises of 13,233 face images from 5,749 identities and has 6,000 face pairs for evaluating person verification.", "mention_start": 0, "mention_end": 15, "dataset_mention": "The LFW dataset"}, {"mentioned_in_paper": "112", "context_id": "167", "dataset_context": "The CPLFW dataset is similar to LFW but includes larger head pose variation.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The CPLFW dataset"}, {"mentioned_in_paper": "112", "context_id": "176", "dataset_context": "We notice that the synthesized images from the proposed method are a little bit blurry, we hypothesize that it be caused by the plenty blurry training images in the Voxceleb dataset.", "mention_start": 160, "mention_end": 181, "dataset_mention": "the Voxceleb dataset"}, {"mentioned_in_paper": "112", "context_id": "177", "dataset_context": "We believe that further improvements can be obtained by using other high-quality datasets.", "mention_start": 62, "mention_end": 89, "dataset_mention": "other high-quality datasets"}, {"mentioned_in_paper": "114", "context_id": "6", "dataset_context": "Experimental results on the spatio-temporal action detection dataset -DIVA -show the effectiveness of our system.", "mention_start": 24, "mention_end": 68, "dataset_mention": "the spatio-temporal action detection dataset"}, {"mentioned_in_paper": "114", "context_id": "7", "dataset_context": "For comparison, the performance of our system is also evaluated on the THUMOS'14 temporal action detection dataset.", "mention_start": 66, "mention_end": 114, "dataset_mention": "the THUMOS'14 temporal action detection dataset"}, {"mentioned_in_paper": "114", "context_id": "15", "dataset_context": "Existing publicly available action detection datasets such as THUMOS'14 [20] and AVA [15] do not posses these challenges.", "mention_start": 0, "mention_end": 53, "dataset_mention": "Existing publicly available action detection datasets"}, {"mentioned_in_paper": "114", "context_id": "17", "dataset_context": "In this paper, we work with the DIVA dataset that has untrimmed security videos.", "mention_start": 27, "mention_end": 44, "dataset_mention": "the DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "18", "dataset_context": "Videos that comprise the DIVA dataset are a subset of those in the VIRAT dataset [34], albeit with newly introduced annotations that make them suitable for the activity detection task.", "mention_start": 0, "mention_end": 37, "dataset_mention": "Videos that comprise the DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "18", "dataset_context": "Videos that comprise the DIVA dataset are a subset of those in the VIRAT dataset [34], albeit with newly introduced annotations that make them suitable for the activity detection task.", "mention_start": 63, "mention_end": 80, "dataset_mention": "the VIRAT dataset"}, {"mentioned_in_paper": "114", "context_id": "19", "dataset_context": "Figure 1 shows some sample frames from the DIVA dataset.", "mention_start": 39, "mention_end": 55, "dataset_mention": "the DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "36", "dataset_context": "\u2022 We evaluate our system on the DIVA dataset, which is an untrimmed security video dataset in the wild.", "mention_start": 28, "mention_end": 44, "dataset_mention": "the DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "36", "dataset_context": "\u2022 We evaluate our system on the DIVA dataset, which is an untrimmed security video dataset in the wild.", "mention_start": 54, "mention_end": 90, "dataset_mention": "an untrimmed security video dataset"}, {"mentioned_in_paper": "114", "context_id": "55", "dataset_context": "The DIVA dataset is a new spatio-temporal action detection dataset for untrimmed videos.", "mention_start": 0, "mention_end": 16, "dataset_mention": "The DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "55", "dataset_context": "The DIVA dataset is a new spatio-temporal action detection dataset for untrimmed videos.", "mention_start": 20, "mention_end": 66, "dataset_mention": "a new spatio-temporal action detection dataset"}, {"mentioned_in_paper": "114", "context_id": "56", "dataset_context": "While we present our work on the DIVA dataset, and there are currently no other papers we can cite that reference the DIVA dataset at this time, we would like to emphasize that we did not create the DIVA dataset, nor are we the only ones who have access to it.", "mention_start": 29, "mention_end": 45, "dataset_mention": "the DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "56", "dataset_context": "While we present our work on the DIVA dataset, and there are currently no other papers we can cite that reference the DIVA dataset at this time, we would like to emphasize that we did not create the DIVA dataset, nor are we the only ones who have access to it.", "mention_start": 60, "mention_end": 130, "dataset_mention": "currently no other papers we can cite that reference the DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "56", "dataset_context": "While we present our work on the DIVA dataset, and there are currently no other papers we can cite that reference the DIVA dataset at this time, we would like to emphasize that we did not create the DIVA dataset, nor are we the only ones who have access to it.", "mention_start": 194, "mention_end": 211, "dataset_mention": "the DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "58", "dataset_context": "The current release of the DIVA dataset (DIVA V1) is adapted from the VIRAT dataset [34] with new annotations for 12 simple and complex actions of interest focusing on the public security domain.", "mention_start": 23, "mention_end": 39, "dataset_mention": "the DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "58", "dataset_context": "The current release of the DIVA dataset (DIVA V1) is adapted from the VIRAT dataset [34] with new annotations for 12 simple and complex actions of interest focusing on the public security domain.", "mention_start": 66, "mention_end": 83, "dataset_mention": "the VIRAT dataset"}, {"mentioned_in_paper": "114", "context_id": "67", "dataset_context": "As compared to other action detection datasets, such as the THUMOS'14 [20] and AVA [15] datasets, the DIVA dataset introduces several new challenges for the action detection task that make methods designed for existing action datasets unsuitable.", "mention_start": 15, "mention_end": 46, "dataset_mention": "other action detection datasets"}, {"mentioned_in_paper": "114", "context_id": "67", "dataset_context": "As compared to other action detection datasets, such as the THUMOS'14 [20] and AVA [15] datasets, the DIVA dataset introduces several new challenges for the action detection task that make methods designed for existing action datasets unsuitable.", "mention_start": 55, "mention_end": 96, "dataset_mention": "the THUMOS'14 [20] and AVA [15] datasets"}, {"mentioned_in_paper": "114", "context_id": "67", "dataset_context": "As compared to other action detection datasets, such as the THUMOS'14 [20] and AVA [15] datasets, the DIVA dataset introduces several new challenges for the action detection task that make methods designed for existing action datasets unsuitable.", "mention_start": 97, "mention_end": 114, "dataset_mention": " the DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "67", "dataset_context": "As compared to other action detection datasets, such as the THUMOS'14 [20] and AVA [15] datasets, the DIVA dataset introduces several new challenges for the action detection task that make methods designed for existing action datasets unsuitable.", "mention_start": 209, "mention_end": 234, "dataset_mention": "existing action datasets"}, {"mentioned_in_paper": "114", "context_id": "95", "dataset_context": "In Figure 5, we show some sample results from video frames of the DIVA dataset.", "mention_start": 61, "mention_end": 78, "dataset_mention": "the DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "96", "dataset_context": "We observe that Mask-RCNN is able to detect humans and vehicles at different scales, a feature which is useful for detecting the multi-scale actions of the DIVA datasets.", "mention_start": 151, "mention_end": 169, "dataset_mention": "the DIVA datasets"}, {"mentioned_in_paper": "114", "context_id": "113", "dataset_context": "We choose the proposals frame lengths to be {32, 64, 128, 256} based on the average frame lengths for the actions in the DIVA dataset which range between 32 \u2212 256 (see Figure 7).", "mention_start": 116, "mention_end": 133, "dataset_mention": "the DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "135", "dataset_context": "The original I3D is designed for full sized frames on the Kinetics dataset.", "mention_start": 54, "mention_end": 74, "dataset_mention": "the Kinetics dataset"}, {"mentioned_in_paper": "114", "context_id": "137", "dataset_context": "Unlike the Kinetics dataset, the aspect ratio of generated dense proposal cuboids is arbitrary.", "mention_start": 7, "mention_end": 27, "dataset_mention": "the Kinetics dataset"}, {"mentioned_in_paper": "114", "context_id": "150", "dataset_context": "The two-stream I3D network has been shown to outperform the single stream network on the Kinetics dataset, however through experimentation we discovered that the two-stream network performed worse for overall prediction accuracy when using the ground truth actions, which motivated our choice to use only optical flow, see Section 5.2.", "mention_start": 85, "mention_end": 105, "dataset_mention": "the Kinetics dataset"}, {"mentioned_in_paper": "114", "context_id": "187", "dataset_context": "All experimental results are reported on the DIVA validation dataset unless otherwise indicated.", "mention_start": 41, "mention_end": 68, "dataset_mention": "the DIVA validation dataset"}, {"mentioned_in_paper": "114", "context_id": "200", "dataset_context": "This is surprising because for other action recognition datasets twostream networks generally outperform single stream networks [43, 10].", "mention_start": 31, "mention_end": 64, "dataset_mention": "other action recognition datasets"}, {"mentioned_in_paper": "114", "context_id": "228", "dataset_context": "DIVA Test Data: The DIVA test dataset annotations are unavailable to the authors at the time of writing this manuscript.", "mention_start": 15, "mention_end": 37, "dataset_mention": " The DIVA test dataset"}, {"mentioned_in_paper": "114", "context_id": "229", "dataset_context": "We submitted a single set of results from our system for independent evaluation and have included these results as well as results from other performers on the DIVA dataset in Table 4.", "mention_start": 156, "mention_end": 172, "dataset_mention": "the DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "232", "dataset_context": "THUMOS'14 Dataset: In order to compare to other action detection systems we also evaluated on the Temporal Action Detection task of THUMOS'14 [20].", "mention_start": 0, "mention_end": 17, "dataset_mention": "THUMOS'14 Dataset"}, {"mentioned_in_paper": "114", "context_id": "234", "dataset_context": "Since the THUMOS'14 dataset is fundamentally different from DIVA in that actions generally span the majority of the frame, we omit the hierarchical clustering phase and instead perform temporal jittering on the cuboid spanning the entire video.", "mention_start": 6, "mention_end": 27, "dataset_mention": "the THUMOS'14 dataset"}, {"mentioned_in_paper": "114", "context_id": "238", "dataset_context": "In this work we introduced an action detection system capable of handling arbitrary length actions in untrimmed security video on the difficult DIVA dataset.", "mention_start": 130, "mention_end": 156, "dataset_mention": "the difficult DIVA dataset"}, {"mentioned_in_paper": "114", "context_id": "239", "dataset_context": "The system presented in this work is easily adapted to the THUMOS dataset.", "mention_start": 55, "mention_end": 73, "dataset_mention": "the THUMOS dataset"}, {"mentioned_in_paper": "119", "context_id": "1", "dataset_context": "To this end, we organise a competition that provides a new benchmark dataset that contains 2000 2D facial images of 135 subjects as well as their 3D ground truth face scans.", "mention_start": 12, "mention_end": 76, "dataset_mention": " we organise a competition that provides a new benchmark dataset"}, {"mentioned_in_paper": "119", "context_id": "13", "dataset_context": "For an example of the former, people are using the Florence 2D/3D hybrid face dataset (MICC) [10], which contains 3D data with 2D videos (e.g.", "mention_start": 46, "mention_end": 85, "dataset_mention": "the Florence 2D/3D hybrid face dataset"}, {"mentioned_in_paper": "119", "context_id": "25", "dataset_context": "\u2022 The competition provides a benchmark dataset with 2000 2D images of 135 subjects as well as their highresolution 3D ground truth face scans.", "mention_start": 0, "mention_end": 46, "dataset_mention": "\u2022 The competition provides a benchmark dataset"}, {"mentioned_in_paper": "119", "context_id": "47", "dataset_context": "To this end, the Stirling ESRC 3D face dataset 6 is used to create the test set and the JNU [6] 3D face dataset is used to form the validation set.", "mention_start": 12, "mention_end": 46, "dataset_mention": " the Stirling ESRC 3D face dataset"}, {"mentioned_in_paper": "119", "context_id": "47", "dataset_context": "To this end, the Stirling ESRC 3D face dataset 6 is used to create the test set and the JNU [6] 3D face dataset is used to form the validation set.", "mention_start": 66, "mention_end": 111, "dataset_mention": "the test set and the JNU [6] 3D face dataset"}, {"mentioned_in_paper": "119", "context_id": "48", "dataset_context": "For training, any 2D or 3D face dataset is allowed except for the Stirling ESRC and JNU datasets.", "mention_start": 61, "mention_end": 96, "dataset_mention": "the Stirling ESRC and JNU datasets"}, {"mentioned_in_paper": "119", "context_id": "50", "dataset_context": "The 2D and 3D faces in the Stirling ESRC dataset were captured under 7 different expression variations.", "mention_start": 23, "mention_end": 48, "dataset_mention": "the Stirling ESRC dataset"}, {"mentioned_in_paper": "119", "context_id": "58", "dataset_context": "The validation set is a part of the JNU 3D face dataset collected by the Jiangnan University using a 3dMDface system.", "mention_start": 32, "mention_end": 55, "dataset_mention": "the JNU 3D face dataset"}, {"mentioned_in_paper": "119", "context_id": "59", "dataset_context": "The full JNU 3D face dataset has the high resolution 3D face scans of 774 Asian subjects.", "mention_start": 0, "mention_end": 28, "dataset_mention": "The full JNU 3D face dataset"}, {"mentioned_in_paper": "119", "context_id": "60", "dataset_context": "For more details of the JNU 3D face dataset, please refer to [6].", "mention_start": 20, "mention_end": 43, "dataset_mention": "the JNU 3D face dataset"}, {"mentioned_in_paper": "119", "context_id": "99", "dataset_context": "The model was trained on multiple in-the-wild face datasets, including the HELEN [22], LFPW [23], ibug [24] and AFW [25] datasets.", "mention_start": 25, "mention_end": 59, "dataset_mention": "multiple in-the-wild face datasets"}, {"mentioned_in_paper": "119", "context_id": "99", "dataset_context": "The model was trained on multiple in-the-wild face datasets, including the HELEN [22], LFPW [23], ibug [24] and AFW [25] datasets.", "mention_start": 97, "mention_end": 129, "dataset_mention": " ibug [24] and AFW [25] datasets"}, {"mentioned_in_paper": "119", "context_id": "101", "dataset_context": "In addition, a subset of the Multi-PIE [26] face dataset was also used for the CNN6 model training.", "mention_start": 24, "mention_end": 56, "dataset_mention": "the Multi-PIE [26] face dataset"}, {"mentioned_in_paper": "119", "context_id": "176", "dataset_context": "To this end, a subset of the Stirling ESRC 3D face dataset has been used to create the test set.", "mention_start": 24, "mention_end": 58, "dataset_mention": "the Stirling ESRC 3D face dataset"}, {"mentioned_in_paper": "122", "context_id": "28", "dataset_context": "Finally, we demonstrate and evaluate our approach through hardware-in-the-loop experiments using a real world-dataset collected from 2 commercial UAVs.", "mention_start": 96, "mention_end": 117, "dataset_mention": "a real world-dataset"}, {"mentioned_in_paper": "122", "context_id": "131", "dataset_context": "On the other hand, there also exist few multiview image dataset on which our baseline method could be trained, let alone those with overhead viewpoints.", "mention_start": 35, "mention_end": 63, "dataset_mention": "few multiview image dataset"}, {"mentioned_in_paper": "122", "context_id": "197", "dataset_context": "To overcome this problem, we collected a custom video dataset with two DJI drones for evaluation (similar to the data for fine tuning) and converted them into rosbags.", "mention_start": 38, "mention_end": 61, "dataset_mention": "a custom video dataset"}, {"mentioned_in_paper": "122", "context_id": "260", "dataset_context": "We introduced a powerful procedure to train such a network using large computer-generated datasets of synthetic images in virtual environments, and to fine-tune on a small set of real images.", "mention_start": 65, "mention_end": 98, "dataset_mention": "large computer-generated datasets"}, {"mentioned_in_paper": "123", "context_id": "5", "dataset_context": "The experiments on the S3DFM Dataset investigate the overall 3D lip dynamics based on the proposed motion divergence.", "mention_start": 19, "mention_end": 36, "dataset_mention": "the S3DFM Dataset"}, {"mentioned_in_paper": "123", "context_id": "47", "dataset_context": "The proposed algorithms were verified on a 3D speaking face dataset (S3DFM [20, 22]) 1 and has good detection performance over 100 sequences with 200 events.", "mention_start": 41, "mention_end": 67, "dataset_mention": "a 3D speaking face dataset"}, {"mentioned_in_paper": "123", "context_id": "135", "dataset_context": "This section reports the experiments on a 3D speaking face dataset.", "mention_start": 40, "mention_end": 66, "dataset_mention": "a 3D speaking face dataset"}, {"mentioned_in_paper": "123", "context_id": "137", "dataset_context": "The proposed pipeline was verified on a publicly available dynamic face dataset -Speech-driven 3D Facial Motion Dataset (S3DFM) [1, 22].", "mention_start": 38, "mention_end": 79, "dataset_mention": "a publicly available dynamic face dataset"}, {"mentioned_in_paper": "123", "context_id": "137", "dataset_context": "The proposed pipeline was verified on a publicly available dynamic face dataset -Speech-driven 3D Facial Motion Dataset (S3DFM) [1, 22].", "mention_start": 38, "mention_end": 119, "dataset_mention": "a publicly available dynamic face dataset -Speech-driven 3D Facial Motion Dataset"}, {"mentioned_in_paper": "125", "context_id": "16", "dataset_context": "For example, the DROP dataset (Dua et al., 2019) focused on Wikipedia-based questions that require numerical reasoning, e.g., \"Where did Charles travel to first, Castile or Barcelona?\"", "mention_start": 12, "mention_end": 29, "dataset_mention": " the DROP dataset"}, {"mentioned_in_paper": "125", "context_id": "22", "dataset_context": "This paper introduces FINQA, a expertannotated dataset that contains 8,281 financial QA pairs, along with their numerical reasoning processes.", "mention_start": 28, "mention_end": 54, "dataset_mention": " a expertannotated dataset"}, {"mentioned_in_paper": "125", "context_id": "37", "dataset_context": "There have been several QA datasets involving numerical understandings and calculations.", "mention_start": 11, "mention_end": 35, "dataset_mention": "been several QA datasets"}, {"mentioned_in_paper": "125", "context_id": "40", "dataset_context": "For reading comprehension, the dataset most related to ours is the DROP dataset (Dua et al., 2019), which applies simple calculations over texts.", "mention_start": 62, "mention_end": 79, "dataset_mention": "the DROP dataset"}, {"mentioned_in_paper": "125", "context_id": "45", "dataset_context": "Another kind of QA datasets related to ours is the math word problem datasets, like MaWPS (Koncel-Kedziorski et al., 2016), MathQA (Amini et al., 2019).", "mention_start": 16, "mention_end": 27, "dataset_mention": "QA datasets"}, {"mentioned_in_paper": "125", "context_id": "45", "dataset_context": "Another kind of QA datasets related to ours is the math word problem datasets, like MaWPS (Koncel-Kedziorski et al., 2016), MathQA (Amini et al., 2019).", "mention_start": 47, "mention_end": 77, "dataset_mention": "the math word problem datasets"}, {"mentioned_in_paper": "125", "context_id": "50", "dataset_context": "Previous works in finance domain include risk management to detect fraud (Han et al., 2018; Wang et al., 2019; Nourbakhsh and Bang, 2019), sentiment analysis to assist market prediction (Day and Lee, 2016; Wang et al., 2013; Akhtar et al., 2017), opinionated Question Answering (Liu et al., 2020), such as the FiQA 2 dataset built from forums and social media.", "mention_start": 305, "mention_end": 324, "dataset_mention": "the FiQA 2 dataset"}, {"mentioned_in_paper": "125", "context_id": "52", "dataset_context": "To the best of our knowledge, there is no previous work and dataset on building QA systems of numerical reasoning on financial reports.", "mention_start": 38, "mention_end": 67, "dataset_mention": "no previous work and dataset"}, {"mentioned_in_paper": "125", "context_id": "71", "dataset_context": "4 The FINQA Dataset 4.1 Data Preparation Data Source.", "mention_start": 0, "mention_end": 19, "dataset_mention": "4 The FINQA Dataset"}, {"mentioned_in_paper": "125", "context_id": "72", "dataset_context": "We develop FINQA based on the publicly available earnings reports of S&P 500 companies from 1999 to 2019, collected in the FinTabNet dataset (Zheng et al., 2021).", "mention_start": 118, "mention_end": 140, "dataset_mention": "the FinTabNet dataset"}, {"mentioned_in_paper": "125", "context_id": "74", "dataset_context": "The FinTabNet dataset has annotated the tables in each report.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The FinTabNet dataset"}, {"mentioned_in_paper": "125", "context_id": "93", "dataset_context": "As most existing QA datasets were constructed by MTurk workers (Yang et al., 2018; Dua et al., 2019; Chen et al., 2020c), it requires substantial domain-specific knowledge to compose meaningful questions that are hard for computers to answer.", "mention_start": 3, "mention_end": 28, "dataset_mention": "most existing QA datasets"}, {"mentioned_in_paper": "125", "context_id": "156", "dataset_context": "We use a Seq2seq architecture for the generator, similar to the Seq2seq baseline in the MathQA dataset (Amini et al., 2019).", "mention_start": 83, "mention_end": 102, "dataset_mention": "the MathQA dataset"}, {"mentioned_in_paper": "125", "context_id": "159", "dataset_context": "The Neural Symbolic Reader(NeRd) (Chen et al., 2020d) is also a pointergenerator based model for program generation, with the state of the art results on the MathQA dataset (Amini et al., 2019).", "mention_start": 153, "mention_end": 172, "dataset_mention": "the MathQA dataset"}, {"mentioned_in_paper": "125", "context_id": "207", "dataset_context": "This paper introduces FINQA, a new expertannotated QA dataset that aims to tackle numerical reasoning over real-world financial data.", "mention_start": 28, "mention_end": 61, "dataset_mention": " a new expertannotated QA dataset"}, {"mentioned_in_paper": "125", "context_id": "214", "dataset_context": "We develop FINQA based on the publicly available earnings reports of S&P 500 companies from 1999 to 2019, collected in the FinTabNet dataset (Zheng et al., 2021).", "mention_start": 118, "mention_end": 140, "dataset_mention": "the FinTabNet dataset"}, {"mentioned_in_paper": "125", "context_id": "215", "dataset_context": "The FinTabNet dataset is publicly available under the CDLA-Permissive 6 license, which permits us to create additional annotations on top of the data (\"Enhanced Data\", \u00a71.5 of CDLA) and publish the annotations (\"Publish\", \u00a71.9 of CDLA).", "mention_start": 0, "mention_end": 21, "dataset_mention": "The FinTabNet dataset"}, {"mentioned_in_paper": "125", "context_id": "216", "dataset_context": "For the annotation of our FINQA dataset on Upwork, we first launch interviews of the task introduction with 4 example questions, which is paid as $30, for them to try a few examples to get informed and familiar with the task.", "mention_start": 22, "mention_end": 39, "dataset_mention": "our FINQA dataset"}, {"mentioned_in_paper": "126", "context_id": "5", "dataset_context": "We first prepare a dataset of 5,000 Chest X-rays from the publicly available datasets.", "mention_start": 0, "mention_end": 26, "dataset_mention": "We first prepare a dataset"}, {"mentioned_in_paper": "126", "context_id": "33", "dataset_context": "We then used a subset of images from ChexPert [11] dataset, as the negative samples for COVID-19 detection.", "mention_start": 37, "mention_end": 58, "dataset_mention": "ChexPert [11] dataset"}, {"mentioned_in_paper": "126", "context_id": "38", "dataset_context": "Here, we train 4 popular convolutional networks which have achieved promising results in several tasks during recent years (including ResNet18, ResNet50, SqueezeNet, and DenseNet-161) on COVID-Xray-5k dataset, and analyze their performance for COVID-19 detection.", "mention_start": 186, "mention_end": 208, "dataset_mention": "COVID-Xray-5k dataset"}, {"mentioned_in_paper": "126", "context_id": "59", "dataset_context": "Section 2 provides a summary of the prepared COVID-Xray-5k Dataset.", "mention_start": 45, "mention_end": 66, "dataset_mention": "COVID-Xray-5k Dataset"}, {"mentioned_in_paper": "126", "context_id": "63", "dataset_context": "Chest X-ray images from two datasets formed the COVID-Xray-5k dataset that contains 2,084 training and 3,100 test images.", "mention_start": 24, "mention_end": 69, "dataset_mention": "two datasets formed the COVID-Xray-5k dataset"}, {"mentioned_in_paper": "126", "context_id": "64", "dataset_context": "One of the used datasets is the recently published Covid-Chestxray-Dataset, which contains a set of images from publications on COVID-19 topics, collected by Joseph Paul Cohen [9, 10].", "mention_start": 41, "mention_end": 74, "dataset_mention": "published Covid-Chestxray-Dataset"}, {"mentioned_in_paper": "126", "context_id": "67", "dataset_context": "It is mentioned that this dataset is continuously updated.", "mention_start": 6, "mention_end": 33, "dataset_mention": "mentioned that this dataset"}, {"mentioned_in_paper": "126", "context_id": "73", "dataset_context": "This way, we can provide the community a more cleanly labeled dataset.", "mention_start": 24, "mention_end": 69, "dataset_mention": "the community a more cleanly labeled dataset"}, {"mentioned_in_paper": "126", "context_id": "78", "dataset_context": "Since the number of Non-Covid images was very small in the [9] dataset, additional images were employed from the Chex-Pert dataset [11], a large public dataset for chest radiograph interpretation consisting of 224,316 chest radiographs of 65,240 patients, labeled for the presence of 14 sub-categories (nofinding, Edema, Pneumonia, etc.).", "mention_start": 55, "mention_end": 70, "dataset_mention": "the [9] dataset"}, {"mentioned_in_paper": "126", "context_id": "78", "dataset_context": "Since the number of Non-Covid images was very small in the [9] dataset, additional images were employed from the Chex-Pert dataset [11], a large public dataset for chest radiograph interpretation consisting of 224,316 chest radiographs of 65,240 patients, labeled for the presence of 14 sub-categories (nofinding, Edema, Pneumonia, etc.).", "mention_start": 108, "mention_end": 130, "dataset_mention": "the Chex-Pert dataset"}, {"mentioned_in_paper": "126", "context_id": "87", "dataset_context": "To overcome the limited data sizes, transfer learning was used to fine-tune four popular pre-trained deep neural networks on the training images of COVID-Xray-5k dataset.", "mention_start": 147, "mention_end": 169, "dataset_mention": "COVID-Xray-5k dataset"}, {"mentioned_in_paper": "126", "context_id": "100", "dataset_context": "One of the models used in this work, is the pre-trained ResNet18, trained on ImageNet dataset.", "mention_start": 76, "mention_end": 93, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "126", "context_id": "124", "dataset_context": "Since the current test dataset is highly imbalanced (100 COVID-19 images, 3000 Non-COVID image), sensitivity and specificity are two proper metrics which can be used for reporting the model performance: (2)", "mention_start": 6, "mention_end": 30, "dataset_mention": "the current test dataset"}, {"mentioned_in_paper": "126", "context_id": "167", "dataset_context": "We make this dataset publicly available for the research community to use as a benchmark for training and evaluating future machine learning models for COVID-19 binary classification task.", "mention_start": 0, "mention_end": 20, "dataset_mention": "We make this dataset"}, {"mentioned_in_paper": "126", "context_id": "168", "dataset_context": "We performed a detail experimental analysis evaluating the performance of each of these 4 models on the test set of of COVID-Xray-5k Dataset, in terms of sensitivity, specificity, ROC, and AUC.", "mention_start": 119, "mention_end": 140, "dataset_mention": "COVID-Xray-5k Dataset"}, {"mentioned_in_paper": "126", "context_id": "172", "dataset_context": "The presented work is reflecting one of the earliest Covid-19 chest X-ray analysis and dataset preparation attempts, which brings time-sensitive relevance in combining these two aspects.", "mention_start": 40, "mention_end": 94, "dataset_mention": "the earliest Covid-19 chest X-ray analysis and dataset"}, {"mentioned_in_paper": "127", "context_id": "164", "dataset_context": "The weight decay is set to 0.0002 for SBU and 0.0001 for the rest datasets.", "mention_start": 57, "mention_end": 74, "dataset_mention": "the rest datasets"}, {"mentioned_in_paper": "127", "context_id": "170", "dataset_context": "Table 1 shows the performance gains brought by CAG and VAG with the crosssubject benchmark on the NTU RGB+D dataset.", "mention_start": 94, "mention_end": 115, "dataset_mention": "the NTU RGB+D dataset"}, {"mentioned_in_paper": "127", "context_id": "184", "dataset_context": "Figure 6 visualizes the learned channel attention on the NTU RGB+D dataset.", "mention_start": 53, "mention_end": 74, "dataset_mention": "the NTU RGB+D dataset"}, {"mentioned_in_paper": "127", "context_id": "198", "dataset_context": "Here, we compare our final performance with existing methods on the aforementioned four datasets.", "mention_start": 63, "mention_end": 96, "dataset_mention": "the aforementioned four datasets"}, {"mentioned_in_paper": "127", "context_id": "199", "dataset_context": "On the N-UCLA and CBU datasets, we obtain an accuracy of 97.2% and 98.9% respectively (see Table 7), setting the new state-ofthe-art.", "mention_start": 3, "mention_end": 30, "dataset_mention": "the N-UCLA and CBU datasets"}, {"mentioned_in_paper": "128", "context_id": "19", "dataset_context": "However, these object detectors rely heavily on pre-define anchors, which are sensitive to hyper-parameters (e.g., input size, aspect ratio, scales, etc.) and different datasets.", "mention_start": 148, "mention_end": 177, "dataset_mention": " etc.) and different datasets"}, {"mentioned_in_paper": "129", "context_id": "70", "dataset_context": "Finally, RDF Schema (RDFS) and Web Ontology Languages (OWL) enable the description of vocabulary semantics used in RDF datasets.", "mention_start": 114, "mention_end": 127, "dataset_mention": "RDF datasets"}, {"mentioned_in_paper": "129", "context_id": "72", "dataset_context": "LiteMat is a semantic-aware encoding scheme that compresses RDF data sets and supports reasoning services associated to the RDFS ontology language.", "mention_start": 11, "mention_end": 73, "dataset_mention": "a semantic-aware encoding scheme that compresses RDF data sets"}, {"mentioned_in_paper": "129", "context_id": "144", "dataset_context": "These triples generally represent an important proportion of the triple set in real-world RDF datasets.", "mention_start": 79, "mention_end": 102, "dataset_mention": "real-world RDF datasets"}, {"mentioned_in_paper": "129", "context_id": "267", "dataset_context": "The core RDF4J databases are mainly intended for small to medium-sized datasets and thus it makes sense to consider them for Edge computing.", "mention_start": 58, "mention_end": 79, "dataset_mention": "medium-sized datasets"}, {"mentioned_in_paper": "129", "context_id": "282", "dataset_context": "The back-end construction time corresponds to the time taken by each system to read the dataset file and to construct its proper storage layout (including indexes in the case of all systems except Succint-Edge which is self-index) on which queries can be asked.", "mention_start": 79, "mention_end": 95, "dataset_mention": "read the dataset"}, {"mentioned_in_paper": "129", "context_id": "297", "dataset_context": "In this evaluation, it is not possible to distinguish between the space used for the dictionaries and the datasets.", "mention_start": 80, "mention_end": 114, "dataset_mention": "the dictionaries and the datasets"}, {"mentioned_in_paper": "129", "context_id": "305", "dataset_context": "Table 1 and 2 provide the results of this experimentation for the LUBM1 dataset (over 100.000 triples).", "mention_start": 62, "mention_end": 79, "dataset_mention": "the LUBM1 dataset"}, {"mentioned_in_paper": "130", "context_id": "108", "dataset_context": "We conducted our experiments of human motion prediction on the H3.6m mocap Dataset [Ionescu et al., 2014], which is the largest human motion dataset for 3D body pose analysis.", "mention_start": 59, "mention_end": 82, "dataset_mention": "the H3.6m mocap Dataset"}, {"mentioned_in_paper": "130", "context_id": "108", "dataset_context": "We conducted our experiments of human motion prediction on the H3.6m mocap Dataset [Ionescu et al., 2014], which is the largest human motion dataset for 3D body pose analysis.", "mention_start": 115, "mention_end": 148, "dataset_mention": "the largest human motion dataset"}, {"mentioned_in_paper": "130", "context_id": "110", "dataset_context": "Recorded by a Vicon motion capture system, the H3.6m dataset provides high quality 3D body joint locations in the global coordinate sampled at 50 frames per second (fps).", "mention_start": 42, "mention_end": 60, "dataset_mention": " the H3.6m dataset"}, {"mentioned_in_paper": "130", "context_id": "111", "dataset_context": "For all our experiments, we followed the same data setting in [Fragkiadaki et al., 2015; Jain et al., 2016; Martinez et al., 2017].", "mention_start": 24, "mention_end": 54, "dataset_mention": " we followed the same data set"}, {"mentioned_in_paper": "130", "context_id": "115", "dataset_context": "The three dimension feature of each joint represents the rotation vector with respect to the parent joint predefined in H3.6m dataset.", "mention_start": 120, "mention_end": 133, "dataset_mention": "H3.6m dataset"}, {"mentioned_in_paper": "130", "context_id": "140", "dataset_context": "The overall human motion prediction result of all 15 activities of H3.6m dataset via mean angle error is shown in Table.1.", "mention_start": 67, "mention_end": 80, "dataset_mention": "H3.6m dataset"}, {"mentioned_in_paper": "130", "context_id": "158", "dataset_context": "Since on the H3.6m dataset, there is no ground-truth human motion sequence that contains two different activities, we thus trained our model with the activity labels and feed a specific label at the test time.", "mention_start": 9, "mention_end": 26, "dataset_mention": "the H3.6m dataset"}, {"mentioned_in_paper": "131", "context_id": "21", "dataset_context": "We evaluate MaskFaceGAN on three high-resolution face datasets and in comparison to several state-of-the-art editing techniques from the literature.", "mention_start": 27, "mention_end": 62, "dataset_mention": "three high-resolution face datasets"}, {"mentioned_in_paper": "131", "context_id": "183", "dataset_context": "To foster reproducibility, we use the official version of StyleGAN2 4 trained on images from the FFHQ dataset.", "mention_start": 92, "mention_end": 109, "dataset_mention": "the FFHQ dataset"}, {"mentioned_in_paper": "131", "context_id": "189", "dataset_context": "\u2022 The Face Parser (S) is based on DeepLabV3 [46] and trained on the CelebA-HQ dataset to generate segmentation masks of the following seven classes: \"mouth\", \"eyebrows\", \"eyes\", \"earrings\", \"hair\", \"noise\" and \"skin\".", "mention_start": 64, "mention_end": 85, "dataset_mention": "the CelebA-HQ dataset"}, {"mentioned_in_paper": "131", "context_id": "219", "dataset_context": "Three distinct datasets are used for the experiments to explore the generalization capabilities of the evaluated approaches across various data distributions.", "mention_start": 0, "mention_end": 23, "dataset_mention": "Three distinct datasets"}, {"mentioned_in_paper": "131", "context_id": "221", "dataset_context": "Fig. 5 compares editing results produced by MaskFaceGAN and five competing models on a couple of sample images from the CelebA-HQ and Helen datasets.", "mention_start": 116, "mention_end": 148, "dataset_mention": "the CelebA-HQ and Helen datasets"}, {"mentioned_in_paper": "131", "context_id": "224", "dataset_context": "These can, for example, be seen with the \"Mouth slightly open\" attribute in Fig. 5 and the editing results in Fig. 6, where higher-resolution edits for the SiblingsDB-HQf dataset are shown for three challenging (often entangled) attributes.", "mention_start": 151, "mention_end": 178, "dataset_mention": "the SiblingsDB-HQf dataset"}, {"mentioned_in_paper": "131", "context_id": "278", "dataset_context": "To demonstrate the impact of different component of Mask-FaceGAN on the editing quality, we perform an ablation study on the CelebA-HQ dataset.", "mention_start": 120, "mention_end": 142, "dataset_mention": "the CelebA-HQ dataset"}, {"mentioned_in_paper": "131", "context_id": "290", "dataset_context": "For a quantitative analysis of the ablation results, we report in Table VIII mean FID scores computed over the test images of CelebA-HQ dataset and averaged over all attributes.", "mention_start": 125, "mention_end": 143, "dataset_mention": "CelebA-HQ dataset"}, {"mentioned_in_paper": "131", "context_id": "371", "dataset_context": "FID scores, grouped by attributes, are shown in Tables IX, X and XI for CelebAHQ, Helen and Siblings dataset, respectively.", "mention_start": 81, "mention_end": 108, "dataset_mention": " Helen and Siblings dataset"}, {"mentioned_in_paper": "131", "context_id": "377", "dataset_context": "The results for model ratings are shown in Tables XII, XIII and XIV for the CelebAHQ, Helen and Siblings datasets, respectively and the results for the best model selection ratings are shown in Tables XV, XVI and XVII with the same dataset order.", "mention_start": 85, "mention_end": 113, "dataset_mention": " Helen and Siblings datasets"}, {"mentioned_in_paper": "133", "context_id": "7", "dataset_context": "We observe that the generated images from our framework consistently improves over the performance of deep face recognition network trained with Oxford VGG Face dataset and achieves comparable results to the state-of-the-art.", "mention_start": 145, "mention_end": 168, "dataset_mention": "Oxford VGG Face dataset"}, {"mentioned_in_paper": "133", "context_id": "11", "dataset_context": "Larger and wider datasets usually improve the generalization and overall performance of the model [41, 1].", "mention_start": 0, "mention_end": 25, "dataset_mention": "Larger and wider datasets"}, {"mentioned_in_paper": "133", "context_id": "140", "dataset_context": "For real face images, we use CASIA-Web Face Dataset [56] that consists of 500K face images of 10K individuals.", "mention_start": 28, "mention_end": 51, "dataset_mention": "CASIA-Web Face Dataset"}, {"mentioned_in_paper": "133", "context_id": "142", "dataset_context": "For that, we use a combination of 300W-3D and AFLW2000-3D datasets as our paired training set [64] which consist of 5K real images with their corresponding 3DMM parameter annotations.", "mention_start": 33, "mention_end": 66, "dataset_mention": "300W-3D and AFLW2000-3D datasets"}, {"mentioned_in_paper": "133", "context_id": "145", "dataset_context": "Amazon Handbag dataset used by [22] contains 137K bag images)", "mention_start": 0, "mention_end": 22, "dataset_mention": "Amazon Handbag dataset"}, {"mentioned_in_paper": "133", "context_id": "149", "dataset_context": "Training Details: We train all the components of our framework together from scratch except the classification network C which is pre-trained by using a subset of Oxford VGG Face Dataset [33].", "mention_start": 162, "mention_end": 186, "dataset_mention": "Oxford VGG Face Dataset"}, {"mentioned_in_paper": "133", "context_id": "171", "dataset_context": "In case of extreme poses, the quality of the image generated by our model becomes less sharp as the CASIA-WebFace dataset, which we used to learn the parameters of discriminator network D R , lacks sufficient number of examples with extreme poses.", "mention_start": 95, "mention_end": 121, "dataset_mention": "the CASIA-WebFace dataset"}, {"mentioned_in_paper": "133", "context_id": "172", "dataset_context": "In order to show that synthetic images are effectively transformed to the realistic domain with preserving identities, we perform a face verification experiments on GAN- Faces dataset.", "mention_start": 164, "mention_end": 183, "dataset_mention": "GAN- Faces dataset"}, {"mentioned_in_paper": "133", "context_id": "176", "dataset_context": "Similarly, we also generated the same number of similar and dis-similar face images pairs from VGG face dataset [33] and the synthetic 3DMM rendered faces dataset.", "mention_start": 94, "mention_end": 111, "dataset_mention": "VGG face dataset"}, {"mentioned_in_paper": "133", "context_id": "176", "dataset_context": "Similarly, we also generated the same number of similar and dis-similar face images pairs from VGG face dataset [33] and the synthetic 3DMM rendered faces dataset.", "mention_start": 94, "mention_end": 162, "dataset_mention": "VGG face dataset [33] and the synthetic 3DMM rendered faces dataset"}, {"mentioned_in_paper": "133", "context_id": "178", "dataset_context": "The addition of realism and preservation of identities of the GANFaces can be seen from the comparison of its distribution to the 3DMM synthetic dataset's distribution.", "mention_start": 126, "mention_end": 152, "dataset_mention": "the 3DMM synthetic dataset"}, {"mentioned_in_paper": "133", "context_id": "185", "dataset_context": "Following [32], we use a pre-trained VGGNet by [43] with 19 layers trained on ImageNet dataset [40] and took these parameters as initial parameters.", "mention_start": 77, "mention_end": 94, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "133", "context_id": "186", "dataset_context": "We train the network with different portion of Oxford VGG Face dataset [33] augmented with the GANFaces dataset.", "mention_start": 47, "mention_end": 70, "dataset_mention": "Oxford VGG Face dataset"}, {"mentioned_in_paper": "133", "context_id": "186", "dataset_context": "We train the network with different portion of Oxford VGG Face dataset [33] augmented with the GANFaces dataset.", "mention_start": 91, "mention_end": 111, "dataset_mention": "the GANFaces dataset"}, {"mentioned_in_paper": "133", "context_id": "191", "dataset_context": "VGG+GANF IJB-A@FAR=1e-2 VGG+GANF IJB-A@FAR=1e-3 VGG IJB-A@FAR=1e-2 VGG IJB-A@FAR=1e-3 We train 6 models with %20, %50 and %100 of the VGG Face dataset with and without the augmentation of GANFaces.", "mention_start": 129, "mention_end": 150, "dataset_mention": "the VGG Face dataset"}, {"mentioned_in_paper": "133", "context_id": "192", "dataset_context": "We evaluate the models on LFW and IJB-A datasets and the benchmark scores is improved with the usage of GANFaces dataset even though low resolution images.", "mention_start": 26, "mention_end": 48, "dataset_mention": "LFW and IJB-A datasets"}, {"mentioned_in_paper": "133", "context_id": "192", "dataset_context": "We evaluate the models on LFW and IJB-A datasets and the benchmark scores is improved with the usage of GANFaces dataset even though low resolution images.", "mention_start": 104, "mention_end": 120, "dataset_mention": "GANFaces dataset"}, {"mentioned_in_paper": "133", "context_id": "193", "dataset_context": "The contribution of GANFaces increase inversely proportional to the number of images included from VGG dataset which indicates more synthetic images might improve the results even further.", "mention_start": 99, "mention_end": 110, "dataset_mention": "VGG dataset"}, {"mentioned_in_paper": "133", "context_id": "195", "dataset_context": "We compare our best model with full VGG dataset and GANFaces to the other state of the art methods.", "mention_start": 31, "mention_end": 47, "dataset_mention": "full VGG dataset"}, {"mentioned_in_paper": "133", "context_id": "209", "dataset_context": "In table 1, we show the contribution of each module and compare them to the whole framework as a baseline and to the performance of a model trained by only half of the VGG dataset.", "mention_start": 163, "mention_end": 179, "dataset_mention": "the VGG dataset"}, {"mentioned_in_paper": "135", "context_id": "127", "dataset_context": "Wiki-Editor is extracted from the UMDWikipedia dataset (Kumar et al., 2015).", "mention_start": 30, "mention_end": 54, "dataset_mention": "the UMDWikipedia dataset"}, {"mentioned_in_paper": "135", "context_id": "141", "dataset_context": "Slashdot dataset (Kunegis et al., 2009) is a network of interactions among users on Slashdot.", "mention_start": 0, "mention_end": 16, "dataset_mention": "Slashdot dataset"}, {"mentioned_in_paper": "137", "context_id": "33", "dataset_context": "The current state-of-the-art for the Drone-Action dataset [14] uses a pose-stream that heavily relies on accurate joint estimations.", "mention_start": 33, "mention_end": 57, "dataset_mention": "the Drone-Action dataset"}, {"mentioned_in_paper": "137", "context_id": "34", "dataset_context": "The state-of-the-art on the MOD20 dataset [15] employs a two-stream strategy and relies on motion-CNN for accuracy.", "mention_start": 24, "mention_end": 41, "dataset_mention": "the MOD20 dataset"}, {"mentioned_in_paper": "137", "context_id": "35", "dataset_context": "The state-of-the-art on the Okutama-Action dataset [16] uses features computed by 3D convolution neural networks in addition to a new set of features computed by the Binary Volume Comparison (BVC) layer, which consists of three components: a 3D-Conv layer with 12 non-trainable (i.e., fixed) filters, a non-linear function, and a set of learnable weights.", "mention_start": 24, "mention_end": 50, "dataset_mention": "the Okutama-Action dataset"}, {"mentioned_in_paper": "137", "context_id": "37", "dataset_context": "Our computationally efficient SWTF (Sparse Weighted Temporal Fusion) model advances the stateof-the-art on the Drone-Action, MOD20, and Okutama-Action datasets without separately employing a pose-stream or temporal stream.", "mention_start": 131, "mention_end": 159, "dataset_mention": " and Okutama-Action datasets"}, {"mentioned_in_paper": "137", "context_id": "50", "dataset_context": "\u2022 We perform extensive experimental analysis on three publicly available benchmark datasets, i.e., Okutama dataset [17], MOD20 dataset [15], and Drone-Action dataset [18].", "mention_start": 98, "mention_end": 114, "dataset_mention": " Okutama dataset"}, {"mentioned_in_paper": "137", "context_id": "50", "dataset_context": "\u2022 We perform extensive experimental analysis on three publicly available benchmark datasets, i.e., Okutama dataset [17], MOD20 dataset [15], and Drone-Action dataset [18].", "mention_start": 120, "mention_end": 134, "dataset_mention": " MOD20 dataset"}, {"mentioned_in_paper": "137", "context_id": "50", "dataset_context": "\u2022 We perform extensive experimental analysis on three publicly available benchmark datasets, i.e., Okutama dataset [17], MOD20 dataset [15], and Drone-Action dataset [18].", "mention_start": 140, "mention_end": 165, "dataset_mention": " and Drone-Action dataset"}, {"mentioned_in_paper": "137", "context_id": "61", "dataset_context": "While the MOD20 dataset only comes with ground truth action labels, the Okutama dataset provides ground truth bounding boxes as well.", "mention_start": 6, "mention_end": 23, "dataset_mention": "the MOD20 dataset"}, {"mentioned_in_paper": "137", "context_id": "61", "dataset_context": "While the MOD20 dataset only comes with ground truth action labels, the Okutama dataset provides ground truth bounding boxes as well.", "mention_start": 67, "mention_end": 87, "dataset_mention": " the Okutama dataset"}, {"mentioned_in_paper": "137", "context_id": "62", "dataset_context": "The Drone-Action dataset goes one step ahead and provides ground truth pose annotations along with bounding boxes and frames.", "mention_start": 0, "mention_end": 24, "dataset_mention": "The Drone-Action dataset"}, {"mentioned_in_paper": "137", "context_id": "117", "dataset_context": "In this subsection, we discuss the evaluation results obtained on the ablation studies performed on Okutama, MOD20, and Drone Action datasets.", "mention_start": 115, "mention_end": 141, "dataset_mention": " and Drone Action datasets"}, {"mentioned_in_paper": "137", "context_id": "122", "dataset_context": "Training the model using a basic backbone with RoiAlign, fully connected, batchnormalization, and dropout layers resulted in 61.34% accuracy for the Okutama dataset.", "mention_start": 144, "mention_end": 164, "dataset_mention": "the Okutama dataset"}, {"mentioned_in_paper": "137", "context_id": "126", "dataset_context": "Similarly for MOD20 dataset, which consists of a diverse range of action classes, each significantly different from the other.", "mention_start": 14, "mention_end": 27, "dataset_mention": "MOD20 dataset"}, {"mentioned_in_paper": "137", "context_id": "130", "dataset_context": "The Drone Action dataset contained various action classes which were similar to one another.", "mention_start": 0, "mention_end": 24, "dataset_mention": "The Drone Action dataset"}, {"mentioned_in_paper": "137", "context_id": "136", "dataset_context": "It successfully achieves state-of-the-art results in all three datasets, namely 72.76% on the Okutama dataset, 92.56% on the MOD20 dataset, and 71.79% on the Drone Action dataset without pose-stream whereas 78.86% with pose-stream.", "mention_start": 89, "mention_end": 109, "dataset_mention": "the Okutama dataset"}, {"mentioned_in_paper": "137", "context_id": "136", "dataset_context": "It successfully achieves state-of-the-art results in all three datasets, namely 72.76% on the Okutama dataset, 92.56% on the MOD20 dataset, and 71.79% on the Drone Action dataset without pose-stream whereas 78.86% with pose-stream.", "mention_start": 120, "mention_end": 138, "dataset_mention": "the MOD20 dataset"}, {"mentioned_in_paper": "137", "context_id": "136", "dataset_context": "It successfully achieves state-of-the-art results in all three datasets, namely 72.76% on the Okutama dataset, 92.56% on the MOD20 dataset, and 71.79% on the Drone Action dataset without pose-stream whereas 78.86% with pose-stream.", "mention_start": 153, "mention_end": 178, "dataset_mention": "the Drone Action dataset"}, {"mentioned_in_paper": "137", "context_id": "137", "dataset_context": "For the Okutama dataset specifically, our Weighted Temporal", "mention_start": 4, "mention_end": 23, "dataset_mention": "the Okutama dataset"}, {"mentioned_in_paper": "137", "context_id": "140", "dataset_context": "It is also computationally less expensive, being a single stream network as compared to the approaches used in the previous works for MOD20 and Okutama dataset evaluation.", "mention_start": 133, "mention_end": 159, "dataset_mention": "MOD20 and Okutama dataset"}, {"mentioned_in_paper": "137", "context_id": "146", "dataset_context": "Significant growth is observed in the Okutama-Action dataset, which can be extremely useful for drone-based activity recognition tasks at extremely high altitudes, such as crowd analysis and video surveillance.", "mention_start": 34, "mention_end": 60, "dataset_mention": "the Okutama-Action dataset"}, {"mentioned_in_paper": "137", "context_id": "147", "dataset_context": "Our model achieves state-of-the-art performance on the challenging outdoor scenes depicted in the MOD20 and Drone-Action datasets, despite being less complex than alternative approaches.", "mention_start": 94, "mention_end": 129, "dataset_mention": "the MOD20 and Drone-Action datasets"}, {"mentioned_in_paper": "139", "context_id": "6", "dataset_context": "The overall resulting framework is evaluated on VoxCeleb1 and MAV-Celeb datasets with a multitude of tasks, including cross-modal verification and matching.", "mention_start": 48, "mention_end": 80, "dataset_mention": "VoxCeleb1 and MAV-Celeb datasets"}, {"mentioned_in_paper": "139", "context_id": "11", "dataset_context": "Recently, Nagrani et al. [3], [4], [5] introduced the face-voice association task to vision community with the creation of a large-scale audio-visual dataset, comprising faces and voices of 1, 251 celebrities.", "mention_start": 122, "mention_end": 157, "dataset_mention": "a large-scale audio-visual dataset"}, {"mentioned_in_paper": "139", "context_id": "13", "dataset_context": "In addition, we observe the creation of new audio-visual datasets for studying novel tasks [7], [12], [13], [14].", "mention_start": 39, "mention_end": 65, "dataset_mention": "new audio-visual datasets"}, {"mentioned_in_paper": "139", "context_id": "40", "dataset_context": "The experiments are performed on Multilingual Audio-Visual (MAV-Celeb) dataset [12].", "mention_start": 33, "mention_end": 78, "dataset_mention": "Multilingual Audio-Visual (MAV-Celeb) dataset"}, {"mentioned_in_paper": "139", "context_id": "45", "dataset_context": "Finally, to our knowledge, we provide a first rigorous comparison of loss functions employed in existing face-voice association methods by utilizing the same underlying branch architecture, input features, and evaluation protocols on VoxCeleb1 dataset (see Fig. 3).", "mention_start": 233, "mention_end": 251, "dataset_mention": "VoxCeleb1 dataset"}, {"mentioned_in_paper": "139", "context_id": "155", "dataset_context": "Training Details and Datasets.", "mention_start": 9, "mention_end": 29, "dataset_mention": "Details and Datasets"}, {"mentioned_in_paper": "139", "context_id": "166", "dataset_context": "Recently Nawaz et al. [12] curated MAV-Celeb dataset to study the impact of language on face-voice association  Experimental Setup.", "mention_start": 35, "mention_end": 52, "dataset_mention": "MAV-Celeb dataset"}, {"mentioned_in_paper": "139", "context_id": "230", "dataset_context": "MAV-Celeb dataset provides language annotation of celebrities to analyze the impact of multiple languages on cross-modal verification and matching tasks.", "mention_start": 0, "mention_end": 17, "dataset_mention": "MAV-Celeb dataset"}, {"mentioned_in_paper": "139", "context_id": "242", "dataset_context": "We integrated this module in a two-stream pipeline, used for extracting face and voice embeddings, and the resulting overall framework is evaluated on a large-scale VoxCeleb1 dataset for F-V matching and verification tasks.", "mention_start": 150, "mention_end": 182, "dataset_mention": "a large-scale VoxCeleb1 dataset"}, {"mentioned_in_paper": "140", "context_id": "7", "dataset_context": "We evaluate SSGVS and other stateof-the-art video synthesis models on the Action Genome dataset and demonstrate the positive significance of video scene graphs in video synthesis.", "mention_start": 70, "mention_end": 95, "dataset_mention": "the Action Genome dataset"}, {"mentioned_in_paper": "140", "context_id": "38", "dataset_context": "\u2022 We split a sub-dataset from the Action Genome dataset [20] and conduct experiments to demonstrate the benefits of using semantic video scene graphs as condition for video synthesis.", "mention_start": 11, "mention_end": 24, "dataset_mention": "a sub-dataset"}, {"mentioned_in_paper": "140", "context_id": "38", "dataset_context": "\u2022 We split a sub-dataset from the Action Genome dataset [20] and conduct experiments to demonstrate the benefits of using semantic video scene graphs as condition for video synthesis.", "mention_start": 30, "mention_end": 55, "dataset_mention": "the Action Genome dataset"}, {"mentioned_in_paper": "140", "context_id": "162", "dataset_context": "We split a sub-dataset from the video scene graph dataset Action Genome [20], which is built upon the In-Home dataset Charades [45].", "mention_start": 9, "mention_end": 22, "dataset_mention": "a sub-dataset"}, {"mentioned_in_paper": "140", "context_id": "162", "dataset_context": "We split a sub-dataset from the video scene graph dataset Action Genome [20], which is built upon the In-Home dataset Charades [45].", "mention_start": 28, "mention_end": 57, "dataset_mention": "the video scene graph dataset"}, {"mentioned_in_paper": "140", "context_id": "162", "dataset_context": "We split a sub-dataset from the video scene graph dataset Action Genome [20], which is built upon the In-Home dataset Charades [45].", "mention_start": 97, "mention_end": 117, "dataset_mention": "the In-Home dataset"}, {"mentioned_in_paper": "140", "context_id": "187", "dataset_context": "In addition, when we train the models on the sub-dataset split from Action Genome, VideoGPT does not perform well.", "mention_start": 40, "mention_end": 56, "dataset_mention": "the sub-dataset"}, {"mentioned_in_paper": "140", "context_id": "188", "dataset_context": "We speculate that this is because Action Genome dataset is more complex than the widely-used datasets for video synthesis [14, 46].", "mention_start": 34, "mention_end": 55, "dataset_mention": "Action Genome dataset"}, {"mentioned_in_paper": "140", "context_id": "188", "dataset_context": "We speculate that this is because Action Genome dataset is more complex than the widely-used datasets for video synthesis [14, 46].", "mention_start": 77, "mention_end": 101, "dataset_mention": "the widely-used datasets"}, {"mentioned_in_paper": "140", "context_id": "238", "dataset_context": "We split a sub-dataset from Action Genome [20], which is built upon Charades [45].", "mention_start": 9, "mention_end": 22, "dataset_mention": "a sub-dataset"}, {"mentioned_in_paper": "141", "context_id": "404", "dataset_context": "The project work described to us was not primarily focused around producing large, reliable datasets or peer-reviewed research papers-the typical legitimized products of professional research efforts.", "mention_start": 82, "mention_end": 100, "dataset_mention": " reliable datasets"}, {"mentioned_in_paper": "142", "context_id": "17", "dataset_context": "Other methods leverage the existing datasets to train a model to analyze the difference of the image texture in terms of specular reflection, image blurriness, image chromaticity etc, to distinguish the fraudulent and real images [49, 8].", "mention_start": 0, "mention_end": 44, "dataset_mention": "Other methods leverage the existing datasets"}, {"mentioned_in_paper": "142", "context_id": "73", "dataset_context": "To the best of our knowledge, even so far the largest dataset FER2013 [14] of natural expression collected from the internet has only about 20 thousands images.", "mention_start": 34, "mention_end": 61, "dataset_mention": "so far the largest dataset"}, {"mentioned_in_paper": "142", "context_id": "75", "dataset_context": "Thus the facial expression model based on deep CNNs has normally transferred from the face recognition model pretrained on the relative larger datasets such as CASIA-Webface [44], MSCeleb-1M [15], etc.", "mention_start": 123, "mention_end": 151, "dataset_mention": "the relative larger datasets"}, {"mentioned_in_paper": "142", "context_id": "130", "dataset_context": "Normally the largescale datasets used for training face recognition do not include the facial attributes such as facial expressions or only having some simple attributes such as smile, moustache, gender etc., as in Celeb-A [24].", "mention_start": 0, "mention_end": 32, "dataset_mention": "Normally the largescale datasets"}, {"mentioned_in_paper": "142", "context_id": "133", "dataset_context": "The other facial expression datasets such as FER2013 [14] collecting the images from internet in a wild condition has relative large size of images (more than 35k images), however it does not include the identity information.", "mention_start": 0, "mention_end": 36, "dataset_mention": "The other facial expression datasets"}, {"mentioned_in_paper": "142", "context_id": "148", "dataset_context": "As far we know, we are the first to evaluate the state-of-art face verification algorithms trained on general large-scale datasets with facial expression dataset.", "mention_start": 135, "mention_end": 161, "dataset_mention": "facial expression dataset"}, {"mentioned_in_paper": "142", "context_id": "151", "dataset_context": "By fine-tuning our pretrained CNNs-based model with the facial expression datasets, the performance can obviously be improved (evaluating by the 3, we can see that on CK+ the single-task fine-tuning model has not essentially improved the performance while the proposed multi-task model is evidently better than the other two models.", "mention_start": 52, "mention_end": 82, "dataset_mention": "the facial expression datasets"}, {"mentioned_in_paper": "142", "context_id": "154", "dataset_context": "b) facial expression performance Table 4 compares the proposed multi-task networks for facial expression recognition with other methods on CK+ and OuluCASIA datasets.", "mention_start": 139, "mention_end": 165, "dataset_mention": "CK+ and OuluCASIA datasets"}, {"mentioned_in_paper": "142", "context_id": "158", "dataset_context": "The evaluation of proposed multi-task networks for facial expression recognition task on CK+ and OuluCASIA datasets.", "mention_start": 89, "mention_end": 115, "dataset_mention": "CK+ and OuluCASIA datasets"}, {"mentioned_in_paper": "143", "context_id": "48", "dataset_context": "Importantly, this strategy can train lesion trackers using images from only one time point, meaning non-longitudinal datasets can be used, which are more read-ily collected.", "mention_start": 99, "mention_end": 125, "dataset_mention": "non-longitudinal datasets"}, {"mentioned_in_paper": "143", "context_id": "186", "dataset_context": "In total, our publicly released deep longitudinal study (DLS) dataset inherits 3008, 403, and 480 lesion pairs from the DeepLesion's train, validate, and test splits, respectively.", "mention_start": 55, "mention_end": 69, "dataset_mention": "(DLS) dataset"}, {"mentioned_in_paper": "143", "context_id": "189", "dataset_context": "We apply the best DLT configuration, developed on the DeepLesion dataset, to track the corresponding target lesions, if they exist.", "mention_start": 49, "mention_end": 72, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "143", "context_id": "236", "dataset_context": "Among the top 3 methods, DLT and DLT-Mix run about 4 times faster than DEEDS on the DeepLesion dataset.", "mention_start": 79, "mention_end": 102, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "143", "context_id": "279", "dataset_context": "These results also underscore the value of our DLS dataset.", "mention_start": 43, "mention_end": 58, "dataset_mention": "our DLS dataset"}, {"mentioned_in_paper": "143", "context_id": "288", "dataset_context": "We benchmark the task of lesion tracking on our DLS dataset which will be made available upon request.", "mention_start": 44, "mention_end": 59, "dataset_mention": "our DLS dataset"}, {"mentioned_in_paper": "145", "context_id": "4", "dataset_context": "Experiments on SVHN, scene text recognition and ImageNet dataset demonstrate that we can achieve 3.3\u00d7 speedup or 3.6\u00d7 parameter reduction with less than 1% drop in accuracy, showing the effectiveness and efficiency of our method.", "mention_start": 20, "mention_end": 64, "dataset_mention": " scene text recognition and ImageNet dataset"}, {"mentioned_in_paper": "145", "context_id": "5", "dataset_context": "Moreover, the computation efficiency of Kronecker layer makes using larger feature map possible, which in turn enables us to outperform the previous state-of-the-art on both SVHN(digit recognition) and CASIA-HWDB (handwritten Chinese character recognition) datasets.", "mention_start": 168, "mention_end": 265, "dataset_mention": "both SVHN(digit recognition) and CASIA-HWDB (handwritten Chinese character recognition) datasets"}, {"mentioned_in_paper": "145", "context_id": "164", "dataset_context": "CASIA-HWDB Liu et al. ( 2011) is an offline Chinese handwriting dataset, containing more than 3 million character samples of over 7,000 character classes, produced by over 1,000 writers.", "mention_start": 33, "mention_end": 71, "dataset_mention": "an offline Chinese handwriting dataset"}, {"mentioned_in_paper": "145", "context_id": "181", "dataset_context": "We also experiment on the word recognition model trained on the synthetic word dataset consisting of 9 million images covering about 90k English words from Jaderberg et al. (2014a) and tested on ICDAR 2013Karatzas et al. (2013) dataset.", "mention_start": 60, "mention_end": 86, "dataset_mention": "the synthetic word dataset"}, {"mentioned_in_paper": "145", "context_id": "181", "dataset_context": "We also experiment on the word recognition model trained on the synthetic word dataset consisting of 9 million images covering about 90k English words from Jaderberg et al. (2014a) and tested on ICDAR 2013Karatzas et al. (2013) dataset.", "mention_start": 195, "mention_end": 235, "dataset_mention": "ICDAR 2013Karatzas et al. (2013) dataset"}, {"mentioned_in_paper": "146", "context_id": "3", "dataset_context": "The proposed algorithm has verified the effectiveness by achieving state-of-the-art performance on the image retrieval benchmark datasets  Stanford online product, and Inshop).", "mention_start": 99, "mention_end": 137, "dataset_mention": "the image retrieval benchmark datasets"}, {"mentioned_in_paper": "146", "context_id": "7", "dataset_context": "In particular, deep neural network architectures have evolved by targeting large datasets such as ImageNet (Deng et al. 2009).", "mention_start": 64, "mention_end": 89, "dataset_mention": "targeting large datasets"}, {"mentioned_in_paper": "146", "context_id": "26", "dataset_context": "Through ablation experiments on four image retrieval benchmark datasets, it is verified that the proposed method consistently improve the retrieval performance.", "mention_start": 32, "mention_end": 71, "dataset_mention": "four image retrieval benchmark datasets"}, {"mentioned_in_paper": "146", "context_id": "34", "dataset_context": "In the computer vision fields, Ima-geNet (Deng et al. 2009) dataset has been widely used for a source task.", "mention_start": 30, "mention_end": 67, "dataset_mention": " Ima-geNet (Deng et al. 2009) dataset"}, {"mentioned_in_paper": "146", "context_id": "67", "dataset_context": "The investigation has been conducted on a retrieval dataset CUB-200-2011 (Wah et al. 2011) with ResNet-50 (He et al. 2016).", "mention_start": 40, "mention_end": 59, "dataset_mention": "a retrieval dataset"}, {"mentioned_in_paper": "146", "context_id": "149", "dataset_context": "In this section, we provide the investigation results for the hyper-parameters \u03b1 and \u03b2 on the  As shown in Figure 3-(a), the best performance on CUB-200 dataset is achieved when the \u03b1 and \u03b2 are set to values around 2 and 0.4, respectively.", "mention_start": 144, "mention_end": 160, "dataset_mention": "CUB-200 dataset"}, {"mentioned_in_paper": "146", "context_id": "150", "dataset_context": "In the case of Cars-196 dataset, the setting \u03b1 and \u03b2 to 2 and 0.4 provides relatively high performance as shown in Figure 3-(b) but the setting \u03b1 and \u03b2 to 3 and 0.6 provides the best performance.", "mention_start": 15, "mention_end": 31, "dataset_mention": "Cars-196 dataset"}, {"mentioned_in_paper": "146", "context_id": "159", "dataset_context": "In the case of Inshop dataset, the performance does not improve much, but the pruning of two layers contributes to a reduction in the network complexity while maintaining comparable performance.", "mention_start": 15, "mention_end": 29, "dataset_mention": "Inshop dataset"}, {"mentioned_in_paper": "146", "context_id": "190", "dataset_context": "As shown in Table 3, pruning 12, 13 layers shows the best performance on the CUB-200 dataset.", "mention_start": 72, "mention_end": 92, "dataset_mention": "the CUB-200 dataset"}, {"mentioned_in_paper": "146", "context_id": "205", "dataset_context": "The effectiveness and efficiency of the proposed algorithm has been validated by the experiments on four image retrieval benchmark datasets.", "mention_start": 100, "mention_end": 139, "dataset_mention": "four image retrieval benchmark datasets"}, {"mentioned_in_paper": "148", "context_id": "7", "dataset_context": "We conduct experiments on PASCAL VOC and MS-COCO dataset with both synthetic noise and machine-generated noise.", "mention_start": 26, "mention_end": 56, "dataset_mention": "PASCAL VOC and MS-COCO dataset"}, {"mentioned_in_paper": "148", "context_id": "178", "dataset_context": "Table 2 shows the comparison results on PASCAL VOC dataset, where the training data contains different mixtures of label noise and bbox noise.", "mention_start": 40, "mention_end": 58, "dataset_mention": "PASCAL VOC dataset"}, {"mentioned_in_paper": "148", "context_id": "190", "dataset_context": "Table 3 shows the results on MS-COCO dataset.", "mention_start": 29, "mention_end": 44, "dataset_mention": "MS-COCO dataset"}, {"mentioned_in_paper": "149", "context_id": "128", "dataset_context": "In this section, we evaluated and compared our MRN with state-ofthe-art approaches on two few-shot learning benchmark datasets, i.e. miniImagenet [34] and tieredImagenet [28].", "mention_start": 85, "mention_end": 126, "dataset_mention": "two few-shot learning benchmark datasets"}, {"mentioned_in_paper": "149", "context_id": "147", "dataset_context": "We validate the effectiveness of the proposed MRN on standard miniImagenet and tieredImagenet datasets, and report the main results in Table 1.", "mention_start": 53, "mention_end": 102, "dataset_mention": "standard miniImagenet and tieredImagenet datasets"}, {"mentioned_in_paper": "149", "context_id": "158", "dataset_context": "We conjecture this is primarily due to there existing more categories and more training examples in the training split of tieredImagenet dataset which in turn presents richer intra-class variations that facilitates model training.", "mention_start": 122, "mention_end": 144, "dataset_mention": "tieredImagenet dataset"}, {"mentioned_in_paper": "150", "context_id": "3", "dataset_context": "To address these issues, we present a new driver attention dataset, Co-CAtt (Cognitive-Conditioned Attention).", "mention_start": 35, "mention_end": 66, "dataset_mention": "a new driver attention dataset"}, {"mentioned_in_paper": "150", "context_id": "4", "dataset_context": "Unlike previous driver attention datasets, CoCAtt includes per-frame annotations that describe the distraction state and intention of the driver.", "mention_start": 7, "mention_end": 41, "dataset_mention": "previous driver attention datasets"}, {"mentioned_in_paper": "150", "context_id": "8", "dataset_context": "Furthermore, CoCAtt is currently the largest and the most diverse driver attention dataset in terms of autonomy levels, eye tracker resolutions, and driving scenarios.", "mention_start": 22, "mention_end": 90, "dataset_mention": "currently the largest and the most diverse driver attention dataset"}, {"mentioned_in_paper": "150", "context_id": "23", "dataset_context": "Specifically, many of the existing driver attention datasets use highresolution but expensive eye trackers, e.g., the SMI ETG 2w Eye Tracking Glasses [1], to record driver gaze [7, 31, 43].", "mention_start": 21, "mention_end": 60, "dataset_mention": "the existing driver attention datasets"}, {"mentioned_in_paper": "150", "context_id": "26", "dataset_context": "To address these issues, we propose a new driver attention dataset, CoCAtt (Cognitive-Conditioned Attention), with 11.88 hours of driver attention data, approximately twice the size of the current largest driver attention dataset, DADA-2000 [7].", "mention_start": 35, "mention_end": 66, "dataset_mention": "a new driver attention dataset"}, {"mentioned_in_paper": "150", "context_id": "26", "dataset_context": "To address these issues, we propose a new driver attention dataset, CoCAtt (Cognitive-Conditioned Attention), with 11.88 hours of driver attention data, approximately twice the size of the current largest driver attention dataset, DADA-2000 [7].", "mention_start": 184, "mention_end": 229, "dataset_mention": "the current largest driver attention dataset"}, {"mentioned_in_paper": "150", "context_id": "34", "dataset_context": "With 6 hours of eye-tracking data, the DR(eye)VE dataset is the only dataset collected in-car and also provides distraction-related annotations for 20% of its frames [31].", "mention_start": 34, "mention_end": 56, "dataset_mention": " the DR(eye)VE dataset"}, {"mentioned_in_paper": "150", "context_id": "37", "dataset_context": "The second is that DR(eye)VE dataset has few interesting scenario with only one car per frame on average [43].", "mention_start": 14, "mention_end": 36, "dataset_mention": "that DR(eye)VE dataset"}, {"mentioned_in_paper": "150", "context_id": "38", "dataset_context": "The scenario issue is addressed by the Berkeley Deep-Drive Attention dataset (BDD-A), which contains many diverse driving scenarios in critical situations [43].", "mention_start": 35, "mention_end": 76, "dataset_mention": "the Berkeley Deep-Drive Attention dataset"}, {"mentioned_in_paper": "150", "context_id": "39", "dataset_context": "The attention data in the BDD-A dataset was collected in an in-lab setup, in which participants were asked to imagine themselves as the driver in a driver-perspective video.", "mention_start": 22, "mention_end": 39, "dataset_mention": "the BDD-A dataset"}, {"mentioned_in_paper": "150", "context_id": "60", "dataset_context": "We present a cognitive-conditioned driver attention dataset, CoCAtt, collected in the CARLA simulation environment [5].", "mention_start": 11, "mention_end": 59, "dataset_mention": "a cognitive-conditioned driver attention dataset"}, {"mentioned_in_paper": "150", "context_id": "61", "dataset_context": "Our dataset contains 11.88 hours of driving data, twice the total duration of the current largest driver attention dataset, DADA-2000 [7].", "mention_start": 77, "mention_end": 122, "dataset_mention": "the current largest driver attention dataset"}, {"mentioned_in_paper": "150", "context_id": "62", "dataset_context": "We provide a comparison of the statistics between our dataset and previous driver attention datasets in Tables 1 and 2. The detailed statistics of our dataset are listed in Table 3.", "mention_start": 50, "mention_end": 100, "dataset_mention": "our dataset and previous driver attention datasets"}, {"mentioned_in_paper": "150", "context_id": "64", "dataset_context": "1. CoCAtt is the first cognitive-conditioned driver attention dataset that provides per-frame annotations of driver state, including driver distraction and intention.", "mention_start": 13, "mention_end": 69, "dataset_mention": "the first cognitive-conditioned driver attention dataset"}, {"mentioned_in_paper": "150", "context_id": "65", "dataset_context": "2. CoCAtt is the first driver attention dataset that contains driver gaze data in both automated driving (SAE Level 3) and manual driving settings.", "mention_start": 13, "mention_end": 47, "dataset_mention": "the first driver attention dataset"}, {"mentioned_in_paper": "150", "context_id": "67", "dataset_context": "The first contribution aims to provide a robust dataset that enables driver attention models to incorporate cognitiveconditioned learning into their framework.", "mention_start": 39, "mention_end": 55, "dataset_mention": "a robust dataset"}, {"mentioned_in_paper": "150", "context_id": "148", "dataset_context": "We conduct rigorous experiments with the baseline models on our CoCAtt dataset for both of the proposed tasks.", "mention_start": 60, "mention_end": 78, "dataset_mention": "our CoCAtt dataset"}, {"mentioned_in_paper": "150", "context_id": "205", "dataset_context": "Our main contribution is a large-scale driver attention dataset, which aims to fill in the data gap of missing driver internal states in the existing driver attention dataset.", "mention_start": 25, "mention_end": 63, "dataset_mention": "a large-scale driver attention dataset"}, {"mentioned_in_paper": "150", "context_id": "205", "dataset_context": "Our main contribution is a large-scale driver attention dataset, which aims to fill in the data gap of missing driver internal states in the existing driver attention dataset.", "mention_start": 136, "mention_end": 174, "dataset_mention": "the existing driver attention dataset"}, {"mentioned_in_paper": "151", "context_id": "49", "dataset_context": "3. Experiments on real-life and benchmark datasets show the effectiveness and efficiency of the algorithm in terms of running time and intermediate result size.", "mention_start": 18, "mention_end": 50, "dataset_mention": "real-life and benchmark datasets"}, {"mentioned_in_paper": "151", "context_id": "260", "dataset_context": "In this section, we experimentally evaluate the performance of the proposed algorithms and CMJoin with four real-life and benchmark data sets.", "mention_start": 102, "mention_end": 141, "dataset_mention": "four real-life and benchmark data sets"}, {"mentioned_in_paper": "151", "context_id": "285", "dataset_context": "Specifically in queries Q1-Q6, CMJoin, SJ, VJ, and EH perform better than PG, as the original tree is deeply recursive in the TreeBank dataset [38], and designed tree pattern queries are complex.", "mention_start": 121, "mention_end": 142, "dataset_mention": "the TreeBank dataset"}, {"mentioned_in_paper": "151", "context_id": "294", "dataset_context": "The reason is that in the Xmark dataset [35], the tree data are flat and with less matching results in twig queries.", "mention_start": 22, "mention_end": 39, "dataset_mention": "the Xmark dataset"}, {"mentioned_in_paper": "151", "context_id": "309", "dataset_context": "In contrast in Q16-Q21, it involves XML, JSON and relational data from the UniBench dataset [40].", "mention_start": 70, "mention_end": 91, "dataset_mention": "the UniBench dataset"}, {"mentioned_in_paper": "151", "context_id": "329", "dataset_context": "The evaluation shows that it performs efficiently and stably in dynamical datasets, with various queries and it also scales well.", "mention_start": 64, "mention_end": 82, "dataset_mention": "dynamical datasets"}, {"mentioned_in_paper": "155", "context_id": "116", "dataset_context": "In this section, we will show that IQL can recover the optimal value function under the dataset support constraints.", "mention_start": 29, "mention_end": 95, "dataset_mention": "that IQL can recover the optimal value function under the dataset"}, {"mentioned_in_paper": "155", "context_id": "183", "dataset_context": "We obtained results for \"-v2\" datasets using an author-suggested implementation. 4", "mention_start": 24, "mention_end": 38, "dataset_mention": "\"-v2\" datasets"}, {"mentioned_in_paper": "155", "context_id": "220", "dataset_context": "Following the suggestions of the authors of the dataset, we subtract 1 from rewards for the Ant Maze datasets.", "mention_start": 87, "mention_end": 109, "dataset_mention": "the Ant Maze datasets"}, {"mentioned_in_paper": "156", "context_id": "2", "dataset_context": "We propose a simple yet effective method for leveraging these image priors to improve semantic segmentation of images from sequential driving datasets.", "mention_start": 123, "mention_end": 150, "dataset_mention": "sequential driving datasets"}, {"mentioned_in_paper": "156", "context_id": "57", "dataset_context": "Each model is trained using the CamVid road scene dataset [11] which contains several driving sequences with object class semantic labels, collected at various times of the day.", "mention_start": 28, "mention_end": 57, "dataset_mention": "the CamVid road scene dataset"}, {"mentioned_in_paper": "156", "context_id": "86", "dataset_context": "There we show a generic scheme for incorporating scene graphs from the prior, where objects bounding boxes from the semantic segmentation prior are structured as a scene graph (similar to how synthetic scene graphs are generated with the COCO dataset for image synthesis [16]).", "mention_start": 233, "mention_end": 250, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "158", "context_id": "97", "dataset_context": "We evaluate our method on two datasets with annotated human body joints and two datasets of wild animals.", "mention_start": 44, "mention_end": 88, "dataset_mention": "annotated human body joints and two datasets"}, {"mentioned_in_paper": "158", "context_id": "98", "dataset_context": "MPII Human Pose dataset (Andriluka et al., 2014) ATRW (Li et al., 2019) is a dataset of Amur tigers images captured in multiple wild zoos in unconstrained settings.", "mention_start": 0, "mention_end": 23, "dataset_mention": "MPII Human Pose dataset"}, {"mentioned_in_paper": "158", "context_id": "108", "dataset_context": "The PCK@0.1 (\u03b1 = 0.1) score is reported for LSP, CUB-200-2011 and ATRW datasets.", "mention_start": 48, "mention_end": 79, "dataset_mention": " CUB-200-2011 and ATRW datasets"}, {"mentioned_in_paper": "158", "context_id": "130", "dataset_context": "The baseline performance decreases significantly when the amount of training data is reduced on human poses and tigers datasets (Table 1).", "mention_start": 96, "mention_end": 127, "dataset_mention": "human poses and tigers datasets"}, {"mentioned_in_paper": "158", "context_id": "131", "dataset_context": "On birds dataset, we observe only a small decrease in the baseline score (Table 1).", "mention_start": 3, "mention_end": 16, "dataset_mention": "birds dataset"}, {"mentioned_in_paper": "158", "context_id": "137", "dataset_context": "For example, our method increases the score from 40% to 66% on LSP dataset with 5% of labeled data.", "mention_start": 62, "mention_end": 74, "dataset_mention": "LSP dataset"}, {"mentioned_in_paper": "158", "context_id": "138", "dataset_context": "On the challenging tigers dataset, our approach reaches the score of 92% trained with only 5% labeled examples when the supervised model shows the score 69% while trained on the same labeled data.", "mention_start": 19, "mention_end": 33, "dataset_mention": "tigers dataset"}, {"mentioned_in_paper": "158", "context_id": "156", "dataset_context": "We investigate the influence of different loss components of our methods on LSP dataset (Table 2).", "mention_start": 76, "mention_end": 87, "dataset_mention": "LSP dataset"}, {"mentioned_in_paper": "158", "context_id": "164", "dataset_context": "We replaced our TE loss with an inverse transformation loss of Honari et al. (2018) in our framework, and applied it on ATRW and CUB-200-2011 datasets with 20% of labeled data.", "mention_start": 119, "mention_end": 150, "dataset_mention": "ATRW and CUB-200-2011 datasets"}, {"mentioned_in_paper": "158", "context_id": "172", "dataset_context": "Experiments on the LSP dataset show a decrease of 1-2% in the score for all cases when ground truth heatmaps are not used (Table 4).", "mention_start": 15, "mention_end": 30, "dataset_mention": "the LSP dataset"}, {"mentioned_in_paper": "158", "context_id": "177", "dataset_context": "Experiments on LSP dataset show that our method is not sensitive to variations of TE (\u03bb 3 ) and TI (\u03bb 4 ) losses (Figure 3).", "mention_start": 15, "mention_end": 26, "dataset_mention": "LSP dataset"}, {"mentioned_in_paper": "158", "context_id": "184", "dataset_context": "Moreover, it reaches the same performance as the model trained on the whole labeled dataset with only 10% of labeled images on tigers ATRW dataset and with 50% labeled images on challenging human poses LSP dataset.", "mention_start": 126, "mention_end": 146, "dataset_mention": "tigers ATRW dataset"}, {"mentioned_in_paper": "158", "context_id": "184", "dataset_context": "Moreover, it reaches the same performance as the model trained on the whole labeled dataset with only 10% of labeled images on tigers ATRW dataset and with 50% labeled images on challenging human poses LSP dataset.", "mention_start": 189, "mention_end": 213, "dataset_mention": "human poses LSP dataset"}, {"mentioned_in_paper": "160", "context_id": "5", "dataset_context": "We first sanity check our idea using two synthetic datasets with a known ground truth, and further demonstrate with a benchmark natural image dataset.", "mention_start": 115, "mention_end": 149, "dataset_mention": "a benchmark natural image dataset"}, {"mentioned_in_paper": "160", "context_id": "18", "dataset_context": "Let our dataset be the junction of a set of n samples with d features X \u2208 R n\u00d7d and associated labels y \u2208 {0, 1} n .", "mention_start": 0, "mention_end": 15, "dataset_mention": "Let our dataset"}, {"mentioned_in_paper": "160", "context_id": "41", "dataset_context": "In our work, we propose and test many baselines across model architectures and datasets.", "mention_start": 54, "mention_end": 87, "dataset_mention": "model architectures and datasets"}, {"mentioned_in_paper": "160", "context_id": "96", "dataset_context": "Concretely, we train CAVs on bootstrapped datasets (n=100 resamples).", "mention_start": 28, "mention_end": 50, "dataset_mention": "bootstrapped datasets"}, {"mentioned_in_paper": "160", "context_id": "104", "dataset_context": "To validate our approach, we compare global ICS results to the original formulation of TCAV on two synthetic datasets with ground truth in Section 3.1 and illustrate local explanations using ICS on a benchmark imaging dataset in Section 3.2.", "mention_start": 197, "mention_end": 225, "dataset_mention": "a benchmark imaging dataset"}, {"mentioned_in_paper": "160", "context_id": "107", "dataset_context": "To validate our approach, we use a simple synthetic dataset along with semi-natural images that are synthetically generated.", "mention_start": 32, "mention_end": 59, "dataset_mention": "a simple synthetic dataset"}, {"mentioned_in_paper": "160", "context_id": "109", "dataset_context": "We describe the metrics and datasets used to quantitatively illustrate the performance of TCAV ICS over TCAV sign(CS) .", "mention_start": 12, "mention_end": 36, "dataset_mention": "the metrics and datasets"}, {"mentioned_in_paper": "160", "context_id": "136", "dataset_context": "Using BARS, we showcase the effectiveness of the local explanation aspect of ICS. Figure 2a illustrates this on 4 local examples of the BARS dataset for F o .", "mention_start": 131, "mention_end": 148, "dataset_mention": "the BARS dataset"}, {"mentioned_in_paper": "160", "context_id": "138", "dataset_context": "To showcase the global aspect of ICS, we compare TCAV sign(CS) and TCAV ICS for both BARS and BAM datasets.", "mention_start": 79, "mention_end": 106, "dataset_mention": "both BARS and BAM datasets"}, {"mentioned_in_paper": "160", "context_id": "153", "dataset_context": "We use the ImageNet dataset [DDS + 09] to generate some of the concepts demonstrated in [KWG + 18] and replicate results at the global level.", "mention_start": 7, "mention_end": 27, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "160", "context_id": "285", "dataset_context": "2. Compute the forward passes of F 1 and F 2 on these images to obtain two datasets of activations: D i , \u2200i \u2208 {1, 2} 3.", "mention_start": 64, "mention_end": 83, "dataset_mention": "obtain two datasets"}, {"mentioned_in_paper": "160", "context_id": "286", "dataset_context": "For all models, layers, and concepts, compute K unit CAVs by sampling with replacement N samples from D i to obtain K bootstrapped datasets D * i,k , \u2200k \u2208 {1, K}. 4. Compute bootstrapped TCAV scores for all models, layers, and concepts using the trained CAVs.", "mention_start": 108, "mention_end": 139, "dataset_mention": "obtain K bootstrapped datasets"}, {"mentioned_in_paper": "160", "context_id": "288", "dataset_context": "In the BARS dataset, it is possible to evaluate how much the model relies on a given concept by evaluating the model on counterfactual examples [GSK19].", "mention_start": 3, "mention_end": 19, "dataset_mention": "the BARS dataset"}, {"mentioned_in_paper": "160", "context_id": "295", "dataset_context": "The BARS dataset being very simple, our models (trained to detect either the color or the orientation) reach 100% test accuracy after only a few epochs.", "mention_start": 0, "mention_end": 16, "dataset_mention": "The BARS dataset"}, {"mentioned_in_paper": "160", "context_id": "299", "dataset_context": "In the BARS dataset, we computed the predicted probability associated with all baselines (see Table 2 for a selected sample).", "mention_start": 3, "mention_end": 19, "dataset_mention": "the BARS dataset"}, {"mentioned_in_paper": "161", "context_id": "6", "dataset_context": "(5) Experiments on both real-life and synthetic data sets demonstrate the effectiveness and efficiency of our algorithm, from several times to orders of magnitude faster than state-of-theart algorithms in terms of evaluation time, even for traditional tree pattern queries with only conjunctive operations.", "mention_start": 19, "mention_end": 57, "dataset_mention": "both real-life and synthetic data sets"}, {"mentioned_in_paper": "161", "context_id": "512", "dataset_context": "We generated five XMark datasets with the scaling factors from 0.5 to 4. For each dataset, we generate a graph, where nodes correspond to XML elements and edges represent the internal links (parent-child) and ID/IDREF links.", "mention_start": 13, "mention_end": 32, "dataset_mention": "five XMark datasets"}, {"mentioned_in_paper": "161", "context_id": "538", "dataset_context": "Fig. 8 (b) shows the results on the XMark dataset of scale 0.5 for different queries.", "mention_start": 32, "mention_end": 49, "dataset_mention": "the XMark dataset"}, {"mentioned_in_paper": "161", "context_id": "627", "dataset_context": "In this section, we present the experimental results for GTPQs with the same structure (Fig. 11) on the XMark data set with scale factor 4. Since HGJoin and TwigStackD need to do the same decompose-and-merge operations to process GTPQs and our experiments for conjunctive queries have shown that TwigStackD significantly outperforms HGJoin, we did not include HGJoin in this set of experiments.", "mention_start": 99, "mention_end": 118, "dataset_mention": "the XMark data set"}, {"mentioned_in_paper": "162", "context_id": "156", "dataset_context": "In this section we provide a detailed analysis of the performance of CNN activations on the MIT Indoor dataset [71], from which we are able to deduce two important properties thereof.", "mention_start": 88, "mention_end": 110, "dataset_mention": "the MIT Indoor dataset"}, {"mentioned_in_paper": "162", "context_id": "270", "dataset_context": "For extracting CNN activations from image patches, we consider two state-of-the-art CNN models which are both pre-trained on the ImageNet dataset [21].", "mention_start": 124, "mention_end": 145, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "162", "context_id": "276", "dataset_context": "Pascal VOC 2007 dataset.", "mention_start": 0, "mention_end": 23, "dataset_mention": "Pascal VOC 2007 dataset"}, {"mentioned_in_paper": "162", "context_id": "277", "dataset_context": "The Pascal VOC 2007 dataset [28, 29] contains a total of 9,963 images from 20 object classes, including 5,011 images for training and validation, and 4,952 for testing.", "mention_start": 0, "mention_end": 27, "dataset_mention": "The Pascal VOC 2007 dataset"}, {"mentioned_in_paper": "162", "context_id": "279", "dataset_context": "Pascal VOC 2012 dataset.", "mention_start": 0, "mention_end": 23, "dataset_mention": "Pascal VOC 2012 dataset"}, {"mentioned_in_paper": "162", "context_id": "280", "dataset_context": "The Pascal VOC 2012 dataset [28, 29] is an extension of the VOC 2007 dataset, which contains a total of 22,531 images from 20 object classes, including 11,540 images for training and validation, and 10,991 for testing.", "mention_start": 0, "mention_end": 27, "dataset_mention": "The Pascal VOC 2012 dataset"}, {"mentioned_in_paper": "162", "context_id": "280", "dataset_context": "The Pascal VOC 2012 dataset [28, 29] is an extension of the VOC 2007 dataset, which contains a total of 22,531 images from 20 object classes, including 11,540 images for training and validation, and 10,991 for testing.", "mention_start": 55, "mention_end": 76, "dataset_mention": "the VOC 2007 dataset"}, {"mentioned_in_paper": "162", "context_id": "282", "dataset_context": "MIT Indoor dataset.", "mention_start": 0, "mention_end": 18, "dataset_mention": "MIT Indoor dataset"}, {"mentioned_in_paper": "162", "context_id": "283", "dataset_context": "The MIT Indoor dataset [71] contains 67 classes of indoors scenes.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The MIT Indoor dataset"}, {"mentioned_in_paper": "162", "context_id": "287", "dataset_context": "The evaluation metric for MIT Indoor dataset is the mean classification accuracy.", "mention_start": 26, "mention_end": 44, "dataset_mention": "MIT Indoor dataset"}, {"mentioned_in_paper": "162", "context_id": "289", "dataset_context": "When mining mid-level visual elements, only training images are used to create transactions (trainval set for Pascal VOC datasets).", "mention_start": 109, "mention_end": 129, "dataset_mention": "Pascal VOC datasets"}, {"mentioned_in_paper": "162", "context_id": "294", "dataset_context": "For generating image features for classification, CNN activations are extracted from five scales for the Pascal VOC datasets as compared to three scales for the MIT Indoor dataset (we experimentally found using more than three scales for MIT Indoor does not improve the overall classification performance.", "mention_start": 100, "mention_end": 124, "dataset_mention": "the Pascal VOC datasets"}, {"mentioned_in_paper": "162", "context_id": "294", "dataset_context": "For generating image features for classification, CNN activations are extracted from five scales for the Pascal VOC datasets as compared to three scales for the MIT Indoor dataset (we experimentally found using more than three scales for MIT Indoor does not improve the overall classification performance.", "mention_start": 156, "mention_end": 179, "dataset_mention": "the MIT Indoor dataset"}, {"mentioned_in_paper": "162", "context_id": "301", "dataset_context": "In this section, we provide a detailed analysis of the proposed system for object classification on the Pascal VOC 2007 and 2012 datasets.", "mention_start": 99, "mention_end": 137, "dataset_mention": "the Pascal VOC 2007 and 2012 datasets"}, {"mentioned_in_paper": "162", "context_id": "309", "dataset_context": "On VOC 2007 dataset, the conf min (Eq.", "mention_start": 3, "mention_end": 19, "dataset_mention": "VOC 2007 dataset"}, {"mentioned_in_paper": "162", "context_id": "311", "dataset_context": "On VOC 2012 dataset, we use 40% for conf min when VGG-VD model is adopted.", "mention_start": 3, "mention_end": 19, "dataset_mention": "VOC 2012 dataset"}, {"mentioned_in_paper": "162", "context_id": "344", "dataset_context": "On the VOC 2007 dataset, when using the VGG-VD model and 50 elements per category, this framework gives 85.0% mAP, which is lower than that of our pattern selection method (86.2%) and pattern merging method (87.3%).", "mention_start": 3, "mention_end": 23, "dataset_mention": "the VOC 2007 dataset"}, {"mentioned_in_paper": "162", "context_id": "356", "dataset_context": "As for the VOC 2012 dataset, as shown in Table 5, when using the VGG-VD CNN model and 50 elements per category, the proposed BoE representation reaches a mAP of 85.5%, outperforming most state-of-the-art methods.", "mention_start": 7, "mention_end": 27, "dataset_mention": "the VOC 2012 dataset"}, {"mentioned_in_paper": "162", "context_id": "358", "dataset_context": "Comparison of classification results on the Pascal VOC 2012 dataset.", "mention_start": 40, "mention_end": 67, "dataset_mention": "the Pascal VOC 2012 dataset"}, {"mentioned_in_paper": "162", "context_id": "360", "dataset_context": "of the VOC 2007 dataset in Fig. 7.", "mention_start": 3, "mention_end": 23, "dataset_mention": "the VOC 2007 dataset"}, {"mentioned_in_paper": "162", "context_id": "368", "dataset_context": "The recent work of [67], for example, takes 5 days to find mid-level elements on the MIT Indoor dataset.", "mention_start": 80, "mention_end": 103, "dataset_mention": "the MIT Indoor dataset"}, {"mentioned_in_paper": "162", "context_id": "370", "dataset_context": "Thus, for approximately 0.2 million transactions created from CNN activations of image patches on the Pascal VOC 2007 dataset, association rule mining takes only 23 seconds to discover representative and discriminative patterns.", "mention_start": 97, "mention_end": 125, "dataset_mention": "the Pascal VOC 2007 dataset"}, {"mentioned_in_paper": "162", "context_id": "375", "dataset_context": "We now provide detailed analysis of the proposed system for the task of scene classification on the MIT Indoor dataset.", "mention_start": 96, "mention_end": 118, "dataset_mention": "the MIT Indoor dataset"}, {"mentioned_in_paper": "162", "context_id": "404", "dataset_context": "For this purpose, we first fine-tuned the VGG-VD model on the MIT Indoor dataset with a learning rate of 0.0005.", "mention_start": 57, "mention_end": 80, "dataset_mention": "the MIT Indoor dataset"}, {"mentioned_in_paper": "162", "context_id": "407", "dataset_context": "The underlying reason is probably due to the small training data size of the MIT Indoor dataset and the large capacity of the VGG-VD model.", "mention_start": 73, "mention_end": 95, "dataset_mention": "the MIT Indoor dataset"}, {"mentioned_in_paper": "162", "context_id": "410", "dataset_context": "We visualize some visual elements discovered and their firings on test images of the MIT Indoor dataset in Fig. 8.", "mention_start": 81, "mention_end": 103, "dataset_mention": "the MIT Indoor dataset"}, {"mentioned_in_paper": "162", "context_id": "417", "dataset_context": "In this section, we give answer to this question based on the Pascal VOC07 dataset which has ground truth bounding boxes annotations.", "mention_start": 57, "mention_end": 82, "dataset_mention": "the Pascal VOC07 dataset"}, {"mentioned_in_paper": "162", "context_id": "419", "dataset_context": "For this purpose, we leverage the test set of the segmentation challenge of the Pascal VOC 2007 dataset in which per-pixel labeling is available.", "mention_start": 75, "mention_end": 103, "dataset_mention": "the Pascal VOC 2007 dataset"}, {"mentioned_in_paper": "163", "context_id": "31", "dataset_context": "Using this pretext task, MoCo shows competitive results under the common protocol of linear classification in the ImageNet dataset [11].", "mention_start": 109, "mention_end": 130, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "163", "context_id": "307", "dataset_context": "This shows that MoCo can perform well on this large-scale, relatively uncurated dataset.", "mention_start": 58, "mention_end": 87, "dataset_mention": " relatively uncurated dataset"}, {"mentioned_in_paper": "163", "context_id": "309", "dataset_context": "Our method has shown positive results of unsupervised learning in a variety of computer vision tasks and datasets.", "mention_start": 79, "mention_end": 113, "dataset_mention": "computer vision tasks and datasets"}, {"mentioned_in_paper": "164", "context_id": "39", "dataset_context": "Besides, we build a new dataset named MMP-Retrieval upon the MMPTRACK dataset for unseen-domain gener-alization evaluation, released at https://iccv2021mmp.github.io/subpage/dataset.html.", "mention_start": 56, "mention_end": 77, "dataset_mention": "the MMPTRACK dataset"}, {"mentioned_in_paper": "164", "context_id": "39", "dataset_context": "Besides, we build a new dataset named MMP-Retrieval upon the MMPTRACK dataset for unseen-domain gener-alization evaluation, released at https://iccv2021mmp.github.io/subpage/dataset.html.", "mention_start": 142, "mention_end": 181, "dataset_mention": "//iccv2021mmp.github.io/subpage/dataset"}, {"mentioned_in_paper": "164", "context_id": "66", "dataset_context": "For LUDA person ReID task, we assume that a labeled source dataset", "mention_start": 36, "mention_end": 66, "dataset_mention": "that a labeled source dataset"}, {"mentioned_in_paper": "164", "context_id": "154", "dataset_context": "We employ a synthetic dataset PersonX (PX) [32] for source pre-training and three public real-world image datasets Market1501 (MA) [51], CUHK-SYSU (SY) [41], MSMT17 (MS) [37] for unsupervised fine-tuning.", "mention_start": 52, "mention_end": 114, "dataset_mention": "source pre-training and three public real-world image datasets"}, {"mentioned_in_paper": "164", "context_id": "156", "dataset_context": "Besides, for unseen-domain generalization evaluation, we build a new dataset called MMP-Retrieval upon the training and validation splits of MMPTRACK dataset released in ICCV 2021 multi-camera multiple people tracking workshop.", "mention_start": 140, "mention_end": 157, "dataset_mention": "MMPTRACK dataset"}, {"mentioned_in_paper": "164", "context_id": "214", "dataset_context": "One synthesis dataset PersonX (PX) is employed to simulate the source domain while three realworld datasets Market1501 (MA), CUHK-SYSU (SY), and MSMT17 (MS) are employed to simulate the dynamic target domain.", "mention_start": 0, "mention_end": 21, "dataset_mention": "One synthesis dataset"}, {"mentioned_in_paper": "166", "context_id": "26", "dataset_context": "\u2022 is quantitatively evaluated on two public datasets for dense MVS with accurate ground truth and that is demonstrated on a use-case specific dataset.", "mention_start": 122, "mention_end": 149, "dataset_mention": "a use-case specific dataset"}, {"mentioned_in_paper": "166", "context_id": "106", "dataset_context": "The availability and versatility of appropriate datasets, however, is not very high, especially with respect to real-world scenarios, which still greatly hinders the practical use of deep-learning-based MVS approaches.", "mention_start": 36, "mention_end": 56, "dataset_mention": "appropriate datasets"}, {"mentioned_in_paper": "166", "context_id": "372", "dataset_context": "The presented approach is quantitatively evaluated on two public datasets, which also provide an appropriate ground truth, namely the DTU Robot MVS dataset (Jensen et al., 2014; Aanaes et al., 2016) and the dataset from the 3DOMcity Benchmark ( \u00d6zdemir et al., 2019).", "mention_start": 122, "mention_end": 155, "dataset_mention": " namely the DTU Robot MVS dataset"}, {"mentioned_in_paper": "166", "context_id": "372", "dataset_context": "The presented approach is quantitatively evaluated on two public datasets, which also provide an appropriate ground truth, namely the DTU Robot MVS dataset (Jensen et al., 2014; Aanaes et al., 2016) and the dataset from the 3DOMcity Benchmark ( \u00d6zdemir et al., 2019).", "mention_start": 192, "mention_end": 214, "dataset_mention": " 2016) and the dataset"}, {"mentioned_in_paper": "166", "context_id": "374", "dataset_context": "For a qualitative evaluation and a discussion regarding the usability of the presented approach for online dense image matching and 3D reconstruction, two privately captured datasets of real-world scenes are used, henceforth referred to as the TMB and the FB dataset.", "mention_start": 150, "mention_end": 182, "dataset_mention": " two privately captured datasets"}, {"mentioned_in_paper": "166", "context_id": "374", "dataset_context": "For a qualitative evaluation and a discussion regarding the usability of the presented approach for online dense image matching and 3D reconstruction, two privately captured datasets of real-world scenes are used, henceforth referred to as the TMB and the FB dataset.", "mention_start": 239, "mention_end": 266, "dataset_mention": "the TMB and the FB dataset"}, {"mentioned_in_paper": "166", "context_id": "377", "dataset_context": "DTU Robot MVS Dataset.", "mention_start": 0, "mention_end": 21, "dataset_mention": "DTU Robot MVS Dataset"}, {"mentioned_in_paper": "166", "context_id": "378", "dataset_context": "The DTU Robot MVS dataset (Figure 5, Column 1) is comprised of 124 different table top scenes.", "mention_start": 0, "mention_end": 25, "dataset_mention": "The DTU Robot MVS dataset"}, {"mentioned_in_paper": "166", "context_id": "387", "dataset_context": "3DOMcity Benchmark Dataset.", "mention_start": 0, "mention_end": 26, "dataset_mention": "3DOMcity Benchmark Dataset"}, {"mentioned_in_paper": "166", "context_id": "399", "dataset_context": "Real-World Use-Case-Specific TMB and FB Dataset.", "mention_start": 0, "mention_end": 47, "dataset_mention": "Real-World Use-Case-Specific TMB and FB Dataset"}, {"mentioned_in_paper": "166", "context_id": "404", "dataset_context": "The first part, namely the TMB dataset (Figure 5, Column 3), consists of four sequences captured by a DJI Phantom 3 professional, flying around a free-standing house and containers at altitudes between 8 m to 15 m.", "mention_start": 15, "mention_end": 38, "dataset_mention": " namely the TMB dataset"}, {"mentioned_in_paper": "166", "context_id": "405", "dataset_context": "For the second part, which is denoted as the fire brigade (FB) dataset (Figure 5, Column 4), images were acquired during a fire brigade exercise around a big industrial building.", "mention_start": 40, "mention_end": 70, "dataset_mention": "the fire brigade (FB) dataset"}, {"mentioned_in_paper": "166", "context_id": "411", "dataset_context": "The undistorted images as well as the intrinsic and extrinsic camera data produced by COLMAP serve as input to the presented approach for the evaluation with respect to the TMB and FB dataset.", "mention_start": 169, "mention_end": 191, "dataset_mention": "the TMB and FB dataset"}, {"mentioned_in_paper": "166", "context_id": "447", "dataset_context": "However, the results reveal that in case of the DTU dataset the smallest mean error, even if it is only slightly smaller, is achieved when setting n = 3, while the best result in case of the 3DOMcity dataset is achieved at n = 1.", "mention_start": 43, "mention_end": 59, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "166", "context_id": "447", "dataset_context": "However, the results reveal that in case of the DTU dataset the smallest mean error, even if it is only slightly smaller, is achieved when setting n = 3, while the best result in case of the 3DOMcity dataset is achieved at n = 1.", "mention_start": 186, "mention_end": 207, "dataset_mention": "the 3DOMcity dataset"}, {"mentioned_in_paper": "166", "context_id": "451", "dataset_context": "In case of the camera setup of the DTU dataset and the configuration of this experiment, i.e. having a bundle size of |\u2126| = 3, a pyramid height of 3 is the smallest height at which the number of sampling points at the highest level does not reach or exceed the Table 2: Processing run-time measured for different configurations of the pyramid height on the DTU and 3DOMcity dataset.", "mention_start": 31, "mention_end": 46, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "166", "context_id": "451", "dataset_context": "In case of the camera setup of the DTU dataset and the configuration of this experiment, i.e. having a bundle size of |\u2126| = 3, a pyramid height of 3 is the smallest height at which the number of sampling points at the highest level does not reach or exceed the Table 2: Processing run-time measured for different configurations of the pyramid height on the DTU and 3DOMcity dataset.", "mention_start": 352, "mention_end": 381, "dataset_mention": "the DTU and 3DOMcity dataset"}, {"mentioned_in_paper": "166", "context_id": "457", "dataset_context": "The measurements again show, that up to n = 3 in case of the DTU dataset, the number of sampling planes at the highest pyramid level is equal to the limit of 256 and that with a smaller amount of sampling points the run-time decreased drastically.", "mention_start": 56, "mention_end": 72, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "166", "context_id": "461", "dataset_context": "In case of the DTU dataset, this experiment shows that the best number of pyramid levels to be used is n = 3, which will thus be set for the successive experiments.", "mention_start": 11, "mention_end": 26, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "166", "context_id": "462", "dataset_context": "In case of the 3DOMcity dataset, Table 1 suggests that the best configuration is to use the original image size.", "mention_start": 11, "mention_end": 31, "dataset_mention": "the 3DOMcity dataset"}, {"mentioned_in_paper": "166", "context_id": "464", "dataset_context": "Thus, in case of the 3DOMcity dataset, the successive experiments will be executed with n = 2, which induces only a slightly higher mean error compared to the best configuration.", "mention_start": 16, "mention_end": 37, "dataset_mention": "the 3DOMcity dataset"}, {"mentioned_in_paper": "166", "context_id": "467", "dataset_context": "The height of the Gaussian pyramids is fixed to n = 3 in case of the DTU dataset and n = 2 in case of the data from 3DOMcity dataset.", "mention_start": 65, "mention_end": 80, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "166", "context_id": "467", "dataset_context": "The height of the Gaussian pyramids is fixed to n = 3 in case of the DTU dataset and n = 2 in case of the data from 3DOMcity dataset.", "mention_start": 116, "mention_end": 132, "dataset_mention": "3DOMcity dataset"}, {"mentioned_in_paper": "166", "context_id": "469", "dataset_context": "The results reveal that the best accuracies are achieved, when five input images are used for image matching, even though, in case of the 3DOMcity dataset, it is only a marginal improvement.", "mention_start": 133, "mention_end": 154, "dataset_mention": "the 3DOMcity dataset"}, {"mentioned_in_paper": "166", "context_id": "472", "dataset_context": "In conclusion, in the subsequent experiments, the size of the input bundle is set to |\u2126| = 5, while the height of the Gaussian image pyramids is set to n = 3 and n = 2 in case of the DTU and 3DOMcity dataset, respectively.", "mention_start": 178, "mention_end": 207, "dataset_mention": "the DTU and 3DOMcity dataset"}, {"mentioned_in_paper": "166", "context_id": "490", "dataset_context": "The pyramid height is set to n = 3 and n = 2 for the DTU and 3DOMcity dataset, respectively.", "mention_start": 49, "mention_end": 77, "dataset_mention": "the DTU and 3DOMcity dataset"}, {"mentioned_in_paper": "166", "context_id": "492", "dataset_context": "While, in case of the DTU dataset, the best results are achieved by the SGM \u03a0-pg implementation, on the 3DOMcity dataset, the standard adaptation of the SGM optimization to the plane-sweep sampling, i.e.", "mention_start": 17, "mention_end": 33, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "166", "context_id": "492", "dataset_context": "While, in case of the DTU dataset, the best results are achieved by the SGM \u03a0-pg implementation, on the 3DOMcity dataset, the standard adaptation of the SGM optimization to the plane-sweep sampling, i.e.", "mention_start": 99, "mention_end": 120, "dataset_mention": "the 3DOMcity dataset"}, {"mentioned_in_paper": "166", "context_id": "499", "dataset_context": "A qualitative comparison between the results on the 3DOMcity dataset in Figure 7, however, does not reveal any noticeable differences between the different implementations.", "mention_start": 48, "mention_end": 68, "dataset_mention": "the 3DOMcity dataset"}, {"mentioned_in_paper": "166", "context_id": "500", "dataset_context": "The reason for the small L1-abs error achieved by SGM \u03a0 on the 3DOMcity dataset is assumed to be due to the fact, that the 3DOMcity dataset also contains a subset of nadir images, in which only a small number of slanted surfaces are existing and the fronto-parallel orientation of the sampling planes coincides with most of the scene structure.", "mention_start": 59, "mention_end": 79, "dataset_mention": "the 3DOMcity dataset"}, {"mentioned_in_paper": "166", "context_id": "500", "dataset_context": "The reason for the small L1-abs error achieved by SGM \u03a0 on the 3DOMcity dataset is assumed to be due to the fact, that the 3DOMcity dataset also contains a subset of nadir images, in which only a small number of slanted surfaces are existing and the fronto-parallel orientation of the sampling planes coincides with most of the scene structure.", "mention_start": 113, "mention_end": 139, "dataset_mention": " that the 3DOMcity dataset"}, {"mentioned_in_paper": "166", "context_id": "524", "dataset_context": "For this, the DTU dataset is split into two subsets, one for the horizontal sampling, in which the camera is looking in a more downwards direction, as well as one for the vertical sampling, where the camera pitch is smaller.", "mention_start": 9, "mention_end": 25, "dataset_mention": " the DTU dataset"}, {"mentioned_in_paper": "166", "context_id": "542", "dataset_context": "Table 8 : Final quantitative results of the three SGM extensions on the DTU dataset, in combination with fronto-parallel plane-sweep sampling and post-filtering based on geometric consistency check.", "mention_start": 67, "mention_end": 83, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "166", "context_id": "551", "dataset_context": "Based on the previous experiments, the following setup is selected for the final experiments on the DTU dataset.", "mention_start": 95, "mention_end": 111, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "166", "context_id": "569", "dataset_context": "In a final comparison, Table 10 lists the quantitative results of some representative approaches from literature, including COLMAP, on the evaluation set of the DTU dataset, calculated with the actual evaluation protocol of the benchmark.", "mention_start": 156, "mention_end": 172, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "166", "context_id": "574", "dataset_context": "Finally, to demonstrate the performance of the presented approach on use-case-specific and real-world datasets, experiments using SGM \u03a0-pg with 4 aggregation paths and the configuration mentioned in Section 3.7 are conducted on the TMB and FB dataset (cf.", "mention_start": 68, "mention_end": 110, "dataset_mention": "use-case-specific and real-world datasets"}, {"mentioned_in_paper": "166", "context_id": "574", "dataset_context": "Finally, to demonstrate the performance of the presented approach on use-case-specific and real-world datasets, experiments using SGM \u03a0-pg with 4 aggregation paths and the configuration mentioned in Section 3.7 are conducted on the TMB and FB dataset (cf.", "mention_start": 227, "mention_end": 250, "dataset_mention": "the TMB and FB dataset"}, {"mentioned_in_paper": "166", "context_id": "578", "dataset_context": "The mean processing time in case of the TMB dataset is 690 ms, partly varying from between 320 ms and 1218 ms depending on the arrangement of the input data and, in turn, the number of plane distances \u03b4 at which the scene is sampled.", "mention_start": 36, "mention_end": 51, "dataset_mention": "the TMB dataset"}, {"mentioned_in_paper": "166", "context_id": "579", "dataset_context": "For the FB dataset, the mean processing time is 800 ms, varying between 514 ms and 1419 ms again depending the arrangement of the input images.", "mention_start": 4, "mention_end": 18, "dataset_mention": "the FB dataset"}, {"mentioned_in_paper": "166", "context_id": "592", "dataset_context": "This is mainly due to the lack of appropriate datasets that are publicly available and provide accurate ground truth that in turn allows for a thorough investigation and benchmarking.", "mention_start": 34, "mention_end": 54, "dataset_mention": "appropriate datasets"}, {"mentioned_in_paper": "166", "context_id": "594", "dataset_context": "To the best of our knowledge, current publicly available datasets and benchmarks, used to evaluate the performance of algorithms for the task of DIM and MVS, aim at the benchmarking of offline approaches, providing a fixed set of input images acquired from predefined viewpoints.", "mention_start": 29, "mention_end": 65, "dataset_mention": " current publicly available datasets"}, {"mentioned_in_paper": "166", "context_id": "604", "dataset_context": "First, Scharstein et al. (2017) demonstrate their implementation on a two-view stereo dataset, in which the input images are captured by two cameras mounted on a fixed rig and orientated in the same direction.", "mention_start": 67, "mention_end": 93, "dataset_mention": "a two-view stereo dataset"}, {"mentioned_in_paper": "166", "context_id": "626", "dataset_context": "In case of the TMB dataset, the mean distance between the individual input images is 1.8 m and 1.03 m for a flight altitude of 15 m and 8 m, respectively.", "mention_start": 11, "mention_end": 26, "dataset_mention": "the TMB dataset"}, {"mentioned_in_paper": "166", "context_id": "632", "dataset_context": "As the use-case-specific experiments for the TMB and FB dataset, however, show, the average processing rate of SGM \u03a0-pg , which is the computationally most complex variant, is between 1-2 Hz, depending on the arrangement of the input images.", "mention_start": 41, "mention_end": 63, "dataset_mention": "the TMB and FB dataset"}, {"mentioned_in_paper": "166", "context_id": "642", "dataset_context": "On an excerpt of the TMB dataset with in image size of 1920 \u00d7 1080 pixels, FaSS-MVS with SGM \u03a0 , parameterized by the final configuration as presented in Section 3.7, achieves an average run-time of 727 ms on the Jetson AGX, compared to an average run-time of approximately 403 ms achieved on the NVIDIA Titan X.", "mention_start": 17, "mention_end": 32, "dataset_mention": "the TMB dataset"}, {"mentioned_in_paper": "166", "context_id": "685", "dataset_context": "Concluding experiments on real-world and use-case-specific datasets have shown, that in terms of run-time the presented approach is well-suited for online processing by achieving a processing rate of 1-2 Hz, meaning that it keeps up with the monocular input stream and allows for incremental 3D mapping, while the input data is being received.", "mention_start": 26, "mention_end": 67, "dataset_mention": "real-world and use-case-specific datasets"}, {"mentioned_in_paper": "168", "context_id": "206", "dataset_context": "Algorithms are also tested on 12+1 datasets provided by SRD.", "mention_start": 30, "mention_end": 43, "dataset_mention": "12+1 datasets"}, {"mentioned_in_paper": "168", "context_id": "210", "dataset_context": "A 13th dataset is constructed as the union of the previous 12, therefore containing 145 \u00d7 12 time series 3 .", "mention_start": 0, "mention_end": 14, "dataset_mention": "A 13th dataset"}, {"mentioned_in_paper": "168", "context_id": "225", "dataset_context": "Since the specified diameter threshold is tight for the UCI datasets, we also experimented with a threshold set to 1.2 times the optimal diameter, in order to evaluate the impact of having relaxed constraints and potentially a larger solution space.", "mention_start": 52, "mention_end": 68, "dataset_mention": "the UCI datasets"}, {"mentioned_in_paper": "168", "context_id": "227", "dataset_context": "For instance, with the ionosphere dataset, the greedy version of EQW guarantees to find a solution with at most \u230a2H 2 \u230b = 3 clusters, while HAC provides a solution with 4 clusters.", "mention_start": 18, "mention_end": 41, "dataset_mention": "the ionosphere dataset"}, {"mentioned_in_paper": "168", "context_id": "237", "dataset_context": "Consequently, on the datasets provided by SRD, this heuristic is not used and CP4CC exceeds the time limit of 10 minutes on 26 of the 48 experiments performed with the monthly electric power datasets, despite them being relatively simple compared with other datasets provided by UCI.", "mention_start": 163, "mention_end": 199, "dataset_mention": "the monthly electric power datasets"}, {"mentioned_in_paper": "168", "context_id": "240", "dataset_context": "Otherwise, exact algorithms-DSATUR, CP and CP4CC without the FPF heuristic-are all dependent on this ordering, as most notably seen on the SRD dataset, where there is at least a tenfold difference between the fastest and slowest run.", "mention_start": 134, "mention_end": 150, "dataset_mention": "the SRD dataset"}, {"mentioned_in_paper": "168", "context_id": "252", "dataset_context": "For example with the waveform dataset, EQW-LP fails to provide an optimal result within 10 minutes.", "mention_start": 17, "mention_end": 37, "dataset_mention": "the waveform dataset"}, {"mentioned_in_paper": "171", "context_id": "6", "dataset_context": "We empirically demonstrate the efficacy of our framework on a variety of non-I.I.D. distributed graph-level molecular property prediction datasets with partial labels.", "mention_start": 73, "mention_end": 146, "dataset_mention": "non-I.I.D. distributed graph-level molecular property prediction datasets"}, {"mentioned_in_paper": "171", "context_id": "37", "dataset_context": "We synthesize non-I.I.D. and partially labeled datasets by using curated data from the MoleculeNet [69] machine learning benchmark.", "mention_start": 0, "mention_end": 55, "dataset_mention": "We synthesize non-I.I.D. and partially labeled datasets"}, {"mentioned_in_paper": "171", "context_id": "41", "dataset_context": "We seek to learn graph level representations in a federated learning setting over decentralized graph datasets located in edge servers which cannot be centralized for training due to privacy and regulation restrictions [20].", "mention_start": 82, "mention_end": 110, "dataset_mention": "decentralized graph datasets"}, {"mentioned_in_paper": "171", "context_id": "48", "dataset_context": "Multiple clients are interested in collaborating to improve their GNN models without necessarily revealing their graph datasets.", "mention_start": 60, "mention_end": 127, "dataset_mention": "their GNN models without necessarily revealing their graph datasets"}, {"mentioned_in_paper": "171", "context_id": "156", "dataset_context": "We use molecular datasets from the MoleculeNet [69] machine learning benchmark in our evaluation.", "mention_start": 7, "mention_end": 25, "dataset_mention": "molecular datasets"}, {"mentioned_in_paper": "171", "context_id": "157", "dataset_context": "In particular, we evaluate our approach on molecular property prediction datasets described in Table 1.", "mention_start": 42, "mention_end": 81, "dataset_mention": "molecular property prediction datasets"}, {"mentioned_in_paper": "171", "context_id": "160", "dataset_context": "As such, each multitask dataset can adequately evaluate our learning framework.", "mention_start": 8, "mention_end": 31, "dataset_mention": " each multitask dataset"}, {"mentioned_in_paper": "171", "context_id": "211", "dataset_context": " [66] propose a hybrid of federated and meta learning to solve the semi-supervised graph node classification problem in decentralized social network datasets.", "mention_start": 119, "mention_end": 157, "dataset_mention": "decentralized social network datasets"}, {"mentioned_in_paper": "177", "context_id": "106", "dataset_context": "We use the KITTI dataset [8] as it provides typical urban data.", "mention_start": 7, "mention_end": 24, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "177", "context_id": "117", "dataset_context": "The classifier is trained on 120 manually labelled images from the KITTI dataset using colour, texture, and location features, shown in Table I.", "mention_start": 63, "mention_end": 80, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "177", "context_id": "124", "dataset_context": "This is facilitated by the KITTI dataset providing time synchronised camera images and Velodyne point clouds.", "mention_start": 23, "mention_end": 40, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "177", "context_id": "129", "dataset_context": "The 3D coordinates of the points contained in the selected clusters are then translated into image space coordinates using the extrinsic calibration provided by the KITTI dataset and then associated with super pixels.", "mention_start": 161, "mention_end": 178, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "179", "context_id": "6", "dataset_context": "This way, we can advance state-of-the-art trackers on the MOTChallenge dataset and significantly improve their long-term tracking performance.", "mention_start": 53, "mention_end": 78, "dataset_mention": "the MOTChallenge dataset"}, {"mentioned_in_paper": "179", "context_id": "39", "dataset_context": "We (ii) utilize a synthetic MOT dataset to study how to localize objects in 3D BEV space in a manner that facilitates robust reasoning about plausible future motion and which are the core forecasting components needed to bridge longer occlusion gaps; Finally, (iii) we demonstrate that we can generalize our conclusions from synthetic sandbox to real-world monocular MOTChallenge sequences and demonstrate that our recipe can be used to improve long-term tracking performance for several object trackers.", "mention_start": 16, "mention_end": 39, "dataset_mention": "a synthetic MOT dataset"}, {"mentioned_in_paper": "179", "context_id": "52", "dataset_context": "In MOT datasets [15, 21] the majority of occlusions are short occlusions (\u2264 2s).", "mention_start": 3, "mention_end": 15, "dataset_mention": "MOT datasets"}, {"mentioned_in_paper": "179", "context_id": "66", "dataset_context": "Exceptions are autonomous driving datasets [21, 10] that generally provide 3D sensory data, together with 3D track information.", "mention_start": 15, "mention_end": 42, "dataset_mention": "autonomous driving datasets"}, {"mentioned_in_paper": "179", "context_id": "67", "dataset_context": "However, only a handful of object tracks contain occlusion gaps longer than 2s: 0.6% in BDD100K [77] and 4% in widely-used KITTI tracking [21] dataset.", "mention_start": 110, "mention_end": 150, "dataset_mention": "widely-used KITTI tracking [21] dataset"}, {"mentioned_in_paper": "179", "context_id": "68", "dataset_context": "Therefore, autonomous driving datasets are, at the moment, not well suited for studying long-term tracking.", "mention_start": 10, "mention_end": 38, "dataset_mention": " autonomous driving datasets"}, {"mentioned_in_paper": "179", "context_id": "69", "dataset_context": "Instead we conduct our experiments and analysis using MOTChallenge [15] dataset, where 19.4% of tracks undergo long (> 2s) occlusions gaps.", "mention_start": 54, "mention_end": 79, "dataset_mention": "MOTChallenge [15] dataset"}, {"mentioned_in_paper": "179", "context_id": "175", "dataset_context": "Finally, we show that our forecasting model can be used to establish new state-of-the-art on the real-world MOT17 and MOT20 datasets (Section 4.4).", "mention_start": 92, "mention_end": 132, "dataset_mention": "the real-world MOT17 and MOT20 datasets"}, {"mentioned_in_paper": "179", "context_id": "176", "dataset_context": "For visualization of our tracking method, we refer the reader to Appendix C. Datasets.", "mention_start": 64, "mention_end": 85, "dataset_mention": "Appendix C. Datasets"}, {"mentioned_in_paper": "179", "context_id": "177", "dataset_context": "We evaluate our trajectory prediction framework on different publicly available pedestrian tracking datasets, namely synthetic MOTSynth [19] and two real-world MOT17 and MOT20 datasets [15].", "mention_start": 51, "mention_end": 108, "dataset_mention": "different publicly available pedestrian tracking datasets"}, {"mentioned_in_paper": "179", "context_id": "177", "dataset_context": "We evaluate our trajectory prediction framework on different publicly available pedestrian tracking datasets, namely synthetic MOTSynth [19] and two real-world MOT17 and MOT20 datasets [15].", "mention_start": 109, "mention_end": 184, "dataset_mention": " namely synthetic MOTSynth [19] and two real-world MOT17 and MOT20 datasets"}, {"mentioned_in_paper": "179", "context_id": "181", "dataset_context": "MOT17 [47] and MOT20 [16] are real-world tracking datasets commonly used to benchmark pedestrian tracking models.", "mention_start": 30, "mention_end": 58, "dataset_mention": "real-world tracking datasets"}, {"mentioned_in_paper": "179", "context_id": "266", "dataset_context": "We can substantiate our conclusion by achieving new state-of-the-art performance on the MOT17 and MOT20 datasets.", "mention_start": 84, "mention_end": 112, "dataset_mention": "the MOT17 and MOT20 datasets"}, {"mentioned_in_paper": "179", "context_id": "292", "dataset_context": "For training the trajectory predictor (Appendix B.2) and the depth estimator (Appendix B.3) we use the MOTSynth dataset [19] which provides ground truth 3D positions of objects and image depth information.", "mention_start": 99, "mention_end": 119, "dataset_mention": "the MOTSynth dataset"}, {"mentioned_in_paper": "179", "context_id": "314", "dataset_context": "The network trains on the synthetic MOTsynth dataset to leverage a large number of tracking scenes of varying perspectives, weather, and light conditions.", "mention_start": 22, "mention_end": 52, "dataset_mention": "the synthetic MOTsynth dataset"}, {"mentioned_in_paper": "181", "context_id": "100", "dataset_context": "We randomly sample images from the Pascal VOC dataset [9], where the ground truth of semantic label is provided in 20 categories.", "mention_start": 31, "mention_end": 53, "dataset_mention": "the Pascal VOC dataset"}, {"mentioned_in_paper": "181", "context_id": "181", "dataset_context": "We first synthesis the semantically annotated dataset.", "mention_start": 0, "mention_end": 53, "dataset_mention": "We first synthesis the semantically annotated dataset"}, {"mentioned_in_paper": "181", "context_id": "187", "dataset_context": "On the other hand, it's labor-consuming to label semantic annotations for the existing reflection removal dataset.", "mention_start": 73, "mention_end": 113, "dataset_mention": "the existing reflection removal dataset"}, {"mentioned_in_paper": "181", "context_id": "188", "dataset_context": "Therefore, we generate the reflection removal dataset with semantic labels.", "mention_start": 22, "mention_end": 53, "dataset_mention": "the reflection removal dataset"}, {"mentioned_in_paper": "181", "context_id": "196", "dataset_context": "Then we evaluate the trained SGR 2 N on the synthetic test set, 20 real images from Berkeley dataset [63] and 463 real images from SIR 2 dataset [51].", "mention_start": 83, "mention_end": 100, "dataset_mention": "Berkeley dataset"}, {"mentioned_in_paper": "181", "context_id": "196", "dataset_context": "Then we evaluate the trained SGR 2 N on the synthetic test set, 20 real images from Berkeley dataset [63] and 463 real images from SIR 2 dataset [51].", "mention_start": 130, "mention_end": 144, "dataset_mention": "SIR 2 dataset"}, {"mentioned_in_paper": "181", "context_id": "295", "dataset_context": "We further conduct comparisons on SIR 2 [51], which contains three sub-datasets.", "mention_start": 51, "mention_end": 79, "dataset_mention": "contains three sub-datasets"}, {"mentioned_in_paper": "181", "context_id": "301", "dataset_context": "The first row shows one visual comparison among state-of-the-art methods on Berkeley test set and the last two rows show the results on SIR 2 dataset.", "mention_start": 136, "mention_end": 149, "dataset_mention": "SIR 2 dataset"}, {"mentioned_in_paper": "181", "context_id": "385", "dataset_context": "We compare the final mIoU of DeeplabV3+ [5] (pretraiend on PascalVOC dataset) and the final PSNR of our baseline [63] on such images.", "mention_start": 59, "mention_end": 76, "dataset_mention": "PascalVOC dataset"}, {"mentioned_in_paper": "181", "context_id": "413", "dataset_context": "Here we train our model on standard RESIDE dataset [23] for dehazing and PBRR dataset [12, 30] for deraining, respectively.", "mention_start": 27, "mention_end": 50, "dataset_mention": "standard RESIDE dataset"}, {"mentioned_in_paper": "181", "context_id": "413", "dataset_context": "Here we train our model on standard RESIDE dataset [23] for dehazing and PBRR dataset [12, 30] for deraining, respectively.", "mention_start": 60, "mention_end": 85, "dataset_mention": "dehazing and PBRR dataset"}, {"mentioned_in_paper": "183", "context_id": "442", "dataset_context": "In this benchmark, we measure the average throughput (during 1 minute) of incremental view maintenance (IVM) on a 200MB TPC-H dataset.", "mention_start": 111, "mention_end": 133, "dataset_mention": "a 200MB TPC-H dataset"}, {"mentioned_in_paper": "185", "context_id": "1", "dataset_context": "We introduce a distributed system for Stratified Locality Sensitive Hashing to perform fast similarity-based prediction on large medical waveform datasets.", "mention_start": 123, "mention_end": 154, "dataset_mention": "large medical waveform datasets"}, {"mentioned_in_paper": "185", "context_id": "11", "dataset_context": "Therefore, previous work analyzed the effectiveness of fast approximate nearest neighbor algorithms for prediction on time-series and, in particular, of locality sensitive hashing (LSH) on arterial blood pressure (ABP) datasets [10], [11].", "mention_start": 188, "mention_end": 227, "dataset_mention": "arterial blood pressure (ABP) datasets"}, {"mentioned_in_paper": "187", "context_id": "37", "dataset_context": "To evaluate the quality of the segment-phrase table we test it on object discovery datasets (Section.", "mention_start": 66, "mention_end": 91, "dataset_mention": "object discovery datasets"}, {"mentioned_in_paper": "187", "context_id": "186", "dataset_context": "We evaluate the quality of our segment-phrase table on standard segmentation datasets.", "mention_start": 55, "mention_end": 85, "dataset_mention": "standard segmentation datasets"}, {"mentioned_in_paper": "187", "context_id": "196", "dataset_context": "This strong result brings to light an interesting trade-off: using large datasets with zero human supervision vs. smaller datasets with some human supervision.", "mention_start": 86, "mention_end": 130, "dataset_mention": "zero human supervision vs. smaller datasets"}, {"mentioned_in_paper": "187", "context_id": "207", "dataset_context": "In [25], the problem of automatically populating the ImageNet dataset with pixelwise segmentations was studied.", "mention_start": 23, "mention_end": 69, "dataset_mention": "automatically populating the ImageNet dataset"}, {"mentioned_in_paper": "187", "context_id": "226", "dataset_context": "For qualitative results, please refer to Figure 5. Dataset and Metrics: Building upon the relation phrases of [38], we collected a dataset of 5,710 semantic relations that include entailment (270 true relations), and paraphrasing (25 true relations).", "mention_start": 40, "mention_end": 58, "dataset_mention": "Figure 5. Dataset"}, {"mentioned_in_paper": "188", "context_id": "119", "dataset_context": "The VehiclePart [26] dataset is an extension of Pascal 3D+ [28] and contains images of vehicles including aeroplanes, bicycles, buses, cars, motorbikes and trains.", "mention_start": 0, "mention_end": 28, "dataset_mention": "The VehiclePart [26] dataset"}, {"mentioned_in_paper": "188", "context_id": "121", "dataset_context": "In addition to the landmark labels provided in Pascal 3D+, the Vehi-clePart dataset additionally defines more than 10 semantic parts for each vehicle type.", "mention_start": 58, "mention_end": 83, "dataset_mention": " the Vehi-clePart dataset"}, {"mentioned_in_paper": "188", "context_id": "122", "dataset_context": "CUB The CUB [25] dataset contains 10,000 bird images with 200 categories of birds with various bird poses and from different camera views.", "mention_start": 0, "mention_end": 24, "dataset_mention": "CUB The CUB [25] dataset"}, {"mentioned_in_paper": "188", "context_id": "126", "dataset_context": "The CelebA [12] dataset contains about 200k human face images.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The CelebA [12] dataset"}, {"mentioned_in_paper": "188", "context_id": "145", "dataset_context": "First, following the designs in the VehiclePart dataset, we use mean average precision (mAP) with an IOU threshold of 0.5 as our evaluation on the part bounding boxes provided in the VehiclePart.", "mention_start": 31, "mention_end": 55, "dataset_mention": "the VehiclePart dataset"}, {"mentioned_in_paper": "188", "context_id": "146", "dataset_context": "Moreover, we use mAP with an L2 distance threshold of 0.1 as the performance metric for both the CUB and CelebA dataset.", "mention_start": 87, "mention_end": 119, "dataset_mention": "both the CUB and CelebA dataset"}, {"mentioned_in_paper": "188", "context_id": "154", "dataset_context": "In addition to the aforementioned baselines, we also compare our results on the CelebA and CUB dataset with recent unsupervised methods [7, 6, 24, 32] and other supervised methods [29, 35, 21, 34].", "mention_start": 75, "mention_end": 102, "dataset_mention": "the CelebA and CUB dataset"}, {"mentioned_in_paper": "188", "context_id": "157", "dataset_context": "In the Table 1, we show the mean AP with IOU threshold of 0.5 on the VehiclePart dataset.", "mention_start": 64, "mention_end": 88, "dataset_mention": "the VehiclePart dataset"}, {"mentioned_in_paper": "188", "context_id": "166", "dataset_context": "To further verify the effectiveness of our method and its applicability to other datasets, we test our method on CelebA, which has a training set of 80 times larger compared with the VehiclePart dataset.", "mention_start": 178, "mention_end": 202, "dataset_mention": "the VehiclePart dataset"}, {"mentioned_in_paper": "188", "context_id": "169", "dataset_context": "Hence, we observe that our method a similar effect as on the VehiclePart dataset, namley that our method improves the part layer weights.", "mention_start": 56, "mention_end": 80, "dataset_mention": "the VehiclePart dataset"}, {"mentioned_in_paper": "188", "context_id": "174", "dataset_context": "We surpass all supervised methods on the MAFL dataset in the aligned setting (with an error of 4.9%), while being slightly worse than [32] when comparing with unsupervised methods as shown in Figure 4.", "mention_start": 37, "mention_end": 53, "dataset_mention": "the MAFL dataset"}, {"mentioned_in_paper": "188", "context_id": "175", "dataset_context": "The objects of VehiclePart and CelebA datasets are considered rigid objects (i.e., vehicles and faces), so we would like to verify our method's ability to discovery non-rigid objects parts/landmarks, such as deformable birds.", "mention_start": 15, "mention_end": 46, "dataset_mention": "VehiclePart and CelebA datasets"}, {"mentioned_in_paper": "188", "context_id": "176", "dataset_context": "The CUB dataset is challenging because it consists of 200 bird classes in various poses (see examples in Figure 9), e.g., standing, swimming, or flying, as well as the different camera view- 5 while the Clustering method achieves 26.9%.", "mention_start": 0, "mention_end": 15, "dataset_mention": "The CUB dataset"}, {"mentioned_in_paper": "188", "context_id": "183", "dataset_context": "The detailed analysis is discuss in Section 4.5 Figure 9. CUB dataset contains birds with different shapes and colorful feathers, various bird pose, multiple camera viewpoints.", "mention_start": 36, "mention_end": 69, "dataset_mention": "Section 4.5 Figure 9. CUB dataset"}, {"mentioned_in_paper": "189", "context_id": "178", "dataset_context": "Our initial survey dataset is subject to limitations: surveyed digitized sellers are potentially more likely to respond to an email, resulting in response bias.", "mention_start": 0, "mention_end": 26, "dataset_mention": "Our initial survey dataset"}, {"mentioned_in_paper": "189", "context_id": "191", "dataset_context": "For RQ2, we only utilize data at an international level, and do not collect business categories or segment payments by state/province due to limited observations and dataset noise.", "mention_start": 148, "mention_end": 173, "dataset_mention": "observations and dataset"}, {"mentioned_in_paper": "189", "context_id": "192", "dataset_context": "Furthermore, a potential limitation of our international digitization datasets are a lack of coverage across digitization techniques: in Japan specifically, the financial services company we study supported only digital invoices early in the pandemic (March 2020), and added additional features later.", "mention_start": 38, "mention_end": 78, "dataset_mention": "our international digitization datasets"}, {"mentioned_in_paper": "189", "context_id": "198", "dataset_context": "Segmenting datasets from other countries at an intrantional level would yield sample-size problems; further segmentation into business sectors would also be prohibitive.", "mention_start": 0, "mention_end": 19, "dataset_mention": "Segmenting datasets"}, {"mentioned_in_paper": "189", "context_id": "203", "dataset_context": "First, we collect intranational government intervention data: to identify the type and enforcement date of interventions, we utilized the OxCRGT dataset [34].", "mention_start": 133, "mention_end": 152, "dataset_mention": "the OxCRGT dataset"}, {"mentioned_in_paper": "189", "context_id": "223", "dataset_context": "Furthermore, utilizing a control dataset at a region level allows us to control for geographic factors and seasonality trends within these regions, adjusting for external effects on payment transactions.", "mention_start": 22, "mention_end": 40, "dataset_mention": "a control dataset"}, {"mentioned_in_paper": "189", "context_id": "251", "dataset_context": "Finally, correlation from Granger's test does not imply causation; instead, it indicates that one time series dataset has predictive information regarding another time series dataset.", "mention_start": 75, "mention_end": 117, "dataset_mention": " it indicates that one time series dataset"}, {"mentioned_in_paper": "189", "context_id": "251", "dataset_context": "Finally, correlation from Granger's test does not imply causation; instead, it indicates that one time series dataset has predictive information regarding another time series dataset.", "mention_start": 154, "mention_end": 182, "dataset_mention": "another time series dataset"}, {"mentioned_in_paper": "189", "context_id": "261", "dataset_context": "To understand what elements of stringency may have played a role in the initial set of mandates outlined by state-level governments, we utilize subfactors from the OxCRGT dataset as independent variables to predict digitization rates in a state.", "mention_start": 159, "mention_end": 178, "dataset_mention": "the OxCRGT dataset"}, {"mentioned_in_paper": "189", "context_id": "531", "dataset_context": "We were able to find a comprehensive dataset for subnational interventions and corresponding data on small businesses in only the United States.", "mention_start": 16, "mention_end": 44, "dataset_mention": "find a comprehensive dataset"}, {"mentioned_in_paper": "189", "context_id": "532", "dataset_context": "Other policy datasets were at a national level, or incomplete.", "mention_start": 0, "mention_end": 21, "dataset_mention": "Other policy datasets"}, {"mentioned_in_paper": "190", "context_id": "6", "dataset_context": "In empirical study on two benchmark crowd counting datasets, the stacked pooling beats the vanilla pooling layer in most cases.", "mention_start": 22, "mention_end": 59, "dataset_mention": "two benchmark crowd counting datasets"}, {"mentioned_in_paper": "190", "context_id": "35", "dataset_context": "It beats the vanilla pooling in most experiments on two benchmark crowd counting datasets.", "mention_start": 52, "mention_end": 89, "dataset_mention": "two benchmark crowd counting datasets"}, {"mentioned_in_paper": "190", "context_id": "89", "dataset_context": "In this work, we do empirical study on two popular crowd counting datasets: ShanghaiTech (Zhang et al. 2016) and WorldExpo'10 ( Zhang et al. 2015), as both the two datasets are very challenging due to diverse scene types and varying density levels.", "mention_start": 38, "mention_end": 74, "dataset_mention": "two popular crowd counting datasets"}, {"mentioned_in_paper": "190", "context_id": "90", "dataset_context": "\u2022 The ShanghaiTech dataset (Zhang et al. 2016)", "mention_start": 0, "mention_end": 26, "dataset_mention": "\u2022 The ShanghaiTech dataset"}, {"mentioned_in_paper": "190", "context_id": "104", "dataset_context": "The experiments are conducted on ShanghaiTech dataset and Base-M Net.", "mention_start": 33, "mention_end": 53, "dataset_mention": "ShanghaiTech dataset"}, {"mentioned_in_paper": "190", "context_id": "107", "dataset_context": "Fig. 5 shows that the vanilla pooling performs worse than our multi-kernel pooling on the high density group of ShanghaiTech-A dataset and also worse on the entire ShanghaiTech-B dataset.", "mention_start": 112, "mention_end": 134, "dataset_mention": "ShanghaiTech-A dataset"}, {"mentioned_in_paper": "190", "context_id": "107", "dataset_context": "Fig. 5 shows that the vanilla pooling performs worse than our multi-kernel pooling on the high density group of ShanghaiTech-A dataset and also worse on the entire ShanghaiTech-B dataset.", "mention_start": 153, "mention_end": 186, "dataset_mention": "the entire ShanghaiTech-B dataset"}, {"mentioned_in_paper": "190", "context_id": "113", "dataset_context": "In regard to datasets, A part and B part of ShanghaiTech dataset vary largely with crowd densities, scenes, and camera perspectives.", "mention_start": 43, "mention_end": 64, "dataset_mention": "ShanghaiTech dataset"}, {"mentioned_in_paper": "190", "context_id": "118", "dataset_context": "Table 3 shows that the Deep-Net is empirically better than Wide-Net and Base-Nets on ShanghaiTech dataset.", "mention_start": 85, "mention_end": 105, "dataset_mention": "ShanghaiTech dataset"}, {"mentioned_in_paper": "190", "context_id": "139", "dataset_context": "We conduct this experiment by applying Base-M Net to ShanghaiTech-B dataset, where the network is previously trained on the training set and evaluated on the testing set.", "mention_start": 53, "mention_end": 75, "dataset_mention": "ShanghaiTech-B dataset"}, {"mentioned_in_paper": "190", "context_id": "146", "dataset_context": "Such scale-invariant representation improves the generalization capability of a CNN model, especially for crowd counting datasets which have high intra-image and inter-image visual similarities.", "mention_start": 105, "mention_end": 129, "dataset_mention": "crowd counting datasets"}, {"mentioned_in_paper": "190", "context_id": "153", "dataset_context": "The stacked pooling layer is efficient and easy to implement, showing better performance than vanilla pooling layer in most cases on benchmark crowd counting datasets.", "mention_start": 132, "mention_end": 166, "dataset_mention": "benchmark crowd counting datasets"}, {"mentioned_in_paper": "191", "context_id": "8", "dataset_context": "The evaluation experiments on Chinese city air quality dataset indicate that our GAGNN outperforms existing forecasting models.", "mention_start": 30, "mention_end": 62, "dataset_mention": "Chinese city air quality dataset"}, {"mentioned_in_paper": "191", "context_id": "35", "dataset_context": "(3) We evaluate GAGNN on the Chinese city air quality dataset and compare it with the SOTA spatial-temporal forecasting models.", "mention_start": 25, "mention_end": 61, "dataset_mention": "the Chinese city air quality dataset"}, {"mentioned_in_paper": "191", "context_id": "145", "dataset_context": "In this section, we evaluate the performance of GAGNN on Chinese city air quality dataset, which covers 209 cities over a period of 850 days.", "mention_start": 56, "mention_end": 89, "dataset_mention": "Chinese city air quality dataset"}, {"mentioned_in_paper": "191", "context_id": "153", "dataset_context": "We evaluate the performance of GAGNN on Chinese city air quality dataset.", "mention_start": 40, "mention_end": 72, "dataset_mention": "Chinese city air quality dataset"}, {"mentioned_in_paper": "191", "context_id": "247", "dataset_context": "We evaluate GAGNN on Chinese city air quality dataset and obtain the following conclusions from the experiment results: (1) HGNNs can effectively relieve the conflict between expanding receptive fields and preserving local features; (2) differentiable grouping network can effectively discover the latent dependencies between cities; (3) the group correlation encoding module can effectively capture the dependencies between city groups.", "mention_start": 21, "mention_end": 53, "dataset_mention": "Chinese city air quality dataset"}, {"mentioned_in_paper": "193", "context_id": "121", "dataset_context": "The AFW dataset is a face image library created using Flickr images, containing 205 images, including 468 marked faces, which contain complex background changes and face pose changes.", "mention_start": 0, "mention_end": 15, "dataset_mention": "The AFW dataset"}, {"mentioned_in_paper": "193", "context_id": "142", "dataset_context": "We present a quantitative comparison of the proposed methods with E-3DMM and FW-3DMM [30] in the datasets AFW and LFPW, and qualitative comparisons on the AFLW dataset.", "mention_start": 150, "mention_end": 167, "dataset_mention": "the AFLW dataset"}, {"mentioned_in_paper": "195", "context_id": "22", "dataset_context": "\u2022 An extension of the attention mechanism from MPRNet that uses PS layers for channels reorganization [8]; \u2022 A new loss function that incorporates both LPR predictions and quality metrics in its formulation; \u2022 The datasets we built for this research (images degraded at different SSIM intervals) are publicly available 1.", "mention_start": 207, "mention_end": 222, "dataset_mention": " \u2022 The datasets"}, {"mentioned_in_paper": "195", "context_id": "96", "dataset_context": "1) Dataset: we performed our experiments on LP images extracted from the RodoSol-ALPR dataset [20].", "mention_start": 0, "mention_end": 10, "dataset_mention": "1) Dataset"}, {"mentioned_in_paper": "195", "context_id": "96", "dataset_context": "1) Dataset: we performed our experiments on LP images extracted from the RodoSol-ALPR dataset [20].", "mention_start": 68, "mention_end": 93, "dataset_mention": "the RodoSol-ALPR dataset"}, {"mentioned_in_paper": "195", "context_id": "99", "dataset_context": "While all Brazilian LPs consist of three letters followed by four digits, the initial pattern adopted in Brazil for Mercosur LPs consists of three letters, one digit, one letter and two digits, in that order [20] (this is the pattern adopted on all Mercosur LPs in the RodoSol-ALPR dataset).", "mention_start": 264, "mention_end": 289, "dataset_mention": "the RodoSol-ALPR dataset"}, {"mentioned_in_paper": "195", "context_id": "101", "dataset_context": "Fig. 3 shows some LPs cropped from the RodoSol-ALPR dataset.", "mention_start": 35, "mention_end": 59, "dataset_mention": "the RodoSol-ALPR dataset"}, {"mentioned_in_paper": "195", "context_id": "104", "dataset_context": "For each image from the RodoSol-ALPR dataset, we first cropped the LP region using the annotations provided by the authors.", "mention_start": 20, "mention_end": 44, "dataset_mention": "the RodoSol-ALPR dataset"}, {"mentioned_in_paper": "195", "context_id": "148", "dataset_context": "We also intend to carry out experiments in cross-dataset setups to assess and eliminate the impact of dataset bias [36], [37].", "mention_start": 43, "mention_end": 56, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "197", "context_id": "12", "dataset_context": "In fact, most of 3D body shape dataset are limited to the age range from young adult to middle age.", "mention_start": 16, "mention_end": 38, "dataset_mention": "3D body shape dataset"}, {"mentioned_in_paper": "197", "context_id": "13", "dataset_context": "Also, MoCap dataset for 3D human pose estimation are limited to a small variety of subjects, since it needs a complicated experimental setup where MoCap and RGB video cameras have to be synchronized.", "mention_start": 5, "mention_end": 19, "dataset_mention": " MoCap dataset"}, {"mentioned_in_paper": "197", "context_id": "21", "dataset_context": "Unlike previous approaches, our method does not require statistical body shape models, 3D pose annotations from MoCap dataset or human interventions to validate 3D pose.", "mention_start": 111, "mention_end": 125, "dataset_mention": "MoCap dataset"}, {"mentioned_in_paper": "197", "context_id": "33", "dataset_context": "The first approach in this line of work is done by Allen et al. [ACP03] where the authors fit a template 3D body model to Caesar dataset that contains a couple of thousand subjects and used principal component analysis (PCA) to model the space of human body shape.", "mention_start": 122, "mention_end": 136, "dataset_mention": "Caesar dataset"}, {"mentioned_in_paper": "197", "context_id": "153", "dataset_context": "We use ResNet50 [HZRS15] pretrained on the ImageNet dataset as the base network.", "mention_start": 39, "mention_end": 59, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "197", "context_id": "166", "dataset_context": "dataset and MS COCO dataset.", "mention_start": 0, "mention_end": 27, "dataset_mention": "dataset and MS COCO dataset"}, {"mentioned_in_paper": "197", "context_id": "175", "dataset_context": "DensePose DensePose dataset [RNI18] contains images with dense annotations of part-specific UV coordinates (Fig. 4), which are provided on the MS COCO images.", "mention_start": 0, "mention_end": 27, "dataset_mention": "DensePose DensePose dataset"}, {"mentioned_in_paper": "197", "context_id": "182", "dataset_context": "Human3.6M Human 3.6M dataset is a large scale dataset [IPOS14] for 3D human pose detection.", "mention_start": 0, "mention_end": 28, "dataset_mention": "Human3.6M Human 3.6M dataset"}, {"mentioned_in_paper": "197", "context_id": "188", "dataset_context": "The images from MPII 2D human pose dataset [APGB] is used for testing and was not used in training.", "mention_start": 16, "mention_end": 42, "dataset_mention": "MPII 2D human pose dataset"}, {"mentioned_in_paper": "197", "context_id": "206", "dataset_context": "Our method is on par with HMR (unpaired) that uses 3D pose and body shape dataset for training GANs to provide 3D constraints in an unsupervised learning manner.", "mention_start": 51, "mention_end": 81, "dataset_mention": "3D pose and body shape dataset"}, {"mentioned_in_paper": "197", "context_id": "209", "dataset_context": "Per-pixel and per-vertex error In order to evaluate alignment of a body model to images, we measured the mean per-vertex error and mean per-pixel error and compare with HMR (paired), which is shown in Table 2. HMR (paired) obtained better results on Human 3.6M dataset than ours in both vertex alignment and pixel alignment, as they use a large amount of 3D pose dataset paired with images whereas ours only use 2D annotations.", "mention_start": 249, "mention_end": 268, "dataset_mention": "Human 3.6M dataset"}, {"mentioned_in_paper": "197", "context_id": "211", "dataset_context": "For MS COCO dataset, our refinement was effective because this dataset is challenging for deep ConvNets to predict body parameters from due to a large variations in background, body shape/pose and clothing.", "mention_start": 4, "mention_end": 19, "dataset_mention": "MS COCO dataset"}, {"mentioned_in_paper": "198", "context_id": "21", "dataset_context": "We perform ablation studies on both synthetic data in section 8 and the actual dataset in section 9.", "mention_start": 54, "mention_end": 86, "dataset_mention": "section 8 and the actual dataset"}, {"mentioned_in_paper": "198", "context_id": "98", "dataset_context": "Combining features stated in previous sections, here is a compact way of defining the overall algorithm with custom- As a result, users can modify the behaviour of the architecture when performing approximation with varied models and datasets.", "mention_start": 215, "mention_end": 242, "dataset_mention": "varied models and datasets"}, {"mentioned_in_paper": "198", "context_id": "116", "dataset_context": "Algorithms described in this paper have been able to capture contexts in each case's dataset.", "mention_start": 73, "mention_end": 92, "dataset_mention": "each case's dataset"}, {"mentioned_in_paper": "198", "context_id": "117", "dataset_context": "For a series of economic observations, ML-APPROX has been able to interpret economic timeseries data with user-defined states using the case's historical dataset as background context.", "mention_start": 131, "mention_end": 161, "dataset_mention": "the case's historical dataset"}, {"mentioned_in_paper": "199", "context_id": "43", "dataset_context": "While these works provide comprehensive theoretical insights, they are typically applied to toy or lowresolution datasets and exhibit inferior results in terms of generation quality and diversity compared to state-of-theart GANs, such as ProgGAN [17] or StyleGAN2 [19].", "mention_start": 98, "mention_end": 121, "dataset_mention": "lowresolution datasets"}, {"mentioned_in_paper": "199", "context_id": "48", "dataset_context": "More specifically, [30, 18] use classifiers, pretrained on the CelebA dataset [24], in order to predict certain face attributes.", "mention_start": 58, "mention_end": 77, "dataset_mention": "the CelebA dataset"}, {"mentioned_in_paper": "199", "context_id": "114", "dataset_context": "In the experiments, we use the LeNet [21] backbone for SN-GAN (MNIST and Ani-meFaces datasets) and ResNet-18 [12] for BigGAN (Ima-geNet), ProgGAN (CelebA-HQ), and StyleGAN2 (FFHQ).", "mention_start": 54, "mention_end": 93, "dataset_mention": "SN-GAN (MNIST and Ani-meFaces datasets"}, {"mentioned_in_paper": "199", "context_id": "133", "dataset_context": "Path ID Non-linearity coefficient Pretrained GAN generators and datasets We evaluate the proposed method using the following pretrained GANs: a) Spectrally Normalized GAN (SN-GAN) [25] trained on MNIST [20] and AnimeFaces [15], b) BigGAN [3] trained on ImageNet [5], c) ProgGAN [17] trained on CelebA-HQ [24], and d) StyleGAN2 [19] trained on FFHQ [19].", "mention_start": 0, "mention_end": 72, "dataset_mention": "Path ID Non-linearity coefficient Pretrained GAN generators and datasets"}, {"mentioned_in_paper": "200", "context_id": "13", "dataset_context": "The recent work for the generation of scene graphs using an end-to-end model [25, 13, 26] gives the best results on the Visual Genome Dataset [11].", "mention_start": 115, "mention_end": 141, "dataset_mention": "the Visual Genome Dataset"}, {"mentioned_in_paper": "200", "context_id": "31", "dataset_context": "The main contributions of this paper are: 1) a novel structured representation (Social Relationship Graph) for social understanding in visual scenes; 2) a novel end-toend-trainable neural network architecture using GRUs and semantic attributes for graph-generation; 3) new state-ofthe art results for social relationship recognition on the PIPA-relation [22] and PISC [12] datasets.", "mention_start": 335, "mention_end": 381, "dataset_mention": "the PIPA-relation [22] and PISC [12] datasets"}, {"mentioned_in_paper": "200", "context_id": "114", "dataset_context": "The PIPA-relation dataset [22] has 16 fine-grained relationship categories 1.", "mention_start": 0, "mention_end": 25, "dataset_mention": "The PIPA-relation dataset"}, {"mentioned_in_paper": "200", "context_id": "115", "dataset_context": "We extend their dataset to a PIPArelation graph dataset.", "mention_start": 27, "mention_end": 55, "dataset_mention": "a PIPArelation graph dataset"}, {"mentioned_in_paper": "200", "context_id": "119", "dataset_context": "We construct our PIPA-relation graph dataset using two attributes (age and gender) from the attribute annotations published on the PIPA dataset [17].", "mention_start": 13, "mention_end": 44, "dataset_mention": "our PIPA-relation graph dataset"}, {"mentioned_in_paper": "200", "context_id": "119", "dataset_context": "We construct our PIPA-relation graph dataset using two attributes (age and gender) from the attribute annotations published on the PIPA dataset [17].", "mention_start": 127, "mention_end": 143, "dataset_mention": "the PIPA dataset"}, {"mentioned_in_paper": "200", "context_id": "121", "dataset_context": "We further validate the performance of our model on the large-scale People in Social Context (PISC) dataset released by Li et al. [12].", "mention_start": 78, "mention_end": 107, "dataset_mention": "Social Context (PISC) dataset"}, {"mentioned_in_paper": "200", "context_id": "122", "dataset_context": "The PISC dataset has 22,670 images where the person pairs are annotated for 3 coarse-grained relationships (intimate, not-intimate and no relation) and 6 fine-grained relationships (commercial, couple, family, friends, professional and no-relation).", "mention_start": 0, "mention_end": 16, "dataset_mention": "The PISC dataset"}, {"mentioned_in_paper": "200", "context_id": "124", "dataset_context": "Comparison models for PIPA-relation dataset: Our baselines are the two end-to-end models trained on the PIPA-relation dataset by Sun et al. [22] and the end-to-end model for Scene Graph Generation by Xu et al. [25] as below:", "mention_start": 22, "mention_end": 43, "dataset_mention": "PIPA-relation dataset"}, {"mentioned_in_paper": "200", "context_id": "124", "dataset_context": "Comparison models for PIPA-relation dataset: Our baselines are the two end-to-end models trained on the PIPA-relation dataset by Sun et al. [22] and the end-to-end model for Scene Graph Generation by Xu et al. [25] as below:", "mention_start": 99, "mention_end": 125, "dataset_mention": "the PIPA-relation dataset"}, {"mentioned_in_paper": "200", "context_id": "126", "dataset_context": "Finetuned model from pre-trained on Imagenet: Uses fixed weights of the conv layers from the Imagenet pretrained weights and fine-tuned the fully-connected layers on the PIPA-relation dataset.", "mention_start": 165, "mention_end": 191, "dataset_mention": "the PIPA-relation dataset"}, {"mentioned_in_paper": "200", "context_id": "127", "dataset_context": "Primal-Dual graph model: Trained the primal-dual graph model [25] on the PIPA-relation graph dataset.", "mention_start": 68, "mention_end": 100, "dataset_mention": "the PIPA-relation graph dataset"}, {"mentioned_in_paper": "200", "context_id": "128", "dataset_context": "Comparison models for PISC dataset: We compare our models with the models proposed by Li et al. [12].", "mention_start": 22, "mention_end": 34, "dataset_mention": "PISC dataset"}, {"mentioned_in_paper": "200", "context_id": "137", "dataset_context": "In case of PISC dataset, we only get scores for domain and relationships as there are no labels for attributes.", "mention_start": 11, "mention_end": 23, "dataset_mention": "PISC dataset"}, {"mentioned_in_paper": "200", "context_id": "142", "dataset_context": "We evaluate the performance of our model on the PIPArelation graph dataset and the PISC dataset.", "mention_start": 44, "mention_end": 74, "dataset_mention": "the PIPArelation graph dataset"}, {"mentioned_in_paper": "200", "context_id": "142", "dataset_context": "We evaluate the performance of our model on the PIPArelation graph dataset and the PISC dataset.", "mention_start": 44, "mention_end": 95, "dataset_mention": "the PIPArelation graph dataset and the PISC dataset"}, {"mentioned_in_paper": "200", "context_id": "143", "dataset_context": "The PIPArelation graph dataset additionally has 6 age labels (infant, child, young adult, middle age, senior and unknown) and 2 gender labels (male and female).", "mention_start": 0, "mention_end": 30, "dataset_mention": "The PIPArelation graph dataset"}, {"mentioned_in_paper": "200", "context_id": "151", "dataset_context": "Results on PIPA-relation dataset: In Table 1, we provide the accuracy for our first setup, SRRec.", "mention_start": 11, "mention_end": 32, "dataset_mention": "PIPA-relation dataset"}, {"mentioned_in_paper": "200", "context_id": "160", "dataset_context": "Results on PISC dataset: Table 4 compares the meanaverage precision evaluated on the PISC dataset for Social Relationship Recognition (SRRec).", "mention_start": 11, "mention_end": 23, "dataset_mention": "PISC dataset"}, {"mentioned_in_paper": "200", "context_id": "160", "dataset_context": "Results on PISC dataset: Table 4 compares the meanaverage precision evaluated on the PISC dataset for Social Relationship Recognition (SRRec).", "mention_start": 80, "mention_end": 97, "dataset_mention": "the PISC dataset"}, {"mentioned_in_paper": "200", "context_id": "161", "dataset_context": "Our final model with mean pooling and 2 time steps notably outperforms the state-of-the-art model on PISC dataset by \u223c8.5%.", "mention_start": 101, "mention_end": 113, "dataset_mention": "PISC dataset"}, {"mentioned_in_paper": "200", "context_id": "163", "dataset_context": "One possible reason is that the scene context in PISC dataset has similar contextual information for the relationships unlike in the PIPA-relation graph dataset.", "mention_start": 49, "mention_end": 61, "dataset_mention": "PISC dataset"}, {"mentioned_in_paper": "200", "context_id": "163", "dataset_context": "One possible reason is that the scene context in PISC dataset has similar contextual information for the relationships unlike in the PIPA-relation graph dataset.", "mention_start": 129, "mention_end": 160, "dataset_mention": "the PIPA-relation graph dataset"}, {"mentioned_in_paper": "200", "context_id": "170", "dataset_context": "Figure 3 shows qualitative results on PIPA-relation graph dataset to compare the SRG generated from our model and the ground truth.", "mention_start": 38, "mention_end": 65, "dataset_mention": "PIPA-relation graph dataset"}, {"mentioned_in_paper": "200", "context_id": "175", "dataset_context": "Figure 5 gives examples of the correct predictions on PISC dataset.", "mention_start": 54, "mention_end": 66, "dataset_mention": "PISC dataset"}, {"mentioned_in_paper": "200", "context_id": "180", "dataset_context": "In this section, we examine the performance of our SRG-GN model variations on the PIPA-Relation graph dataset.", "mention_start": 77, "mention_end": 109, "dataset_mention": "the PIPA-Relation graph dataset"}, {"mentioned_in_paper": "202", "context_id": "3", "dataset_context": "In this paper, we present the ApolloScape dataset [1] and its applications for autonomous driving.", "mention_start": 25, "mention_end": 49, "dataset_mention": "the ApolloScape dataset"}, {"mentioned_in_paper": "202", "context_id": "9", "dataset_context": "We expect our dataset and proposed relevant algorithms can support and motivate researchers for further development of multi-sensor fusion and multi-task learning in the field of computer vision.", "mention_start": 0, "mention_end": 21, "dataset_mention": "We expect our dataset"}, {"mentioned_in_paper": "202", "context_id": "47", "dataset_context": "Autonomous driving datasets and related algorithms has been an active research area for years.", "mention_start": 0, "mention_end": 27, "dataset_mention": "Autonomous driving datasets"}, {"mentioned_in_paper": "202", "context_id": "56", "dataset_context": "Most recently, the Cityscapes dataset [3] is specially collected for 2D segmentation which contains 30 semantic classes.", "mention_start": 14, "mention_end": 37, "dataset_mention": " the Cityscapes dataset"}, {"mentioned_in_paper": "202", "context_id": "60", "dataset_context": "Similarly, the Mapillary Vistas dataset [28] provides a larger set of images with fine annotations, which has 25,000 images with 66 object categories.", "mention_start": 10, "mention_end": 39, "dataset_mention": " the Mapillary Vistas dataset"}, {"mentioned_in_paper": "202", "context_id": "72", "dataset_context": "In Tab. 1, we compare the properties our dataset and other SOTA datasets for autonomous driving, and show that Apol-loScape is unique in terms of data scale, granularity of labelling, task variations within real environments.", "mention_start": 21, "mention_end": 48, "dataset_mention": "the properties our dataset"}, {"mentioned_in_paper": "202", "context_id": "72", "dataset_context": "In Tab. 1, we compare the properties our dataset and other SOTA datasets for autonomous driving, and show that Apol-loScape is unique in terms of data scale, granularity of labelling, task variations within real environments.", "mention_start": 21, "mention_end": 72, "dataset_mention": "the properties our dataset and other SOTA datasets"}, {"mentioned_in_paper": "202", "context_id": "131", "dataset_context": "The specifications of the classes are partially borrowed from the Cityscapes dataset.", "mention_start": 62, "mention_end": 84, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "202", "context_id": "338", "dataset_context": "( 3)) to train a model on Zpark dataset.", "mention_start": 26, "mention_end": 39, "dataset_mention": "Zpark dataset"}, {"mentioned_in_paper": "202", "context_id": "356", "dataset_context": "Finally at bottom of Tab. 4, we list corresponding results on Dlake dataset, which draws similar conclusion with that from Zpark dataset.", "mention_start": 61, "mention_end": 75, "dataset_mention": "Dlake dataset"}, {"mentioned_in_paper": "202", "context_id": "356", "dataset_context": "Finally at bottom of Tab. 4, we list corresponding results on Dlake dataset, which draws similar conclusion with that from Zpark dataset.", "mention_start": 122, "mention_end": 136, "dataset_mention": "Zpark dataset"}, {"mentioned_in_paper": "202", "context_id": "358", "dataset_context": "At top part of Tab. 5, we show the scene parsing results of Zpark dataset.", "mention_start": 59, "mention_end": 73, "dataset_mention": "Zpark dataset"}, {"mentioned_in_paper": "202", "context_id": "359", "dataset_context": "Firstly, we adopt one of the SOTA parsing network on the CityScapes, i.e., ResNet38 [76], and train it with Zpark dataset.", "mention_start": 107, "mention_end": 121, "dataset_mention": "Zpark dataset"}, {"mentioned_in_paper": "202", "context_id": "360", "dataset_context": "It utilizes pre-trained parameters from the CityScapes [3] dataset, and run with a 1.03s per-frame with our resolution.", "mention_start": 40, "mention_end": 66, "dataset_mention": "the CityScapes [3] dataset"}, {"mentioned_in_paper": "202", "context_id": "371", "dataset_context": "Bottom part of Tab. 5 shows the results over the larger Dlake dataset with more object labelling, where we see clearer improvement, i.e., from 62.36 to 67.00, and here the rendered label provides a background context for object segmentation, which also improve the object parsing performance.", "mention_start": 45, "mention_end": 69, "dataset_mention": "the larger Dlake dataset"}, {"mentioned_in_paper": "202", "context_id": "426", "dataset_context": "ApolloScape is significantly larger than existing autonomous driving datasets, e.g., KITTI [2] and Cityscapes [3], yielding more challenges for computer vision research field.", "mention_start": 41, "mention_end": 77, "dataset_mention": "existing autonomous driving datasets"}, {"mentioned_in_paper": "202", "context_id": "431", "dataset_context": "Last but not the least, ApolloScape is an evolving dataset, not only in terms of data scale, but also in terms of various driving conditions, tasks and acquisition devices.", "mention_start": 38, "mention_end": 58, "dataset_mention": "an evolving dataset"}, {"mentioned_in_paper": "204", "context_id": "7", "dataset_context": "Extensive experiments on benchmark molecular property prediction datasets show that PAR consistently outperforms existing methods and can obtain property-aware molecular embeddings and model molecular relation graph properly.", "mention_start": 25, "mention_end": 73, "dataset_mention": "benchmark molecular property prediction datasets"}, {"mentioned_in_paper": "204", "context_id": "27", "dataset_context": "This can be commonly observed in benchmark molecular property prediction datasets.", "mention_start": 33, "mention_end": 81, "dataset_mention": "benchmark molecular property prediction datasets"}, {"mentioned_in_paper": "204", "context_id": "28", "dataset_context": "As shown in Figure 1, Mol-1 and Mol-4 from the Tox21 dataset [19] have the same activity in SR-HSE task while acting differently in SR-MMP task.", "mention_start": 42, "mention_end": 60, "dataset_mention": "the Tox21 dataset"}, {"mentioned_in_paper": "204", "context_id": "35", "dataset_context": "\u2022 We conduct extensive empirical studies on real molecular property prediction datasets.", "mention_start": 44, "mention_end": 87, "dataset_mention": "real molecular property prediction datasets"}, {"mentioned_in_paper": "204", "context_id": "141", "dataset_context": "We perform experiments on widely used benchmark few-shot molecular property prediction datasets (Table 1) included in MoleculeNet [42].", "mention_start": 38, "mention_end": 95, "dataset_mention": "benchmark few-shot molecular property prediction datasets"}, {"mentioned_in_paper": "204", "context_id": "213", "dataset_context": "We perform experiments on widely used benchmark few-shot molecular property prediction datasets 3 : (i) Tox21 [19] contains assays each measuring the human toxicity of a biological target; (ii) SIDER [39] records the side effects for compounds used in marketed medicines, where the original 5868 side effect categories are grouped into 27 categories as in [3, 8]; (iii) MUV [1] is designed to validate virtual screening where active molecules are chosen to be structurally distinct from each another; and (iv) ToxCast [40] is a collection of compounds with toxicity labels which are obtained via high-throughput screening.", "mention_start": 38, "mention_end": 95, "dataset_mention": "benchmark few-shot molecular property prediction datasets"}, {"mentioned_in_paper": "205", "context_id": "28", "dataset_context": "First, we present a novel way of generating universal dataset for training universal detectors.", "mention_start": 43, "mention_end": 61, "dataset_mention": "universal dataset"}, {"mentioned_in_paper": "205", "context_id": "32", "dataset_context": "After training on the universal dataset, our NIX-Net can reliably recognize the regions inpainted by different deep inpainting methods.", "mention_start": 18, "mention_end": 39, "dataset_mention": "the universal dataset"}, {"mentioned_in_paper": "205", "context_id": "55", "dataset_context": "In contrast, we propose to generate a universal training dataset to capture the common characteristics shared by different deep inpainting methods, and train the detection model on this universal dataset.", "mention_start": 180, "mention_end": 203, "dataset_mention": "this universal dataset"}, {"mentioned_in_paper": "205", "context_id": "74", "dataset_context": "Our universal dataset U T distinguishes itself from existing inpainting-method-aware datasets as a general formulation of real versus generated contents.", "mention_start": 0, "mention_end": 21, "dataset_mention": "Our universal dataset"}, {"mentioned_in_paper": "205", "context_id": "74", "dataset_context": "Our universal dataset U T distinguishes itself from existing inpainting-method-aware datasets as a general formulation of real versus generated contents.", "mention_start": 52, "mention_end": 93, "dataset_mention": "existing inpainting-method-aware datasets"}, {"mentioned_in_paper": "205", "context_id": "75", "dataset_context": "However, having this dataset is not enough to train accurate deep inpainting detectors.", "mention_start": 8, "mention_end": 28, "dataset_mention": " having this dataset"}, {"mentioned_in_paper": "205", "context_id": "109", "dataset_context": "Note that the network can also be trained on any other inpainting detection datasets including the inpaintingmethod-aware datasets.", "mention_start": 45, "mention_end": 84, "dataset_mention": "any other inpainting detection datasets"}, {"mentioned_in_paper": "205", "context_id": "109", "dataset_context": "Note that the network can also be trained on any other inpainting detection datasets including the inpaintingmethod-aware datasets.", "mention_start": 95, "mention_end": 130, "dataset_mention": "the inpaintingmethod-aware datasets"}, {"mentioned_in_paper": "205", "context_id": "114", "dataset_context": "Inpainting methods and datasets.", "mention_start": 0, "mention_end": 31, "dataset_mention": "Inpainting methods and datasets"}, {"mentioned_in_paper": "205", "context_id": "131", "dataset_context": "Besides, we run our NIX-Net on a hybrid dataset that combines UT with one out of the 3 inpainting-method-aware datasets.", "mention_start": 30, "mention_end": 47, "dataset_mention": "a hybrid dataset"}, {"mentioned_in_paper": "205", "context_id": "131", "dataset_context": "Besides, we run our NIX-Net on a hybrid dataset that combines UT with one out of the 3 inpainting-method-aware datasets.", "mention_start": 80, "mention_end": 119, "dataset_mention": "the 3 inpainting-method-aware datasets"}, {"mentioned_in_paper": "205", "context_id": "137", "dataset_context": "When trained on the hybrid datasets, our NIX-Net achieved the best overall performance.", "mention_start": 16, "mention_end": 35, "dataset_mention": "the hybrid datasets"}, {"mentioned_in_paper": "205", "context_id": "146", "dataset_context": "Revisit Table 1, we find that, whenever the UT dataset is used in conjunction with one inpainting-method-aware dataset, the generalization performance of our NIX-Net can be significantly improved.", "mention_start": 30, "mention_end": 54, "dataset_mention": " whenever the UT dataset"}, {"mentioned_in_paper": "205", "context_id": "146", "dataset_context": "Revisit Table 1, we find that, whenever the UT dataset is used in conjunction with one inpainting-method-aware dataset, the generalization performance of our NIX-Net can be significantly improved.", "mention_start": 82, "mention_end": 118, "dataset_mention": "one inpainting-method-aware dataset"}, {"mentioned_in_paper": "205", "context_id": "147", "dataset_context": "Moreover, the UT dataset alone can lead to much better generalizability of existing methods LDICN and ManTra-Net.", "mention_start": 9, "mention_end": 24, "dataset_mention": " the UT dataset"}, {"mentioned_in_paper": "205", "context_id": "150", "dataset_context": "Another important observation is that NIX-Net trained on UT alone can achieve a similar level of performance as it was trained on the hybrid datasets, though combining more training data does improve the performance.", "mention_start": 130, "mention_end": 149, "dataset_mention": "the hybrid datasets"}, {"mentioned_in_paper": "205", "context_id": "158", "dataset_context": "All these networks are trained on the UT dataset generated from Places2 and tested on test images from Places2 by GL, CA and GC.", "mention_start": 34, "mention_end": 48, "dataset_mention": "the UT dataset"}, {"mentioned_in_paper": "206", "context_id": "48", "dataset_context": "This trend leads to and is further promoted by a number of publicly available computer vision data sets with human-labeled ground truth [15], [1], [16], [17], [18], [19].", "mention_start": 59, "mention_end": 103, "dataset_mention": "publicly available computer vision data sets"}, {"mentioned_in_paper": "206", "context_id": "53", "dataset_context": "Yu et al. [24] present a piecewise flat embedding learning algorithm and report the best published results so far on Berkeley Segmentation Data Set using the MCG framework.", "mention_start": 117, "mention_end": 147, "dataset_mention": "Berkeley Segmentation Data Set"}, {"mentioned_in_paper": "206", "context_id": "57", "dataset_context": "Moreover, impressive results in the extensive evaluations on six public segmentation data sets are reported in [25].", "mention_start": 60, "mention_end": 94, "dataset_mention": "six public segmentation data sets"}, {"mentioned_in_paper": "206", "context_id": "201", "dataset_context": "4) PASCAL Visual Object Classes Data Set (VOC12) [18] :", "mention_start": 0, "mention_end": 40, "dataset_mention": "4) PASCAL Visual Object Classes Data Set"}, {"mentioned_in_paper": "206", "context_id": "205", "dataset_context": "5) Stanford Background Data Set (SBD) [17] : 715 approximately 320 \u00d7 240 images of outdoor scenes with one ground truth per image.", "mention_start": 0, "mention_end": 31, "dataset_mention": "5) Stanford Background Data Set"}, {"mentioned_in_paper": "206", "context_id": "206", "dataset_context": "6) NYU Depth Data Set v2 (NYU) [19] : 1449 indoor scene images with one ground truth per image.", "mention_start": 0, "mention_end": 21, "dataset_mention": "6) NYU Depth Data Set"}, {"mentioned_in_paper": "206", "context_id": "269", "dataset_context": "It is noteworthy that the generalization of our method is almost as good as ISCRA [25] by being trained only on BSDS (general natural photos) and achieving competitive results on the NYU data set (indoor scene photos).", "mention_start": 179, "mention_end": 195, "dataset_mention": "the NYU data set"}, {"mentioned_in_paper": "206", "context_id": "294", "dataset_context": "We fixed the water level at 0.01 for all five datasets (BSDS300/500, MSRC, SBD, and VOC12), except the NYU data set.", "mention_start": 98, "mention_end": 115, "dataset_mention": "the NYU data set"}, {"mentioned_in_paper": "206", "context_id": "295", "dataset_context": "For the NYU data set of indoor scene images, we observe the decrease in gPb boundary detection strength, so we lower the water level to 0.001.", "mention_start": 4, "mention_end": 20, "dataset_mention": "the NYU data set"}, {"mentioned_in_paper": "207", "context_id": "11", "dataset_context": "High classification performances have been achieved on the BU-3DFE and Bosphorus datasets compared to the state-of-the-art methods.", "mention_start": 55, "mention_end": 89, "dataset_mention": "the BU-3DFE and Bosphorus datasets"}, {"mentioned_in_paper": "207", "context_id": "57", "dataset_context": "However, the FC layers of the pre-trained models are more dataset-specific features (generally pretrained on ImageNet dataset) which is a very different dataset, thus, this strategy is not suitable for FER in such a case.", "mention_start": 108, "mention_end": 125, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "207", "context_id": "61", "dataset_context": "Despite the high performances achieved by these models, the problem of over-fitting is still a grand challenge because the pre-trained models are generally trained with ImageNet dataset which has a lot of data belonging to very different classes, and the performance is highly depend on the ensemble strategy that cannot be meaningful for FER.", "mention_start": 168, "mention_end": 185, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "207", "context_id": "64", "dataset_context": "Our objectives are threefold: I) we avoid the overfitting that can be occurred after fine-tuning the pre-trained models to small FER datasets.", "mention_start": 122, "mention_end": 141, "dataset_mention": "small FER datasets"}, {"mentioned_in_paper": "207", "context_id": "66", "dataset_context": "III) Ensure a high discriminative power from the pre-trained models instead of applying scratch training, which is timeconsuming, and is not suitable for small 3D FER datasets.", "mention_start": 153, "mention_end": 175, "dataset_mention": "small 3D FER datasets"}, {"mentioned_in_paper": "207", "context_id": "91", "dataset_context": "In order to exploit the discriminative power of the deeply learned features using CNN, and the efficiency of covariance descriptors as a compact representation, we propose in this paper a deep and shallow feature combination method using covariance descriptors to handle the problem of FER on the two challenging BU-3DFE and Bosphorus datasets.", "mention_start": 312, "mention_end": 343, "dataset_mention": "BU-3DFE and Bosphorus datasets"}, {"mentioned_in_paper": "207", "context_id": "108", "dataset_context": "It contains 16 layers, trained on the ImageNet dataset which has over 14 million images and 1000 classes This model has been successfully used for face recognition [35] and facial expression recognition [36].", "mention_start": 33, "mention_end": 54, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "207", "context_id": "170", "dataset_context": "This section presents the datasets used to evaluate the proposed method and the experimental results compared to other methods in the literature using the same datasets.", "mention_start": 0, "mention_end": 34, "dataset_mention": "This section presents the datasets"}, {"mentioned_in_paper": "207", "context_id": "171", "dataset_context": "The Bosphorus dataset [12] which was made for testing the algorithms using 3D and 2D facial images for facial analysis and recognition tasks.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The Bosphorus dataset"}, {"mentioned_in_paper": "207", "context_id": "175", "dataset_context": "Fig. 4 presents some examples from the Bosphorus dataset for the same subject.", "mention_start": 35, "mention_end": 56, "dataset_mention": "the Bosphorus dataset"}, {"mentioned_in_paper": "207", "context_id": "176", "dataset_context": "BU-3DFE dataset (Binghamton University 3D Facial Expression) [11] is a multi-view facial expression database of 2500 images captured in lab-controlled environment.", "mention_start": 0, "mention_end": 15, "dataset_mention": "BU-3DFE dataset"}, {"mentioned_in_paper": "207", "context_id": "180", "dataset_context": "Fig. 5 presents an example of six expressive faces from BU-3DFE dataset.", "mention_start": 56, "mention_end": 71, "dataset_mention": "BU-3DFE dataset"}, {"mentioned_in_paper": "207", "context_id": "195", "dataset_context": "90 subjects of the BU-3DFE dataset are then used for training, where 10 subjects are used for the test.", "mention_start": 15, "mention_end": 34, "dataset_mention": "the BU-3DFE dataset"}, {"mentioned_in_paper": "207", "context_id": "203", "dataset_context": "To evaluate the proposed system on the Bosphorus dataset, we follow the standard protocol (10 fold-cross validation) as in prior methods [43].", "mention_start": 35, "mention_end": 56, "dataset_mention": "the Bosphorus dataset"}, {"mentioned_in_paper": "207", "context_id": "207", "dataset_context": "thus, distinguishing the subjects of Bosphorus dataset with fear and surprise expressions is a very hard task, which explains their confusion as presented in Fig. 8.", "mention_start": 36, "mention_end": 54, "dataset_mention": "Bosphorus dataset"}, {"mentioned_in_paper": "207", "context_id": "213", "dataset_context": "Table 2 presents the performance comparison between the proposed method with those of the literature on the Bosphorus dataset.", "mention_start": 104, "mention_end": 125, "dataset_mention": "the Bosphorus dataset"}, {"mentioned_in_paper": "207", "context_id": "230", "dataset_context": "Fig. 9a presents the improvement of the classification rate according to the codebook size using the shallow and VGG-16 co-varied features on BU-3DFE dataset.", "mention_start": 142, "mention_end": 157, "dataset_mention": "BU-3DFE dataset"}, {"mentioned_in_paper": "207", "context_id": "235", "dataset_context": "The same study has been conducted on the Bosphorus dataset.", "mention_start": 37, "mention_end": 58, "dataset_mention": "the Bosphorus dataset"}, {"mentioned_in_paper": "207", "context_id": "257", "dataset_context": "More particularly, the reported results show that the pre-trained deep features on depth map images outperformed the curvature-based one when dealing with BU-3DFE and Bosphorus datasets.", "mention_start": 154, "mention_end": 185, "dataset_mention": "BU-3DFE and Bosphorus datasets"}, {"mentioned_in_paper": "207", "context_id": "269", "dataset_context": "The reported results are the overall accuracies obtained using Bosphoros dataset.", "mention_start": 63, "mention_end": 80, "dataset_mention": "Bosphoros dataset"}, {"mentioned_in_paper": "207", "context_id": "286", "dataset_context": "The displayed performances obtained on the BU-3DFE and Bosphorus datasets demonstrate that the deep codebook gives higher classification rates comparing to the shallow codebook.", "mention_start": 39, "mention_end": 73, "dataset_mention": "the BU-3DFE and Bosphorus datasets"}, {"mentioned_in_paper": "207", "context_id": "290", "dataset_context": "In future work, dynamic features could be added to the proposed system to boost the classification rate of the 3D FER (BU-4DFE dataset).", "mention_start": 106, "mention_end": 134, "dataset_mention": "the 3D FER (BU-4DFE dataset"}, {"mentioned_in_paper": "207", "context_id": "292", "dataset_context": "We also look at the application of generative models to provide additional training data to deal with the problem of small 3D FER datasets and to further enhance the efficiency of the proposed framework.", "mention_start": 117, "mention_end": 138, "dataset_mention": "small 3D FER datasets"}, {"mentioned_in_paper": "209", "context_id": "238", "dataset_context": "We compare our tracking accuracy with BodyFusion [43] using their public vicon dataset.", "mention_start": 60, "mention_end": 86, "dataset_mention": "their public vicon dataset"}, {"mentioned_in_paper": "210", "context_id": "9", "dataset_context": "Experiments show that Rend-Net can achieve state-of-the-art performance on 2D and 3D object recognition tasks on various VG datasets.", "mention_start": 113, "mention_end": 132, "dataset_mention": "various VG datasets"}, {"mentioned_in_paper": "210", "context_id": "166", "dataset_context": "In this section, we evaluate RendNet on different tasks and datasets to examine the effectiveness of RendNet on 2D and 3D vector graphics recognition problems.", "mention_start": 39, "mention_end": 68, "dataset_mention": "different tasks and datasets"}, {"mentioned_in_paper": "210", "context_id": "192", "dataset_context": "We evaluate RendNet for 3D object recognition on the FeatureNet [54] and Cluster3D [51] datasets.", "mention_start": 49, "mention_end": 96, "dataset_mention": "the FeatureNet [54] and Cluster3D [51] datasets"}, {"mentioned_in_paper": "210", "context_id": "206", "dataset_context": "In Cluster3D, around 200000 similarity/dissimilarity pairs are annotated in a subset of the ABC dataset [22] that contains around 20000 CAD models.", "mention_start": 87, "mention_end": 103, "dataset_mention": "the ABC dataset"}, {"mentioned_in_paper": "210", "context_id": "208", "dataset_context": "Thus, topology does not account for so much in Cluster3D as other VG-based datasets, such as FeatureNet.", "mention_start": 59, "mention_end": 83, "dataset_mention": "other VG-based datasets"}, {"mentioned_in_paper": "210", "context_id": "219", "dataset_context": "Here we validate our model design by running a classification task on the SESYD floor plan dataset.", "mention_start": 70, "mention_end": 98, "dataset_mention": "the SESYD floor plan dataset"}, {"mentioned_in_paper": "210", "context_id": "239", "dataset_context": "To investigate the representations learnt by RendNet, we trained a RendNet in an unsupervised manner with BYOL [13] on the ABC [22] dataset.", "mention_start": 118, "mention_end": 139, "dataset_mention": "the ABC [22] dataset"}, {"mentioned_in_paper": "210", "context_id": "251", "dataset_context": "In the future, we can integrate more techniques to further improve the performance, such as pre-training on large VG datasets.", "mention_start": 107, "mention_end": 125, "dataset_mention": "large VG datasets"}, {"mentioned_in_paper": "212", "context_id": "1", "dataset_context": "To facilitate further research on this topic, we provide an injury specific 2D dataset for alpine skiing, covering in total 533 images.", "mention_start": 56, "mention_end": 86, "dataset_mention": "an injury specific 2D dataset"}, {"mentioned_in_paper": "212", "context_id": "18", "dataset_context": "To overcome some of these challenges, Bachmann et al. [1] created a skiing specific 2D dataset and trained OpenPose [4] on this dataset, achieving good results in regular skiing scenarios.", "mention_start": 65, "mention_end": 94, "dataset_mention": "a skiing specific 2D dataset"}, {"mentioned_in_paper": "212", "context_id": "21", "dataset_context": "With the goal of developing a deep learning based motion capture tool for the analysis of injury mechanisms, we created an injury specific 2D dataset for alpine skiing, covering 533 manually annotated video frames.", "mention_start": 119, "mention_end": 149, "dataset_mention": "an injury specific 2D dataset"}, {"mentioned_in_paper": "212", "context_id": "23", "dataset_context": "Running state-of-the-art algorithms on our injury dataset [7, 6], we evaluated their performance during 'regular skiing', as well as in 'out-ofbalance' and fall scenarios and identified major difficulties and failure cases.", "mention_start": 39, "mention_end": 57, "dataset_mention": "our injury dataset"}, {"mentioned_in_paper": "212", "context_id": "26", "dataset_context": "We created an 'out-of-balance' and fall situation-specific 2D dataset for alpine skiing covering a total of 533 sample images (Figure 1).", "mention_start": 11, "mention_end": 69, "dataset_mention": "an 'out-of-balance' and fall situation-specific 2D dataset"}, {"mentioned_in_paper": "212", "context_id": "37", "dataset_context": "Our injury specific 2D dataset is made available online for further research.", "mention_start": 0, "mention_end": 30, "dataset_mention": "Our injury specific 2D dataset"}, {"mentioned_in_paper": "212", "context_id": "53", "dataset_context": "To optimize these parameters ideally for our injury specific dataset, we conducted a grid search over all parameters k, \u03b1, \u03b2, \u03b3, which revealed a best overall performance for k = 12, \u03b1 = 1, \u03b2 = 1 and \u03b3 = 0.", "mention_start": 41, "mention_end": 68, "dataset_mention": "our injury specific dataset"}, {"mentioned_in_paper": "212", "context_id": "56", "dataset_context": "To evaluate the performance of the proposed post processing routine, we ran DCPose and AlphaPose in their pretrained configuration on our injury specific dataset, with and without applying the post processing.", "mention_start": 133, "mention_end": 161, "dataset_mention": "our injury specific dataset"}, {"mentioned_in_paper": "212", "context_id": "78", "dataset_context": "Running state-of-the-art algorithms, DCPose and Alpha-Pose, on our injury specific dataset, we observed high performance for 'regular skiing' scenarios in PCK@0.2 (0.9 and higher) as well as in AP (0.78).", "mention_start": 62, "mention_end": 90, "dataset_mention": "our injury specific dataset"}, {"mentioned_in_paper": "212", "context_id": "97", "dataset_context": "Using our injury specific dataset for training and refining keypoint detection algorithms could bring further improvements, e.g. by a transfer learning approach.", "mention_start": 6, "mention_end": 33, "dataset_mention": "our injury specific dataset"}, {"mentioned_in_paper": "213", "context_id": "7", "dataset_context": "On a 64 GPU cluster, for fine-tuning on the AG's news dataset, AutoFreeze is able to achieve up to 4.38\u00d7 speedup when optimizing for end-to-end training time and 5.03\u00d7 reduction in total cost when optimizing for efficiency, without affecting model accuracy.", "mention_start": 39, "mention_end": 61, "dataset_mention": "the AG's news dataset"}, {"mentioned_in_paper": "213", "context_id": "26", "dataset_context": "with the Yelp dataset [69] can take around 27 hours 1 in a four P100 GPU cluster.", "mention_start": 5, "mention_end": 21, "dataset_mention": "the Yelp dataset"}, {"mentioned_in_paper": "213", "context_id": "33", "dataset_context": "For example with the MRPC dataset [11] this approach can reduce the time for an epoch by 2\u00d7, but we find that the accuracy of the fine-tuned model suffers, dropping from 87% to 76.5%.", "mention_start": 17, "mention_end": 33, "dataset_mention": "the MRPC dataset"}, {"mentioned_in_paper": "213", "context_id": "58", "dataset_context": "including topic classification on the AG's News dataset [69] and Sogou News dataset [55], sentiment analysis on Yelp Full dataset [69] and IMDb dataset [32], question answering on SQuAD2.0 dataset [46], multiple choice task on SWAG dataset [67], and text summarization on CNN/DailyMail dataset [16].", "mention_start": 34, "mention_end": 55, "dataset_mention": "the AG's News dataset"}, {"mentioned_in_paper": "213", "context_id": "58", "dataset_context": "including topic classification on the AG's News dataset [69] and Sogou News dataset [55], sentiment analysis on Yelp Full dataset [69] and IMDb dataset [32], question answering on SQuAD2.0 dataset [46], multiple choice task on SWAG dataset [67], and text summarization on CNN/DailyMail dataset [16].", "mention_start": 34, "mention_end": 83, "dataset_mention": "the AG's News dataset [69] and Sogou News dataset"}, {"mentioned_in_paper": "213", "context_id": "58", "dataset_context": "including topic classification on the AG's News dataset [69] and Sogou News dataset [55], sentiment analysis on Yelp Full dataset [69] and IMDb dataset [32], question answering on SQuAD2.0 dataset [46], multiple choice task on SWAG dataset [67], and text summarization on CNN/DailyMail dataset [16].", "mention_start": 111, "mention_end": 129, "dataset_mention": "Yelp Full dataset"}, {"mentioned_in_paper": "213", "context_id": "58", "dataset_context": "including topic classification on the AG's News dataset [69] and Sogou News dataset [55], sentiment analysis on Yelp Full dataset [69] and IMDb dataset [32], question answering on SQuAD2.0 dataset [46], multiple choice task on SWAG dataset [67], and text summarization on CNN/DailyMail dataset [16].", "mention_start": 111, "mention_end": 151, "dataset_mention": "Yelp Full dataset [69] and IMDb dataset"}, {"mentioned_in_paper": "213", "context_id": "58", "dataset_context": "including topic classification on the AG's News dataset [69] and Sogou News dataset [55], sentiment analysis on Yelp Full dataset [69] and IMDb dataset [32], question answering on SQuAD2.0 dataset [46], multiple choice task on SWAG dataset [67], and text summarization on CNN/DailyMail dataset [16].", "mention_start": 179, "mention_end": 196, "dataset_mention": "SQuAD2.0 dataset"}, {"mentioned_in_paper": "213", "context_id": "58", "dataset_context": "including topic classification on the AG's News dataset [69] and Sogou News dataset [55], sentiment analysis on Yelp Full dataset [69] and IMDb dataset [32], question answering on SQuAD2.0 dataset [46], multiple choice task on SWAG dataset [67], and text summarization on CNN/DailyMail dataset [16].", "mention_start": 226, "mention_end": 239, "dataset_mention": "SWAG dataset"}, {"mentioned_in_paper": "213", "context_id": "58", "dataset_context": "including topic classification on the AG's News dataset [69] and Sogou News dataset [55], sentiment analysis on Yelp Full dataset [69] and IMDb dataset [32], question answering on SQuAD2.0 dataset [46], multiple choice task on SWAG dataset [67], and text summarization on CNN/DailyMail dataset [16].", "mention_start": 271, "mention_end": 293, "dataset_mention": "CNN/DailyMail dataset"}, {"mentioned_in_paper": "213", "context_id": "61", "dataset_context": "In distributed fine-tuning AutoFreeze can significantly improve training time and efficiency, reducing the end-to-end training time by 4.4\u00d7 with the performance packing mode and reducing total cost by 5.03\u00d7 when using the efficiency packing mode for fine tuning on the AG's News dataset in a 64 GPU cluster.", "mention_start": 264, "mention_end": 286, "dataset_mention": "the AG's News dataset"}, {"mentioned_in_paper": "213", "context_id": "65", "dataset_context": "Transfer learning and fine tuning of large pre-trained models has enabled easy use of deep models for new tasks and new datasets [55, 60, 70, 28].", "mention_start": 102, "mention_end": 128, "dataset_mention": "new tasks and new datasets"}, {"mentioned_in_paper": "213", "context_id": "73", "dataset_context": "For example, fine tuning BERT on the relatively small IMDB dataset [32], containing 25K points, takes around 3 hours on a single P100 GPU.", "mention_start": 32, "mention_end": 66, "dataset_mention": "the relatively small IMDB dataset"}, {"mentioned_in_paper": "213", "context_id": "76", "dataset_context": "Even on the latest A100 GPU, fine-tuning BERT LARGE on the Yelp dataset can take around two hours.", "mention_start": 54, "mention_end": 71, "dataset_mention": "the Yelp dataset"}, {"mentioned_in_paper": "213", "context_id": "87", "dataset_context": "For example, when fine-tuning with the IMDb dataset, we see that doing one iteration takes around 435ms of which more than 50% is taken by the backward pass.", "mention_start": 34, "mention_end": 51, "dataset_mention": "the IMDb dataset"}, {"mentioned_in_paper": "213", "context_id": "99", "dataset_context": "In Figure 4 we compare static freezing schemes when fine-tuning BERT BASE with IMDb and Sogou dataset.", "mention_start": 79, "mention_end": 101, "dataset_mention": "IMDb and Sogou dataset"}, {"mentioned_in_paper": "213", "context_id": "103", "dataset_context": "We also see similar results for an image classification workload where we fine-tune ResNet-18 model [15] pre-trained on CINIC-10 [9] with CIFAR-10 dataset.", "mention_start": 138, "mention_end": 154, "dataset_mention": "CIFAR-10 dataset"}, {"mentioned_in_paper": "213", "context_id": "120", "dataset_context": "We use the same IMDB dataset as before and compute the SVCCA scores by comparing each layer's weights periodically (5 times every epoch in this case) with the final weights of the model.", "mention_start": 7, "mention_end": 28, "dataset_mention": "the same IMDB dataset"}, {"mentioned_in_paper": "213", "context_id": "155", "dataset_context": "To solve the first consideration, we measure the size and time taken for reading intermediate outputs when fine-tuning the Yelp dataset.", "mention_start": 106, "mention_end": 135, "dataset_mention": "fine-tuning the Yelp dataset"}, {"mentioned_in_paper": "213", "context_id": "222", "dataset_context": "We next evaluate AutoFreeze on a number of NLP datasets and tasks, and measure the performance benefits and model accuracy.", "mention_start": 43, "mention_end": 55, "dataset_mention": "NLP datasets"}, {"mentioned_in_paper": "213", "context_id": "224", "dataset_context": "In our experimental study we evaluate AutoFreeze on-(i) four text classification datasets, (ii) one question answering dataset, (iii) one multiple choice dataset, (iv) one combined text summarization dataset.", "mention_start": 38, "mention_end": 89, "dataset_mention": "AutoFreeze on-(i) four text classification datasets"}, {"mentioned_in_paper": "213", "context_id": "224", "dataset_context": "In our experimental study we evaluate AutoFreeze on-(i) four text classification datasets, (ii) one question answering dataset, (iii) one multiple choice dataset, (iv) one combined text summarization dataset.", "mention_start": 90, "mention_end": 126, "dataset_mention": " (ii) one question answering dataset"}, {"mentioned_in_paper": "213", "context_id": "224", "dataset_context": "In our experimental study we evaluate AutoFreeze on-(i) four text classification datasets, (ii) one question answering dataset, (iii) one multiple choice dataset, (iv) one combined text summarization dataset.", "mention_start": 127, "mention_end": 161, "dataset_mention": " (iii) one multiple choice dataset"}, {"mentioned_in_paper": "213", "context_id": "224", "dataset_context": "In our experimental study we evaluate AutoFreeze on-(i) four text classification datasets, (ii) one question answering dataset, (iii) one multiple choice dataset, (iv) one combined text summarization dataset.", "mention_start": 180, "mention_end": 207, "dataset_mention": "text summarization dataset"}, {"mentioned_in_paper": "213", "context_id": "240", "dataset_context": "For example, we observe that freezing up to the 9 th layer of the BERT encoder results in around 11% accuracy loss for MRPC, while it only results in around 1% accuracy loss for the SST-2 dataset.", "mention_start": 177, "mention_end": 195, "dataset_mention": "the SST-2 dataset"}, {"mentioned_in_paper": "213", "context_id": "247", "dataset_context": "From the figures we see that for the Sogou and IMDb datasets, we observe 0.07% and 0.1% reduction in mean of the best accuracy, while for AG News and Yelp F. datasets, we do not see any accuracy loss.", "mention_start": 33, "mention_end": 60, "dataset_mention": "the Sogou and IMDb datasets"}, {"mentioned_in_paper": "213", "context_id": "247", "dataset_context": "From the figures we see that for the Sogou and IMDb datasets, we observe 0.07% and 0.1% reduction in mean of the best accuracy, while for AG News and Yelp F. datasets, we do not see any accuracy loss.", "mention_start": 137, "mention_end": 166, "dataset_mention": "AG News and Yelp F. datasets"}, {"mentioned_in_paper": "213", "context_id": "252", "dataset_context": "We observe AutoFreeze is particularly helpful on large datasets, on AG's News dataset, a large dataset with 120K samples, AutoFreeze is able to save around 5 hours fine-tuning time.", "mention_start": 67, "mention_end": 85, "dataset_mention": "AG's News dataset"}, {"mentioned_in_paper": "213", "context_id": "269", "dataset_context": "As for the total cost, Efficiency Packing reduces the total cost by 5.03\u00d7 and 5.21\u00d7 for AG's News and Sogou News datasets, while Performance Packing reduces the total cost by 4.38\u00d7 and 4.74\u00d7 for AG's News and Sogou News datasets when compared against full fine tuning.", "mention_start": 87, "mention_end": 121, "dataset_mention": "AG's News and Sogou News datasets"}, {"mentioned_in_paper": "213", "context_id": "269", "dataset_context": "As for the total cost, Efficiency Packing reduces the total cost by 5.03\u00d7 and 5.21\u00d7 for AG's News and Sogou News datasets, while Performance Packing reduces the total cost by 4.38\u00d7 and 4.74\u00d7 for AG's News and Sogou News datasets when compared against full fine tuning.", "mention_start": 194, "mention_end": 228, "dataset_mention": "AG's News and Sogou News datasets"}, {"mentioned_in_paper": "213", "context_id": "270", "dataset_context": "Breaking down the benefits, when we freeze 11 BERT layers for an epoch with the AG's News dataset, the average iteration time for Performance Packing with 64 machines is 1.05 seconds and there are 35 minibatches in an epoch.", "mention_start": 75, "mention_end": 97, "dataset_mention": "the AG's News dataset"}, {"mentioned_in_paper": "213", "context_id": "272", "dataset_context": "We also measured the accuracy for both Efficiency Packing and Performance Packing and compare them to full fine-tuning without changing number of GPUs or per GPU batch size on AG's News and Sogou datasets (Table 3).", "mention_start": 176, "mention_end": 204, "dataset_mention": "AG's News and Sogou datasets"}, {"mentioned_in_paper": "213", "context_id": "288", "dataset_context": "CINIC-10 transfer learning: Although BERT fine tuning is our primary focus, we also evaluate Aut-oFreeze on the vision task described in Section 2. We take a ResNet-18 model which is pre-trained with CINIC-10 and fine-tune this model for the CIFAR-10 dataset.", "mention_start": 237, "mention_end": 258, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "213", "context_id": "292", "dataset_context": "We set up the experiments on a p4d.24xlarge instance with eight A100s on AWS, and ran AutoFreeze on the AG's News dataset.", "mention_start": 99, "mention_end": 121, "dataset_mention": "the AG's News dataset"}, {"mentioned_in_paper": "213", "context_id": "299", "dataset_context": "For example, if we set N to be 75th percentile which results in a more aggressive policy, we get 0.39% accuracy loss for the IMDb dataset.", "mention_start": 120, "mention_end": 137, "dataset_mention": "the IMDb dataset"}, {"mentioned_in_paper": "213", "context_id": "341", "dataset_context": "Table 7 shows similar measurements for the SWAG dataset.", "mention_start": 39, "mention_end": 55, "dataset_mention": "the SWAG dataset"}, {"mentioned_in_paper": "213", "context_id": "352", "dataset_context": "We fine-tune BERT BASE on Sogou dataset for 6 epochs using stepped learning rate schedule with initial learning rate of 1e-5.", "mention_start": 26, "mention_end": 39, "dataset_mention": "Sogou dataset"}, {"mentioned_in_paper": "213", "context_id": "359", "dataset_context": ": Benefits from AutoFreeze when the Sogou dataset is fine-tuned for more epochs (6 epochs).", "mention_start": 31, "mention_end": 49, "dataset_mention": "the Sogou dataset"}, {"mentioned_in_paper": "215", "context_id": "20", "dataset_context": "Aggregating and curating these disparate datasets lead to a valuable commodity that clients are willing to pay for gleaned insights, and to enrich and clean their individual databases.", "mention_start": 25, "mention_end": 49, "dataset_mention": "these disparate datasets"}, {"mentioned_in_paper": "215", "context_id": "382", "dataset_context": "PrivateClean provides a mechanism to generate -differentially private datasets on numerical and discrete values from which a data analyst can apply data cleaning operations.", "mention_start": 46, "mention_end": 78, "dataset_mention": "-differentially private datasets"}, {"mentioned_in_paper": "215", "context_id": "384", "dataset_context": "To guarantee a privatized dataset with discrete attributes is -differentially private at confidence (1 \u2212 \u03b1), a sufficient number of distinct records is needed.", "mention_start": 3, "mention_end": 33, "dataset_mention": "guarantee a privatized dataset"}, {"mentioned_in_paper": "215", "context_id": "406", "dataset_context": "Figure 5 shows the relative proportions for varying B values over the clinical dataset.", "mention_start": 66, "mention_end": 86, "dataset_mention": "the clinical dataset"}, {"mentioned_in_paper": "215", "context_id": "424", "dataset_context": "This is reflected in longer runtimes for the larger clinical and census datasets.", "mention_start": 41, "mention_end": 80, "dataset_mention": "the larger clinical and census datasets"}, {"mentioned_in_paper": "215", "context_id": "445", "dataset_context": "Despite the differing privacy models, our evaluation aims to explore the influence of increasing error rates on the repair error \u03b4, and algorithm runtimes using the Clinical dataset.", "mention_start": 160, "mention_end": 181, "dataset_mention": "the Clinical dataset"}, {"mentioned_in_paper": "215", "context_id": "482", "dataset_context": "In contrast, PACAS applies generalization to target predicates in the given query requests, without randomizing the entire dataset, albeit with looser privacy guarantees, but with higher data utility.", "mention_start": 91, "mention_end": 130, "dataset_mention": " without randomizing the entire dataset"}, {"mentioned_in_paper": "216", "context_id": "25", "dataset_context": "On multioriented ship detection dataset HRSC2016 [Liu et al., 2017], our method can still perform the best, further showing its promising generalization ability.", "mention_start": 3, "mention_end": 39, "dataset_mention": "multioriented ship detection dataset"}, {"mentioned_in_paper": "216", "context_id": "83", "dataset_context": "ICDAR 2017 MLT. [Nayef et al., 2017] is the largest multi-lingual (9 languages) oriented scene text dataset, including 7.2k training samples, 1.8k validation samples and 9k testing samples.", "mention_start": 39, "mention_end": 107, "dataset_mention": "the largest multi-lingual (9 languages) oriented scene text dataset"}, {"mentioned_in_paper": "216", "context_id": "91", "dataset_context": " [Yao et al., 2012] is a text-line based oriented dataset with 300 training images and 200 testing images captured from indoor and outdoor scenes.", "mention_start": 22, "mention_end": 57, "dataset_mention": "a text-line based oriented dataset"}, {"mentioned_in_paper": "216", "context_id": "103", "dataset_context": "To demonstrate generalization ability of SBD, we further evaluated and compared SBD on Level 1 task of the HRSC2016 dataset [Liu et al., 2017] to show our method's performance on multi-directional object detection.", "mention_start": 102, "mention_end": 123, "dataset_mention": "the HRSC2016 dataset"}, {"mentioned_in_paper": "216", "context_id": "107", "dataset_context": "Table 6 : Experimental results on HRS 2016 dataset.", "mention_start": 33, "mention_end": 50, "dataset_mention": "HRS 2016 dataset"}, {"mentioned_in_paper": "216", "context_id": "113", "dataset_context": "To test generalization ability, we further conducted an experiment on oriented general object dataset HRSC2016, and the results showed that our method can outperform recent state-of-the-art methods with a large margin.", "mention_start": 69, "mention_end": 101, "dataset_mention": "oriented general object dataset"}, {"mentioned_in_paper": "218", "context_id": "5", "dataset_context": "Empirical results show the benefits of the approach, obtaining state-ofthe-art performance on the open-sourced nuScenes data set.", "mention_start": 93, "mention_end": 128, "dataset_mention": "the open-sourced nuScenes data set"}, {"mentioned_in_paper": "218", "context_id": "92", "dataset_context": "We evaluated our method on the public nuScenes [19] data set.", "mention_start": 27, "mention_end": 60, "dataset_mention": "the public nuScenes [19] data set"}, {"mentioned_in_paper": "218", "context_id": "150", "dataset_context": "Experiments on the real-world, open-source nuScenes data set indicate the benefits of the proposed method.", "mention_start": 30, "mention_end": 60, "dataset_mention": " open-source nuScenes data set"}, {"mentioned_in_paper": "219", "context_id": "121", "dataset_context": "Table 1 : Runtimes (wall clock time in seconds) of ADMM used in [Steck, 2020] and of the proposed approximation for different model-ranks k on the ML-20M dataset, using an AWS instance with 128 GB memory and 16 vCPUs.", "mention_start": 142, "mention_end": 161, "dataset_mention": "the ML-20M dataset"}, {"mentioned_in_paper": "219", "context_id": "124", "dataset_context": "Only for very large model-ranks k, ADMM is faster on the ML-20M data set in Table 1.", "mention_start": 52, "mention_end": 72, "dataset_mention": "the ML-20M data set"}, {"mentioned_in_paper": "220", "context_id": "4", "dataset_context": "Experiments on two standard benchmarks, Dataset-CASIA and Dataset-ICDAR, yielded outstanding results, with correct rates of 97.10% and 97.15%, respectively, which are significantly better than the best result reported thus far in the literature.", "mention_start": 39, "mention_end": 65, "dataset_mention": " Dataset-CASIA and Dataset"}, {"mentioned_in_paper": "220", "context_id": "235", "dataset_context": "To evaluate the effectiveness of the proposed system, we conducted experiments on the standard benchmark dataset CASIA-OLHWDB [55] and the ICDAR2013 Chinese handwriting recognition competition dataset [56] for unconstrained online handwritten Chinese text recognition.", "mention_start": 81, "mention_end": 200, "dataset_mention": "the standard benchmark dataset CASIA-OLHWDB [55] and the ICDAR2013 Chinese handwriting recognition competition dataset"}, {"mentioned_in_paper": "220", "context_id": "244", "dataset_context": "Two popular benchmark datasets for unconstrained online handwritten Chinese text recognition were used for performance evaluation, i.e., the test set of CASIA2.0-2.2 (Dataset-CASIA) and the test set of the online handwritten Chinese text recognition task of the ICDAR 2013 Chinese handwriting recognition competition [56] (Dataset-ICDAR).", "mention_start": 152, "mention_end": 174, "dataset_mention": "CASIA2.0-2.2 (Dataset"}, {"mentioned_in_paper": "220", "context_id": "244", "dataset_context": "Two popular benchmark datasets for unconstrained online handwritten Chinese text recognition were used for performance evaluation, i.e., the test set of CASIA2.0-2.2 (Dataset-CASIA) and the test set of the online handwritten Chinese text recognition task of the ICDAR 2013 Chinese handwriting recognition competition [56] (Dataset-ICDAR).", "mention_start": 257, "mention_end": 330, "dataset_mention": "the ICDAR 2013 Chinese handwriting recognition competition [56] (Dataset"}, {"mentioned_in_paper": "220", "context_id": "268", "dataset_context": "The experiments showed that the system performance improved monotonically for both Dataset-ICDAR and Dataset-CASIA in the order of FCRN, 2C-FCRN, and 3C-FCRN, suggesting that we successfully leveraged the multiple spatial contexts by using multiple receptive fields and improved the system performance.", "mention_start": 78, "mention_end": 108, "dataset_mention": "both Dataset-ICDAR and Dataset"}, {"mentioned_in_paper": "220", "context_id": "283", "dataset_context": "Our MC-FCRN significantly outperformed CRNN on both Dataset-ICDAR and Dataset-CASIA, suggesting that MC-FCRN captures more essential spatial context information and online information from the pen-tip trajectories and is thus a better choice for the OHCTR problem.", "mention_start": 47, "mention_end": 77, "dataset_mention": "both Dataset-ICDAR and Dataset"}, {"mentioned_in_paper": "220", "context_id": "305", "dataset_context": "In the experiments, our best result significantly outperformed all other existing methods, with relative error reductions of 43% and 38% in terms of the correct rate on two standard benchmarks, Dataset-ICDAR and Dataset-CASIA, respectively.", "mention_start": 193, "mention_end": 219, "dataset_mention": " Dataset-ICDAR and Dataset"}, {"mentioned_in_paper": "222", "context_id": "21", "dataset_context": "First, the model compresses a MOCAP dataset compactly, encoding it in its weights by developing an internal latent space representation of motion.", "mention_start": 6, "mention_end": 43, "dataset_mention": " the model compresses a MOCAP dataset"}, {"mentioned_in_paper": "222", "context_id": "160", "dataset_context": "The Anidance dataset was originally proposed as a music-to-dance generation dataset (Tang, Mao, and Jia 2018).", "mention_start": 0, "mention_end": 20, "dataset_mention": "The Anidance dataset"}, {"mentioned_in_paper": "222", "context_id": "160", "dataset_context": "The Anidance dataset was originally proposed as a music-to-dance generation dataset (Tang, Mao, and Jia 2018).", "mention_start": 48, "mention_end": 83, "dataset_mention": "a music-to-dance generation dataset"}, {"mentioned_in_paper": "222", "context_id": "167", "dataset_context": "Table 2 does not contain L2Q and NPSS metrics because the Anidance dataset does not contain angular information.", "mention_start": 54, "mention_end": 74, "dataset_mention": "the Anidance dataset"}, {"mentioned_in_paper": "222", "context_id": "270", "dataset_context": " Duan et al. (2022) extracted the human motion data and formulated a dataset similar to LaFAN1 that contains only global joint coordinates.", "mention_start": 0, "mention_end": 76, "dataset_mention": " Duan et al. (2022) extracted the human motion data and formulated a dataset"}, {"mentioned_in_paper": "224", "context_id": "5", "dataset_context": "In this paper, we investigated the effect of pre-trained features from the massive-scale, irrelevant ImageNet dataset and a relatively moderate-scale, but relevant peanut root dataset on switchgrass root imagery segmentation applications.", "mention_start": 89, "mention_end": 117, "dataset_mention": " irrelevant ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "5", "dataset_context": "In this paper, we investigated the effect of pre-trained features from the massive-scale, irrelevant ImageNet dataset and a relatively moderate-scale, but relevant peanut root dataset on switchgrass root imagery segmentation applications.", "mention_start": 154, "mention_end": 183, "dataset_mention": "relevant peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "6", "dataset_context": "We compiled two minirhizotron image datasets to accomplish this study: one with 17,550 peanut root images and another with 28 switchgrass root images.", "mention_start": 0, "mention_end": 44, "dataset_mention": "We compiled two minirhizotron image datasets"}, {"mentioned_in_paper": "224", "context_id": "9", "dataset_context": "We observed that features pre-trained on a closely related but relatively moderate size dataset like our peanut dataset were more effective than features pre-trained on the large but unrelated ImageNet dataset.", "mention_start": 63, "mention_end": 95, "dataset_mention": "relatively moderate size dataset"}, {"mentioned_in_paper": "224", "context_id": "9", "dataset_context": "We observed that features pre-trained on a closely related but relatively moderate size dataset like our peanut dataset were more effective than features pre-trained on the large but unrelated ImageNet dataset.", "mention_start": 63, "mention_end": 119, "dataset_mention": "relatively moderate size dataset like our peanut dataset"}, {"mentioned_in_paper": "224", "context_id": "9", "dataset_context": "We observed that features pre-trained on a closely related but relatively moderate size dataset like our peanut dataset were more effective than features pre-trained on the large but unrelated ImageNet dataset.", "mention_start": 183, "mention_end": 209, "dataset_mention": "unrelated ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "10", "dataset_context": "We achieved high quality segmentation on peanut root dataset with 99.04% accuracy at the pixel-level and overcame errors in human-labeled ground truth masks.", "mention_start": 41, "mention_end": 60, "dataset_mention": "peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "11", "dataset_context": "By applying transfer learning technique on limited switchgrass dataset with features pre-trained on peanut dataset, we obtained 99% segmentation accuracy in switchgrass imagery using only 21 images for training (fine tuning).", "mention_start": 51, "mention_end": 70, "dataset_mention": "switchgrass dataset"}, {"mentioned_in_paper": "224", "context_id": "11", "dataset_context": "By applying transfer learning technique on limited switchgrass dataset with features pre-trained on peanut dataset, we obtained 99% segmentation accuracy in switchgrass imagery using only 21 images for training (fine tuning).", "mention_start": 100, "mention_end": 114, "dataset_mention": "peanut dataset"}, {"mentioned_in_paper": "224", "context_id": "36", "dataset_context": "The ImageNet dataset (Deng et al., 2009) contains more than 14 million pictures which is divided into 1000 classes.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "39", "dataset_context": "Great breakthroughs have been achieved based on ImageNet dataset in different research areas such as image classification, object localization, object detection, scene classification and Scene parsing.", "mention_start": 48, "mention_end": 64, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "41", "dataset_context": "Huh et al. (Huh et al., 2016) illustrated that transfer learning performance is similar with features pre-trained only on half of ImageNet dataset as opposed to the full dataset.", "mention_start": 129, "mention_end": 146, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "44", "dataset_context": "Thus, we suppose that ImageNet pre-training is less effective as compared to discipline-specific data sets that include plant root im-agery, due to the lack of relevance of the ImageNet dataset relative to plant root datasets.", "mention_start": 76, "mention_end": 106, "dataset_mention": "discipline-specific data sets"}, {"mentioned_in_paper": "224", "context_id": "44", "dataset_context": "Thus, we suppose that ImageNet pre-training is less effective as compared to discipline-specific data sets that include plant root im-agery, due to the lack of relevance of the ImageNet dataset relative to plant root datasets.", "mention_start": 172, "mention_end": 193, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "44", "dataset_context": "Thus, we suppose that ImageNet pre-training is less effective as compared to discipline-specific data sets that include plant root im-agery, due to the lack of relevance of the ImageNet dataset relative to plant root datasets.", "mention_start": 205, "mention_end": 225, "dataset_mention": "plant root datasets"}, {"mentioned_in_paper": "224", "context_id": "45", "dataset_context": "In this work, we collected a moderately sized peanut root minirhizotron imagery dataset and a small sized switchgrass root minirhizotron imagery dataset and manually traced root segments in both sets.", "mention_start": 26, "mention_end": 87, "dataset_mention": "a moderately sized peanut root minirhizotron imagery dataset"}, {"mentioned_in_paper": "224", "context_id": "45", "dataset_context": "In this work, we collected a moderately sized peanut root minirhizotron imagery dataset and a small sized switchgrass root minirhizotron imagery dataset and manually traced root segments in both sets.", "mention_start": 26, "mention_end": 152, "dataset_mention": "a moderately sized peanut root minirhizotron imagery dataset and a small sized switchgrass root minirhizotron imagery dataset"}, {"mentioned_in_paper": "224", "context_id": "46", "dataset_context": "We trained U-net based models with different depths on the peanut root dataset to achieve automated, precise pixel-wise root segmentation.", "mention_start": 55, "mention_end": 78, "dataset_mention": "the peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "48", "dataset_context": "Then, we used a transfer learning approach to apply pre-trained features from the peanut root data and the ImageNet dataset on the small-scale switchgrass root dataset to explore the effect of different pre-trained features and the accuracy of segmentation.", "mention_start": 77, "mention_end": 123, "dataset_mention": "the peanut root data and the ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "48", "dataset_context": "Then, we used a transfer learning approach to apply pre-trained features from the peanut root data and the ImageNet dataset on the small-scale switchgrass root dataset to explore the effect of different pre-trained features and the accuracy of segmentation.", "mention_start": 126, "mention_end": 167, "dataset_mention": "the small-scale switchgrass root dataset"}, {"mentioned_in_paper": "224", "context_id": "49", "dataset_context": "Our results affirm that features pre-trained on a moderate-sized peanut dataset that was highly related to the target switchgrass dataset were more effective for our root segmentation problem than those pre-trained on the large-scale but less relevant ImageNet dataset.", "mention_start": 48, "mention_end": 79, "dataset_mention": "a moderate-sized peanut dataset"}, {"mentioned_in_paper": "224", "context_id": "49", "dataset_context": "Our results affirm that features pre-trained on a moderate-sized peanut dataset that was highly related to the target switchgrass dataset were more effective for our root segmentation problem than those pre-trained on the large-scale but less relevant ImageNet dataset.", "mention_start": 118, "mention_end": 137, "dataset_mention": "switchgrass dataset"}, {"mentioned_in_paper": "224", "context_id": "49", "dataset_context": "Our results affirm that features pre-trained on a moderate-sized peanut dataset that was highly related to the target switchgrass dataset were more effective for our root segmentation problem than those pre-trained on the large-scale but less relevant ImageNet dataset.", "mention_start": 238, "mention_end": 268, "dataset_mention": "less relevant ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "50", "dataset_context": "Furthermore, our results also confirm that pre-training via transfer learning techniques is effective (and necessary) for small-scale plant root datasets.", "mention_start": 121, "mention_end": 153, "dataset_mention": "small-scale plant root datasets"}, {"mentioned_in_paper": "224", "context_id": "52", "dataset_context": "In the following sections we describe our datasets, the semantic segmentation methods employed, our experiments using those methods with our datasets and other visual imagery datasets, and finally draw some conclusions based upon that work.", "mention_start": 136, "mention_end": 183, "dataset_mention": "our datasets and other visual imagery datasets"}, {"mentioned_in_paper": "224", "context_id": "53", "dataset_context": "We have compiled two minirhizotron root image datasets.", "mention_start": 8, "mention_end": 54, "dataset_mention": "compiled two minirhizotron root image datasets"}, {"mentioned_in_paper": "224", "context_id": "54", "dataset_context": "The first dataset contains 17,550 peanut root RGB images and the second dataset has 28 switchgrass root RGB images.", "mention_start": 30, "mention_end": 79, "dataset_mention": "550 peanut root RGB images and the second dataset"}, {"mentioned_in_paper": "224", "context_id": "58", "dataset_context": "Peanut root dataset was collected in a field trial at the Plant Science Research and Education Unit (PSREU) located in Citra, Florida during the 2016 growing season.", "mention_start": 0, "mention_end": 19, "dataset_mention": "Peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "68", "dataset_context": "Switchgrass root dataset was collected using a CI-602 insitu root imager (CID Bio-Science, Camas, WA, USA) in minirhizotron tubes in a 2-year old switchgrass field at the U.S. Department of Energy National Environmental Research Park at Fermilab in Batavia, IL, USA.", "mention_start": 0, "mention_end": 24, "dataset_mention": "Switchgrass root dataset"}, {"mentioned_in_paper": "224", "context_id": "101", "dataset_context": "We first trained our model on the peanut root dataset to investigate the segmentation performance on plant root minirhizotron imagery when moderate-sized dataset is available.", "mention_start": 30, "mention_end": 53, "dataset_mention": "the peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "101", "dataset_context": "We first trained our model on the peanut root dataset to investigate the segmentation performance on plant root minirhizotron imagery when moderate-sized dataset is available.", "mention_start": 139, "mention_end": 161, "dataset_mention": "moderate-sized dataset"}, {"mentioned_in_paper": "224", "context_id": "102", "dataset_context": "Then, we designed experiments to demonstrate how the pre-trained features can help improve segmentation performance when a limited switchgrass dataset is used.", "mention_start": 130, "mention_end": 150, "dataset_mention": "switchgrass dataset"}, {"mentioned_in_paper": "224", "context_id": "103", "dataset_context": "We also compared the effect of features pre-trained on the well-known massive scale ImageNet dataset and features pre-trained on our peanut root dataset.", "mention_start": 55, "mention_end": 100, "dataset_mention": "the well-known massive scale ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "103", "dataset_context": "We also compared the effect of features pre-trained on the well-known massive scale ImageNet dataset and features pre-trained on our peanut root dataset.", "mention_start": 129, "mention_end": 152, "dataset_mention": "our peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "107", "dataset_context": "To find the proper model depth for our peanut root dataset and future use on transfer learning experiment on switchgrass dataset, we implemented three models with depth 4, depth 5 and depth 6, where model depth refers to the number of encoders and decoders in the down-sampling and upsampling path.", "mention_start": 35, "mention_end": 58, "dataset_mention": "our peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "107", "dataset_context": "To find the proper model depth for our peanut root dataset and future use on transfer learning experiment on switchgrass dataset, we implemented three models with depth 4, depth 5 and depth 6, where model depth refers to the number of encoders and decoders in the down-sampling and upsampling path.", "mention_start": 109, "mention_end": 128, "dataset_mention": "switchgrass dataset"}, {"mentioned_in_paper": "224", "context_id": "147", "dataset_context": "For specific applications such as plant root segmentation, the dataset is often limited like our switchgrass root dataset.", "mention_start": 87, "mention_end": 121, "dataset_mention": "like our switchgrass root dataset"}, {"mentioned_in_paper": "224", "context_id": "149", "dataset_context": "In our experiments, we had two available datasets to get pre-trained features: the pre-trained features from popular massive-scale ImageNet dataset and our selfcollected peanut root dataset.", "mention_start": 108, "mention_end": 147, "dataset_mention": "popular massive-scale ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "149", "dataset_context": "In our experiments, we had two available datasets to get pre-trained features: the pre-trained features from popular massive-scale ImageNet dataset and our selfcollected peanut root dataset.", "mention_start": 108, "mention_end": 189, "dataset_mention": "popular massive-scale ImageNet dataset and our selfcollected peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "150", "dataset_context": "Our situation is similar to most data-limited applications where we often have some moderatescale datasets that relevant to the target dataset.", "mention_start": 79, "mention_end": 106, "dataset_mention": "some moderatescale datasets"}, {"mentioned_in_paper": "224", "context_id": "151", "dataset_context": "We designed experiments on the small-scale switchgrass root dataset to explore the effect of pre-trained features from these two different datasets.", "mention_start": 27, "mention_end": 67, "dataset_mention": "the small-scale switchgrass root dataset"}, {"mentioned_in_paper": "224", "context_id": "152", "dataset_context": "Compared with the ImageNet dataset that has 14 million images, our peanut root dataset is quite small, but much more relevant to the switchgrass root dataset.", "mention_start": 14, "mention_end": 34, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "152", "dataset_context": "Compared with the ImageNet dataset that has 14 million images, our peanut root dataset is quite small, but much more relevant to the switchgrass root dataset.", "mention_start": 62, "mention_end": 86, "dataset_mention": " our peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "152", "dataset_context": "Compared with the ImageNet dataset that has 14 million images, our peanut root dataset is quite small, but much more relevant to the switchgrass root dataset.", "mention_start": 128, "mention_end": 157, "dataset_mention": "the switchgrass root dataset"}, {"mentioned_in_paper": "224", "context_id": "155", "dataset_context": "Unlike the ImageNet dataset, our peanut dataset had pixel-level labels, which could be used to directly train our U-net based model for segmentation.", "mention_start": 7, "mention_end": 27, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "155", "dataset_context": "Unlike the ImageNet dataset, our peanut dataset had pixel-level labels, which could be used to directly train our U-net based model for segmentation.", "mention_start": 28, "mention_end": 47, "dataset_mention": " our peanut dataset"}, {"mentioned_in_paper": "224", "context_id": "156", "dataset_context": "An advantage of the pre-trained features using the peanut dataset is that we could compute the pretrained features for both encoder and decoder.", "mention_start": 47, "mention_end": 65, "dataset_mention": "the peanut dataset"}, {"mentioned_in_paper": "224", "context_id": "159", "dataset_context": "To make a comprehensive comparison of different pre-trained features, we implemented four models namely: 1) S-model whose weights are randomly initialized; 2) I-model whose encoder is initialized with pre-trained weights on Im-ageNet dataset; 3) P-En-model whose encoder is initialized with pre-trained weights on our peanut dataset; and 4) P-EnDemodel whose encoder as well as decoder are initialized with pre-trained weights on our peanut dataset.", "mention_start": 223, "mention_end": 241, "dataset_mention": "Im-ageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "159", "dataset_context": "To make a comprehensive comparison of different pre-trained features, we implemented four models namely: 1) S-model whose weights are randomly initialized; 2) I-model whose encoder is initialized with pre-trained weights on Im-ageNet dataset; 3) P-En-model whose encoder is initialized with pre-trained weights on our peanut dataset; and 4) P-EnDemodel whose encoder as well as decoder are initialized with pre-trained weights on our peanut dataset.", "mention_start": 313, "mention_end": 332, "dataset_mention": "our peanut dataset"}, {"mentioned_in_paper": "224", "context_id": "159", "dataset_context": "To make a comprehensive comparison of different pre-trained features, we implemented four models namely: 1) S-model whose weights are randomly initialized; 2) I-model whose encoder is initialized with pre-trained weights on Im-ageNet dataset; 3) P-En-model whose encoder is initialized with pre-trained weights on our peanut dataset; and 4) P-EnDemodel whose encoder as well as decoder are initialized with pre-trained weights on our peanut dataset.", "mention_start": 429, "mention_end": 448, "dataset_mention": "our peanut dataset"}, {"mentioned_in_paper": "224", "context_id": "162", "dataset_context": "All the models were trained on the switchgrass dataset for 300 epochs.", "mention_start": 31, "mention_end": 54, "dataset_mention": "the switchgrass dataset"}, {"mentioned_in_paper": "224", "context_id": "167", "dataset_context": "Unlike the peanut root dataset, the number of root pixels versus nonroot pixels is heavily imbalanced in our switchgrass root imagery.", "mention_start": 7, "mention_end": 30, "dataset_mention": "the peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "190", "dataset_context": "Specifically, features pre-trained on the peanut root dataset were more effective than features pretrained on ImageNet dataset, even though the ImageNet dataset is much larger than the peanut dataset.", "mention_start": 37, "mention_end": 61, "dataset_mention": "the peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "190", "dataset_context": "Specifically, features pre-trained on the peanut root dataset were more effective than features pretrained on ImageNet dataset, even though the ImageNet dataset is much larger than the peanut dataset.", "mention_start": 109, "mention_end": 126, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "190", "dataset_context": "Specifically, features pre-trained on the peanut root dataset were more effective than features pretrained on ImageNet dataset, even though the ImageNet dataset is much larger than the peanut dataset.", "mention_start": 132, "mention_end": 160, "dataset_mention": "though the ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "190", "dataset_context": "Specifically, features pre-trained on the peanut root dataset were more effective than features pretrained on ImageNet dataset, even though the ImageNet dataset is much larger than the peanut dataset.", "mention_start": 180, "mention_end": 199, "dataset_mention": "the peanut dataset"}, {"mentioned_in_paper": "224", "context_id": "199", "dataset_context": "In our experiments, we explored the effectiveness of pretrained features from the popular ImageNet dataset and a peanut root dataset for switchgrass root segmentation.", "mention_start": 77, "mention_end": 106, "dataset_mention": "the popular ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "199", "dataset_context": "In our experiments, we explored the effectiveness of pretrained features from the popular ImageNet dataset and a peanut root dataset for switchgrass root segmentation.", "mention_start": 77, "mention_end": 132, "dataset_mention": "the popular ImageNet dataset and a peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "201", "dataset_context": "Although the ImageNet dataset is massive in scale, it appears that pre-trained features from peanut dataset had better performance.", "mention_start": 9, "mention_end": 29, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "201", "dataset_context": "Although the ImageNet dataset is massive in scale, it appears that pre-trained features from peanut dataset had better performance.", "mention_start": 92, "mention_end": 107, "dataset_mention": "peanut dataset"}, {"mentioned_in_paper": "224", "context_id": "202", "dataset_context": "Thus, (perhaps, intuitively) pre-trained features from the massive-scale ImageNet dataset are not always the best for imagery with different visual appearances such as plant root images.", "mention_start": 54, "mention_end": 89, "dataset_mention": "the massive-scale ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "203", "dataset_context": "The relevance of the pre-trained dataset with respect to the target dataset is more crucial for segmentation performance.", "mention_start": 17, "mention_end": 40, "dataset_mention": "the pre-trained dataset"}, {"mentioned_in_paper": "224", "context_id": "204", "dataset_context": "Since the ImageNet dataset and the switchgrass dataset are very different, it is likely that only the features in shallow layers can help with segmentation results, since low-level features are more general such as edges or textures (Yosinski et al., 2014).", "mention_start": 6, "mention_end": 26, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "204", "dataset_context": "Since the ImageNet dataset and the switchgrass dataset are very different, it is likely that only the features in shallow layers can help with segmentation results, since low-level features are more general such as edges or textures (Yosinski et al., 2014).", "mention_start": 6, "mention_end": 54, "dataset_mention": "the ImageNet dataset and the switchgrass dataset"}, {"mentioned_in_paper": "224", "context_id": "207", "dataset_context": "In contrast, features from the peanut root dataset that is much smaller but highly related to the switchgrass dataset are more valuable regardless of how deep the model is, because both low-level and high-level features are useful to the switchgrass root images.", "mention_start": 26, "mention_end": 50, "dataset_mention": "the peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "207", "dataset_context": "In contrast, features from the peanut root dataset that is much smaller but highly related to the switchgrass dataset are more valuable regardless of how deep the model is, because both low-level and high-level features are useful to the switchgrass root images.", "mention_start": 93, "mention_end": 117, "dataset_mention": "the switchgrass dataset"}, {"mentioned_in_paper": "224", "context_id": "213", "dataset_context": "Our model achieved high quality segmentation masks with 99.04% ROC AUC at the pixel-level and overcame errors in human-labeled ground truth masks on peanut root dataset.", "mention_start": 149, "mention_end": 168, "dataset_mention": "peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "215", "dataset_context": "Furthermore, we improved the segmentation performance on a small-scale switchgrass root dataset by using pre-trained features from the massive-scale ImageNet dataset and a mid-scale peanut root dataset, followed by fine tuning on a small switchgrass root dataset.", "mention_start": 56, "mention_end": 95, "dataset_mention": "a small-scale switchgrass root dataset"}, {"mentioned_in_paper": "224", "context_id": "215", "dataset_context": "Furthermore, we improved the segmentation performance on a small-scale switchgrass root dataset by using pre-trained features from the massive-scale ImageNet dataset and a mid-scale peanut root dataset, followed by fine tuning on a small switchgrass root dataset.", "mention_start": 130, "mention_end": 165, "dataset_mention": "the massive-scale ImageNet dataset"}, {"mentioned_in_paper": "224", "context_id": "215", "dataset_context": "Furthermore, we improved the segmentation performance on a small-scale switchgrass root dataset by using pre-trained features from the massive-scale ImageNet dataset and a mid-scale peanut root dataset, followed by fine tuning on a small switchgrass root dataset.", "mention_start": 130, "mention_end": 201, "dataset_mention": "the massive-scale ImageNet dataset and a mid-scale peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "215", "dataset_context": "Furthermore, we improved the segmentation performance on a small-scale switchgrass root dataset by using pre-trained features from the massive-scale ImageNet dataset and a mid-scale peanut root dataset, followed by fine tuning on a small switchgrass root dataset.", "mention_start": 229, "mention_end": 262, "dataset_mention": "a small switchgrass root dataset"}, {"mentioned_in_paper": "224", "context_id": "216", "dataset_context": "We obtained above 99% segmentation accuracy in switchgrass root segmentation with pre-trained encoder and decoder from our peanut root dataset.", "mention_start": 119, "mention_end": 142, "dataset_mention": "our peanut root dataset"}, {"mentioned_in_paper": "224", "context_id": "219", "dataset_context": "Also, features pre-trained on peanut dataset that is relatively small but highly related to the switchgrass dataset were more effective than those pre-trained on a massive-scale but less relevant ImageNet dataset.", "mention_start": 29, "mention_end": 44, "dataset_mention": "peanut dataset"}, {"mentioned_in_paper": "224", "context_id": "219", "dataset_context": "Also, features pre-trained on peanut dataset that is relatively small but highly related to the switchgrass dataset were more effective than those pre-trained on a massive-scale but less relevant ImageNet dataset.", "mention_start": 91, "mention_end": 115, "dataset_mention": "the switchgrass dataset"}, {"mentioned_in_paper": "224", "context_id": "219", "dataset_context": "Also, features pre-trained on peanut dataset that is relatively small but highly related to the switchgrass dataset were more effective than those pre-trained on a massive-scale but less relevant ImageNet dataset.", "mention_start": 181, "mention_end": 212, "dataset_mention": "less relevant ImageNet dataset"}, {"mentioned_in_paper": "225", "context_id": "47", "dataset_context": "Importantly, labels are shuffled from dataset-to-dataset.", "mention_start": 37, "mention_end": 56, "dataset_mention": "dataset-to-dataset"}, {"mentioned_in_paper": "225", "context_id": "91", "dataset_context": "The Omniglot dataset consists of over 1600 separate classes with only a few examples per class, aptly lending to it being called the transpose of MNIST (Lake et al., 2015).", "mention_start": 0, "mention_end": 20, "dataset_mention": "The Omniglot dataset"}, {"mentioned_in_paper": "225", "context_id": "226", "dataset_context": "First, N unique classes are sampled from the Omniglot dataset, where N is the maximum number of unique classes per episode.", "mention_start": 40, "mention_end": 61, "dataset_mention": "the Omniglot dataset"}, {"mentioned_in_paper": "225", "context_id": "240", "dataset_context": "At the 100 000 episode mark, the task continues; however, data are pulled from a disjoint test set (i.e., samples from classes 1201-1623 in the omniglot dataset), and weight updates are ceased.", "mention_start": 139, "mention_end": 160, "dataset_mention": "the omniglot dataset"}, {"mentioned_in_paper": "227", "context_id": "7", "dataset_context": "MESSI is the first to answer exact similarity search queries on 100GB datasets in \u223c50msec (30-75msec across diverse datasets), which enables real-time, interactive data exploration on very large data series collections.", "mention_start": 64, "mention_end": 78, "dataset_mention": "100GB datasets"}, {"mentioned_in_paper": "227", "context_id": "12", "dataset_context": "For example, ADS+ [89], the state-of-the-art sequential (i.e., non-parallel) indexing technique, requires more than 2min to answer exactly a single 1-NN (Nearest Neighbor) query on a (moderately sized) 100GB sequence dataset.", "mention_start": 180, "mention_end": 224, "dataset_mention": "a (moderately sized) 100GB sequence dataset"}, {"mentioned_in_paper": "227", "context_id": "19", "dataset_context": "For instance, ParIS+ answers a 1-NN (Nearest Neighbor) exact query on a 100GB dataset in 15sec, which is above the limit for keeping the user's attention (i.e., 10sec), and for supporting interactive analysis (i.e., 100msec) [29].", "mention_start": 69, "mention_end": 85, "dataset_mention": "a 100GB dataset"}, {"mentioned_in_paper": "227", "context_id": "46", "dataset_context": "Consequently, MESSI answers exact 1-NN queries on 100GB datasets within 30-70msec across diverse synthetic and real datasets.", "mention_start": 49, "mention_end": 64, "dataset_mention": "100GB datasets"}, {"mentioned_in_paper": "227", "context_id": "425", "dataset_context": "In all cases, the algorithms operated exclusively in main memory (the datasets were already loaded in memory, as well).", "mention_start": 52, "mention_end": 78, "dataset_mention": "main memory (the datasets"}, {"mentioned_in_paper": "227", "context_id": "427", "dataset_context": "[Datasets] In order to evaluate the performance of the proposed approach, we use several synthetic datasets for a fine grained analysis, and two real datasets from diverse domains.", "mention_start": 0, "mention_end": 9, "dataset_mention": "[Datasets"}, {"mentioned_in_paper": "227", "context_id": "484", "dataset_context": "We observe that MESSI performs up to 4.2x faster than ParIS+ (for the 200GB dataset), with the improvement becoming larger with the dataset size.", "mention_start": 54, "mention_end": 83, "dataset_mention": "ParIS+ (for the 200GB dataset"}, {"mentioned_in_paper": "227", "context_id": "543", "dataset_context": "[Real Datasets] Figures 17 and 18 reaffirm that MESSI exhibits the best performance for both index creation and query answering, even when executing on the real datasets, SALD and Seismic (for a 100GB dataset), for the reasons listed in the previous paragraphs.", "mention_start": 0, "mention_end": 14, "dataset_mention": "[Real Datasets"}, {"mentioned_in_paper": "227", "context_id": "543", "dataset_context": "[Real Datasets] Figures 17 and 18 reaffirm that MESSI exhibits the best performance for both index creation and query answering, even when executing on the real datasets, SALD and Seismic (for a 100GB dataset), for the reasons listed in the previous paragraphs.", "mention_start": 170, "mention_end": 208, "dataset_mention": " SALD and Seismic (for a 100GB dataset"}, {"mentioned_in_paper": "227", "context_id": "544", "dataset_context": "Regarding index creation, MESSI is 3.6x faster than ParIS+ on SALD and 3.7x faster than ParIS on Seismic, for a 100GB dataset.", "mention_start": 109, "mention_end": 125, "dataset_mention": "a 100GB dataset"}, {"mentioned_in_paper": "227", "context_id": "556", "dataset_context": "Figure 26 shows that the pruning proportion of all algorithms increases as we increase the level of noise in the query workloads, while Real is even more difficult: for the Seismic dataset, we can only prune 40-55% of the real distance calculations.", "mention_start": 168, "mention_end": 188, "dataset_mention": "the Seismic dataset"}, {"mentioned_in_paper": "227", "context_id": "562", "dataset_context": "MESSI is always better than all competitors: it performs 3.5x-100x faster than UCR Suite-p on the Seismic dataset, and 16x-135x faster on SALD.", "mention_start": 93, "mention_end": 113, "dataset_mention": "the Seismic dataset"}, {"mentioned_in_paper": "227", "context_id": "565", "dataset_context": "We observe that for our 100GB datasets, the MESSI index occupies \u223c5GB of space for Synthetic and Seismic, and \u223c10GB for SALD.", "mention_start": 20, "mention_end": 38, "dataset_mention": "our 100GB datasets"}, {"mentioned_in_paper": "227", "context_id": "571", "dataset_context": "The experimental results on a 100GB dataset show that as we increase the warping window size from 1% to 20% of the data series length, the query answering time of MESSI increases as well: the LB Keogh envelope of the query becomes wider, and consequently, pruning in the index is smaller (refer to Figure 28).", "mention_start": 28, "mention_end": 43, "dataset_mention": "a 100GB dataset"}, {"mentioned_in_paper": "227", "context_id": "578", "dataset_context": "The results, depicted in Figure 30, report the performance of MESSI and ParIS+ for different values of k on a 100GB dataset (100M series of size 256 values, generated with our synthetic data generator).", "mention_start": 107, "mention_end": 123, "dataset_mention": "a 100GB dataset"}, {"mentioned_in_paper": "227", "context_id": "602", "dataset_context": "The work closest to ours is Paris/ParIS+ [65, 67], which exploits modern hardware, but was designed for disk-resident datasets (see also Section 2).", "mention_start": 103, "mention_end": 126, "dataset_mention": "disk-resident datasets"}, {"mentioned_in_paper": "227", "context_id": "630", "dataset_context": "MESSI is up to 4x faster in index construction and up to 11x faster in query answering than the state-of-the-art solution, and is the first technique to answer answering exact similarity search queries on 100GB datasets in \u223c50msec.", "mention_start": 204, "mention_end": 219, "dataset_mention": "100GB datasets"}, {"mentioned_in_paper": "228", "context_id": "27", "dataset_context": "For our analysis, we resort to HICO-Det dataset [3].", "mention_start": 30, "mention_end": 47, "dataset_mention": "HICO-Det dataset"}, {"mentioned_in_paper": "229", "context_id": "9", "dataset_context": "We achieve the state-of-the-art for weakly supervised semantic image segmentation on VOC 2012 dataset, assuming no manually labeled pixel level information is available.", "mention_start": 85, "mention_end": 101, "dataset_mention": "VOC 2012 dataset"}, {"mentioned_in_paper": "229", "context_id": "91", "dataset_context": "Images in the ImageNet dataset are assumed to contain only one foreground object, and hence #y = 2.", "mention_start": 10, "mention_end": 30, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "229", "context_id": "100", "dataset_context": "For images in the ImageNet dataset, #y = 2, and hence the above optimization problem contains only two variables, \u03b2 0 and \u03b2 , where is the label of the foreground object is present in the image.", "mention_start": 14, "mention_end": 34, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "229", "context_id": "191", "dataset_context": "Models that use weak-supervision for training often employ a much larger dataset than fully-supervised models.", "mention_start": 59, "mention_end": 80, "dataset_mention": "a much larger dataset"}, {"mentioned_in_paper": "229", "context_id": "193", "dataset_context": "Similarly, the model in [21] uses a subset of Flickr dataset [8] for training.", "mention_start": 45, "mention_end": 60, "dataset_mention": "Flickr dataset"}, {"mentioned_in_paper": "229", "context_id": "194", "dataset_context": "A clean subset of the ImageNet dataset is used in [7], while a much larger subset of the same dataset is used in [19].", "mention_start": 18, "mention_end": 38, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "229", "context_id": "195", "dataset_context": "More importantly, almost all the approaches for semantic segmentation utilize the ImageNet dataset for pretraining the network.", "mention_start": 77, "mention_end": 98, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "229", "context_id": "197", "dataset_context": "In particular, we downloaded images of objects belonging to the 20 object classes in the VOC 2012 dataset [6] from the Imagenet database [20].", "mention_start": 84, "mention_end": 105, "dataset_mention": "the VOC 2012 dataset"}, {"mentioned_in_paper": "229", "context_id": "201", "dataset_context": "For testing, we use the validation and test set of VOC 2012 dataset.", "mention_start": 50, "mention_end": 67, "dataset_mention": "VOC 2012 dataset"}, {"mentioned_in_paper": "229", "context_id": "204", "dataset_context": "Almost all approaches for weakly supervised semantic segmentation report the results on VOC 2012 dataset.", "mention_start": 88, "mention_end": 104, "dataset_mention": "VOC 2012 dataset"}, {"mentioned_in_paper": "230", "context_id": "4", "dataset_context": "We demonstrate the robustness and accuracy of our approach on achallenging dataset.", "mention_start": 62, "mention_end": 82, "dataset_mention": "achallenging dataset"}, {"mentioned_in_paper": "230", "context_id": "111", "dataset_context": "To demonstrate the applicability of OUf approach, we captured a dataset of 32 images with an Apple iPad Air in Uf-   As a consequence, the accuracy of the pose estimated with the sensors only ranges from very accUfate, about 0.4 m position and 2\u00b0 rotation error, to very poor, up to 16.5 m position and about 30\u00b0 rotation error.", "mention_start": 49, "mention_end": 71, "dataset_mention": " we captured a dataset"}, {"mentioned_in_paper": "230", "context_id": "139", "dataset_context": "We hope that our work and dataset will encoUfage new research on the problem we introduce in this paper.", "mention_start": 8, "mention_end": 33, "dataset_mention": "that our work and dataset"}, {"mentioned_in_paper": "231", "context_id": "156", "dataset_context": "In particular, we focus on object detection across different domains of driving video datasets which is of great importance in autonomous driving and remains as a challenging task:", "mention_start": 71, "mention_end": 94, "dataset_mention": "driving video datasets"}, {"mentioned_in_paper": "231", "context_id": "159", "dataset_context": "Foggy Cityscapes [40] is a foggy version of the Cityscapes dataset with same train/val split and annotations as those in the Cityscapes dataset.", "mention_start": 44, "mention_end": 66, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "231", "context_id": "159", "dataset_context": "Foggy Cityscapes [40] is a foggy version of the Cityscapes dataset with same train/val split and annotations as those in the Cityscapes dataset.", "mention_start": 121, "mention_end": 143, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "231", "context_id": "162", "dataset_context": "BDD100k [52] is a large driving video dataset with 100k annotated images.", "mention_start": 16, "mention_end": 45, "dataset_mention": "a large driving video dataset"}, {"mentioned_in_paper": "231", "context_id": "188", "dataset_context": "The Cityscapes dataset are used as the source domain and the Foggy Cityscapes dataset are used as the target domain.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The Cityscapes dataset"}, {"mentioned_in_paper": "231", "context_id": "188", "dataset_context": "The Cityscapes dataset are used as the source domain and the Foggy Cityscapes dataset are used as the target domain.", "mention_start": 35, "mention_end": 85, "dataset_mention": "the source domain and the Foggy Cityscapes dataset"}, {"mentioned_in_paper": "231", "context_id": "196", "dataset_context": "Here evaluate the crossdomain performance from one real-world KITTI dataset to another real-world Citiscapes dataset to examine the adaptation ability across different camera devices.", "mention_start": 47, "mention_end": 75, "dataset_mention": "one real-world KITTI dataset"}, {"mentioned_in_paper": "231", "context_id": "196", "dataset_context": "Here evaluate the crossdomain performance from one real-world KITTI dataset to another real-world Citiscapes dataset to examine the adaptation ability across different camera devices.", "mention_start": 79, "mention_end": 116, "dataset_mention": "another real-world Citiscapes dataset"}, {"mentioned_in_paper": "232", "context_id": "43", "dataset_context": "As for training data, we only consider the binary attribute labels presented in the large-scale CelebFaces Attributes (CelebA) dataset [21].", "mention_start": 79, "mention_end": 134, "dataset_mention": "the large-scale CelebFaces Attributes (CelebA) dataset"}, {"mentioned_in_paper": "232", "context_id": "274", "dataset_context": "Our DIAT models are trained using a subset of the aligned CelebA dataset [21] by removing the images with poor quality.", "mention_start": 46, "mention_end": 72, "dataset_mention": "the aligned CelebA dataset"}, {"mentioned_in_paper": "234", "context_id": "64", "dataset_context": "Recent work shows that deep convolutional neural network trained on a sufficiently large dataset such as ImageNet [35] can be transferred to other computer vision tasks [8, 13, 19, 34].", "mention_start": 68, "mention_end": 96, "dataset_mention": "a sufficiently large dataset"}, {"mentioned_in_paper": "234", "context_id": "144", "dataset_context": "The recognition precision over 10 runs, large standard deviation of reflectance and KTH dataset due to randomly selecting one sample surface from each class as test set in each iteration (described in Section 6.1).", "mention_start": 67, "mention_end": 95, "dataset_mention": "reflectance and KTH dataset"}, {"mentioned_in_paper": "234", "context_id": "146", "dataset_context": "The first one is Flickr Material Dataset (FMD) [37], which contains 10 material classes.", "mention_start": 17, "mention_end": 40, "dataset_mention": "Flickr Material Dataset"}, {"mentioned_in_paper": "234", "context_id": "147", "dataset_context": "The second one is the Describable Texture Datasets (DTD) [7], containing texture images labeled with 47 describable attributes.", "mention_start": 18, "mention_end": 50, "dataset_mention": "the Describable Texture Datasets"}, {"mentioned_in_paper": "234", "context_id": "148", "dataset_context": "The third texture dataset is KTH-TIPS-2b (KTH) [4], which contains 11 material categories with 4 samples per category and a number of example images for each sample.", "mention_start": 0, "mention_end": 25, "dataset_mention": "The third texture dataset"}, {"mentioned_in_paper": "234", "context_id": "168", "dataset_context": "We can see the randomized and optimized version of DRC hashing outperform the other methods in FTD, DTD and KTH datasets.", "mention_start": 99, "mention_end": 120, "dataset_mention": " DTD and KTH datasets"}, {"mentioned_in_paper": "234", "context_id": "171", "dataset_context": "DRC-opt (60.2%) is much better than directly using CNN activations (CNN-ITQ 51.9%) for the reflectance dataset.", "mention_start": 87, "mention_end": 110, "dataset_mention": "the reflectance dataset"}, {"mentioned_in_paper": "234", "context_id": "172", "dataset_context": "The methods using CNN features are slightly worse than hashing with FV pooling SIFT feature, since the reflectance disks dataset is very different from the CNN pre-trained dataset, Ima-geNet, which consists real world images.", "mention_start": 98, "mention_end": 128, "dataset_mention": "the reflectance disks dataset"}, {"mentioned_in_paper": "234", "context_id": "172", "dataset_context": "The methods using CNN features are slightly worse than hashing with FV pooling SIFT feature, since the reflectance disks dataset is very different from the CNN pre-trained dataset, Ima-geNet, which consists real world images.", "mention_start": 151, "mention_end": 179, "dataset_mention": "the CNN pre-trained dataset"}, {"mentioned_in_paper": "234", "context_id": "173", "dataset_context": "Reflectance Hashing with textons [46] has an overall recognition precision of 56.69% on this reflectance dataset.", "mention_start": 88, "mention_end": 112, "dataset_mention": "this reflectance dataset"}, {"mentioned_in_paper": "234", "context_id": "174", "dataset_context": "We test this method on the reflectance dataset only since it is specifically developed for reflectance disks.", "mention_start": 23, "mention_end": 46, "dataset_mention": "the reflectance dataset"}, {"mentioned_in_paper": "234", "context_id": "177", "dataset_context": "For reflectance-friction dataset, each surface instance (from same or different material class) can have a different friction coefficient, as shown in Table 1.", "mention_start": 4, "mention_end": 32, "dataset_mention": "reflectance-friction dataset"}, {"mentioned_in_paper": "236", "context_id": "0", "dataset_context": "The hypothesis that image datasets gathered online \"in the wild\" can produce biased object recognizers, e.g.", "mention_start": 0, "mention_end": 34, "dataset_mention": "The hypothesis that image datasets"}, {"mentioned_in_paper": "236", "context_id": "6", "dataset_context": "Denoted as OOWL500, it contains 120, 000 images of 500 objects and is the largest \"in the lab\" image dataset available when both number of classes and objects per class are considered.", "mention_start": 85, "mention_end": 108, "dataset_mention": "the lab\" image dataset"}, {"mentioned_in_paper": "236", "context_id": "10", "dataset_context": "Second, it is used to show that the augmentation of in the wild datasets, such as ImageNet, with in the lab data, such as OOWL500, can significantly decrease these biases, leading to object recognizers of improved generalization.", "mention_start": 54, "mention_end": 72, "dataset_mention": "the wild datasets"}, {"mentioned_in_paper": "236", "context_id": "12", "dataset_context": "It is revealed that data augmentation with synthetic images does not suffice to eliminate in the wild datasets biases, and that camera shake and pose diversity play a more important role in object recognition robustness than previously thought.", "mention_start": 93, "mention_end": 110, "dataset_mention": "the wild datasets"}, {"mentioned_in_paper": "236", "context_id": "26", "dataset_context": "Nevertheless, experience has shown that most datasets have biases [19].", "mention_start": 28, "mention_end": 53, "dataset_mention": "shown that most datasets"}, {"mentioned_in_paper": "236", "context_id": "27", "dataset_context": "Like many recent vision datasets, ImageNet is the product of data collection \"in the wild\" -images are first collected on the internet, then annotated on platforms like MTurk.", "mention_start": 0, "mention_end": 32, "dataset_mention": "Like many recent vision datasets"}, {"mentioned_in_paper": "236", "context_id": "44", "dataset_context": "We believe that progress along these directions requires bringing dataset collection back \"to the lab\" and enabling explicit control over variables such as object pose, distance, or lighting direction.", "mention_start": 57, "mention_end": 73, "dataset_mention": "bringing dataset"}, {"mentioned_in_paper": "236", "context_id": "60", "dataset_context": "OOWL500 has several unique properties compared to past pose datasets.", "mention_start": 50, "mention_end": 68, "dataset_mention": "past pose datasets"}, {"mentioned_in_paper": "236", "context_id": "62", "dataset_context": "Second, unlike datasets collected with domes or turntables, there is a small \"camera shake\" effect that produces images with statistics different than those of ImageNet style datasets.", "mention_start": 159, "mention_end": 183, "dataset_mention": "ImageNet style datasets"}, {"mentioned_in_paper": "236", "context_id": "74", "dataset_context": "In this section, we present the proposed drone-based setup for image collection \"in the lab\" and the resulting OOWL500 dataset.", "mention_start": 83, "mention_end": 126, "dataset_mention": "the lab\" and the resulting OOWL500 dataset"}, {"mentioned_in_paper": "236", "context_id": "112", "dataset_context": "The above setup was used to create the OOWL500 dataset, which contains 500 objects captured at varying angles, for a total of 120, 000 images.", "mention_start": 35, "mention_end": 54, "dataset_mention": "the OOWL500 dataset"}, {"mentioned_in_paper": "236", "context_id": "128", "dataset_context": "In the wild datasets Table 1 compares OOWL500 to previous datasets.", "mention_start": 3, "mention_end": 20, "dataset_mention": "the wild datasets"}, {"mentioned_in_paper": "236", "context_id": "129", "dataset_context": "The top third of the table summarizes the properties of \"in the wild\" datasets.", "mention_start": 60, "mention_end": 78, "dataset_mention": "the wild\" datasets"}, {"mentioned_in_paper": "236", "context_id": "131", "dataset_context": "They are listed mostly for calibration of what constitutes a large object recognition dataset today.", "mention_start": 42, "mention_end": 93, "dataset_mention": "what constitutes a large object recognition dataset"}, {"mentioned_in_paper": "236", "context_id": "132", "dataset_context": "Note that, with less than 30, 000 images, Pascal and the two Caltech datasets are now considered \"small\" in comparison to MS-COCO and ImageNet.", "mention_start": 41, "mention_end": 77, "dataset_mention": " Pascal and the two Caltech datasets"}, {"mentioned_in_paper": "236", "context_id": "135", "dataset_context": "The goal of this work is not to replace in the wild datasets.", "mention_start": 43, "mention_end": 60, "dataset_mention": "the wild datasets"}, {"mentioned_in_paper": "236", "context_id": "143", "dataset_context": "The second limitation of \"in the wild\" datasets is the lack of information needed to study important questions in object recognition, such as invariance to pose, scale, lighting, etc.", "mention_start": 29, "mention_end": 47, "dataset_mention": "the wild\" datasets"}, {"mentioned_in_paper": "236", "context_id": "150", "dataset_context": "None of the two hold for \"in the wild\" datasets, which favor instance diversity.", "mention_start": 29, "mention_end": 47, "dataset_mention": "the wild\" datasets"}, {"mentioned_in_paper": "236", "context_id": "154", "dataset_context": "In the wild datasets are shown in red, in the lab in green, and synthetic in purple.", "mention_start": 3, "mention_end": 20, "dataset_mention": "the wild datasets"}, {"mentioned_in_paper": "236", "context_id": "156", "dataset_context": "In the lab datasets The study of these questions has motivated several \"in the lab\" datasets, annotated for factors like pose, distance, or lighting.", "mention_start": 3, "mention_end": 19, "dataset_mention": "the lab datasets"}, {"mentioned_in_paper": "236", "context_id": "156", "dataset_context": "In the lab datasets The study of these questions has motivated several \"in the lab\" datasets, annotated for factors like pose, distance, or lighting.", "mention_start": 75, "mention_end": 92, "dataset_mention": "the lab\" datasets"}, {"mentioned_in_paper": "236", "context_id": "160", "dataset_context": "First, they exhibit a professional photography bias similar to that of in the wild datasets.", "mention_start": 73, "mention_end": 91, "dataset_mention": "the wild datasets"}, {"mentioned_in_paper": "236", "context_id": "163", "dataset_context": "Due to this, many of these datasets have few classes of overlap with in the wild datasets like ImageNet (only HM comes close to OOWL500 in this regard).", "mention_start": 71, "mention_end": 89, "dataset_mention": "the wild datasets"}, {"mentioned_in_paper": "236", "context_id": "166", "dataset_context": "Even the largest in the lab datasets can be small in this respect.", "mention_start": 20, "mention_end": 36, "dataset_mention": "the lab datasets"}, {"mentioned_in_paper": "236", "context_id": "173", "dataset_context": "Among in the lab datasets (shown in green), OOWL500 achieves the best trade-off between the two factors.", "mention_start": 9, "mention_end": 25, "dataset_mention": "the lab datasets"}, {"mentioned_in_paper": "236", "context_id": "174", "dataset_context": "Naturally, in the wild datasets tend to be larger in size.", "mention_start": 13, "mention_end": 31, "dataset_mention": "the wild datasets"}, {"mentioned_in_paper": "236", "context_id": "203", "dataset_context": "Correcting ImageNet biases We next studied the usefulness of \"in the lab\" datasets such as OOWL500 to correct ImageNet biases.", "mention_start": 65, "mention_end": 82, "dataset_mention": "the lab\" datasets"}, {"mentioned_in_paper": "236", "context_id": "217", "dataset_context": "This is a well known problem, wherein fine-tuning causes the classifier to \"forget\" the original dataset [44, 45].", "mention_start": 74, "mention_end": 104, "dataset_mention": "\"forget\" the original dataset"}, {"mentioned_in_paper": "236", "context_id": "235", "dataset_context": "This confirms the need for in the lab datasets that, like OOWL500, enable training that specifically corrects ImageNet biases.", "mention_start": 30, "mention_end": 46, "dataset_mention": "the lab datasets"}, {"mentioned_in_paper": "236", "context_id": "238", "dataset_context": "Alternative training strategies Computer generated synthetic datasets provide a simple alternative to \"in the lab\" datasets.", "mention_start": 106, "mention_end": 123, "dataset_mention": "the lab\" datasets"}, {"mentioned_in_paper": "236", "context_id": "265", "dataset_context": "In the lab datasets such as OOWL500 enable the study of how this coverage impacts the quality of the resulting classifiers.", "mention_start": 3, "mention_end": 19, "dataset_mention": "the lab datasets"}, {"mentioned_in_paper": "236", "context_id": "282", "dataset_context": "Note that most in the wild datasets only exhibit object diversity, because pose diversity is almost impossible to control in this setting.", "mention_start": 18, "mention_end": 35, "dataset_mention": "the wild datasets"}, {"mentioned_in_paper": "236", "context_id": "284", "dataset_context": "In the lab datasets, like OOWL500, enable such research.", "mention_start": 3, "mention_end": 19, "dataset_mention": "the lab datasets"}, {"mentioned_in_paper": "236", "context_id": "291", "dataset_context": "Again, while camera shake is usually simulated for (by shifting, cropping, etc.) in classifier design, pose diversity is much harder to address with in the wild datasets.", "mention_start": 151, "mention_end": 169, "dataset_mention": "the wild datasets"}, {"mentioned_in_paper": "236", "context_id": "292", "dataset_context": "In this work, we have investigated the hypothesis that dataset collection \"in the wild\" is not sufficient to guarantee unbiased object recognizers.", "mention_start": 21, "mention_end": 62, "dataset_mention": "investigated the hypothesis that dataset"}, {"mentioned_in_paper": "236", "context_id": "301", "dataset_context": "The usefulness of the procedure was demonstrated through the collection of the OOWL500 dataset -the largest in the lab dataset recognition dataset in the literature when both number of classes and objects per class are considered.", "mention_start": 75, "mention_end": 94, "dataset_mention": "the OOWL500 dataset"}, {"mentioned_in_paper": "236", "context_id": "301", "dataset_context": "The usefulness of the procedure was demonstrated through the collection of the OOWL500 dataset -the largest in the lab dataset recognition dataset in the literature when both number of classes and objects per class are considered.", "mention_start": 111, "mention_end": 126, "dataset_mention": "the lab dataset"}, {"mentioned_in_paper": "236", "context_id": "301", "dataset_context": "The usefulness of the procedure was demonstrated through the collection of the OOWL500 dataset -the largest in the lab dataset recognition dataset in the literature when both number of classes and objects per class are considered.", "mention_start": 111, "mention_end": 146, "dataset_mention": "the lab dataset recognition dataset"}, {"mentioned_in_paper": "236", "context_id": "305", "dataset_context": "Second, it was used to show that the augmentation of in the wild datasets, such as ImageNet, with in the lab data, such as OOWL500, can significantly decrease these biases, leading to object recognizers of improved generalization.", "mention_start": 55, "mention_end": 73, "dataset_mention": "the wild datasets"}, {"mentioned_in_paper": "236", "context_id": "307", "dataset_context": "This has shown that data augmentation with synthetic images does not suffice to eliminate in the wild datasets biases, and that camera shake and pose diversity play a more important role in object recognizers' robustness than previously thought.", "mention_start": 93, "mention_end": 110, "dataset_mention": "the wild datasets"}, {"mentioned_in_paper": "237", "context_id": "4", "dataset_context": "We find that people don't behave consistently with Bayesian predictions for large sample size datasets, and this difference cannot be explained by elicitation technique.", "mention_start": 89, "mention_end": 102, "dataset_mention": "size datasets"}, {"mentioned_in_paper": "237", "context_id": "86", "dataset_context": "For the purposes of understanding how Bayesian cognitive modeling might provide insight into visualization interpretation, we sought a realistic yet relatively simple dataset similar those shown in the media or public facing reports.", "mention_start": 122, "mention_end": 174, "dataset_mention": " we sought a realistic yet relatively simple dataset"}, {"mentioned_in_paper": "237", "context_id": "125", "dataset_context": "By manipulating both representation of uncertainty and the dataset, we aim to gain a better sense of how robust our observation of approximate Bayesian inference is.", "mention_start": 39, "mention_end": 66, "dataset_mention": "uncertainty and the dataset"}, {"mentioned_in_paper": "237", "context_id": "160", "dataset_context": "Dataset and Presentation: We reuse the same proportion dataset used in Study 1 (mental health outcomes among women in the tech industry) and the same icon array visualization.", "mention_start": 25, "mention_end": 62, "dataset_mention": " We reuse the same proportion dataset"}, {"mentioned_in_paper": "237", "context_id": "169", "dataset_context": "We created a visualization (Fig. 5 (b)) that shows this proportion in a similar icon array format to that used for the mental health in tech dataset.", "mention_start": 136, "mention_end": 148, "dataset_mention": "tech dataset"}, {"mentioned_in_paper": "237", "context_id": "177", "dataset_context": "Based on a prospective power analysis conducted on pilot data with a desired power of at least 0.8 assuming \u03b1=0.05, we recruited 800 workers with an approval rating of 98% or more (400 per dataset, 200 per elicitation condition) in the U.S from Amazon Mechanical Turk (AMT).", "mention_start": 174, "mention_end": 196, "dataset_mention": "more (400 per dataset"}, {"mentioned_in_paper": "237", "context_id": "195", "dataset_context": "For the tech dataset (N=158) used in Study 2, we observed a similar pattern as in Study 1, with errors roughly equally distributed about zero for means, and around zero but with a slight bias toward the degree of variance of priors (i.e., overestimating variance in the data).", "mention_start": 4, "mention_end": 20, "dataset_mention": "the tech dataset"}, {"mentioned_in_paper": "237", "context_id": "196", "dataset_context": "For the elderly dataset (N=750,000), residuals for means are again roughly symmetric about zero, but residuals for standard deviations are nearly entirely to the right of zero.", "mention_start": 4, "mention_end": 23, "dataset_mention": "the elderly dataset"}, {"mentioned_in_paper": "237", "context_id": "197", "dataset_context": "This suggests a strong tendency for people to be more uncertainty about the true proportion than they should rationally be, given the size of the observed dataset.", "mention_start": 141, "mention_end": 162, "dataset_mention": "the observed dataset"}, {"mentioned_in_paper": "237", "context_id": "199", "dataset_context": "For example, those using the graphical balls and bins interface (Fig. 8 fourth column) appear to be slightly more consistent (i.e., more concentrated distribution) and slightly less likely to be biased in their estimates of standard deviation of the elderly dataset (Fig. 8 bottom row).", "mention_start": 245, "mention_end": 265, "dataset_mention": "the elderly dataset"}, {"mentioned_in_paper": "237", "context_id": "204", "dataset_context": "The small differences in techniques, however, are far less pronounced that the more obvious differences between people's residuals for standard deviation for the (large) elderly dataset versus the (small) tech dataset.", "mention_start": 157, "mention_end": 185, "dataset_mention": "the (large) elderly dataset"}, {"mentioned_in_paper": "237", "context_id": "204", "dataset_context": "The small differences in techniques, however, are far less pronounced that the more obvious differences between people's residuals for standard deviation for the (large) elderly dataset versus the (small) tech dataset.", "mention_start": 157, "mention_end": 217, "dataset_mention": "the (large) elderly dataset versus the (small) tech dataset"}, {"mentioned_in_paper": "237", "context_id": "208", "dataset_context": "To disambiguate whether the difference between the tech dataset and the elderly dataset is due to the different domains of the data or the different sample sizes, we introduced additional datasets by manipulating sample size.", "mention_start": 47, "mention_end": 63, "dataset_mention": "the tech dataset"}, {"mentioned_in_paper": "237", "context_id": "208", "dataset_context": "To disambiguate whether the difference between the tech dataset and the elderly dataset is due to the different domains of the data or the different sample sizes, we introduced additional datasets by manipulating sample size.", "mention_start": 47, "mention_end": 87, "dataset_mention": "the tech dataset and the elderly dataset"}, {"mentioned_in_paper": "237", "context_id": "209", "dataset_context": "We reran the study with the sample sizes switched for the two datasets (tech dataset N=720,000, elderly dataset N=150).", "mention_start": 54, "mention_end": 84, "dataset_mention": "the two datasets (tech dataset"}, {"mentioned_in_paper": "237", "context_id": "209", "dataset_context": "We reran the study with the sample sizes switched for the two datasets (tech dataset N=720,000, elderly dataset N=150).", "mention_start": 95, "mention_end": 111, "dataset_mention": " elderly dataset"}, {"mentioned_in_paper": "237", "context_id": "210", "dataset_context": "We observed the same pattern of results in residual plots (presented in supplemental material), where elicitation techniques did not appear to reliably impact individual's residuals in means or standard deviations, but the larger sample size datasets led to residual standard deviations that were strongly biased toward greatly overestimating the amount of uncertainty one should feel given their prior and the observed data.", "mention_start": 236, "mention_end": 250, "dataset_mention": "size datasets"}, {"mentioned_in_paper": "237", "context_id": "213", "dataset_context": "Hence participants did not weight the value of information captured by the observed elderly dataset as much as they should, given its large sample size (N=750,000).", "mention_start": 71, "mention_end": 99, "dataset_mention": "the observed elderly dataset"}, {"mentioned_in_paper": "237", "context_id": "215", "dataset_context": "We found that while people's responses were consistent with an approximate or samplebased Bayesian hypothesis for the small sample size datasets, we don't see analogous evidence that people act as samplebased Bayesians for the large sample datasets (Fig. 9(a, b)).", "mention_start": 131, "mention_end": 144, "dataset_mention": "size datasets"}, {"mentioned_in_paper": "237", "context_id": "222", "dataset_context": "Figure 10 shows how the perceived sample size of the observed data was roughly the same across elicitation techniques and datasets.", "mention_start": 95, "mention_end": 130, "dataset_mention": "elicitation techniques and datasets"}, {"mentioned_in_paper": "237", "context_id": "223", "dataset_context": "The mean across all techniques for the tech dataset (N=158) was 212.47 (median=41.14)", "mention_start": 35, "mention_end": 51, "dataset_mention": "the tech dataset"}, {"mentioned_in_paper": "237", "context_id": "224", "dataset_context": "whereas the mean of elderly dataset (N=750,000) was 359.58 (median=51.51),", "mention_start": 20, "mention_end": 35, "dataset_mention": "elderly dataset"}, {"mentioned_in_paper": "237", "context_id": "232", "dataset_context": "To evaluate our questions, we used the tech dataset (N=158) and the elderly dataset (N=750,000) that we used in Study 2 (Fig. 5).", "mention_start": 34, "mention_end": 51, "dataset_mention": "the tech dataset"}, {"mentioned_in_paper": "237", "context_id": "232", "dataset_context": "To evaluate our questions, we used the tech dataset (N=158) and the elderly dataset (N=750,000) that we used in Study 2 (Fig. 5).", "mention_start": 34, "mention_end": 83, "dataset_mention": "the tech dataset (N=158) and the elderly dataset"}, {"mentioned_in_paper": "237", "context_id": "235", "dataset_context": "To create HOPs for each dataset, we constructed a binomial distribution using parameters of the dataset (e.g., \u03b2(n = 158, p = 0.17) for the tech dataset), then sampled multiple hypothetical modes from the distribution to present as hypothetical outcomes, using a frame rate of 400ms as suggested by prior work [23, 26] (Fig. 11).", "mention_start": 135, "mention_end": 152, "dataset_mention": "the tech dataset"}, {"mentioned_in_paper": "237", "context_id": "241", "dataset_context": "The Elicitation-No uncertainty condition responses consisted of participants' responses from the text sample-based conditions from Study 2 (responses from a total of 200 participants, 100 per dataset).", "mention_start": 183, "mention_end": 199, "dataset_mention": " 100 per dataset"}, {"mentioned_in_paper": "237", "context_id": "242", "dataset_context": "For the remaining conditions, we recruited an additional 600 participants (100 per condition, a total of 300 per dataset) in the U.S from AMT.", "mention_start": 104, "mention_end": 120, "dataset_mention": "300 per dataset"}, {"mentioned_in_paper": "237", "context_id": "248", "dataset_context": "To compute the normative posterior for No-elicitation conditions, we used the aggregate priors from participants in the text sample-based condition in Study 2 (Tech dataset: \u03b1 = 10.79,", "mention_start": 156, "mention_end": 172, "dataset_mention": "2 (Tech dataset"}, {"mentioned_in_paper": "237", "context_id": "250", "dataset_context": "Elderly dataset:\u03b1 = 31.25,", "mention_start": 0, "mention_end": 15, "dataset_mention": "Elderly dataset"}, {"mentioned_in_paper": "237", "context_id": "262", "dataset_context": "Being assigned to view the large sample size dataset (i.e., elderly dataset) still had a large impact on results at the individual level, with the average log KLD for those who viewed the large sample dataset being on average 1.54 log KLD units larger than those for the small sample size dataset (95% CI:[1.42,1.67]).", "mention_start": 40, "mention_end": 52, "dataset_mention": "size dataset"}, {"mentioned_in_paper": "237", "context_id": "262", "dataset_context": "Being assigned to view the large sample size dataset (i.e., elderly dataset) still had a large impact on results at the individual level, with the average log KLD for those who viewed the large sample dataset being on average 1.54 log KLD units larger than those for the small sample size dataset (95% CI:[1.42,1.67]).", "mention_start": 59, "mention_end": 75, "dataset_mention": " elderly dataset"}, {"mentioned_in_paper": "237", "context_id": "262", "dataset_context": "Being assigned to view the large sample size dataset (i.e., elderly dataset) still had a large impact on results at the individual level, with the average log KLD for those who viewed the large sample dataset being on average 1.54 log KLD units larger than those for the small sample size dataset (95% CI:[1.42,1.67]).", "mention_start": 283, "mention_end": 296, "dataset_mention": "size dataset"}, {"mentioned_in_paper": "237", "context_id": "266", "dataset_context": "Even though participants assigned to examine the large sample size dataset had relatively high log KLDs relative to the small sample size dataset, viewing HOPs did have some impact on how accurately they perceived the sample size of the observed data.", "mention_start": 62, "mention_end": 74, "dataset_mention": "size dataset"}, {"mentioned_in_paper": "237", "context_id": "266", "dataset_context": "Even though participants assigned to examine the large sample size dataset had relatively high log KLDs relative to the small sample size dataset, viewing HOPs did have some impact on how accurately they perceived the sample size of the observed data.", "mention_start": 133, "mention_end": 145, "dataset_mention": "size dataset"}, {"mentioned_in_paper": "237", "context_id": "268", "dataset_context": "For the tech dataset (N=158), while the means of the No uncertainty and Uncertainty conditions were similar (326.0 vs. 327.3), the median was much closer to the actual sample size of the dataset for the Uncertainty conditions (median perceived: 166.3, true sample size: 158) than the No uncertainty conditions (median perceived: 97.2).", "mention_start": 4, "mention_end": 20, "dataset_mention": "the tech dataset"}, {"mentioned_in_paper": "237", "context_id": "269", "dataset_context": "For the elderly dataset (N=750,000), both the mean and median of the Uncertainty conditions were closer to the true observed sample size (mean perceived: 60,268.9,", "mention_start": 4, "mention_end": 23, "dataset_mention": "the elderly dataset"}, {"mentioned_in_paper": "237", "context_id": "276", "dataset_context": "While no single study can definitively establish whether people reason about presented data in a Bayesian way, our experiments showed that on average, people's responses were consistent with a sample-based Bayesian account when examining small sample size datasets.", "mention_start": 250, "mention_end": 264, "dataset_mention": "size datasets"}, {"mentioned_in_paper": "237", "context_id": "277", "dataset_context": "On average, people's responses deviated from Bayesian reasoning when presented with large sample size datasets.", "mention_start": 96, "mention_end": 110, "dataset_mention": "size datasets"}, {"mentioned_in_paper": "238", "context_id": "24", "dataset_context": "The engine combines the publicly available datasets of satellite-derived soil moisture measurements from the European Space Agency (ESA) and generates fine-grained, gap-free soil moisture predictions using three implementations of machine learning algorithms: a kernel-based approach (kernel-weighted k-nearest neighbors or KKNN), the Hybrid Piecewise Polynomial approach (HYPPO), and a treebased approach (Random Forests or RF).", "mention_start": 0, "mention_end": 51, "dataset_mention": "The engine combines the publicly available datasets"}, {"mentioned_in_paper": "238", "context_id": "43", "dataset_context": "Our topography dataset, consisting of multiple terrain parameters, is based on DSMs, which are available at several resolutions and are useful to represent multiple terrain characteristics [13].", "mention_start": 0, "mention_end": 22, "dataset_mention": "Our topography dataset"}, {"mentioned_in_paper": "238", "context_id": "48", "dataset_context": "To define the spatial limits of our soil moisture prediction, we use the 2011 update of the Commission for Environmental Cooperation (CEC) ecoregion dataset, developed jointly by Mexico, the United States, and Canada and based on the analysis of ecosystem elements such as geology, physiography, vegetation, climate, soils, land use, wildlife, and hydrology [8].", "mention_start": 106, "mention_end": 156, "dataset_mention": "Environmental Cooperation (CEC) ecoregion dataset"}, {"mentioned_in_paper": "238", "context_id": "115", "dataset_context": "Such techniques do not assume a particular functional or geometric form of the model, and are thus suitable to deal with sparse datasets (e.g., areas with large gaps of soil moisture satellite estimates).", "mention_start": 120, "mention_end": 136, "dataset_mention": "sparse datasets"}, {"mentioned_in_paper": "238", "context_id": "190", "dataset_context": "We reiterate that other satellitederived datasets of soil moisture can be used in our workflow such as AMSR-E (Advanced Microwave Scanning Radiometer -Earth Observing System Sensor on the NASA Aqua Satellite [25]), ASCAT (Advanced SCATterometer aboard the EUMETSAT MetOp satellite [26] - [28]), and AQUARIUS (Satellite instrument from NASA SMAP mission [29]).", "mention_start": 0, "mention_end": 49, "dataset_mention": "We reiterate that other satellitederived datasets"}, {"mentioned_in_paper": "238", "context_id": "191", "dataset_context": "Despite technological advances, satellite datasets still have coarse spatial resolution and present temporal gaps making support tools such as SOMOSPIE useful to provide insights for research, environmental management, and precision agriculture based on remote sensing data.", "mention_start": 31, "mention_end": 50, "dataset_mention": " satellite datasets"}, {"mentioned_in_paper": "241", "context_id": "367", "dataset_context": "SEAFOG will give us the first large-scale homogeneous dataset of H i absorption and X-ray absorption (also extremely well-matched in sensitivity and angular resolution), enabling us to perform an unbiased census of the population of H i and X-ray absorbers in order to definitively uncover the relationship between them.", "mention_start": 0, "mention_end": 61, "dataset_mention": "SEAFOG will give us the first large-scale homogeneous dataset"}, {"mentioned_in_paper": "242", "context_id": "82", "dataset_context": "In contrast, the IUI dataset is small enough to allow us to combine a large-scale automated text analysis with an in-depth manual content assessment.", "mention_start": 12, "mention_end": 28, "dataset_mention": " the IUI dataset"}, {"mentioned_in_paper": "243", "context_id": "103", "dataset_context": "A. Implementation Details 1) Training Scene.: All experiments are implemented on an Ubuntu workstation with a 24-core Intel CPU, 4 NVIDIA TITAN RTX GPU, and 64G RAM. 2) Dataset and Training Strategy.:", "mention_start": 152, "mention_end": 176, "dataset_mention": " and 64G RAM. 2) Dataset"}, {"mentioned_in_paper": "244", "context_id": "11", "dataset_context": "Moreover, vision algorithms are typically trained on offline image datasets, which suffer from a significant camera sensor specific bias [3].", "mention_start": 52, "mention_end": 75, "dataset_mention": "offline image datasets"}, {"mentioned_in_paper": "244", "context_id": "41", "dataset_context": "One of the fundamental problems of common image datasets, such as [6, 9, 24, 16], is that they are image sensor biased.", "mention_start": 35, "mention_end": 56, "dataset_mention": "common image datasets"}, {"mentioned_in_paper": "244", "context_id": "47", "dataset_context": "Samples of this dataset can be found in Figure 2. The full dataset is published at http://jtl.lassonde.yorku.ca/software/datasets/.", "mention_start": 40, "mention_end": 66, "dataset_mention": "Figure 2. The full dataset"}, {"mentioned_in_paper": "244", "context_id": "47", "dataset_context": "Samples of this dataset can be found in Figure 2. The full dataset is published at http://jtl.lassonde.yorku.ca/software/datasets/.", "mention_start": 88, "mention_end": 129, "dataset_mention": "//jtl.lassonde.yorku.ca/software/datasets"}, {"mentioned_in_paper": "244", "context_id": "67", "dataset_context": "There were twenty class-specific detectors trained on the PASCAL VOC 2007 dataset [9], and only five of them were used for the purpose of this evaluation.", "mention_start": 54, "mention_end": 81, "dataset_mention": "the PASCAL VOC 2007 dataset"}, {"mentioned_in_paper": "244", "context_id": "70", "dataset_context": "To make it consistent with the other algorithms, it was trained only on the PASCAL VOC 2007 dataset.", "mention_start": 71, "mention_end": 99, "dataset_mention": "the PASCAL VOC 2007 dataset"}, {"mentioned_in_paper": "244", "context_id": "75", "dataset_context": "For the R-CNN and SPP-net, the neural networks were pre-trained on ImageNet and fine-tuned on the PASCAL VOC 2007 dataset.", "mention_start": 93, "mention_end": 121, "dataset_mention": "the PASCAL VOC 2007 dataset"}, {"mentioned_in_paper": "244", "context_id": "76", "dataset_context": "Twenty detectors were trained for the objects in the PASCAL dataset, and only five of them were used for evaluating.", "mention_start": 49, "mention_end": 67, "dataset_mention": "the PASCAL dataset"}, {"mentioned_in_paper": "245", "context_id": "312", "dataset_context": "Real/Synthetic Data Sets.", "mention_start": 0, "mention_end": 24, "dataset_mention": "Real/Synthetic Data Sets"}, {"mentioned_in_paper": "245", "context_id": "314", "dataset_context": "Specifically, for real data sets, we used two check-in data sets, Gowalla [10] and Foursquare [20].", "mention_start": 41, "mention_end": 64, "dataset_mention": "two check-in data sets"}, {"mentioned_in_paper": "245", "context_id": "315", "dataset_context": "In the Gowalla data set, there are 196,591 nodes (users), with 6,442,890 checkin records.", "mention_start": 3, "mention_end": 23, "dataset_mention": "the Gowalla data set"}, {"mentioned_in_paper": "245", "context_id": "316", "dataset_context": "In the Foursquare data set, there are 2,153,471 users, 1,143,092 venues, and 1,021,970 check-ins, extracted from the Foursquare application through the public API.", "mention_start": 3, "mention_end": 26, "dataset_mention": "the Foursquare data set"}, {"mentioned_in_paper": "246", "context_id": "385", "dataset_context": "Fig. 6 shows the curve of loss ratios (compensated loss : original loss) when > 2 max i {||u i ||} on the CIFAR-100 data set.", "mention_start": 101, "mention_end": 124, "dataset_mention": "the CIFAR-100 data set"}, {"mentioned_in_paper": "246", "context_id": "428", "dataset_context": "For IMDB data set, the batch size is set as 64; the learning rate is set as 0.001; the number of epochs is set as 6; the proportion of train/val/test data is 4:1:5; the embedding dropout is set as 0.5; the dimension of hidden vectors is 100.", "mention_start": 4, "mention_end": 17, "dataset_mention": "IMDB data set"}, {"mentioned_in_paper": "246", "context_id": "432", "dataset_context": "For SST-2 data set, the batch size is set as 32; the learning rate is set as 0.0001; the number of epochs is set as 50; the division of train/val/test data follows the default split; the embedding dropout is set as 0.7; the dimension of hidden vectors is 256.", "mention_start": 4, "mention_end": 18, "dataset_mention": "SST-2 data set"}, {"mentioned_in_paper": "246", "context_id": "454", "dataset_context": "In addition, we plot the distribution of l1-norm of compensated logit vectors when using LogComp on CIFAR-10 and CIFAR-100 data sets when no added label noises are present (0%).", "mention_start": 99, "mention_end": 132, "dataset_mention": "CIFAR-10 and CIFAR-100 data sets"}, {"mentioned_in_paper": "247", "context_id": "4", "dataset_context": "A series of experiments on the MORPH-II and ORL datasets are conducted to demonstrate the effectiveness of this approach.", "mention_start": 27, "mention_end": 56, "dataset_mention": "the MORPH-II and ORL datasets"}, {"mentioned_in_paper": "247", "context_id": "10", "dataset_context": "Encouraged by the work in [5], in which the random subspace method is applied to twodimensional PCA, we propose a new algorithm and evaluate its performance on the MORPH-II and ORL datasets.", "mention_start": 159, "mention_end": 189, "dataset_mention": "the MORPH-II and ORL datasets"}, {"mentioned_in_paper": "247", "context_id": "252", "dataset_context": "A. Introduction to the Data 1) MORPH-II: The MORPH-II dataset [8] is a longitudinal dataset collected over five years.", "mention_start": 40, "mention_end": 61, "dataset_mention": " The MORPH-II dataset"}, {"mentioned_in_paper": "247", "context_id": "252", "dataset_context": "A. Introduction to the Data 1) MORPH-II: The MORPH-II dataset [8] is a longitudinal dataset collected over five years.", "mention_start": 68, "mention_end": 91, "dataset_mention": "a longitudinal dataset"}, {"mentioned_in_paper": "247", "context_id": "260", "dataset_context": "2) ORL: The ORL dataset [1] contains 40 people each with 10 images of size 112 \u00d7 92.", "mention_start": 7, "mention_end": 23, "dataset_mention": " The ORL dataset"}, {"mentioned_in_paper": "247", "context_id": "261", "dataset_context": "There are minor variations in lighting and facial expression, but it is an easy dataset for face recognition.", "mention_start": 71, "mention_end": 87, "dataset_mention": "an easy dataset"}, {"mentioned_in_paper": "247", "context_id": "288", "dataset_context": "A novel algorithm for face recognition, RS-2DLDA, is presented and evaluated on MORPH-II and ORL datasets.", "mention_start": 79, "mention_end": 105, "dataset_mention": "MORPH-II and ORL datasets"}, {"mentioned_in_paper": "249", "context_id": "32", "dataset_context": "Similar to the related work [5, 16, 24, 31, 42] we test our method by adapting both the GTA5 [36] and SYNTHIA [37] synthetic datasets to Cityscapes [6] and show that our results surpass the current state-of-the-art for the commonly used segmentation networks.", "mention_start": 69, "mention_end": 133, "dataset_mention": "adapting both the GTA5 [36] and SYNTHIA [37] synthetic datasets"}, {"mentioned_in_paper": "249", "context_id": "66", "dataset_context": "Our objective is to train a deep neural network M to perform semantic segmentation on a target (real) dataset T .", "mention_start": 95, "mention_end": 109, "dataset_mention": "(real) dataset"}, {"mentioned_in_paper": "249", "context_id": "68", "dataset_context": "In order to do this, we use synthetic data from a source (synthetic) dataset S, where we have both the images X S and the labels Y S .", "mention_start": 47, "mention_end": 76, "dataset_mention": "a source (synthetic) dataset"}, {"mentioned_in_paper": "249", "context_id": "155", "dataset_context": "For each experiment, we report 3 values: the mIoU; the gain wrt the lower bound, which is a naive training on the source dataset; the remaining gap wrt the upper bound, which is the result for training with target labels (called oracle prediction).", "mention_start": 109, "mention_end": 128, "dataset_mention": "the source dataset"}, {"mentioned_in_paper": "250", "context_id": "160", "dataset_context": "The top-1-accuracy 7 of the models, when being trained on the ImageNet data set, increases with an increasing number of layers.", "mention_start": 57, "mention_end": 79, "dataset_mention": "the ImageNet data set"}, {"mentioned_in_paper": "250", "context_id": "161", "dataset_context": "However, this also increases their computational Figure 7 : Top-1-accuracies of different ResNet variants, when being trained on the ImageNet data set, versus their computational costs, i.e. the number of operations, needed for a single forward pass (data from [32], inspired by [33]).", "mention_start": 128, "mention_end": 150, "dataset_mention": "the ImageNet data set"}, {"mentioned_in_paper": "250", "context_id": "166", "dataset_context": "On the one hand for randomly initialized weights and on the other hand for initializations with weights resulting from previous trainings on the ImageNet and the COCO data set (see Figure 8).", "mention_start": 141, "mention_end": 175, "dataset_mention": "the ImageNet and the COCO data set"}, {"mentioned_in_paper": "250", "context_id": "169", "dataset_context": "\u2022 data set yield lower validation losses than weights resulting from a previous training on the ImageNet data set.", "mention_start": 0, "mention_end": 10, "dataset_mention": "\u2022 data set"}, {"mentioned_in_paper": "250", "context_id": "169", "dataset_context": "\u2022 data set yield lower validation losses than weights resulting from a previous training on the ImageNet data set.", "mention_start": 92, "mention_end": 113, "dataset_mention": "the ImageNet data set"}, {"mentioned_in_paper": "250", "context_id": "172", "dataset_context": "The last of the three insights is rather counterintuitive, considering the higher performance of the ResNet-101 backbone on the ImageNet data set (see Figure 7).", "mention_start": 123, "mention_end": 145, "dataset_mention": "the ImageNet data set"}, {"mentioned_in_paper": "250", "context_id": "176", "dataset_context": "Ultimately, the ResNet-50 model initialized with weights resulting from a previous training on the COCO data set was used as backbone for the DPN, due to it exhibiting the highest performance at the lowest computational cost.", "mention_start": 94, "mention_end": 112, "dataset_mention": "the COCO data set"}, {"mentioned_in_paper": "252", "context_id": "1", "dataset_context": "This work drastically reduces the human effort required to apply saliency algorithms to new tasks and datasets, while also ensuring consistency and procedural correctness for results and conclusions produced by different parties.", "mention_start": 88, "mention_end": 110, "dataset_mention": "new tasks and datasets"}, {"mentioned_in_paper": "252", "context_id": "8", "dataset_context": "For example, Table 1 shows scores computed by the similarity (SIM) metric [35] as calculated in three different studies (Vig et al. [55], Wang and Shen [56], and Berga and Otazu [5]) on the Toronto dataset [10].", "mention_start": 185, "mention_end": 205, "dataset_mention": "the Toronto dataset"}, {"mentioned_in_paper": "252", "context_id": "47", "dataset_context": "Here we show the SIM [35] scores for five algorithms over the Toronto dataset [10] as reported by three recent publications (note that [55] report two scores, one with added center bias and one without).", "mention_start": 58, "mention_end": 77, "dataset_mention": "the Toronto dataset"}, {"mentioned_in_paper": "252", "context_id": "80", "dataset_context": "[63]) -Image domains outside the natural images which form the bulk of fixation datasets, such as websites [43] or satellite imagery [21] -Image quality assessment (e.g.", "mention_start": 71, "mention_end": 88, "dataset_mention": "fixation datasets"}, {"mentioned_in_paper": "254", "context_id": "15", "dataset_context": "We merge Facebook and Twitter datasets with logs from Microsoft's web browsers and Bing search engine, thus fusing data from three major internet Figure 1 : IRA campaign structure.", "mention_start": 0, "mention_end": 38, "dataset_mention": "We merge Facebook and Twitter datasets"}, {"mentioned_in_paper": "254", "context_id": "57", "dataset_context": "IRA-linked Tweets: Alongside the Facebook ads data, the House Intelligence Committee also released a dataset on June 8th consisting of 3841 Twitter account handles believed to be associated with the IRA (Schiff 2018b).", "mention_start": 89, "mention_end": 108, "dataset_mention": "released a dataset"}, {"mentioned_in_paper": "254", "context_id": "76", "dataset_context": "We join the primary datasets to recover the scope of the IRA campaign.", "mention_start": 0, "mention_end": 28, "dataset_mention": "We join the primary datasets"}, {"mentioned_in_paper": "254", "context_id": "95", "dataset_context": "In addition to the primary datasets, we employ numerous external secondary datasets.", "mention_start": 15, "mention_end": 35, "dataset_mention": "the primary datasets"}, {"mentioned_in_paper": "254", "context_id": "95", "dataset_context": "In addition to the primary datasets, we employ numerous external secondary datasets.", "mention_start": 46, "mention_end": 83, "dataset_mention": "numerous external secondary datasets"}, {"mentioned_in_paper": "254", "context_id": "303", "dataset_context": "This work was motivated by the need to better understand the IRA campaign in 2016 and was enabled by the release of the recent datasets from Facebook and Twitter on this matter during the respective testimonies at the United States House of Representatives (Schiff 2018a; 2018b).", "mention_start": 116, "mention_end": 135, "dataset_mention": "the recent datasets"}, {"mentioned_in_paper": "254", "context_id": "310", "dataset_context": "The released Facebook ads data set and the web search data set we use, on the other hand, do include mobile traffic.", "mention_start": 0, "mention_end": 34, "dataset_mention": "The released Facebook ads data set"}, {"mentioned_in_paper": "254", "context_id": "310", "dataset_context": "The released Facebook ads data set and the web search data set we use, on the other hand, do include mobile traffic.", "mention_start": 0, "mention_end": 62, "dataset_mention": "The released Facebook ads data set and the web search data set"}, {"mentioned_in_paper": "254", "context_id": "315", "dataset_context": "Likewise, we restrict our analysis to short-term trends because our browsing instrumentation data set does not maintain longterm histories.", "mention_start": 63, "mention_end": 101, "dataset_mention": "our browsing instrumentation data set"}, {"mentioned_in_paper": "254", "context_id": "322", "dataset_context": "Circumstances around the 2016 U.S. presidential elections and rising concerns about the influence of propaganda campaigns led to the public availability of valuable datasets for understanding IRA activities.", "mention_start": 156, "mention_end": 173, "dataset_mention": "valuable datasets"}, {"mentioned_in_paper": "254", "context_id": "323", "dataset_context": "Weaving together the public datasets with proprietary data on search and browsing activity provides a previously unavailable lens on the workings of the IRA campaign.", "mention_start": 0, "mention_end": 36, "dataset_mention": "Weaving together the public datasets"}, {"mentioned_in_paper": "254", "context_id": "387", "dataset_context": "We formatted our dataset as (group url, date) pairs.", "mention_start": 0, "mention_end": 24, "dataset_mention": "We formatted our dataset"}, {"mentioned_in_paper": "257", "context_id": "10", "dataset_context": "Data integration -combining diverse data sets, from different organizations or with heterogeneous schemas -has been a long standing challenge for the database community, which we continue to struggle with today.", "mention_start": 0, "mention_end": 45, "dataset_mention": "Data integration -combining diverse data sets"}, {"mentioned_in_paper": "257", "context_id": "18", "dataset_context": "Instead of insisting on clean data and a standardized schema, we argue that we should accept that many closely related data sets will never be fully integrated into a single relational system.", "mention_start": 61, "mention_end": 128, "dataset_mention": " we argue that we should accept that many closely related data sets"}, {"mentioned_in_paper": "257", "context_id": "19", "dataset_context": "Instead, we propose Termite, a\"dirt-loving\" database system that provides as much of the power of declarative querying as possible, but on top of these non-uniform datasets.", "mention_start": 145, "mention_end": 172, "dataset_mention": "these non-uniform datasets"}, {"mentioned_in_paper": "257", "context_id": "25", "dataset_context": "These relational embeddings are similar to the word embeddings used for modelling language in text processing [17, 20], but are specially constructed to work well for tabular datasets that are relational in nature.", "mention_start": 166, "mention_end": 183, "dataset_mention": "tabular datasets"}, {"mentioned_in_paper": "257", "context_id": "147", "dataset_context": "We built a dataset with information of faculty at CSAIL.", "mention_start": 0, "mention_end": 18, "dataset_mention": "We built a dataset"}, {"mentioned_in_paper": "257", "context_id": "187", "dataset_context": "To understand this empirically, we retrieved ground truth for both tasks on the same CSAIL faculty dataset mentioned above, and then implemented functions on Termite to perform each task.", "mention_start": 75, "mention_end": 106, "dataset_mention": "the same CSAIL faculty dataset"}, {"mentioned_in_paper": "258", "context_id": "8", "dataset_context": "Experiments are made respectively on the structured data sets KDD-Cup 1999 and CIC-IDS 2017, in which the dimensions of the data are relatively low, and also on the unstructured data sets MNIST and CIFAR-10 with the data of the relatively high dimensions.", "mention_start": 37, "mention_end": 61, "dataset_mention": "the structured data sets"}, {"mentioned_in_paper": "258", "context_id": "8", "dataset_context": "Experiments are made respectively on the structured data sets KDD-Cup 1999 and CIC-IDS 2017, in which the dimensions of the data are relatively low, and also on the unstructured data sets MNIST and CIFAR-10 with the data of the relatively high dimensions.", "mention_start": 160, "mention_end": 187, "dataset_mention": "the unstructured data sets"}, {"mentioned_in_paper": "258", "context_id": "25", "dataset_context": " [3] proposed IDSGAN that can generate adversarial attacks and they generated adversarial malicious traffic for the IDS (Instrusion Detection System) data set through a black attack approach, but they completely replaced almost all features.", "mention_start": 111, "mention_end": 158, "dataset_mention": "the IDS (Instrusion Detection System) data set"}, {"mentioned_in_paper": "258", "context_id": "99", "dataset_context": "To test the performance of the proposed FFA-GAN approach, four experiments are made on the structured and unstructured data sets.", "mention_start": 86, "mention_end": 128, "dataset_mention": "the structured and unstructured data sets"}, {"mentioned_in_paper": "258", "context_id": "100", "dataset_context": "A. Structured Data Set 1) i. KDD-Cup 1999: KDD-Cup 1999 [14] is a data set used for a big data analysis competition on the Fifth International Conference on Knowledge Discovery and Data Mining in 1999.", "mention_start": 0, "mention_end": 22, "dataset_mention": "A. Structured Data Set"}, {"mentioned_in_paper": "258", "context_id": "128", "dataset_context": "B. Unstructured Data Set 1) i. MNIST: MNIST data set [20] is a classic data set in the domain of image processing.", "mention_start": 0, "mention_end": 24, "dataset_mention": "B. Unstructured Data Set"}, {"mentioned_in_paper": "258", "context_id": "128", "dataset_context": "B. Unstructured Data Set 1) i. MNIST: MNIST data set [20] is a classic data set in the domain of image processing.", "mention_start": 37, "mention_end": 52, "dataset_mention": " MNIST data set"}, {"mentioned_in_paper": "258", "context_id": "128", "dataset_context": "B. Unstructured Data Set 1) i. MNIST: MNIST data set [20] is a classic data set in the domain of image processing.", "mention_start": 60, "mention_end": 79, "dataset_mention": "a classic data set"}, {"mentioned_in_paper": "258", "context_id": "133", "dataset_context": "have the classification accuracy over 99.0% on MNIST data set.", "mention_start": 47, "mention_end": 61, "dataset_mention": "MNIST data set"}, {"mentioned_in_paper": "258", "context_id": "142", "dataset_context": "CIFAR-10: CIFAR-10 data set [23] is an image data set, which were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.", "mention_start": 9, "mention_end": 27, "dataset_mention": " CIFAR-10 data set"}, {"mentioned_in_paper": "258", "context_id": "145", "dataset_context": "The difference between the CIFAR-10 data set and the MNIST data set is their channels: the image of CIFAR-10 has 3 channels and the image of MNIST has only one channel, which means it is more difficult to train FFA-GAN on the data set CIFAR-10 than on MNIST.", "mention_start": 23, "mention_end": 44, "dataset_mention": "the CIFAR-10 data set"}, {"mentioned_in_paper": "258", "context_id": "145", "dataset_context": "The difference between the CIFAR-10 data set and the MNIST data set is their channels: the image of CIFAR-10 has 3 channels and the image of MNIST has only one channel, which means it is more difficult to train FFA-GAN on the data set CIFAR-10 than on MNIST.", "mention_start": 23, "mention_end": 67, "dataset_mention": "the CIFAR-10 data set and the MNIST data set"}, {"mentioned_in_paper": "258", "context_id": "153", "dataset_context": "From the four experiments, we can draw the conclusion that FFA-GAN is an effective and robust approach to generate the adversarial samples on the structural and unstructural data set.", "mention_start": 141, "mention_end": 182, "dataset_mention": "the structural and unstructural data set"}, {"mentioned_in_paper": "258", "context_id": "165", "dataset_context": "Experiments on the structured data sets and unstructured data sets results show the good performance of the FFA-GAN approach.", "mention_start": 15, "mention_end": 39, "dataset_mention": "the structured data sets"}, {"mentioned_in_paper": "258", "context_id": "165", "dataset_context": "Experiments on the structured data sets and unstructured data sets results show the good performance of the FFA-GAN approach.", "mention_start": 15, "mention_end": 66, "dataset_mention": "the structured data sets and unstructured data sets"}, {"mentioned_in_paper": "259", "context_id": "44", "dataset_context": "Steiner et al. in [26] study the inter-play between regularization, data augmentation and dataset size when training ViTs for image classification.", "mention_start": 67, "mention_end": 97, "dataset_mention": " data augmentation and dataset"}, {"mentioned_in_paper": "259", "context_id": "109", "dataset_context": "We conduct our experiments on the ADE20k dataset [34].", "mention_start": 30, "mention_end": 48, "dataset_mention": "the ADE20k dataset"}, {"mentioned_in_paper": "259", "context_id": "127", "dataset_context": "We evaluate the generalization capability of the pretrained representation by fine-tuning the network on a depth estimation task on the NYU-Depth V2 dataset [25].", "mention_start": 132, "mention_end": 156, "dataset_mention": "the NYU-Depth V2 dataset"}, {"mentioned_in_paper": "259", "context_id": "130", "dataset_context": "In both cases, the final activation function is a dilated sigmoid that matches the depth range of the NYU-Depth V2 dataset.", "mention_start": 97, "mention_end": 122, "dataset_mention": "the NYU-Depth V2 dataset"}, {"mentioned_in_paper": "259", "context_id": "137", "dataset_context": "These two models have indeed been pre-trained on the training set of the ImageNet-1k dataset [23] -without using labels -and fine-tuned on the NYU-Depth V2 dataset.", "mention_start": 69, "mention_end": 92, "dataset_mention": "the ImageNet-1k dataset"}, {"mentioned_in_paper": "259", "context_id": "137", "dataset_context": "These two models have indeed been pre-trained on the training set of the ImageNet-1k dataset [23] -without using labels -and fine-tuned on the NYU-Depth V2 dataset.", "mention_start": 139, "mention_end": 163, "dataset_mention": "the NYU-Depth V2 dataset"}, {"mentioned_in_paper": "259", "context_id": "140", "dataset_context": "For the same reason, we also exclude results from models pre-trained on much larger depth datasets, like [21].", "mention_start": 71, "mention_end": 98, "dataset_mention": "much larger depth datasets"}, {"mentioned_in_paper": "261", "context_id": "28", "dataset_context": "The notion is extended in (Cheng and Xie, 2014) to achieve tracking control in a sampled-data setting and in (Li et al., 2011) to enable finite-time leader-follower consensus.", "mention_start": 78, "mention_end": 97, "dataset_mention": "a sampled-data set"}, {"mentioned_in_paper": "262", "context_id": "73", "dataset_context": "We normalize the dataset by subtraction of the mean of the training patches and division by their standard deviation.", "mention_start": 0, "mention_end": 24, "dataset_mention": "We normalize the dataset"}, {"mentioned_in_paper": "262", "context_id": "80", "dataset_context": "For evaluation we use the Multi-view Stereo Correspondence dataset (Brown et al., 2011), which consists of 64\u00d764 grayscale image patches sampled from 3D reconstructions of the Statue of Liberty (LY), Notre Dame (ND) and Half Dome in Yosemite (YO).", "mention_start": 22, "mention_end": 66, "dataset_mention": "the Multi-view Stereo Correspondence dataset"}, {"mentioned_in_paper": "262", "context_id": "155", "dataset_context": "The results are summarized in Table 6 and shown in Fig. 9. Our approach outperforms all descriptors, with the largest relative improvement on the 'Liberty' (LY) dataset.", "mention_start": 141, "mention_end": 168, "dataset_mention": "the 'Liberty' (LY) dataset"}, {"mentioned_in_paper": "263", "context_id": "5", "dataset_context": "We evaluate our approach using the MovieLens 100K and 1M datasets.", "mention_start": 31, "mention_end": 65, "dataset_mention": "the MovieLens 100K and 1M datasets"}, {"mentioned_in_paper": "263", "context_id": "7", "dataset_context": "We also develop a distinction between meta-learners that operate per-instance (micro-level), per-data subset (mid-level), and per-dataset (global level).", "mention_start": 121, "mention_end": 137, "dataset_mention": " and per-dataset"}, {"mentioned_in_paper": "263", "context_id": "8", "dataset_context": "Our evaluation shows that a hypothetically perfect micro-level meta-learner would improve RMSE by 25.5% for the MovieLens 100K and 1M datasets, compared to the overall-best algorithms used.", "mention_start": 108, "mention_end": 142, "dataset_mention": "the MovieLens 100K and 1M datasets"}, {"mentioned_in_paper": "263", "context_id": "48", "dataset_context": "Our meta-learner aims to use the best algorithm for each user-item pair in a recommendation dataset, from a pool of single recommendation algorithms.", "mention_start": 75, "mention_end": 99, "dataset_mention": "a recommendation dataset"}, {"mentioned_in_paper": "263", "context_id": "50", "dataset_context": "1. Co-clustering 2. KNN (Baseline) 3. KNN (Basic) 4. KNN (with Means) 5. Non-negative Matrix Factorization (NMF) 6. SVD 7. SVD++ 8. Slope One 9. Baseline -A collaborative filtering baseline which always predicts the overallaverage rating, biased by the overall-average rating for the user, and overallaverage rating for the item We performed our experiments using the MovieLens 100K and 1M datasets [14].", "mention_start": 363, "mention_end": 398, "dataset_mention": "the MovieLens 100K and 1M datasets"}, {"mentioned_in_paper": "263", "context_id": "52", "dataset_context": "To assess the potential improvements that a hypothetically 'perfect' micro-level meta-learner could offer over an overall-best algorithm, we first evaluate our pool of algorithms individually on the MovieLens 100K and 1M datasets.", "mention_start": 194, "mention_end": 229, "dataset_mention": "the MovieLens 100K and 1M datasets"}, {"mentioned_in_paper": "263", "context_id": "69", "dataset_context": "Table 1 : An illustration of the MovieLens dataset.", "mention_start": 28, "mention_end": 50, "dataset_mention": "the MovieLens dataset"}, {"mentioned_in_paper": "263", "context_id": "97", "dataset_context": "The average error (RMSE) of different collaborative-filtering algorithms on the Mov-ieLens 100K and 1M datasets.", "mention_start": 76, "mention_end": 111, "dataset_mention": "the Mov-ieLens 100K and 1M datasets"}, {"mentioned_in_paper": "263", "context_id": "101", "dataset_context": "However, for each row in the 100K dataset, SVD++ is not the best algorithm most often (ML100K; SVD++: 15.85%, vs. KNN (Basic): 16.7%) (Fig. 4).", "mention_start": 24, "mention_end": 41, "dataset_mention": "the 100K dataset"}, {"mentioned_in_paper": "263", "context_id": "102", "dataset_context": "The second-best algorithm by RMSE (KNN (Baseline)) is least often the best algorithm for each user-item in the 100K dataset.", "mention_start": 107, "mention_end": 123, "dataset_mention": "the 100K dataset"}, {"mentioned_in_paper": "263", "context_id": "103", "dataset_context": "In the 1M dataset, the second most-frequently accurate algorithm KNN (Basic) (16.10%), is the least accurate with regards to RMSE (0.936).", "mention_start": 3, "mention_end": 17, "dataset_mention": "the 1M dataset"}, {"mentioned_in_paper": "263", "context_id": "105", "dataset_context": "In a hypothetical scenario in which the best algorithm per dataset instance could be choseni.e. with a perfect meta-learner -RMSE would be improved by 25.5% for both 100K and 1M when compared to their respective overall-best algorithms (Fig. 3).", "mention_start": 36, "mention_end": 66, "dataset_mention": "the best algorithm per dataset"}, {"mentioned_in_paper": "264", "context_id": "39", "dataset_context": "In particular, the ImageNet dataset [54] is organized by following the hierarchical structure of WordNet [41].", "mention_start": 14, "mention_end": 35, "dataset_mention": " the ImageNet dataset"}, {"mentioned_in_paper": "264", "context_id": "140", "dataset_context": "We explored the la- tent representations of GAE [27] and our models on the Cora dataset [57] by constraining the latent space as a 2dimensional hyperbolic or Euclidean space.", "mention_start": 71, "mention_end": 87, "dataset_mention": "the Cora dataset"}, {"mentioned_in_paper": "264", "context_id": "155", "dataset_context": "In this experiment, we observe the unsupervised hyperbolic image embeddings' ability to recognize the latent structure of visual datasets that have hierarchical structures.", "mention_start": 121, "mention_end": 137, "dataset_mention": "visual datasets"}, {"mentioned_in_paper": "264", "context_id": "158", "dataset_context": "Thus, we have constructed a new dataset, ImageNet-BNCR, that has a Balanced Number of Classes across Roots.", "mention_start": 13, "mention_end": 39, "dataset_mention": "constructed a new dataset"}, {"mentioned_in_paper": "264", "context_id": "163", "dataset_context": "We extracted 1000-dimensional features by training a convolutional auto-encoder (CAE) [37] on the ImageNet-10 and ImageNet-BNCR datasets.", "mention_start": 94, "mention_end": 136, "dataset_mention": "the ImageNet-10 and ImageNet-BNCR datasets"}, {"mentioned_in_paper": "264", "context_id": "186", "dataset_context": "We observed that the proposed method could yield unsupervised hyperbolic image embeddings reflecting the latent structure of the visual datasets that have hierarchical structure.", "mention_start": 125, "mention_end": 144, "dataset_mention": "the visual datasets"}, {"mentioned_in_paper": "264", "context_id": "203", "dataset_context": "ImageNet-10 [9] and ImageNet-Dogs [9] are subsets of the ImageNet dataset [31].", "mention_start": 53, "mention_end": 73, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "264", "context_id": "215", "dataset_context": "For visual datasets, we construct the mutual k nearest neighbors graph, A, as follows:", "mention_start": 4, "mention_end": 19, "dataset_mention": "visual datasets"}, {"mentioned_in_paper": "264", "context_id": "224", "dataset_context": "[9] and ImageNet-BNCR datasets on the experiment of Section 5.3 in the manuscript.", "mention_start": 0, "mention_end": 30, "dataset_mention": "[9] and ImageNet-BNCR datasets"}, {"mentioned_in_paper": "264", "context_id": "250", "dataset_context": "The experiments were conducted on Pubmed [57], BlogCatalog [60], and Amazon Photo [39] datasets.", "mention_start": 64, "mention_end": 95, "dataset_mention": " and Amazon Photo [39] datasets"}, {"mentioned_in_paper": "264", "context_id": "253", "dataset_context": "Notably, on BlogCatalog and Amazon Photo datasets, HGCAE achieves more than 30% higher performances compared to Euclidean counterparts.", "mention_start": 11, "mention_end": 49, "dataset_mention": "BlogCatalog and Amazon Photo datasets"}, {"mentioned_in_paper": "264", "context_id": "255", "dataset_context": "We explored the latent representations of GAE [27] and our models on Pubmed [57], BlogCatalog [60], Citesser [57], and Amazon Photo [39] datasets by constraining the latent space as a 2-dimensional hyperbolic or Euclidean space.", "mention_start": 114, "mention_end": 145, "dataset_mention": " and Amazon Photo [39] datasets"}, {"mentioned_in_paper": "264", "context_id": "269", "dataset_context": "However, not all datasets' latent structures have the topological properties of the tree.", "mention_start": 8, "mention_end": 25, "dataset_mention": " not all datasets"}, {"mentioned_in_paper": "266", "context_id": "25", "dataset_context": "These methods require a considerable overlap between the initialized contour and an object to be segmented, which may not be the case in the low-frame rate datasets.", "mention_start": 136, "mention_end": 164, "dataset_mention": "the low-frame rate datasets"}, {"mentioned_in_paper": "266", "context_id": "45", "dataset_context": "Rather than aiming at deriving a single lineage, Uncertainty-Aware Tracking relies on a Bayesian approach and keeps track of all possible lineages, ranking them by their probability, shows promising results for low-frame rate datasets [2].", "mention_start": 210, "mention_end": 234, "dataset_mention": "low-frame rate datasets"}, {"mentioned_in_paper": "266", "context_id": "46", "dataset_context": "However, the approach is computationally expensive, extremely space demanding, and, thus, unsuitable for the analysis of big cultivation datasets, like [18].", "mention_start": 120, "mention_end": 145, "dataset_mention": "big cultivation datasets"}, {"mentioned_in_paper": "266", "context_id": "82", "dataset_context": "The calculation of intensity difference of two consecutive frames was utilized by [15] as a preliminary step for the cell tracking to estimate the cells' motion vectors, but limited to high frame rate (less than five minutes) datasets.", "mention_start": 211, "mention_end": 234, "dataset_mention": "five minutes) datasets"}, {"mentioned_in_paper": "266", "context_id": "137", "dataset_context": "To create a ground truth, we applied the proposed and a baseline approach (see below) to the segmented highest timeresolution dataset.", "mention_start": 88, "mention_end": 133, "dataset_mention": "the segmented highest timeresolution dataset"}, {"mentioned_in_paper": "266", "context_id": "140", "dataset_context": "In order to evaluate our algorithm for different frame rates, i.e. 2, 4, 6, 8, 10, and 12 minutes for C. glutamicum and 5, 10, 15 minutes for E. coli, we downsample our original datasets, respectively.", "mention_start": 150, "mention_end": 186, "dataset_mention": " we downsample our original datasets"}, {"mentioned_in_paper": "266", "context_id": "149", "dataset_context": "In case of E. coli dataset evaluation, both settings lead to the same result, so we only report the value at default settings (Fig. 5).", "mention_start": 11, "mention_end": 26, "dataset_mention": "E. coli dataset"}, {"mentioned_in_paper": "266", "context_id": "152", "dataset_context": "As shown in Fig. 5, the proposed method outperforms the baseline method when applied to a C. glutamicum dataset and is on par with the baseline approach applied to the E. coli dataset.", "mention_start": 87, "mention_end": 111, "dataset_mention": "a C. glutamicum dataset"}, {"mentioned_in_paper": "266", "context_id": "152", "dataset_context": "As shown in Fig. 5, the proposed method outperforms the baseline method when applied to a C. glutamicum dataset and is on par with the baseline approach applied to the E. coli dataset.", "mention_start": 163, "mention_end": 183, "dataset_mention": "the E. coli dataset"}, {"mentioned_in_paper": "266", "context_id": "154", "dataset_context": "Moreover, in case of the E. coli dataset, the septum formation as a part of the division process may not be captured in the phase contrast images.", "mention_start": 20, "mention_end": 40, "dataset_mention": "the E. coli dataset"}, {"mentioned_in_paper": "267", "context_id": "2", "dataset_context": "Compared to the image domain where large scale and fully annotated benchmark datasets exists, VideoQA datasets are limited to small scale and are automatically generated, etc.", "mention_start": 93, "mention_end": 110, "dataset_mention": " VideoQA datasets"}, {"mentioned_in_paper": "267", "context_id": "4", "dataset_context": "Here we introduce ActivityNet-QA, a fully annotated and large scale VideoQA dataset.", "mention_start": 33, "mention_end": 83, "dataset_mention": " a fully annotated and large scale VideoQA dataset"}, {"mentioned_in_paper": "267", "context_id": "5", "dataset_context": "The dataset consists of 58,000 QA pairs on 5,800 complex web videos derived from the popular ActivityNet dataset.", "mention_start": 81, "mention_end": 112, "dataset_mention": "the popular ActivityNet dataset"}, {"mentioned_in_paper": "267", "context_id": "6", "dataset_context": "We present a statistical analysis of our ActivityNet-QA dataset and conduct extensive experiments on it by comparing existing VideoQA baselines.", "mention_start": 37, "mention_end": 63, "dataset_mention": "our ActivityNet-QA dataset"}, {"mentioned_in_paper": "267", "context_id": "23", "dataset_context": "As noted above, high-quality datasets are of considerable value for VQA research.", "mention_start": 15, "mention_end": 37, "dataset_mention": " high-quality datasets"}, {"mentioned_in_paper": "267", "context_id": "24", "dataset_context": "Several VideoQA datasets have been compiled for different scenarios, such as MovieQA (Tapaswi et al. 2016), TGIF-QA (Jang et al. 2017), MSVD-QA, MSRVTT-QA (Xu et al. 2017), and Video-QA (Zeng et al. 2017).", "mention_start": 0, "mention_end": 24, "dataset_mention": "Several VideoQA datasets"}, {"mentioned_in_paper": "267", "context_id": "25", "dataset_context": "Most of these VideoQA datasets exploit video source data from other datasets and then add questionanswer pairs to them.", "mention_start": 8, "mention_end": 30, "dataset_mention": "these VideoQA datasets"}, {"mentioned_in_paper": "267", "context_id": "28", "dataset_context": "\u2022 The datasets are small scale.", "mention_start": 0, "mention_end": 14, "dataset_mention": "\u2022 The datasets"}, {"mentioned_in_paper": "267", "context_id": "39", "dataset_context": "Our dataset exploits 5,800 videos from the ActivityNet dataset, which contains about 20,000 untrimmed web videos representing 200 action classes (Fabian Caba Heilbron and Niebles 2015).", "mention_start": 39, "mention_end": 62, "dataset_mention": "the ActivityNet dataset"}, {"mentioned_in_paper": "267", "context_id": "41", "dataset_context": "Compared with other VideoQA datasets, ActivityNet-QA is of large scale, fully annotated by humans, and with very long videos.", "mention_start": 14, "mention_end": 36, "dataset_mention": "other VideoQA datasets"}, {"mentioned_in_paper": "267", "context_id": "44", "dataset_context": "We first introduce the ActivityNet-QA dataset from three perspectives, namely video collection, QA generation, and statistical analysis.", "mention_start": 19, "mention_end": 45, "dataset_mention": "the ActivityNet-QA dataset"}, {"mentioned_in_paper": "267", "context_id": "47", "dataset_context": "Instead, we sample 5,800 videos from the 20,000 videos in Ac-tivityNet dataset.", "mention_start": 58, "mention_end": 78, "dataset_mention": "Ac-tivityNet dataset"}, {"mentioned_in_paper": "267", "context_id": "53", "dataset_context": "To reduce the labor costs, some VideoQA datasets exploit the narrative descriptions or captions of videos (Jang et al. 2017; Xu et al. 2017), to automatically generate QA pairs using off-the-shelf algorithms (Ren, Kiros, and Zemel 2015).", "mention_start": 26, "mention_end": 48, "dataset_mention": " some VideoQA datasets"}, {"mentioned_in_paper": "267", "context_id": "69", "dataset_context": "Referring to the taxonomy in existing VQA datasets (Ren, Kiros, and Zemel 2015; Antol et al. 2015), we manually categorize the samples into the following six classes by their answer types: Yes/No, Number, Color, Object, Location and Other.", "mention_start": 29, "mention_end": 50, "dataset_mention": "existing VQA datasets"}, {"mentioned_in_paper": "267", "context_id": "85", "dataset_context": "Here we present the detailed statistics of our ActivityNet-QA dataset.", "mention_start": 43, "mention_end": 69, "dataset_mention": "our ActivityNet-QA dataset"}, {"mentioned_in_paper": "267", "context_id": "97", "dataset_context": "In this section, we explore the difficulty of the ActivityNet-QA dataset using several baseline models based on different types of video features.", "mention_start": 45, "mention_end": 72, "dataset_mention": "the ActivityNet-QA dataset"}, {"mentioned_in_paper": "267", "context_id": "114", "dataset_context": "As we have obtained the sampled video unit set, for each unit u i , we used the VGG-16 network pre-trained on the ImageNet dataset (Simonyan and Zisserman 2014) to extract the appearance feature (the fc7 feature x i \u2208 R 4096 ) given the central frame of the unit, and the C3D network pretrained on the Sport-1M dataset (Tran et al. 2015) to extract the motion features (the fc7 feature y i \u2208 R 4096 ) given the whole 16 consecutive frames of the unit.", "mention_start": 109, "mention_end": 130, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "267", "context_id": "114", "dataset_context": "As we have obtained the sampled video unit set, for each unit u i , we used the VGG-16 network pre-trained on the ImageNet dataset (Simonyan and Zisserman 2014) to extract the appearance feature (the fc7 feature x i \u2208 R 4096 ) given the central frame of the unit, and the C3D network pretrained on the Sport-1M dataset (Tran et al. 2015) to extract the motion features (the fc7 feature y i \u2208 R 4096 ) given the whole 16 consecutive frames of the unit.", "mention_start": 297, "mention_end": 318, "dataset_mention": "the Sport-1M dataset"}, {"mentioned_in_paper": "267", "context_id": "119", "dataset_context": "Note that the focus of this paper is the constructed ActivityNet-QA dataset and a discussion of what influences the performance on the dataset.", "mention_start": 37, "mention_end": 75, "dataset_mention": "the constructed ActivityNet-QA dataset"}, {"mentioned_in_paper": "267", "context_id": "130", "dataset_context": "We evaluate the aforementioned VideoQA models on our ActivityNet-QA dataset.", "mention_start": 49, "mention_end": 75, "dataset_mention": "our ActivityNet-QA dataset"}, {"mentioned_in_paper": "267", "context_id": "184", "dataset_context": "Compared with existing VideoQA datasets, our dataset is unique in that: 1) the videos originate from ActivityNet, a large-scale video understanding dataset with long web videos; 2) the QA pairs are fully annotated by crowdsourcing.", "mention_start": 14, "mention_end": 39, "dataset_mention": "existing VideoQA datasets"}, {"mentioned_in_paper": "267", "context_id": "184", "dataset_context": "Compared with existing VideoQA datasets, our dataset is unique in that: 1) the videos originate from ActivityNet, a large-scale video understanding dataset with long web videos; 2) the QA pairs are fully annotated by crowdsourcing.", "mention_start": 113, "mention_end": 155, "dataset_mention": " a large-scale video understanding dataset"}, {"mentioned_in_paper": "267", "context_id": "186", "dataset_context": "Based on the constructed dataset, we apply several baselines to analyze the difficulty of our dataset and also investigate the strategies to learn better video feature representation; and 3) the QA pairs of our dataset are bilingual with alignment.", "mention_start": 9, "mention_end": 32, "dataset_mention": "the constructed dataset"}, {"mentioned_in_paper": "268", "context_id": "44", "dataset_context": "\u2022 We experiment with different public medical image datasets of different modalities, and MultiResUNet shows superior accuracy.", "mention_start": 21, "mention_end": 60, "dataset_mention": "different public medical image datasets"}, {"mentioned_in_paper": "268", "context_id": "132", "dataset_context": "Curation of medical imaging datasets are challenging compared to the traditional computer vision datasets.", "mention_start": 12, "mention_end": 36, "dataset_mention": "medical imaging datasets"}, {"mentioned_in_paper": "268", "context_id": "132", "dataset_context": "Curation of medical imaging datasets are challenging compared to the traditional computer vision datasets.", "mention_start": 65, "mention_end": 105, "dataset_mention": "the traditional computer vision datasets"}, {"mentioned_in_paper": "268", "context_id": "133", "dataset_context": "Expensive imaging equipments, sophisticated image acquisition pipelines, necessity of expert annotation, issues of privacy-all adds to the complexity of developing medical imaging datasets [22].", "mention_start": 163, "mention_end": 188, "dataset_mention": "medical imaging datasets"}, {"mentioned_in_paper": "268", "context_id": "134", "dataset_context": "As a result, only a few public medical imaging benchmark datasets exist, and they only contain a handful of images each.", "mention_start": 17, "mention_end": 65, "dataset_mention": "a few public medical imaging benchmark datasets"}, {"mentioned_in_paper": "268", "context_id": "139", "dataset_context": "We used the fluorescence microscopy image dataset developed by Murphy Lab [36].", "mention_start": 8, "mention_end": 49, "dataset_mention": "the fluorescence microscopy image dataset"}, {"mentioned_in_paper": "268", "context_id": "149", "dataset_context": "We acquired the dermoscopy images from the ISIC-2018 : Lesion Boundary Segmentation challenge dataset.", "mention_start": 54, "mention_end": 101, "dataset_mention": " Lesion Boundary Segmentation challenge dataset"}, {"mentioned_in_paper": "268", "context_id": "150", "dataset_context": "The data for this challenge were extracted from the ISIC-2017 dataset [3] and the HAM10000 dataset [42].", "mention_start": 48, "mention_end": 69, "dataset_mention": "the ISIC-2017 dataset"}, {"mentioned_in_paper": "268", "context_id": "150", "dataset_context": "The data for this challenge were extracted from the ISIC-2017 dataset [3] and the HAM10000 dataset [42].", "mention_start": 48, "mention_end": 98, "dataset_mention": "the ISIC-2017 dataset [3] and the HAM10000 dataset"}, {"mentioned_in_paper": "268", "context_id": "151", "dataset_context": "The compiled dataset contains a total of 2594 images of different types of skin lesions with expert annotation.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The compiled dataset"}, {"mentioned_in_paper": "268", "context_id": "201", "dataset_context": "Cross-Validation tests estimate the general effectiveness of an algorithm on an independent dataset, ensuring a balance between bias and variance.", "mention_start": 77, "mention_end": 99, "dataset_mention": "an independent dataset"}, {"mentioned_in_paper": "268", "context_id": "253", "dataset_context": "Fortunately, the Dermatoscopy image dataset we have used contains images with such confusing cases, allowing us to analyze and compare the behaviour and performance of the two models.", "mention_start": 12, "mention_end": 43, "dataset_mention": " the Dermatoscopy image dataset"}, {"mentioned_in_paper": "268", "context_id": "264", "dataset_context": "Particularly, in the Fluorescence Microscopy image dataset, there exist some images with bright objects, that are apparently almost indistinguishable from the actual nuclei.", "mention_start": 16, "mention_end": 58, "dataset_mention": "the Fluorescence Microscopy image dataset"}, {"mentioned_in_paper": "268", "context_id": "281", "dataset_context": "Among the handful publicly available biomedical image datasets, we selected the ones that were drastically different from each other.", "mention_start": 6, "mention_end": 62, "dataset_mention": "the handful publicly available biomedical image datasets"}, {"mentioned_in_paper": "268", "context_id": "283", "dataset_context": "The Murphy Lab Fluorescence Microscopy dataset is possibly the simplest dataset for performing segmentation, having an acute difference in contrast between the forground, i.e., the cell nuclei and the background, but contains some outliers.", "mention_start": 0, "mention_end": 46, "dataset_mention": "The Murphy Lab Fluorescence Microscopy dataset"}, {"mentioned_in_paper": "268", "context_id": "283", "dataset_context": "The Murphy Lab Fluorescence Microscopy dataset is possibly the simplest dataset for performing segmentation, having an acute difference in contrast between the forground, i.e., the cell nuclei and the background, but contains some outliers.", "mention_start": 50, "mention_end": 79, "dataset_mention": "possibly the simplest dataset"}, {"mentioned_in_paper": "268", "context_id": "284", "dataset_context": "The CVC-ClinicDB dataset contains colon endoscopy images where the boundaries between the polyps and the background are so vague that often it becomes difficult to distinguish even for a trained operator.", "mention_start": 0, "mention_end": 24, "dataset_mention": "The CVC-ClinicDB dataset"}, {"mentioned_in_paper": "268", "context_id": "286", "dataset_context": "On the other hand, the dermoscopy dataset of ISIC-2018 competition contains images of poor contrast to the extent that sometimes the skin lesions seem identical to the background and vice versa.", "mention_start": 18, "mention_end": 41, "dataset_mention": " the dermoscopy dataset"}, {"mentioned_in_paper": "268", "context_id": "288", "dataset_context": "ISBI-2012 electron microscopy dataset presents a different type of challenge.", "mention_start": 0, "mention_end": 37, "dataset_mention": "ISBI-2012 electron microscopy dataset"}, {"mentioned_in_paper": "268", "context_id": "290", "dataset_context": "The BraTS17 MRI dataset, on the other hand contains multimodal 3D images, which is a different problem alltogether.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The BraTS17 MRI dataset"}, {"mentioned_in_paper": "269", "context_id": "11", "dataset_context": "However, because there is no commonly agreed-upon taxonomy for actions, different datasets define completely different action categories, which makes it challenging to combine action datasets and reuse models.", "mention_start": 175, "mention_end": 191, "dataset_mention": "action datasets"}, {"mentioned_in_paper": "269", "context_id": "51", "dataset_context": "As part of this work, we will release both our software for learning embeddings and the trained embeddings we generated for all of the benchmark action recognition datasets.", "mention_start": 130, "mention_end": 172, "dataset_mention": "the benchmark action recognition datasets"}, {"mentioned_in_paper": "269", "context_id": "77", "dataset_context": "There are many efforts to ameliorate this problem, however most require the use of auxiliary datasets or self-training.", "mention_start": 82, "mention_end": 101, "dataset_mention": "auxiliary datasets"}, {"mentioned_in_paper": "269", "context_id": "111", "dataset_context": "For visual features we use the C3D architecture pretrained on Sports-1M dataset [12], given its success in action recognition [30, 25].", "mention_start": 62, "mention_end": 79, "dataset_mention": "Sports-1M dataset"}, {"mentioned_in_paper": "269", "context_id": "146", "dataset_context": "For example, in the HMDB51 dataset, the action class \"push\" has a variety of videos, from children pushing toy trains to adults pushing tables.", "mention_start": 15, "mention_end": 34, "dataset_mention": "the HMDB51 dataset"}, {"mentioned_in_paper": "269", "context_id": "171", "dataset_context": "Superior performance on a smaller and harder dataset suggests that Action2Vec is capturing meaningful semantic and temporal properties.", "mention_start": 24, "mention_end": 52, "dataset_mention": "a smaller and harder dataset"}, {"mentioned_in_paper": "269", "context_id": "215", "dataset_context": "This experiment is designed to easily scale to most action datasets, in which multiple action classes contain the same verb with different nouns.", "mention_start": 47, "mention_end": 67, "dataset_mention": "most action datasets"}, {"mentioned_in_paper": "269", "context_id": "225", "dataset_context": "We were unable to preform this experiment on the HMDB51 dataset because it does not contain multiple action classes with the same verb.", "mention_start": 45, "mention_end": 63, "dataset_mention": "the HMDB51 dataset"}, {"mentioned_in_paper": "270", "context_id": "154", "dataset_context": "To test the generalizability, we also designed a different dataset, which consists of polygonal shapes that are quite different from the shapes of digits.", "mention_start": 37, "mention_end": 66, "dataset_mention": "designed a different dataset"}, {"mentioned_in_paper": "271", "context_id": "4", "dataset_context": "The resulting algorithm consistently outperforms the original BN on various types of network architectures and datasets.", "mention_start": 85, "mention_end": 119, "dataset_mention": "network architectures and datasets"}, {"mentioned_in_paper": "271", "context_id": "21", "dataset_context": "Experiments show that the resulting algorithm consistently outperforms the original BN algorithm on various types of network architectures and datasets.", "mention_start": 117, "mention_end": 151, "dataset_mention": "network architectures and datasets"}, {"mentioned_in_paper": "271", "context_id": "160", "dataset_context": "SVHN [17] is a digit classification benchmark dataset that contains 73,257 images in the training set, 26,032 images in the test set, and 531,131 images in the extra set.", "mention_start": 13, "mention_end": 53, "dataset_mention": "a digit classification benchmark dataset"}, {"mentioned_in_paper": "271", "context_id": "186", "dataset_context": "Tables 1 and 2 compare the performance of the baseline SGD and two proposed algorithms described in Sec. 4 and 5, on CIFAR-10, CIFAR-100, and SVHN datasets.", "mention_start": 137, "mention_end": 155, "dataset_mention": " and SVHN datasets"}, {"mentioned_in_paper": "272", "context_id": "2", "dataset_context": "We also substantially strengthen a recent result of Phuong and Lampert on directional convergence of gradient flow, and obtain as a corollary that training two-layer ReLU neural networks on orthogonally separable datasets can cause their adversarial reprogramming to fail.", "mention_start": 189, "mention_end": 221, "dataset_mention": "orthogonally separable datasets"}, {"mentioned_in_paper": "272", "context_id": "13", "dataset_context": "Like the universal adversarial perturbation of Moosavi-Dezfooli, Fawzi, Fawzi, and Frossard [2017], a single adversarial program is combined with every input from the adversarial task, but in contrast to the former where the goal is to cause natural images to be Both for the neworks with random weights and for the networks trained to infinity on orthogonally separable datasets, we then show that similar theoretical results can be obtained with a different kind of adversarial task, namely those given by the Gaussian distributions also of Schmidt et al. [2018].", "mention_start": 347, "mention_end": 379, "dataset_mention": "orthogonally separable datasets"}, {"mentioned_in_paper": "272", "context_id": "23", "dataset_context": "We conduct the same experiments also on the more challenging Fashion-MNIST [Xiao, Rasul, and Vollgraf, 2017] and Kuzushiji-MNIST [Clanuwat, Bober-Irizar, Kitamoto, Lamb, Yamamoto, and Ha, 2018] datasets, and obtain broadly similar results, however with lower test accuracies in several cases; they are reported in Appendix H.1.", "mention_start": 187, "mention_end": 202, "dataset_mention": " 2018] datasets"}, {"mentioned_in_paper": "272", "context_id": "47", "dataset_context": "These binary classification data models on hypercube vertices are inspired by the MNIST dataset [LeCun et al., 1998].", "mention_start": 78, "mention_end": 95, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "272", "context_id": "71", "dataset_context": "Orthogonally separable dataset.", "mention_start": 0, "mention_end": 30, "dataset_mention": "Orthogonally separable dataset"}, {"mentioned_in_paper": "272", "context_id": "141", "dataset_context": "Adversarial task dataset.", "mention_start": 0, "mention_end": 24, "dataset_mention": "Adversarial task dataset"}, {"mentioned_in_paper": "272", "context_id": "142", "dataset_context": " Elsayed et al. [2019] evaluate adversarial reprogramming on random networks using the MNIST [LeCun et al., 1998] dataset.", "mention_start": 107, "mention_end": 121, "dataset_mention": " 1998] dataset"}, {"mentioned_in_paper": "272", "context_id": "143", "dataset_context": "In other words, they were asking whether it is possible to repurpose a random network for the task of classifying the handwritten digits from the MNIST dataset.", "mention_start": 141, "mention_end": 159, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "272", "context_id": "147", "dataset_context": "We map the 10 labels of the MNIST dataset onto the first 10 of these classes.", "mention_start": 24, "mention_end": 41, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "272", "context_id": "148", "dataset_context": "For additional experimental results on the Fashion-MNIST and Kuzushiji-MNIST datasets, please see Appendix H.1.", "mention_start": 39, "mention_end": 85, "dataset_mention": "the Fashion-MNIST and Kuzushiji-MNIST datasets"}, {"mentioned_in_paper": "272", "context_id": "171", "dataset_context": "Provided the input dimension is sufficiently large, and for a wide variety of parameter regimes, our results show that: firstly, arbitrarily high reprogramming accuracies are achievable in expectation for networks with random weights; and secondly, reprogramming accuracies that are no better than guessing may be unavoidable for networks that were trained for many iterations with small learning rates on orthogonally separable datasets.", "mention_start": 405, "mention_end": 437, "dataset_mention": "orthogonally separable datasets"}, {"mentioned_in_paper": "272", "context_id": "177", "dataset_context": "The outcomes of our experiments, which are on six realistic convolutional network architectures designed for image classification, and on three adversarial tasks provided by the MNIST, Fashion-MNIST and Kuzushiji-MNIST datasets, are consistent with our theoretical results on high reprogramming accuracies.", "mention_start": 184, "mention_end": 227, "dataset_mention": " Fashion-MNIST and Kuzushiji-MNIST datasets"}, {"mentioned_in_paper": "272", "context_id": "279", "dataset_context": "Here we work with the notations and assumptions from Section 3, so we have a trajectory \u03b8(t) : [0, \u221e) \u2192 R k(d+1) of gradient flow for a two-layer ReLU network N with input dimension d and width k, trained on an orthogonally separable binary classification dataset S = {(x 1 , y 1 ), . . .", "mention_start": 207, "mention_end": 263, "dataset_mention": "an orthogonally separable binary classification dataset"}, {"mentioned_in_paper": "272", "context_id": "350", "dataset_context": "Another recent result is the next lemma, which shows that maximum-margin KKT points for twolayer ReLU networks on orthogonally separable datasets are particularly streamlined.", "mention_start": 113, "mention_end": 145, "dataset_mention": "orthogonally separable datasets"}, {"mentioned_in_paper": "272", "context_id": "354", "dataset_context": "Lemma 15 (Lyu et al. [2021, Lemma H.3]). Suppose \u03b8 is a Karush-Kuhn-Tucker point of the problem in Theorem 14 for a two-layer ReLU network N \u03b8 and an orthogonally separable dataset.", "mention_start": 113, "mention_end": 180, "dataset_mention": "a two-layer ReLU network N \u03b8 and an orthogonally separable dataset"}, {"mentioned_in_paper": "272", "context_id": "415", "dataset_context": "We consider, as in Section 3, a trajectory of gradient flow for a two-layer ReLU network N \u03b8 with input dimension d and width k, trained on an orthogonally separable binary classification dataset, from a balanced and live initialisation, using either the exponential or the logistic loss function.", "mention_start": 139, "mention_end": 195, "dataset_mention": "an orthogonally separable binary classification dataset"}, {"mentioned_in_paper": "273", "context_id": "37", "dataset_context": "While in [26], the source data is transferred to the target style, and the deep neural network is then trained on the D s\u2192t dataset, we note that in the clinical setting it would mean re-training the model for each new target domain (e.g., a new clinic), which might complicate certification and clinical deployment.", "mention_start": 113, "mention_end": 131, "dataset_mention": "the D s\u2192t dataset"}, {"mentioned_in_paper": "273", "context_id": "57", "dataset_context": "We conduct all the experiments on a public brain MR dataset called CC359 [20], which is formed of 359 scans and various masks, among which are the brain segmentation masks.", "mention_start": 34, "mention_end": 59, "dataset_mention": "a public brain MR dataset"}, {"mentioned_in_paper": "275", "context_id": "159", "dataset_context": "We conduct a thorough experiment on a real-world dataset: the NSF research award dataset 1 and evaluate the performance of LRSE compared with the MRSE schemes [6].", "mention_start": 57, "mention_end": 88, "dataset_mention": " the NSF research award dataset"}, {"mentioned_in_paper": "275", "context_id": "178", "dataset_context": "Extensive experiments based on real-word dataset demonstrate that the proposed scheme achieves the design goals.", "mention_start": 31, "mention_end": 48, "dataset_mention": "real-word dataset"}, {"mentioned_in_paper": "276", "context_id": "43", "dataset_context": "4) The CPNet results in a significant reduction in computational cost while delivering equivalent performance on Ped2 [28] and Avenue [17] datasets.", "mention_start": 113, "mention_end": 147, "dataset_mention": "Ped2 [28] and Avenue [17] datasets"}, {"mentioned_in_paper": "276", "context_id": "102", "dataset_context": "The UCSD Pedestrian 2 (Ped2) Dataset contains 16 training videos and 12 testing videos with 12 abnormal events.", "mention_start": 0, "mention_end": 36, "dataset_mention": "The UCSD Pedestrian 2 (Ped2) Dataset"}, {"mentioned_in_paper": "276", "context_id": "105", "dataset_context": "CUHK Avenue Dataset contains 16 training and 21 testing videos with 47 abnormal events in total, including running and throwing of objects.", "mention_start": 0, "mention_end": 19, "dataset_mention": "CUHK Avenue Dataset"}, {"mentioned_in_paper": "276", "context_id": "137", "dataset_context": "As Tang et al. [29] delivers equivalent AUC on both Ped2 and Avenue datasets, it requires far more computational cost as it implements both reconstruction and prediction tasks with two U-Nets.", "mention_start": 47, "mention_end": 76, "dataset_mention": "both Ped2 and Avenue datasets"}, {"mentioned_in_paper": "276", "context_id": "139", "dataset_context": "Figure 3 shows anomaly detection results of CPNet-0.37 on Ped2 and Avenue datasets.", "mention_start": 58, "mention_end": 82, "dataset_mention": "Ped2 and Avenue datasets"}, {"mentioned_in_paper": "276", "context_id": "141", "dataset_context": "For the Ped2 dataset, our model well captures abnormal regions, i.e., riding a bicycle, riding a skateboard and driving a vehicle, as shown in Figure 3.  left column shows some abnormal regions of ground truth (GT) and predicted frames (Pred.) on the Ped2 dataset.", "mention_start": 4, "mention_end": 20, "dataset_mention": "the Ped2 dataset"}, {"mentioned_in_paper": "276", "context_id": "141", "dataset_context": "For the Ped2 dataset, our model well captures abnormal regions, i.e., riding a bicycle, riding a skateboard and driving a vehicle, as shown in Figure 3.  left column shows some abnormal regions of ground truth (GT) and predicted frames (Pred.) on the Ped2 dataset.", "mention_start": 245, "mention_end": 263, "dataset_mention": "the Ped2 dataset"}, {"mentioned_in_paper": "276", "context_id": "143", "dataset_context": "For the Avenue dataset, the abnormal regions, i.e., throwing a bag and running, are successfully detected.", "mention_start": 4, "mention_end": 22, "dataset_mention": "the Avenue dataset"}, {"mentioned_in_paper": "276", "context_id": "152", "dataset_context": "The CPNet resulted in a significant reduction in computational cost while delivering equivalent performance on Ped2 and Avenue datasets compared with the state-of-the-art methods.", "mention_start": 111, "mention_end": 135, "dataset_mention": "Ped2 and Avenue datasets"}, {"mentioned_in_paper": "280", "context_id": "3", "dataset_context": "Without any bells and whistles, PolarMask achieves 32.9% in mask mAP with single-model and single-scale training/testing on the challenging COCO dataset.", "mention_start": 139, "mention_end": 152, "dataset_mention": "COCO dataset"}, {"mentioned_in_paper": "280", "context_id": "35", "dataset_context": "Without bells and whistles, PolarMask achieves 32.9% in mask mAP with single-model and single-scale training/testing on the challenging COCO dataset [24].", "mention_start": 135, "mention_end": 148, "dataset_mention": "COCO dataset"}, {"mentioned_in_paper": "280", "context_id": "216", "dataset_context": "We evaluate PolarMask on the COCO dataset and compare test-dev results to state-of-the-art methods including both one-stage and two-stage models, shown in Table 2. Po-larMask outputs are visualized in Figure 8.", "mention_start": 25, "mention_end": 41, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "282", "context_id": "47", "dataset_context": "Majhee blocks are represented as irregular polygons in the NPM shapefile dataset available on the Humanitarian Data Exchange [8], as shown in Figure 2. It is worth noting that manual population estimation is a challenging and time-consuming task.", "mention_start": 55, "mention_end": 80, "dataset_mention": "the NPM shapefile dataset"}, {"mentioned_in_paper": "282", "context_id": "74", "dataset_context": "Nevertheless, we believe this approach and dataset constitute an important step towards the development of tools that enable the humanitarian community to effectively and rapidly respond to the global displacement crisis.", "mention_start": 24, "mention_end": 50, "dataset_mention": "this approach and dataset"}, {"mentioned_in_paper": "284", "context_id": "69", "dataset_context": "The collected sensor dataset is also split into one-stride long data by our algorithm.", "mention_start": 14, "mention_end": 28, "dataset_mention": "sensor dataset"}, {"mentioned_in_paper": "284", "context_id": "113", "dataset_context": "Note that, we also use random normal noise augmentation on labeled sensor dataset to increase diversity of the labeled dataset.", "mention_start": 58, "mention_end": 81, "dataset_mention": "labeled sensor dataset"}, {"mentioned_in_paper": "284", "context_id": "136", "dataset_context": "The performance evaluation for the model is done by averaging the error and accuracy of all k testing dataset.", "mention_start": 88, "mention_end": 109, "dataset_mention": "all k testing dataset"}, {"mentioned_in_paper": "285", "context_id": "3", "dataset_context": "Most existing matrix factorization methods adopt the same proximity for different tasks, while it is observed that different tasks and datasets may require different proximity, limiting their representation power.", "mention_start": 100, "mention_end": 143, "dataset_mention": "observed that different tasks and datasets"}, {"mentioned_in_paper": "285", "context_id": "4", "dataset_context": "Motivated by this, we propose Lemane, a framework with trainable proximity measures, which can be learned to best suit the datasets and tasks at hand automatically.", "mention_start": 108, "mention_end": 131, "dataset_mention": "best suit the datasets"}, {"mentioned_in_paper": "285", "context_id": "10", "dataset_context": "Extensive experiments show that our proposed solution outperforms existing solutions on both link prediction and node classification tasks on almost all datasets.", "mention_start": 142, "mention_end": 161, "dataset_mention": "almost all datasets"}, {"mentioned_in_paper": "285", "context_id": "22", "dataset_context": "However, it is observed that different tasks and datasets may require different proximities to achieve high performance, limiting the representation power of such solutions.", "mention_start": 14, "mention_end": 57, "dataset_mention": "observed that different tasks and datasets"}, {"mentioned_in_paper": "285", "context_id": "29", "dataset_context": "Motivated by the limitation of existing solutions, we present an effective framework Lemane 1 with trainable proximity measures, which can be learned to best suit the datasets and tasks at hand automatically.", "mention_start": 152, "mention_end": 175, "dataset_mention": "best suit the datasets"}, {"mentioned_in_paper": "285", "context_id": "52", "dataset_context": "Extensive experiments show that our Lemane outperforms existing methods on both link prediction and node classification tasks on almost all datasets.", "mention_start": 129, "mention_end": 148, "dataset_mention": "almost all datasets"}, {"mentioned_in_paper": "285", "context_id": "277", "dataset_context": "On the Wikipedia dataset, a co-occurrence network of words appearing in the Wikipedia, our Lemane still achieves high performance and is the best method among all matrix factorization methods.", "mention_start": 3, "mention_end": 24, "dataset_mention": "the Wikipedia dataset"}, {"mentioned_in_paper": "285", "context_id": "292", "dataset_context": "Besides, compared to STRAP, which takes PPR without training the stopping probabilities, our Lemane achieves more than 1% lead on Wikipedia datasets and up to 3% on the Orkut dataset.", "mention_start": 129, "mention_end": 148, "dataset_mention": "Wikipedia datasets"}, {"mentioned_in_paper": "285", "context_id": "292", "dataset_context": "Besides, compared to STRAP, which takes PPR without training the stopping probabilities, our Lemane achieves more than 1% lead on Wikipedia datasets and up to 3% on the Orkut dataset.", "mention_start": 164, "mention_end": 182, "dataset_mention": "the Orkut dataset"}, {"mentioned_in_paper": "285", "context_id": "293", "dataset_context": "Compared to the second-best matrix factorization method NRP, our Lemane further achieves about 1% lead on the BlogCatalog dataset in all of the tested training ratios.", "mention_start": 105, "mention_end": 129, "dataset_mention": "the BlogCatalog dataset"}, {"mentioned_in_paper": "285", "context_id": "296", "dataset_context": "In this paper, we present Lemane that learns trainable proximity measures to best suit the datasets and tasks at hand automatically.", "mention_start": 76, "mention_end": 99, "dataset_mention": "best suit the datasets"}, {"mentioned_in_paper": "286", "context_id": "4", "dataset_context": "The experimental results on public light field datasets have demonstrated the advantages of the proposed algorithm compared with other state-of-the-art light field depth estimation algorithms, especially in multi-occluder areas.", "mention_start": 28, "mention_end": 55, "dataset_mention": "public light field datasets"}, {"mentioned_in_paper": "286", "context_id": "139", "dataset_context": "The performance of the proposed algorithm is evaluated by using the most popular light field datasets [17].", "mention_start": 64, "mention_end": 101, "dataset_mention": "the most popular light field datasets"}, {"mentioned_in_paper": "286", "context_id": "163", "dataset_context": "Our algorithm outperforms previous state-of-the-art algorithms in almost all datasets.", "mention_start": 66, "mention_end": 85, "dataset_mention": "almost all datasets"}, {"mentioned_in_paper": "287", "context_id": "92", "dataset_context": "We applied MoCo-MSLR to free breathing 3D radial imaging acquisitions in the lung and placenta from previously acquired datasets.", "mention_start": 100, "mention_end": 128, "dataset_mention": "previously acquired datasets"}, {"mentioned_in_paper": "287", "context_id": "120", "dataset_context": "One healthy volunteer UTE lung dataset (17) was acquired with a 32 channel coil, scan time of 5 minutes and 45 seconds, TE=0.25ms,", "mention_start": 0, "mention_end": 38, "dataset_mention": "One healthy volunteer UTE lung dataset"}, {"mentioned_in_paper": "287", "context_id": "126", "dataset_context": "One UTE lung dataset of a cystic fibrosis (CF) patient was acquired with an 8-channel coil array, an overall scan time of 4 minutes 18 seconds, TE=80\u00b5s, TR=3.48ms, flip angle 4 degrees and 1.25 mm isotropic resolution.", "mention_start": 0, "mention_end": 20, "dataset_mention": "One UTE lung dataset"}, {"mentioned_in_paper": "287", "context_id": "132", "dataset_context": "One UTE lung dataset of a patient with idiopathic pulmonary fibrosis (IPF) was acquired with an 8-channel coil array, an overall scan time of 4 minutes 54 seconds, TE=80\u00b5s, TR=3.27ms, flip angle 4 degrees and 1.25 mm isotropic resolution.", "mention_start": 0, "mention_end": 20, "dataset_mention": "One UTE lung dataset"}, {"mentioned_in_paper": "287", "context_id": "136", "dataset_context": "One placental dataset of a healthy pregnant patient in the third trimester was acquired with GE Air Coil, an overall scan time of 4 minutes, 2 seconds, TE=1.3ms,", "mention_start": 0, "mention_end": 21, "dataset_mention": "One placental dataset"}, {"mentioned_in_paper": "287", "context_id": "140", "dataset_context": "The healthy volunteer and CF datasets were acquired on a 3 Tesla GE scanner.", "mention_start": 0, "mention_end": 37, "dataset_mention": "The healthy volunteer and CF datasets"}, {"mentioned_in_paper": "287", "context_id": "141", "dataset_context": "The IPF and placental datasets acquired on a 1.5 Tesla GE scanner", "mention_start": 0, "mention_end": 30, "dataset_mention": "The IPF and placental datasets"}, {"mentioned_in_paper": "289", "context_id": "7", "dataset_context": "Furthermore, we perform our study on two generic image datasets and one real-world federated medical image dataset.", "mention_start": 36, "mention_end": 63, "dataset_mention": "two generic image datasets"}, {"mentioned_in_paper": "289", "context_id": "7", "dataset_context": "Furthermore, we perform our study on two generic image datasets and one real-world federated medical image dataset.", "mention_start": 36, "mention_end": 114, "dataset_mention": "two generic image datasets and one real-world federated medical image dataset"}, {"mentioned_in_paper": "289", "context_id": "8", "dataset_context": "We also systematically investigate the effect of the proportion of affected clients and the dataset distribution factors", "mention_start": 67, "mention_end": 99, "dataset_mention": "affected clients and the dataset"}, {"mentioned_in_paper": "289", "context_id": "25", "dataset_context": "In recent years, mutation testing has also been applied to DL applications by defining mutation operators working on DL models and datasets (Hu et al., 2019; Shen et al., 2018; Wang et al., 2019).", "mention_start": 116, "mention_end": 139, "dataset_mention": "DL models and datasets"}, {"mentioned_in_paper": "289", "context_id": "33", "dataset_context": "We also run the experiments on generic and real-world federated medical image datasets.", "mention_start": 31, "mention_end": 86, "dataset_mention": "generic and real-world federated medical image datasets"}, {"mentioned_in_paper": "289", "context_id": "34", "dataset_context": "Since generic datasets are not distributed, we distribute them between multiple clients to see the effect of distribution, and we make the distribution with three levels of non-iid.", "mention_start": 6, "mention_end": 22, "dataset_mention": "generic datasets"}, {"mentioned_in_paper": "289", "context_id": "38", "dataset_context": "The results show that even the baseline FL aggregator (Federated Averaging) is quite robust against mutation operators on the generic image datasets.", "mention_start": 122, "mention_end": 148, "dataset_mention": "the generic image datasets"}, {"mentioned_in_paper": "289", "context_id": "39", "dataset_context": "However, the Overlap mutator causes noticeable damage when applied to the medical dataset.", "mention_start": 69, "mention_end": 89, "dataset_mention": "the medical dataset"}, {"mentioned_in_paper": "289", "context_id": "41", "dataset_context": "Our comparison of FL aggregators shows that Krum faces issues on more noniid datasets and does not work as well as others, where all clients are benign.", "mention_start": 65, "mention_end": 85, "dataset_mention": "more noniid datasets"}, {"mentioned_in_paper": "289", "context_id": "48", "dataset_context": "-Analysis of different configurations of FL (cross-device and cross-silo) using both generic and medical datasets with different distributions.", "mention_start": 80, "mention_end": 113, "dataset_mention": "both generic and medical datasets"}, {"mentioned_in_paper": "289", "context_id": "159", "dataset_context": "RQ1: How robust is Federated Averaging against attacks and faults in a wellknown image domain dataset?", "mention_start": 68, "mention_end": 101, "dataset_mention": "a wellknown image domain dataset"}, {"mentioned_in_paper": "289", "context_id": "166", "dataset_context": "RQ3: In a real-world federated dataset, what is the effect of attacks and faults on different aggregators?", "mention_start": 7, "mention_end": 38, "dataset_mention": "a real-world federated dataset"}, {"mentioned_in_paper": "289", "context_id": "168", "dataset_context": "Note that we do experiments similar to RQ1 and RQ2 just on a real Federated dataset and a cross-silo setting.", "mention_start": 59, "mention_end": 83, "dataset_mention": "a real Federated dataset"}, {"mentioned_in_paper": "289", "context_id": "177", "dataset_context": "Generic datasets: For the first two questions, which are focused on the generic image dataset, we choose two popular datasets from the image classification task, Fashion MNIST (Xiao et al., 2017), and CIFAR-10 (Krizhevsky et al., 2009).", "mention_start": 0, "mention_end": 16, "dataset_mention": "Generic datasets"}, {"mentioned_in_paper": "289", "context_id": "177", "dataset_context": "Generic datasets: For the first two questions, which are focused on the generic image dataset, we choose two popular datasets from the image classification task, Fashion MNIST (Xiao et al., 2017), and CIFAR-10 (Krizhevsky et al., 2009).", "mention_start": 67, "mention_end": 93, "dataset_mention": "the generic image dataset"}, {"mentioned_in_paper": "289", "context_id": "178", "dataset_context": "The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6000 images per class.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The CIFAR-10 dataset"}, {"mentioned_in_paper": "289", "context_id": "184", "dataset_context": "We also chose these centralized datasets to control how we distribute them and study the effect of data distribution.", "mention_start": 14, "mention_end": 40, "dataset_mention": "these centralized datasets"}, {"mentioned_in_paper": "289", "context_id": "185", "dataset_context": "Federated dataset: In RQ3, we aim to study a more realistic scenario and see its effect on FL quality.", "mention_start": 0, "mention_end": 17, "dataset_mention": "Federated dataset"}, {"mentioned_in_paper": "289", "context_id": "186", "dataset_context": "Since tasks on medical images are one of the main FL ap-plications (Kairouz et al., 2021), given patients' privacy concerns, a distributed medical image dataset is a perfect match for RQ3's goal.", "mention_start": 124, "mention_end": 160, "dataset_mention": " a distributed medical image dataset"}, {"mentioned_in_paper": "289", "context_id": "187", "dataset_context": "Our medical imaging dataset is obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (Mueller et al., 2005).", "mention_start": 0, "mention_end": 27, "dataset_mention": "Our medical imaging dataset"}, {"mentioned_in_paper": "289", "context_id": "199", "dataset_context": "Data distribution: There is no need for synthetic distribution for the ADNI dataset since it is already distributed.", "mention_start": 66, "mention_end": 83, "dataset_mention": "the ADNI dataset"}, {"mentioned_in_paper": "289", "context_id": "200", "dataset_context": "However, since CIFAR-10 and Fashion MNIST datasets are centralized, we partition them and distribute samples to different clients.", "mention_start": 14, "mention_end": 50, "dataset_mention": "CIFAR-10 and Fashion MNIST datasets"}, {"mentioned_in_paper": "289", "context_id": "212", "dataset_context": "Models: Our models for generic datasets are simple convolutional neural networks with 12 and 7 layers for CIFAR-10 and Fashion MNIST, respectively, which are taken from Keras' tutorials and examples (cif, 2021; mni, 2021).", "mention_start": 22, "mention_end": 39, "dataset_mention": "generic datasets"}, {"mentioned_in_paper": "289", "context_id": "213", "dataset_context": "Table 1 shows a summary of the models used for Fashion MNIST and CIFAR-10 datasets.", "mention_start": 47, "mention_end": 82, "dataset_mention": "Fashion MNIST and CIFAR-10 datasets"}, {"mentioned_in_paper": "289", "context_id": "214", "dataset_context": "For the ADNI dataset, we use a transfer learning (TL) approach using the VGG16 model (Simonyan and Zisserman, 2015) pre-trained on ImageNet (Russakovsky et al., 2015).", "mention_start": 4, "mention_end": 20, "dataset_mention": "the ADNI dataset"}, {"mentioned_in_paper": "289", "context_id": "251", "dataset_context": "For the RQ3, since we have six centers in the ADNI dataset and a cross-silo setting, we select all clients at each round for training.", "mention_start": 41, "mention_end": 58, "dataset_mention": "the ADNI dataset"}, {"mentioned_in_paper": "289", "context_id": "253", "dataset_context": "In RQ4, we conduct experiments on the CIFAR-10 (with the non-iid degree of 0.4) and ADNI datasets to compare our proposed aggregator with existing ones.", "mention_start": 74, "mention_end": 97, "dataset_mention": "0.4) and ADNI datasets"}, {"mentioned_in_paper": "289", "context_id": "296", "dataset_context": "The results for the CIFAR-10 and Fashion MNIST datasets are reported in Figures 6 and  7, respectively.", "mention_start": 16, "mention_end": 55, "dataset_mention": "the CIFAR-10 and Fashion MNIST datasets"}, {"mentioned_in_paper": "289", "context_id": "319", "dataset_context": "The same can be seen from Figures 8d and  8f for the Fashion MNIST dataset, and the model is essentially guessing randomly.", "mention_start": 48, "mention_end": 74, "dataset_mention": "the Fashion MNIST dataset"}, {"mentioned_in_paper": "289", "context_id": "325", "dataset_context": "Regarding the Backdoor attack, we showed how it is applied in the background section in Figure 2. In the CIFAR-10 dataset, images will be misclassified as a car with that specific pixel pattern.", "mention_start": 100, "mention_end": 121, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "289", "context_id": "326", "dataset_context": "Also, in the Fashion MNIST dataset, they will be misclassified as a trouser.", "mention_start": 8, "mention_end": 34, "dataset_mention": "the Fashion MNIST dataset"}, {"mentioned_in_paper": "289", "context_id": "337", "dataset_context": "Also, in Figure 9a for the CIFAR-10 dataset, we see that in the tiniest proportion, a more non-iid distribution increases the backdoor accuracy and makes it more powerful.", "mention_start": 22, "mention_end": 43, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "289", "context_id": "338", "dataset_context": "This is not obvious in higher proportions as the backdoor reaches the highest feasible (considering the attack method and dataset) accuracy with an iid distribution.", "mention_start": 45, "mention_end": 129, "dataset_mention": "the backdoor reaches the highest feasible (considering the attack method and dataset"}, {"mentioned_in_paper": "289", "context_id": "340", "dataset_context": "For the same reason, non-iid does not change the backdoor accuracy for the Fashion MNIST dataset as the backdoor accuracy is already 100%.", "mention_start": 70, "mention_end": 96, "dataset_mention": "the Fashion MNIST dataset"}, {"mentioned_in_paper": "289", "context_id": "341", "dataset_context": "Answer to RQ1: In generic image datasets (e.g., CIFAR-10 and Fashion MNIST), Federated Averaging is not robust against any attacks, and the FL process faces quality issues, as shown by the final model accuracy Also, generally, model attacks are more powerful than data attacks.", "mention_start": 17, "mention_end": 40, "dataset_mention": "generic image datasets"}, {"mentioned_in_paper": "289", "context_id": "358", "dataset_context": "Figure 13a shows similar patterns on the Fashion MNIST dataset in an iid case.", "mention_start": 37, "mention_end": 62, "dataset_mention": "the Fashion MNIST dataset"}, {"mentioned_in_paper": "289", "context_id": "361", "dataset_context": "Looking at Figures 12c and 12e for the CIFAR-10 dataset, as the non-iid degree increases, Federated Averaging outperforms other techniques in almost all the cases, and its advantage is more noticeable, with 30% of malicious clients.", "mention_start": 35, "mention_end": 55, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "289", "context_id": "366", "dataset_context": "Looking at the untargeted model poisoning attacks (Random Update and Sign Flip) for the CIFAR-10 dataset.", "mention_start": 84, "mention_end": 104, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "289", "context_id": "373", "dataset_context": "The observations are a bit different on the Fashion MNSIT dataset.", "mention_start": 40, "mention_end": 65, "dataset_mention": "the Fashion MNSIT dataset"}, {"mentioned_in_paper": "289", "context_id": "390", "dataset_context": "As it can be seen, Krum achieves the best accuracy on average and achieves the top rank the most in the CIFAR-10 dataset, so all in all, it is the most robust aggregation method for that dataset.", "mention_start": 99, "mention_end": 120, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "289", "context_id": "412", "dataset_context": "However, there is a significant difference in the ADNI dataset for Krum.", "mention_start": 45, "mention_end": 62, "dataset_mention": "the ADNI dataset"}, {"mentioned_in_paper": "289", "context_id": "418", "dataset_context": "To illustrate this better, Figure 16 shows Krum's test accuracy convergence trend over the communication rounds for ADNI and CIFAR-10 datasets on worst and best runs.", "mention_start": 115, "mention_end": 142, "dataset_mention": "ADNI and CIFAR-10 datasets"}, {"mentioned_in_paper": "289", "context_id": "426", "dataset_context": "To see how aggregation methods work here and what the differences are compared to generic image datasets, we report the mutator results in Figure 17.", "mention_start": 82, "mention_end": 104, "dataset_mention": "generic image datasets"}, {"mentioned_in_paper": "289", "context_id": "429", "dataset_context": "However, interestingly, in the ADNI dataset, the Overlap mutator has a significant impact on Federated Averaging.", "mention_start": 26, "mention_end": 43, "dataset_mention": "the ADNI dataset"}, {"mentioned_in_paper": "289", "context_id": "434", "dataset_context": "Krum is also more robust than Federated Averaging, but because of its problems with the ADNI dataset almost always performs worse than Median and Trimmed Mean.", "mention_start": 83, "mention_end": 100, "dataset_mention": "the ADNI dataset"}, {"mentioned_in_paper": "289", "context_id": "436", "dataset_context": "Effect of attacks on ADNI dataset: According to Figure 18a, in the Label Flip attack, we see that in a small proportion of affected clients, all aggregators perform very similarly except Krum, which is worse, much like the previous RQ.", "mention_start": 21, "mention_end": 33, "dataset_mention": "ADNI dataset"}, {"mentioned_in_paper": "289", "context_id": "454", "dataset_context": "Note that Krum's backdoor task accuracy in this dataset is much higher than in generic datasets used in RQ2 (more than 50% in this question compared to less than 10% in RQ2).", "mention_start": 79, "mention_end": 95, "dataset_mention": "generic datasets"}, {"mentioned_in_paper": "289", "context_id": "484", "dataset_context": "and ADNI datasets and test the new method on Label Flip and Sign Flip attacks.", "mention_start": 0, "mention_end": 17, "dataset_mention": "and ADNI datasets"}, {"mentioned_in_paper": "289", "context_id": "487", "dataset_context": "Given that the Fashion MNIST dataset is less challenging than CIFAR-10, based on RQ1-2 results, we only focus on these two datasets for the sake of space.", "mention_start": 6, "mention_end": 36, "dataset_mention": "that the Fashion MNIST dataset"}, {"mentioned_in_paper": "289", "context_id": "516", "dataset_context": "Since in a real case, essential factors like the dataset and attack type are unknown to the server (consequently the aggregator), i.e., there is no way to know which aggregator would work best.", "mention_start": 21, "mention_end": 56, "dataset_mention": " essential factors like the dataset"}, {"mentioned_in_paper": "289", "context_id": "544", "dataset_context": "To mitigate this problem, regarding datasets, we used well-known generic Fashion MNIST and CIFAR-10 datasets and an application-specific ADNI dataset, which is a real-world federated dataset, to improve the generalizability of the findings.", "mention_start": 53, "mention_end": 108, "dataset_mention": "well-known generic Fashion MNIST and CIFAR-10 datasets"}, {"mentioned_in_paper": "289", "context_id": "544", "dataset_context": "To mitigate this problem, regarding datasets, we used well-known generic Fashion MNIST and CIFAR-10 datasets and an application-specific ADNI dataset, which is a real-world federated dataset, to improve the generalizability of the findings.", "mention_start": 53, "mention_end": 149, "dataset_mention": "well-known generic Fashion MNIST and CIFAR-10 datasets and an application-specific ADNI dataset"}, {"mentioned_in_paper": "289", "context_id": "544", "dataset_context": "To mitigate this problem, regarding datasets, we used well-known generic Fashion MNIST and CIFAR-10 datasets and an application-specific ADNI dataset, which is a real-world federated dataset, to improve the generalizability of the findings.", "mention_start": 159, "mention_end": 190, "dataset_mention": "a real-world federated dataset"}, {"mentioned_in_paper": "289", "context_id": "545", "dataset_context": "In terms of the models, although like many related works (Li et al., 2019 (Li et al., , 2021) ) and to make the systematic experiments with all combinations manageable, we used only one model per dataset, we used a diverse set of models across datasets (from simple CNNs to more complex VGG16).", "mention_start": 181, "mention_end": 203, "dataset_mention": "one model per dataset"}, {"mentioned_in_paper": "289", "context_id": "603", "dataset_context": "Lastly, they only used naturally centralized datasets in a cross-device setting; in contrast, we also did a comprehensive study on a federated dataset and cross-silo FL setting.", "mention_start": 22, "mention_end": 53, "dataset_mention": "naturally centralized datasets"}, {"mentioned_in_paper": "289", "context_id": "603", "dataset_context": "Lastly, they only used naturally centralized datasets in a cross-device setting; in contrast, we also did a comprehensive study on a federated dataset and cross-silo FL setting.", "mention_start": 130, "mention_end": 150, "dataset_mention": "a federated dataset"}, {"mentioned_in_paper": "289", "context_id": "605", "dataset_context": "They introduced a targeted model poisoning attack and showed its effectiveness against Federated Averaging on the Fashion MNIST dataset.", "mention_start": 110, "mention_end": 135, "dataset_mention": "the Fashion MNIST dataset"}, {"mentioned_in_paper": "289", "context_id": "616", "dataset_context": "We performed our experiments on two generic image datasets, each with three different distributions, one federated medical dataset, eight In addition, our study shows that there is no single best robust aggregator, and their accuracy depends on factors such as attack type, dataset, and data distribution.", "mention_start": 32, "mention_end": 58, "dataset_mention": "two generic image datasets"}, {"mentioned_in_paper": "289", "context_id": "616", "dataset_context": "We performed our experiments on two generic image datasets, each with three different distributions, one federated medical dataset, eight In addition, our study shows that there is no single best robust aggregator, and their accuracy depends on factors such as attack type, dataset, and data distribution.", "mention_start": 100, "mention_end": 130, "dataset_mention": " one federated medical dataset"}, {"mentioned_in_paper": "290", "context_id": "68", "dataset_context": "It would be possible to use existing RDF datasets as prototypes, but it would result in a KB which does not use the inheritance feature specific to prototypes.", "mention_start": 28, "mention_end": 49, "dataset_mention": "existing RDF datasets"}, {"mentioned_in_paper": "290", "context_id": "82", "dataset_context": "Hence, we will call these baseline data sets.", "mention_start": 6, "mention_end": 44, "dataset_mention": " we will call these baseline data sets"}, {"mentioned_in_paper": "290", "context_id": "100", "dataset_context": "For the fixpoint, the baseline dataset provides close to linear performance, which is expected.", "mention_start": 17, "mention_end": 38, "dataset_mention": " the baseline dataset"}, {"mentioned_in_paper": "290", "context_id": "161", "dataset_context": "In this benchmark we use the blocks(30) data set as presented in section 2.3 and perform benchmarks in two environments.", "mention_start": 25, "mention_end": 48, "dataset_mention": "the blocks(30) data set"}, {"mentioned_in_paper": "294", "context_id": "2", "dataset_context": "To investigate this, we employ multilevel models on a real-world driving dataset consisting of 10,139 sequences.", "mention_start": 51, "mention_end": 80, "dataset_mention": "a real-world driving dataset"}, {"mentioned_in_paper": "294", "context_id": "20", "dataset_context": "Therefore, we employ multilevel modeling on a real-world driving dataset consisting of 10,139 user interaction sequences and the accompanying driving and eye tracking data.", "mention_start": 43, "mention_end": 72, "dataset_mention": "a real-world driving dataset"}, {"mentioned_in_paper": "294", "context_id": "121", "dataset_context": "The final dataset contains 10,139 sequences of which 8,655 are manual driving, 770 are ACC driving, and 714 are ACC+LCA driving 1 . 1 The dataset statistics are given here: https://doi.org/10.6084/m9.figshare.19668918.v1", "mention_start": 111, "mention_end": 145, "dataset_mention": "ACC+LCA driving 1 . 1 The dataset"}, {"mentioned_in_paper": "296", "context_id": "143", "dataset_context": "(However, data limitations-see the analysis of Section 4-mean that we use a single, extremely long, residual energy dataset of 324,360 hourly observations both to estimate the parameters \u03bb i and to examine the effectiveness of the resulting policies.", "mention_start": 99, "mention_end": 123, "dataset_mention": " residual energy dataset"}, {"mentioned_in_paper": "298", "context_id": "8", "dataset_context": "On two OAD benchmark datasets, THUMOS-14 and TVSeries, our IEN outperforms stateof-the-art OAD methods using only RGB frames.", "mention_start": 3, "mention_end": 29, "dataset_mention": "two OAD benchmark datasets"}, {"mentioned_in_paper": "298", "context_id": "9", "dataset_context": "Furthermore, on the THUMOS-14 dataset, our IEN outperforms the state-ofthe-art OAD methods using two-stream features based on RGB frames and optical flows.", "mention_start": 15, "mention_end": 37, "dataset_mention": "the THUMOS-14 dataset"}, {"mentioned_in_paper": "298", "context_id": "45", "dataset_context": "To show the effectiveness and efficiency of our IEN consisting of IEUs, we conduct experiments using two OAD benchmark datasets, THUMOS-14 and TVSeries.", "mention_start": 100, "mention_end": 127, "dataset_mention": "two OAD benchmark datasets"}, {"mentioned_in_paper": "298", "context_id": "51", "dataset_context": "\u2022 For the THUMOS-14 and TVSeries datasets, the proposed IEN achieves the best OAD performance compared to state-of-the-art OAD methods using visual features from only RGB frames.", "mention_start": 6, "mention_end": 41, "dataset_mention": "the THUMOS-14 and TVSeries datasets"}, {"mentioned_in_paper": "298", "context_id": "52", "dataset_context": "In addition, on the THUMOS-14 dataset, our IEN outperforms the state-of-the-art OAD methods using visual features from RGB frames as well as optical flows.", "mention_start": 15, "mention_end": 37, "dataset_mention": "the THUMOS-14 dataset"}, {"mentioned_in_paper": "298", "context_id": "139", "dataset_context": "We conducted experiments with two OAD benchmark datasets, THUMOS-14 and TVSeries.", "mention_start": 30, "mention_end": 56, "dataset_mention": "two OAD benchmark datasets"}, {"mentioned_in_paper": "298", "context_id": "142", "dataset_context": "Third, we compared the performances of state-of-the-art methods to our IEN on both the OAD datasets.", "mention_start": 77, "mention_end": 99, "dataset_mention": "both the OAD datasets"}, {"mentioned_in_paper": "298", "context_id": "144", "dataset_context": "A. Datasets 1) THUMOS-14: THUMOS-14 [22] is a dataset initially publicized for a competition for offline action detection and localization.", "mention_start": 0, "mention_end": 11, "dataset_mention": "A. Datasets"}, {"mentioned_in_paper": "298", "context_id": "149", "dataset_context": "2) TVSeries: TVSeries [9] is a realistic dataset consisting of 27 episodes from six famous TV series.", "mention_start": 28, "mention_end": 48, "dataset_mention": "a realistic dataset"}, {"mentioned_in_paper": "298", "context_id": "161", "dataset_context": "2) Mean calibrated average precision: For the TVSeries dataset, the per-frame mean calibrated average precision (mcAP) is used as a performance evaluation metric.", "mention_start": 41, "mention_end": 62, "dataset_mention": "the TVSeries dataset"}, {"mentioned_in_paper": "298", "context_id": "179", "dataset_context": "For RGB features, our IEN use TSM [14] with only RGB frames, which was pretrained using the Kinetics dataset [12].", "mention_start": 87, "mention_end": 108, "dataset_mention": "the Kinetics dataset"}, {"mentioned_in_paper": "298", "context_id": "228", "dataset_context": "On two OAD benchmark datasets, THUMOS-14 [22] and TVSeries [9], IEN achieves a per-frame mAP of 60.4% and 81.4%, respectively.", "mention_start": 3, "mention_end": 29, "dataset_mention": "two OAD benchmark datasets"}, {"mentioned_in_paper": "299", "context_id": "119", "dataset_context": "There is an unweighted and undirected link between user i and object \u03b2, if the rating value from user i to object \u03b2 is no less than 3 (both datasets with a 5-point rating scale from 1 to 5).", "mention_start": 131, "mention_end": 148, "dataset_mention": "3 (both datasets"}, {"mentioned_in_paper": "301", "context_id": "9", "dataset_context": "terms, and we introduce the Semantic Trails Datasets (STDs) which are two different datasets of semantically annotated trails created starting from check-ins performed on the Foursquare social network.", "mention_start": 23, "mention_end": 52, "dataset_mention": "the Semantic Trails Datasets"}, {"mentioned_in_paper": "301", "context_id": "11", "dataset_context": "Furthermore, we enriched the datasets by adding valuable semantic information, that is the Schema.org", "mention_start": 12, "mention_end": 37, "dataset_mention": " we enriched the datasets"}, {"mentioned_in_paper": "301", "context_id": "17", "dataset_context": "In the following, we distinguish among works related to data-driven studies (Section 2.1), next POI recommendation (Section 2.2), and check-in datasets (Section 2.3).", "mention_start": 129, "mention_end": 151, "dataset_mention": " and check-in datasets"}, {"mentioned_in_paper": "301", "context_id": "20", "dataset_context": "Li et al. [4] performed a statistical study with the aim of unraveling the correlations among venue categories and their popularity using a large check-ins dataset with 2.4 million venues collected from different geographical regions.", "mention_start": 138, "mention_end": 163, "dataset_mention": "a large check-ins dataset"}, {"mentioned_in_paper": "301", "context_id": "29", "dataset_context": "Some check-in datasets collected from LBSNs are already publicly available.", "mention_start": 0, "mention_end": 22, "dataset_mention": "Some check-in datasets"}, {"mentioned_in_paper": "301", "context_id": "30", "dataset_context": "The NYC Restaurant Rich Dataset [16] includes check-ins of restaurant venues in New York City only, as well as tip and tag data collected from Foursquare from October 2011 to February 2012.", "mention_start": 0, "mention_end": 31, "dataset_mention": "The NYC Restaurant Rich Dataset"}, {"mentioned_in_paper": "301", "context_id": "31", "dataset_context": "The NYC and Tokyo Check-in Dataset [17] contains check-ins performed in New York City and Tokyo collected from April 2012 to February 2013, together with their timestamp, GPS coordinates and venue category.", "mention_start": 0, "mention_end": 34, "dataset_mention": "The NYC and Tokyo Check-in Dataset"}, {"mentioned_in_paper": "301", "context_id": "32", "dataset_context": "The Global-Scale Check-in Dataset (GSCD) [15] includes long-term check-in data collected from April 2012 to September 2013 only considering the 415 most popular cities on Foursquare.", "mention_start": 0, "mention_end": 33, "dataset_mention": "The Global-Scale Check-in Dataset"}, {"mentioned_in_paper": "301", "context_id": "36", "dataset_context": "In this section, we detail the process that we followed for building the Semantic Trails Datasets (STDs) from the collections of check-ins at our disposal, that will be described in Section 4. The exploited algorithm is publicly available in our GitHub repository, 3 while the resulting datasets have been published on figshare [6].", "mention_start": 68, "mention_end": 97, "dataset_mention": "the Semantic Trails Datasets"}, {"mentioned_in_paper": "301", "context_id": "36", "dataset_context": "In this section, we detail the process that we followed for building the Semantic Trails Datasets (STDs) from the collections of check-ins at our disposal, that will be described in Section 4. The exploited algorithm is publicly available in our GitHub repository, 3 while the resulting datasets have been published on figshare [6].", "mention_start": 272, "mention_end": 295, "dataset_mention": "the resulting datasets"}, {"mentioned_in_paper": "301", "context_id": "48", "dataset_context": "In order to construct the semantic trails from the initial datasets, we processed the check-ins and we analyzed their timestamps, for obtaining an unambiguous time representation that also includes the time zone.", "mention_start": 47, "mention_end": 67, "dataset_mention": "the initial datasets"}, {"mentioned_in_paper": "301", "context_id": "68", "dataset_context": "In order to enrich the available datasets, we identified the city where each venue is probably located by performing the reverse geocoding of its coordinates.", "mention_start": 12, "mention_end": 41, "dataset_mention": "enrich the available datasets"}, {"mentioned_in_paper": "301", "context_id": "100", "dataset_context": "The first one is the Global-Scale Check-in Dataset (GSCD), created by the authors of [15] and publicly available on the Web.", "mention_start": 17, "mention_end": 50, "dataset_mention": "the Global-Scale Check-in Dataset"}, {"mentioned_in_paper": "301", "context_id": "107", "dataset_context": "We constructed two different versions of the STDs by applying the procedure described in Section 3 to these initial datasets.", "mention_start": 102, "mention_end": 124, "dataset_mention": "these initial datasets"}, {"mentioned_in_paper": "301", "context_id": "111", "dataset_context": "This result is associated with the different collection protocols of the GSCD and our initial dataset.", "mention_start": 69, "mention_end": 101, "dataset_mention": "the GSCD and our initial dataset"}, {"mentioned_in_paper": "301", "context_id": "116", "dataset_context": "The differences in the number of trails and venues are consistent with the size of the initial dataset.", "mention_start": 83, "mention_end": 102, "dataset_mention": "the initial dataset"}, {"mentioned_in_paper": "301", "context_id": "142", "dataset_context": "In order to demonstrate the usefulness of a semantically annotated dataset, we computed additional statistics by also relying on external information obtained from GeoNames.", "mention_start": 42, "mention_end": 74, "dataset_mention": "a semantically annotated dataset"}, {"mentioned_in_paper": "301", "context_id": "170", "dataset_context": "We characterized the two datasets by analyzing them considering different dimensions and we demonstrated the usefulness of semantically annotated data by relying on external information to compute additional statistics.", "mention_start": 0, "mention_end": 33, "dataset_mention": "We characterized the two datasets"}, {"mentioned_in_paper": "302", "context_id": "5", "dataset_context": "Experimental results on three egocentric interaction datasets show the effectiveness of our method and advantage over state-of-the-arts.", "mention_start": 24, "mention_end": 61, "dataset_mention": "three egocentric interaction datasets"}, {"mentioned_in_paper": "302", "context_id": "10", "dataset_context": "(b) shows adjacent frames with obvious egomotion in an egocentric video from UTokyo PEV dataset [39].", "mention_start": 77, "mention_end": 95, "dataset_mention": "UTokyo PEV dataset"}, {"mentioned_in_paper": "302", "context_id": "116", "dataset_context": "We evaluate our method on three egocentric humanhuman interaction datasets.", "mention_start": 26, "mention_end": 74, "dataset_mention": "three egocentric humanhuman interaction datasets"}, {"mentioned_in_paper": "302", "context_id": "117", "dataset_context": "UTokyo Paired Ego-Video (PEV) Dataset contains 1226 paired egocentric videos recording dyadic human-human interactions [39].", "mention_start": 0, "mention_end": 37, "dataset_mention": "UTokyo Paired Ego-Video (PEV) Dataset"}, {"mentioned_in_paper": "302", "context_id": "120", "dataset_context": "NUS First-person Interaction Dataset contains 152 firstperson videos and 133 third-person videos of both humanhuman and human-object interactions [25].", "mention_start": 0, "mention_end": 36, "dataset_mention": "NUS First-person Interaction Dataset"}, {"mentioned_in_paper": "302", "context_id": "125", "dataset_context": "JPL First-Person Interaction Dataset consists of 84 videos of humans interacting with a humanoid model with a camera mounted on its head [31].", "mention_start": 0, "mention_end": 36, "dataset_mention": "JPL First-Person Interaction Dataset"}, {"mentioned_in_paper": "302", "context_id": "149", "dataset_context": "The third part reports the results of our method and the fourth part compares the performance using multiple POV videos on PEV dataset.", "mention_start": 123, "mention_end": 134, "dataset_mention": "PEV dataset"}, {"mentioned_in_paper": "302", "context_id": "155", "dataset_context": "On PEV dataset, Yonetani et al. [39] achieves 69.2% of accuracy with paired videos, certainly surpassing others using single POV video.", "mention_start": 3, "mention_end": 14, "dataset_mention": "PEV dataset"}, {"mentioned_in_paper": "302", "context_id": "164", "dataset_context": "We obtain clearer improvements on PEV dataset since it contains more samples dependent on relations.", "mention_start": 34, "mention_end": 45, "dataset_mention": "PEV dataset"}, {"mentioned_in_paper": "302", "context_id": "165", "dataset_context": "While in NUS(first h-h) dataset, most samples have weaker relations between the two interacting persons.", "mention_start": 9, "mention_end": 31, "dataset_mention": "NUS(first h-h) dataset"}, {"mentioned_in_paper": "302", "context_id": "181", "dataset_context": "Without attention module, our framework could only capture the global appearance and motion cues, and fails to model the relations between the camera wearer and the interactor, which leads to 9.0% and 12.3% accuracy degradation on PEV and NUS(first h-h) dataset, demonstrating the importance of attention module.", "mention_start": 230, "mention_end": 261, "dataset_mention": "PEV and NUS(first h-h) dataset"}, {"mentioned_in_paper": "302", "context_id": "186", "dataset_context": "On different datasets, global motion and local motion contribute differently to recognition, probably because global motion is important to distinguish interactions such as positive and negtive response on PEV dataset, but such interactions highly relevant to global motion are not included in NUS(first h-h) dataset.", "mention_start": 205, "mention_end": 217, "dataset_mention": "PEV dataset"}, {"mentioned_in_paper": "302", "context_id": "186", "dataset_context": "On different datasets, global motion and local motion contribute differently to recognition, probably because global motion is important to distinguish interactions such as positive and negtive response on PEV dataset, but such interactions highly relevant to global motion are not included in NUS(first h-h) dataset.", "mention_start": 293, "mention_end": 316, "dataset_mention": "NUS(first h-h) dataset"}, {"mentioned_in_paper": "302", "context_id": "193", "dataset_context": "Without the motion module, our method fails to capture motion information and can only use appearance features, which leads to 18.3% and 12.0% accuracy drop on PEV and NUS(first h-h) dataset, showing the necessity of motion modeling.", "mention_start": 159, "mention_end": 190, "dataset_mention": "PEV and NUS(first h-h) dataset"}, {"mentioned_in_paper": "304", "context_id": "30", "dataset_context": "These approaches are used to assess the similarity of a new dataset to previously analysed datasets [10].", "mention_start": 71, "mention_end": 99, "dataset_mention": "previously analysed datasets"}, {"mentioned_in_paper": "304", "context_id": "37", "dataset_context": "Although its knowledge-base was built through expert-driven knowledge engineering rather than via MLL, it still stood out as the first automatic tool that systematically relates application domain and dataset characteristics.", "mention_start": 124, "mention_end": 208, "dataset_mention": "the first automatic tool that systematically relates application domain and dataset"}, {"mentioned_in_paper": "304", "context_id": "107", "dataset_context": "The first section is discussing ways of gathering real-world datasets and techniques to create synthetic datasets which are known as EoD.", "mention_start": 40, "mention_end": 69, "dataset_mention": "gathering real-world datasets"}, {"mentioned_in_paper": "304", "context_id": "109", "dataset_context": "MF are combined with performance measures to build MK dataset which becomes the input of MLL.", "mention_start": 51, "mention_end": 61, "dataset_mention": "MK dataset"}, {"mentioned_in_paper": "304", "context_id": "116", "dataset_context": "In the following subsections, studies that are dealing with the real-world data, those which elaborate the techniques to generate artificial datasets, and published resources are discussed.", "mention_start": 129, "mention_end": 149, "dataset_mention": "artificial datasets"}, {"mentioned_in_paper": "304", "context_id": "119", "dataset_context": "Table 1 presents datasets that are used in different researches for MLL purpose.", "mention_start": 0, "mention_end": 25, "dataset_mention": "Table 1 presents datasets"}, {"mentioned_in_paper": "304", "context_id": "126", "dataset_context": "[78] used 320 artificially generated boolean datasets with 5 to 12 features in each one.", "mention_start": 37, "mention_end": 53, "dataset_mention": "boolean datasets"}, {"mentioned_in_paper": "304", "context_id": "127", "dataset_context": "These artificial datasets were benchmarked on 16 UCI and DaimlerChrysler real-world datasets.", "mention_start": 0, "mention_end": 25, "dataset_mention": "These artificial datasets"}, {"mentioned_in_paper": "304", "context_id": "127", "dataset_context": "These artificial datasets were benchmarked on 16 UCI and DaimlerChrysler real-world datasets.", "mention_start": 46, "mention_end": 92, "dataset_mention": "16 UCI and DaimlerChrysler real-world datasets"}, {"mentioned_in_paper": "304", "context_id": "128", "dataset_context": "Similarly [11] generated 222 datasets, each containing 20 numeric and nominal features having 1K to 10K instances classified between 2 to 5 classes.", "mention_start": 25, "mention_end": 37, "dataset_mention": "222 datasets"}, {"mentioned_in_paper": "304", "context_id": "130", "dataset_context": "[79] proposed a method to generate a large number of datasets by transforming the existing datasets, known as datasetoids.", "mention_start": 65, "mention_end": 99, "dataset_mention": "transforming the existing datasets"}, {"mentioned_in_paper": "304", "context_id": "131", "dataset_context": "An artificial dataset was generated against each symbolic attribute of a given dataset, obtained by switching the selected attribute with the target variable.", "mention_start": 0, "mention_end": 21, "dataset_mention": "An artificial dataset"}, {"mentioned_in_paper": "304", "context_id": "132", "dataset_context": "This method was used on 64 datasets gathered from the UCI repository and it generated a total of 983 datasetoids.", "mention_start": 97, "mention_end": 108, "dataset_mention": "983 dataset"}, {"mentioned_in_paper": "304", "context_id": "133", "dataset_context": "At the end potential anomalies related to the artificial datasets were also discussed as well as their proposed solutions were presented.", "mention_start": 42, "mention_end": 65, "dataset_mention": "the artificial datasets"}, {"mentioned_in_paper": "304", "context_id": "135", "dataset_context": "One very simple solution proposed for these problems was to simply discard the datasetoids which showed any of the above mentioned properties.", "mention_start": 60, "mention_end": 86, "dataset_mention": "simply discard the dataset"}, {"mentioned_in_paper": "304", "context_id": "136", "dataset_context": "This method produced promising results, therefore enabling the generation of new datasets that could solve the scarce datasets problems.", "mention_start": 106, "mention_end": 126, "dataset_mention": "the scarce datasets"}, {"mentioned_in_paper": "304", "context_id": "139", "dataset_context": "A total of 264 artificial datasets were generated to exhibit a number of different characteristics including, for instance, perfect and strong trend, perfect seasonality, or certain type and level of noise.", "mention_start": 11, "mention_end": 34, "dataset_mention": "264 artificial datasets"}, {"mentioned_in_paper": "304", "context_id": "141", "dataset_context": "[80] generated 160 artificial datasets to obtain a wide range of cluster structures.", "mention_start": 15, "mention_end": 38, "dataset_mention": "160 artificial datasets"}, {"mentioned_in_paper": "304", "context_id": "144", "dataset_context": "For each of the 8 combinations of cluster number and dimension, 10 different instances were generated, giving 80 datasets in each method.", "mention_start": 102, "mention_end": 121, "dataset_mention": " giving 80 datasets"}, {"mentioned_in_paper": "304", "context_id": "148", "dataset_context": "[81] presented a novel data generator approach for numerical features and classification datasets that could be used as input datasets for MLL which represented an entirely different approach from [79].", "mention_start": 51, "mention_end": 97, "dataset_mention": "numerical features and classification datasets"}, {"mentioned_in_paper": "304", "context_id": "161", "dataset_context": "ML researchers can share datasets, algorithms, their configurations, and experiment setups on this platform which other researchers can use to compare results.", "mention_start": 0, "mention_end": 33, "dataset_mention": "ML researchers can share datasets"}, {"mentioned_in_paper": "304", "context_id": "164", "dataset_context": "Similarly, at the Meta-level, the MK dataset is used as a training-set of MLL, and the quality of this MK dataset is dependent on sufficient number and quality of EoD from different domains.", "mention_start": 29, "mention_end": 44, "dataset_mention": " the MK dataset"}, {"mentioned_in_paper": "304", "context_id": "164", "dataset_context": "Similarly, at the Meta-level, the MK dataset is used as a training-set of MLL, and the quality of this MK dataset is dependent on sufficient number and quality of EoD from different domains.", "mention_start": 97, "mention_end": 113, "dataset_mention": "this MK dataset"}, {"mentioned_in_paper": "304", "context_id": "165", "dataset_context": "These EoD are used to generate MFs which act as predictors and the estimated predictive performance evaluated ML algorithms for these EoD are used as the target variable in the MK dataset.", "mention_start": 173, "mention_end": 187, "dataset_mention": "the MK dataset"}, {"mentioned_in_paper": "304", "context_id": "168", "dataset_context": "Most of the studies gathered datasets from the UCI with different filtering options and the remaining few studies gathered datasets from different data-mining competitions.", "mention_start": 8, "mention_end": 37, "dataset_mention": "the studies gathered datasets"}, {"mentioned_in_paper": "304", "context_id": "168", "dataset_context": "Most of the studies gathered datasets from the UCI with different filtering options and the remaining few studies gathered datasets from different data-mining competitions.", "mention_start": 56, "mention_end": 131, "dataset_mention": "different filtering options and the remaining few studies gathered datasets"}, {"mentioned_in_paper": "304", "context_id": "171", "dataset_context": "Some MLL researches resolved the problem of the number and quality of available datasets by building their MK datasets using artificially generated EoD.", "mention_start": 101, "mention_end": 118, "dataset_mention": "their MK datasets"}, {"mentioned_in_paper": "304", "context_id": "172", "dataset_context": "They have adopted two different approaches to generate these synthetic datasets: 1) by transforming real-world datasets; and 2) by utilizing statistical and genetic programming approaches.", "mention_start": 86, "mention_end": 119, "dataset_mention": "transforming real-world datasets"}, {"mentioned_in_paper": "304", "context_id": "177", "dataset_context": "Combining all the proposed approaches iteratively could offer a potential solution to the dataset scarcity; i.e., initially gathering the existing available real-world problems, then transforming those datasets by generating several others and finally applying various other techniques to generate artificial datasets independently (see Figure 2).", "mention_start": 177, "mention_end": 210, "dataset_mention": " then transforming those datasets"}, {"mentioned_in_paper": "304", "context_id": "177", "dataset_context": "Combining all the proposed approaches iteratively could offer a potential solution to the dataset scarcity; i.e., initially gathering the existing available real-world problems, then transforming those datasets by generating several others and finally applying various other techniques to generate artificial datasets independently (see Figure 2).", "mention_start": 297, "mention_end": 317, "dataset_mention": "artificial datasets"}, {"mentioned_in_paper": "304", "context_id": "179", "dataset_context": "Considering all three necessary components of an MLL system, gathering datasets from published experimental evaluations and benchmarking of ML algorithms would seem to be more attractive, however, there are a lot of challenges with such data related to reporting only the best learning algorithms, publishing limited information of experimentations, availability of datasets used in the research, lack of detailed configurations of evaluated learning algorithms, etc. OpenML platform has attempted to address most of these issues focusing on the consistency and completeness of the gathered information but as it is in a preliminary stage of development it currently lacks a sufficiently large number of problems from different domains and sufficiently robust and comprehensive number of ML algorithms tested for each of the datasets to be very useful in its current form.", "mention_start": 60, "mention_end": 79, "dataset_mention": " gathering datasets"}, {"mentioned_in_paper": "304", "context_id": "182", "dataset_context": "As directly comparing large and complex datasets is normally infeasible, the similarity between different problems/datasets is carried out using a number of so called MFs offering a simplified representation of the problems/datasets.", "mention_start": 95, "mention_end": 123, "dataset_mention": "different problems/datasets"}, {"mentioned_in_paper": "304", "context_id": "182", "dataset_context": "As directly comparing large and complex datasets is normally infeasible, the similarity between different problems/datasets is carried out using a number of so called MFs offering a simplified representation of the problems/datasets.", "mention_start": 210, "mention_end": 232, "dataset_mention": "the problems/datasets"}, {"mentioned_in_paper": "304", "context_id": "203", "dataset_context": "[83] presented a novel approach of generating informative MFs by simply averaging overall attributes of the source datasets.", "mention_start": 104, "mention_end": 123, "dataset_mention": "the source datasets"}, {"mentioned_in_paper": "304", "context_id": "210", "dataset_context": "Another technique of MF generation is Landmarking which characterizes a dataset using the performance of a set of simple learners.", "mention_start": 56, "mention_end": 79, "dataset_mention": "characterizes a dataset"}, {"mentioned_in_paper": "304", "context_id": "225", "dataset_context": "The last experiment benchmarked 16 real-world datasets from the UCI [63] and the DaimlerChrysler where again the landmarking approach resulted in the best overall performance.", "mention_start": 0, "mention_end": 54, "dataset_mention": "The last experiment benchmarked 16 real-world datasets"}, {"mentioned_in_paper": "304", "context_id": "230", "dataset_context": "In addition to 3 landmarkers, 5 descriptive MFs (shown in the descriptive approach in Figure 3) have also been extracted from 216 datasets.", "mention_start": 125, "mention_end": 138, "dataset_mention": "216 datasets"}, {"mentioned_in_paper": "304", "context_id": "233", "dataset_context": "Here only C4.5 Decision Tree algorithm (C4.5) was used as a Meta-learner trained with 222 artificial boolean datasets and tested with 18 UCI problems [63].", "mention_start": 86, "mention_end": 117, "dataset_mention": "222 artificial boolean datasets"}, {"mentioned_in_paper": "304", "context_id": "249", "dataset_context": "MFs were constructed for the experiments from the UCI datasets (see Table 1) which contained up to 25% missing values.", "mention_start": 46, "mention_end": 62, "dataset_mention": "the UCI datasets"}, {"mentioned_in_paper": "304", "context_id": "270", "dataset_context": "[12] effort was towards improving the dataset characterization by capturing the structural shape and size of the decision tree induced from the dataset.", "mention_start": 0, "mention_end": 45, "dataset_mention": "[12] effort was towards improving the dataset"}, {"mentioned_in_paper": "304", "context_id": "277", "dataset_context": "The k-Nearest Neighbour algorithm, with various values of k between 1 to 40, was used to select k datasets for ranking the performance of learning algorithms.", "mention_start": 88, "mention_end": 106, "dataset_mention": "select k datasets"}, {"mentioned_in_paper": "304", "context_id": "323", "dataset_context": "[69] proposed an MLL based approach to rank candidate algorithms where k-NN was used to identify the datasets that were most similar to the query dataset.", "mention_start": 136, "mention_end": 153, "dataset_mention": "the query dataset"}, {"mentioned_in_paper": "304", "context_id": "335", "dataset_context": "The one-class performance evaluator computed each Base-classifier on only positively labelled instances using 4 algorithms including 1) global density estimation, 2) peer group analysis, 3) SVM, and 4) attribute distribution function approximation (ADIFA) on 53 distinct datasets (details can be seen in Table 1).", "mention_start": 258, "mention_end": 279, "dataset_mention": "53 distinct datasets"}, {"mentioned_in_paper": "304", "context_id": "342", "dataset_context": "Furthermore, most of them selected only the best algorithm from the pool to minimize the representation complexity of MK dataset, therefore very few of them stored information about the ranking and relative performance of evaluated BLLs.", "mention_start": 117, "mention_end": 128, "dataset_mention": "MK dataset"}, {"mentioned_in_paper": "304", "context_id": "367", "dataset_context": "Although its knowledge base had been built through an expert-driven knowledge engineering rather than via MLL it still stands out as the first automatic tool that systematically related application domain and dataset characteristics to the most suitable classification algorithms.", "mention_start": 133, "mention_end": 216, "dataset_mention": "the first automatic tool that systematically related application domain and dataset"}, {"mentioned_in_paper": "304", "context_id": "416", "dataset_context": "Furthermore, DSIT dataset characteristics from StatLog and DCT were combined to create an MK dataset consisting of 33 MFs.", "mention_start": 12, "mention_end": 25, "dataset_mention": " DSIT dataset"}, {"mentioned_in_paper": "304", "context_id": "416", "dataset_context": "Furthermore, DSIT dataset characteristics from StatLog and DCT were combined to create an MK dataset consisting of 33 MFs.", "mention_start": 86, "mention_end": 100, "dataset_mention": "an MK dataset"}, {"mentioned_in_paper": "304", "context_id": "483", "dataset_context": "In order to evaluate the framework, a case study using cancer gene expression microarray datasets was conducted.", "mention_start": 54, "mention_end": 97, "dataset_mention": "cancer gene expression microarray datasets"}, {"mentioned_in_paper": "304", "context_id": "495", "dataset_context": "However, there was no significant difference between the correlation values of MLP and SVR methods for both Meta-datasets.", "mention_start": 102, "mention_end": 121, "dataset_mention": "both Meta-datasets"}, {"mentioned_in_paper": "304", "context_id": "525", "dataset_context": "Additionally, these systems have the same issue which was discussed in the previous sections that the MK dataset did not have a sufficient number of Meta-examples (MEs).", "mention_start": 70, "mention_end": 112, "dataset_mention": "the previous sections that the MK dataset"}, {"mentioned_in_paper": "304", "context_id": "546", "dataset_context": "Additionally, from the results, it could be observed that the METAL(B) approach proved to be effective in domains (datasets) with high noise rates and several irrelevant attributes whereas the instance-based approach showed higher accuracy for the remaining domains.", "mention_start": 105, "mention_end": 123, "dataset_mention": "domains (datasets"}, {"mentioned_in_paper": "304", "context_id": "554", "dataset_context": "For the business news dataset, both adaptive techniques outperformed trivial non-adaptive approaches.", "mention_start": 4, "mention_end": 29, "dataset_mention": "the business news dataset"}, {"mentioned_in_paper": "304", "context_id": "555", "dataset_context": "Two evaluations were performed for the business cycle dataset where the data was split into 5 and 15 equally sized batches where the fixed size window approach performed slightly better than the adaptive techniques.", "mention_start": 35, "mention_end": 61, "dataset_mention": "the business cycle dataset"}, {"mentioned_in_paper": "304", "context_id": "608", "dataset_context": "The experimental results showed that for the TTP dataset the pair of regressors, regardless of the presence of the tie resolution strategy, outperformed the default and ensemble-based approaches.", "mention_start": 41, "mention_end": 56, "dataset_mention": "the TTP dataset"}, {"mentioned_in_paper": "304", "context_id": "610", "dataset_context": "Moreover, a slightly higher error rate was recorded for RF Meta-learner of the MetaStream than the default but was lower than the ensemble approach for the TTP dataset, whereas for the EDP dataset the MetaStream outperformed the default but was worse than the ensemble.", "mention_start": 151, "mention_end": 167, "dataset_mention": "the TTP dataset"}, {"mentioned_in_paper": "304", "context_id": "610", "dataset_context": "Moreover, a slightly higher error rate was recorded for RF Meta-learner of the MetaStream than the default but was lower than the ensemble approach for the TTP dataset, whereas for the EDP dataset the MetaStream outperformed the default but was worse than the ensemble.", "mention_start": 180, "mention_end": 196, "dataset_mention": "the EDP dataset"}, {"mentioned_in_paper": "304", "context_id": "655", "dataset_context": "1. Pre-processing Steps Recommendation: MLL can be applied to find the most appropriate combination of pre-processing steps for MK dataset.", "mention_start": 127, "mention_end": 138, "dataset_mention": "MK dataset"}, {"mentioned_in_paper": "304", "context_id": "656", "dataset_context": "As MLL is proposed for four different areas within a system which means in case a concept drift is detected a maximum of four MK datasets, which will be representing different problems, will require pre-processing.", "mention_start": 121, "mention_end": 137, "dataset_mention": "four MK datasets"}, {"mentioned_in_paper": "304", "context_id": "657", "dataset_context": "The applicability of MLL on changing environment requires dynamically growing MK dataset where a fixed set of pre-processing methods and techniques can be ineffective.", "mention_start": 58, "mention_end": 88, "dataset_mention": "dynamically growing MK dataset"}, {"mentioned_in_paper": "304", "context_id": "664", "dataset_context": "Using MLL the historical batches (concepts) of data can be extracted from MK dataset which can be used as effective data for training of the current concept.", "mention_start": 74, "mention_end": 84, "dataset_mention": "MK dataset"}, {"mentioned_in_paper": "304", "context_id": "665", "dataset_context": "This process can be named as Reverse Knowledge Extraction where MFs of the current concept can be used to extract the MEs of relevant concepts from MK datasets.", "mention_start": 148, "mention_end": 159, "dataset_mention": "MK datasets"}, {"mentioned_in_paper": "304", "context_id": "671", "dataset_context": "For instance, the MFs of incoming data can be computed as well as cumulated on the arrival of every batch and simultaneously compared with the set of MEs, from MK dataset, whose learning algorithm (used as target variable in MK) is used to score the current batches of data.", "mention_start": 159, "mention_end": 170, "dataset_mention": "MK dataset"}, {"mentioned_in_paper": "304", "context_id": "675", "dataset_context": "A MK dataset can be gathered containing the various parameters of the adaptive mechanism as MFs and mapped with the algorithm or combination that is performing the best for those parameters.", "mention_start": 0, "mention_end": 12, "dataset_mention": "A MK dataset"}, {"mentioned_in_paper": "304", "context_id": "676", "dataset_context": "Based on the currently selected algorithm the appropriate set of parameters can be extracted from the MK dataset.", "mention_start": 98, "mention_end": 112, "dataset_mention": "the MK dataset"}, {"mentioned_in_paper": "304", "context_id": "687", "dataset_context": "At Meta-level this impact would greatly effect because one ME in MK dataset will be extracted from one concept drift which might consist of several batches of data.", "mention_start": 65, "mention_end": 75, "dataset_mention": "MK dataset"}, {"mentioned_in_paper": "304", "context_id": "730", "dataset_context": "The review of the existing research has been structured into the coverage of five key components of an MLL system: (i) Available real and synthetic datasets for modelling at the Meta-level; (ii) Meta-features generation and selection approaches; (iii) Base-level learners as an input to the Meta-learning; (iv) Meta-learning; (v) Meta-learning based adaptive mechanisms for non-stationary environments.", "mention_start": 114, "mention_end": 156, "dataset_mention": " (i) Available real and synthetic datasets"}, {"mentioned_in_paper": "305", "context_id": "7", "dataset_context": "Our results on two egocentric video datasets show the method's promise relative to existing techniques for saliency and summarization.", "mention_start": 15, "mention_end": 44, "dataset_mention": "two egocentric video datasets"}, {"mentioned_in_paper": "305", "context_id": "56", "dataset_context": "In Section 4, we add new comparisons to multiple existing video summarization methods, analyze object prominence in the summaries, conduct new user studies with over 25 users to systematically gauge the summaries' quality, and produce new results on the Activities of Daily Living dataset [12].", "mention_start": 267, "mention_end": 288, "dataset_mention": "Daily Living dataset"}, {"mentioned_in_paper": "305", "context_id": "105", "dataset_context": "We call this the UT Egocentric (UT Ego) dataset.", "mention_start": 0, "mention_end": 47, "dataset_mention": "We call this the UT Egocentric (UT Ego) dataset"}, {"mentioned_in_paper": "305", "context_id": "190", "dataset_context": "While shot boundary detection has been frequently used to perform event segmentation for videos, it is impractical for our wearable camera data setting.", "mention_start": 118, "mention_end": 147, "dataset_mention": "our wearable camera data set"}, {"mentioned_in_paper": "305", "context_id": "254", "dataset_context": "In this section we evaluate our approach on our new UT Egocentric (UT Ego) dataset and on the Activities of Daily Living (ADL) dataset [12], which consists of 17 and 10 hours of egocentric video, respectively.", "mention_start": 44, "mention_end": 82, "dataset_mention": "our new UT Egocentric (UT Ego) dataset"}, {"mentioned_in_paper": "305", "context_id": "254", "dataset_context": "In this section we evaluate our approach on our new UT Egocentric (UT Ego) dataset and on the Activities of Daily Living (ADL) dataset [12], which consists of 17 and 10 hours of egocentric video, respectively.", "mention_start": 108, "mention_end": 134, "dataset_mention": "Daily Living (ADL) dataset"}, {"mentioned_in_paper": "305", "context_id": "257", "dataset_context": "For our UT Ego dataset, we collected 10 videos from four subjects, each 3-5 hours long. 1 Each person contributed one video, except one who contributed seven.", "mention_start": 4, "mention_end": 22, "dataset_mention": "our UT Ego dataset"}, {"mentioned_in_paper": "306", "context_id": "201", "dataset_context": "Trancos is a vehicle counting dataset which includes 1244 images with 41976 vehicles annotations.", "mention_start": 11, "mention_end": 37, "dataset_mention": "a vehicle counting dataset"}, {"mentioned_in_paper": "306", "context_id": "219", "dataset_context": "The Trancos dataset employs a different evaluation metric called Grid Average Mean absolute Error (GAME).", "mention_start": 0, "mention_end": 19, "dataset_mention": "The Trancos dataset"}, {"mentioned_in_paper": "306", "context_id": "318", "dataset_context": "In Table VIII, we report the performance of our three experts in the first level (E 1 \u223c E 3 ), groups in the second level (Gr 1 \u223c Gr 3 ) and final output E out on the JHU dataset.", "mention_start": 162, "mention_end": 178, "dataset_mention": "the JHU dataset"}, {"mentioned_in_paper": "307", "context_id": "7", "dataset_context": "Data itself is also scarce, as labelling datasets to train supervised models is a very laborious task.", "mention_start": 30, "mention_end": 49, "dataset_mention": "labelling datasets"}, {"mentioned_in_paper": "307", "context_id": "10", "dataset_context": "Experiments using a pre-trained model show that it is possible to achieve 89.75% accuracy in NER even with a small manually labeled dataset.", "mention_start": 107, "mention_end": 139, "dataset_mention": "a small manually labeled dataset"}, {"mentioned_in_paper": "307", "context_id": "37", "dataset_context": "Section 4 lists all the experiments done, including details of the small in-house labeled dataset used to train the NER model.", "mention_start": 62, "mention_end": 97, "dataset_mention": "the small in-house labeled dataset"}, {"mentioned_in_paper": "307", "context_id": "41", "dataset_context": "At Farfetch, we had the manual resources available to annotate a small NER dataset and so we decided to have the NER as a separate independent component.", "mention_start": 62, "mention_end": 82, "dataset_mention": "a small NER dataset"}, {"mentioned_in_paper": "307", "context_id": "70", "dataset_context": "However, the main contribution of [8] is one of the best known benchmarks in the entity disambiguation task: the AIDA CoNLL-YAGO -a new dataset based on CoNLL 2003, in which they manually annotated all proper nouns with corresponding entities in YAGO2.", "mention_start": 108, "mention_end": 143, "dataset_mention": " the AIDA CoNLL-YAGO -a new dataset"}, {"mentioned_in_paper": "307", "context_id": "75", "dataset_context": "They observe that adding an extra, task-specific, KG context improves the performance of Candidate Entity Disambiguation methods, leading to a new best performance for AIDA-CoNLL dataset.", "mention_start": 167, "mention_end": 186, "dataset_mention": "AIDA-CoNLL dataset"}, {"mentioned_in_paper": "307", "context_id": "80", "dataset_context": "Their model reaches SOTA results on the AIDA/CoNLL dataset (comparing with the other end-toend methods) and, when combined with Stanford NER, it generalizes well to other datasets with different characteristics.", "mention_start": 36, "mention_end": 58, "dataset_mention": "the AIDA/CoNLL dataset"}, {"mentioned_in_paper": "307", "context_id": "125", "dataset_context": "We take the weights from the pre-trained BERTbase-cased model [7] and fine-tune it on top of a small set of manually labeled NER dataset.", "mention_start": 108, "mention_end": 136, "dataset_mention": "manually labeled NER dataset"}, {"mentioned_in_paper": "307", "context_id": "172", "dataset_context": "Both experiments use a small in-house dataset composed of textual product descriptions from the fashion domain.", "mention_start": 21, "mention_end": 45, "dataset_mention": "a small in-house dataset"}, {"mentioned_in_paper": "307", "context_id": "176", "dataset_context": "The pre-trained model was trained on Wikipedia 5 and the BookCorpus 6 datasets using Masked Language Modeling and Next Sentence Prediction tasks [7].", "mention_start": 37, "mention_end": 78, "dataset_mention": "Wikipedia 5 and the BookCorpus 6 datasets"}, {"mentioned_in_paper": "307", "context_id": "184", "dataset_context": "Even though the dataset is small in size, we saw that having access to a pre-trained BERT-base-cased model provides us with the high accuracy necessary for the entity linking step.", "mention_start": 5, "mention_end": 23, "dataset_mention": "though the dataset"}, {"mentioned_in_paper": "307", "context_id": "245", "dataset_context": "Nevertheless, such an approach requires the construction of a new dedicated dataset.", "mention_start": 59, "mention_end": 83, "dataset_mention": "a new dedicated dataset"}, {"mentioned_in_paper": "309", "context_id": "91", "dataset_context": "Our experimental evaluation is much more comprehensive (CIFAR10, CIFAR100, and MNIST datasets with different C2F Nets).", "mention_start": 74, "mention_end": 93, "dataset_mention": " and MNIST datasets"}, {"mentioned_in_paper": "309", "context_id": "244", "dataset_context": "We employ the CIFAR10, CIFAR100 and MNIST [35] [36] image dataset with a 4:1:1 split for training (40000), validation (10000), and testing (10000) to train and test our C2F Nets approach.", "mention_start": 22, "mention_end": 65, "dataset_mention": " CIFAR100 and MNIST [35] [36] image dataset"}, {"mentioned_in_paper": "309", "context_id": "245", "dataset_context": "The input image dimension is 32x32x3 for the CIFAR10 and CIFAR100 datasets while it is 32x32x1 for the MNIST data set.", "mention_start": 41, "mention_end": 74, "dataset_mention": "the CIFAR10 and CIFAR100 datasets"}, {"mentioned_in_paper": "309", "context_id": "245", "dataset_context": "The input image dimension is 32x32x3 for the CIFAR10 and CIFAR100 datasets while it is 32x32x1 for the MNIST data set.", "mention_start": 99, "mention_end": 117, "dataset_mention": "the MNIST data set"}, {"mentioned_in_paper": "309", "context_id": "247", "dataset_context": "We employed the CIFAR10 dataset to train our adaptive C2F nets A and B; MNIST dataset for C2F net C; and CIFAR100 dataset for C2F net D Energy and accuracy trade-off objective O. Recall that the training of C2F Nets is based on the objective", "mention_start": 12, "mention_end": 31, "dataset_mention": "the CIFAR10 dataset"}, {"mentioned_in_paper": "309", "context_id": "247", "dataset_context": "We employed the CIFAR10 dataset to train our adaptive C2F nets A and B; MNIST dataset for C2F net C; and CIFAR100 dataset for C2F net D Energy and accuracy trade-off objective O. Recall that the training of C2F Nets is based on the objective", "mention_start": 71, "mention_end": 85, "dataset_mention": " MNIST dataset"}, {"mentioned_in_paper": "309", "context_id": "247", "dataset_context": "We employed the CIFAR10 dataset to train our adaptive C2F nets A and B; MNIST dataset for C2F net C; and CIFAR100 dataset for C2F net D Energy and accuracy trade-off objective O. Recall that the training of C2F Nets is based on the objective", "mention_start": 100, "mention_end": 121, "dataset_mention": " and CIFAR100 dataset"}, {"mentioned_in_paper": "309", "context_id": "259", "dataset_context": "With Net A and Net B we demonstrate the effect of different architectures on the same CIFAR10 dataset.", "mention_start": 77, "mention_end": 101, "dataset_mention": "the same CIFAR10 dataset"}, {"mentioned_in_paper": "309", "context_id": "260", "dataset_context": "With Net C and Net D we analyze the impact on the performance for a simpler and complex dataset like MNIST and CIFAR100 respectively.", "mention_start": 66, "mention_end": 95, "dataset_mention": "a simpler and complex dataset"}, {"mentioned_in_paper": "309", "context_id": "280", "dataset_context": "However, we observe higher accuracies with Net C because MNIST data set is simple and obtain lower accuracies in NET D because CIFAR100 data set is relatively complex.", "mention_start": 56, "mention_end": 71, "dataset_mention": "MNIST data set"}, {"mentioned_in_paper": "309", "context_id": "280", "dataset_context": "However, we observe higher accuracies with Net C because MNIST data set is simple and obtain lower accuracies in NET D because CIFAR100 data set is relatively complex.", "mention_start": 126, "mention_end": 144, "dataset_mention": "CIFAR100 data set"}, {"mentioned_in_paper": "310", "context_id": "3", "dataset_context": "We generate the few-shot dataset of VQA with a variety of answers and their attributes without any human effort.", "mention_start": 12, "mention_end": 32, "dataset_mention": "the few-shot dataset"}, {"mentioned_in_paper": "310", "context_id": "5", "dataset_context": "Experimental results on the VQA v2.0 validation dataset demonstrate the effectiveness of our proposed attribute network and the constraint between answers and their corresponding attributes, as well as the ability of our method to handle the answers with few training examples.", "mention_start": 24, "mention_end": 55, "dataset_mention": "the VQA v2.0 validation dataset"}, {"mentioned_in_paper": "310", "context_id": "40", "dataset_context": "We conduct experiments on VQA v2.0 dataset.", "mention_start": 26, "mention_end": 42, "dataset_mention": "VQA v2.0 dataset"}, {"mentioned_in_paper": "310", "context_id": "55", "dataset_context": "Furthermore, Vilbert [29], VL-bert [30], Structbert [31], and LXMERT [32] even directly fine-tune the BERT [33] model without considering the different distributions of question and image features, and this procedure requires a large number of out-domain data, such as image caption dataset [34], [35], [36] and text-based question answering dataset [37].", "mention_start": 268, "mention_end": 290, "dataset_mention": "image caption dataset"}, {"mentioned_in_paper": "310", "context_id": "55", "dataset_context": "Furthermore, Vilbert [29], VL-bert [30], Structbert [31], and LXMERT [32] even directly fine-tune the BERT [33] model without considering the different distributions of question and image features, and this procedure requires a large number of out-domain data, such as image caption dataset [34], [35], [36] and text-based question answering dataset [37].", "mention_start": 302, "mention_end": 349, "dataset_mention": " [36] and text-based question answering dataset"}, {"mentioned_in_paper": "310", "context_id": "69", "dataset_context": "Moreover, A Closer Look at Few-shot Classification [49] compares these representative algorithms and develops a simple baseline approach on top of the frozen CNN, which achieves state-of-the-art on both mini-ImageNet [44] and CUB [15] datasets.", "mention_start": 197, "mention_end": 243, "dataset_mention": "both mini-ImageNet [44] and CUB [15] datasets"}, {"mentioned_in_paper": "310", "context_id": "113", "dataset_context": "In this section, we evaluate our method on VQA v2.0 dataset [54].", "mention_start": 42, "mention_end": 59, "dataset_mention": "VQA v2.0 dataset"}, {"mentioned_in_paper": "310", "context_id": "114", "dataset_context": "We first introduce this dataset following by constructing the few-shot dataset and then describe our implementation details and results, and finally the qualitative analysis.", "mention_start": 58, "mention_end": 78, "dataset_mention": "the few-shot dataset"}, {"mentioned_in_paper": "310", "context_id": "116", "dataset_context": "Compared with VQA v1.0 dataset [3], it emphasizes the visual understanding by reducing the text bias learned from the questions.", "mention_start": 14, "mention_end": 30, "dataset_mention": "VQA v1.0 dataset"}, {"mentioned_in_paper": "310", "context_id": "119", "dataset_context": "Constructing the few-shot dataset: The answers of VQA dataset can be classified into three types, i.e. 'yes/no', 'number', and 'others', and we focus on the ones of 'others', as they are more complex and diverse than the other two categories.", "mention_start": 13, "mention_end": 33, "dataset_mention": "the few-shot dataset"}, {"mentioned_in_paper": "310", "context_id": "119", "dataset_context": "Constructing the few-shot dataset: The answers of VQA dataset can be classified into three types, i.e. 'yes/no', 'number', and 'others', and we focus on the ones of 'others', as they are more complex and diverse than the other two categories.", "mention_start": 49, "mention_end": 61, "dataset_mention": "VQA dataset"}, {"mentioned_in_paper": "310", "context_id": "137", "dataset_context": "Due to easily over-fitting on the novel fewshot dataset, we reduce the training epochs of it to 40, 20, and 14 for 1-, 5-, and 10-shot learning respectively.", "mention_start": 30, "mention_end": 55, "dataset_mention": "the novel fewshot dataset"}, {"mentioned_in_paper": "310", "context_id": "144", "dataset_context": "Nevertheless, the performance drops on the 5-shot and 10-shot learning when \u03bb > 0.2, it can be explained that the answers can be represented by the summing of embeddings of attributes to some extent with \u03bb = 0.1 and \u03bb = 0.2, but a higher value like \u03bb = 0.5 limits its potential to learn new information from the domain of few-shot dataset.", "mention_start": 321, "mention_end": 338, "dataset_mention": "few-shot dataset"}, {"mentioned_in_paper": "310", "context_id": "164", "dataset_context": "We advocate the few-shot dataset of VQA, which has two subsets, namely the base set and novel set, and we generate the attributes of answers without human effort.", "mention_start": 0, "mention_end": 32, "dataset_mention": "We advocate the few-shot dataset"}, {"mentioned_in_paper": "310", "context_id": "165", "dataset_context": "Given a large amount of training data from the base dataset, we utilize BGN to model the relationship between words from the question and objects from the image and generate their joint embeddings in the base stage.", "mention_start": 43, "mention_end": 59, "dataset_mention": "the base dataset"}, {"mentioned_in_paper": "311", "context_id": "117", "dataset_context": "Like this, navigating the data set and finding patterns in branch occurrences is facilitated.", "mention_start": 10, "mention_end": 34, "dataset_mention": " navigating the data set"}, {"mentioned_in_paper": "311", "context_id": "119", "dataset_context": "The sea ice data set describes arctic sea ice concentrations and is provided by the National Snow and Ice Data Center [3].", "mention_start": 0, "mention_end": 20, "dataset_mention": "The sea ice data set"}, {"mentioned_in_paper": "311", "context_id": "127", "dataset_context": "Figures 7 and 9 show the T-FCT for the sea ice data set with branches of typical behavior highlighted.", "mention_start": 35, "mention_end": 55, "dataset_mention": "the sea ice data set"}, {"mentioned_in_paper": "311", "context_id": "135", "dataset_context": "Here, we used the overlap metric for the alignment and illustrate the usefulness of optimized branch spacing in discrete data sets.", "mention_start": 111, "mention_end": 130, "dataset_mention": "discrete data sets"}, {"mentioned_in_paper": "313", "context_id": "162", "dataset_context": "We mainly evaluate quantitatively on the synthetic ICL-NUIM livingroom dataset [22].", "mention_start": 37, "mention_end": 78, "dataset_mention": "the synthetic ICL-NUIM livingroom dataset"}, {"mentioned_in_paper": "313", "context_id": "163", "dataset_context": "Additional evaluations are performed on TUM RGB-D Dataset (TUM) [60] and 3D Scene Data (Zhou) [70] with real scans.", "mention_start": 40, "mention_end": 57, "dataset_mention": "TUM RGB-D Dataset"}, {"mentioned_in_paper": "313", "context_id": "166", "dataset_context": "We compare against other representations on both clean and noisy sequences of the ICL-NUIM dataset to further demonstrate the noise-handling ability of each representation.", "mention_start": 78, "mention_end": 98, "dataset_mention": "the ICL-NUIM dataset"}, {"mentioned_in_paper": "313", "context_id": "183", "dataset_context": "For livingroom dataset at the size of about 6m*3m*9m, the voxel grid is allocated to be 256 3 for 4cm and 5cm resolution and 512 3 for 2cm and 3cm.", "mention_start": 4, "mention_end": 22, "dataset_mention": "livingroom dataset"}, {"mentioned_in_paper": "314", "context_id": "81", "dataset_context": "The developed metric was first tested on a newly gathered VASE dataset 2 in order to assess the performance of improved feature coverage.", "mention_start": 41, "mention_end": 70, "dataset_mention": "a newly gathered VASE dataset"}, {"mentioned_in_paper": "314", "context_id": "85", "dataset_context": "A second evaluation was performed on the widely used Oxford dataset 3.", "mention_start": 53, "mention_end": 67, "dataset_mention": "Oxford dataset"}, {"mentioned_in_paper": "314", "context_id": "105", "dataset_context": "Table 1 presents the t-test results on the VASE dataset and it is clear that the evolved set results in a lower value for \u03b1 suggesting better coverage (t is larger than t-Critical for one and two-tail tests with a P value very close to zero) with substantial confidence.", "mention_start": 39, "mention_end": 55, "dataset_mention": "the VASE dataset"}, {"mentioned_in_paper": "314", "context_id": "120", "dataset_context": "q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Number of Features q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Number of Features (b) Oxford dataset q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Coverage (a) Vase dataset 0 10 20 30 40 5000 10000 20000 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Coverage (b) Oxford dataset q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Matched Images Accuracy Fig. 8 Change in the accuracy of the computed homography for original and refined set of image features, squares and circles respectively.", "mention_start": 1179, "mention_end": 1206, "dataset_mention": "Features (b) Oxford dataset"}, {"mentioned_in_paper": "314", "context_id": "120", "dataset_context": "q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Number of Features q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Number of Features (b) Oxford dataset q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Coverage (a) Vase dataset 0 10 20 30 40 5000 10000 20000 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Coverage (b) Oxford dataset q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Matched Images Accuracy Fig. 8 Change in the accuracy of the computed homography for original and refined set of image features, squares and circles respectively.", "mention_start": 1179, "mention_end": 2279, "dataset_mention": "Features (b) Oxford dataset q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Coverage (a) Vase dataset"}, {"mentioned_in_paper": "314", "context_id": "120", "dataset_context": "q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Number of Features q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Number of Features (b) Oxford dataset q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Coverage (a) Vase dataset 0 10 20 30 40 5000 10000 20000 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Coverage (b) Oxford dataset q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Matched Images Accuracy Fig. 8 Change in the accuracy of the computed homography for original and refined set of image features, squares and circles respectively.", "mention_start": 1179, "mention_end": 2441, "dataset_mention": "Features (b) Oxford dataset q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Coverage (a) Vase dataset 0 10 20 30 40 5000 10000 20000 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Images Coverage (b) Oxford dataset"}, {"mentioned_in_paper": "316", "context_id": "138", "dataset_context": "The MSMT17 [20] dataset consists of 126, 441 person images from 4, 101 identities, thus constituting the largest person Re-ID dataset at present.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The MSMT17 [20] dataset"}, {"mentioned_in_paper": "316", "context_id": "148", "dataset_context": "Similarly, in the local branch, the dimensionality of the output feature maps of I L 2,s and I L  datasets.", "mention_start": 87, "mention_end": 106, "dataset_mention": "s and I L  datasets"}, {"mentioned_in_paper": "316", "context_id": "158", "dataset_context": "Table 2 clearly shows the superior performance of CCAN against all the other methods in terms of mAP and Rank-1 accuracies on the Market-1501 dataset.", "mention_start": 126, "mention_end": 149, "dataset_mention": "the Market-1501 dataset"}, {"mentioned_in_paper": "316", "context_id": "161", "dataset_context": "We further evaluated our proposed CCAN on the DukeMTMC-reID [19] dataset.", "mention_start": 42, "mention_end": 72, "dataset_mention": "the DukeMTMC-reID [19] dataset"}, {"mentioned_in_paper": "316", "context_id": "162", "dataset_context": "More variations in resolution and viewpoints due to wider camera views, and more complex environmental layout make DukeMTMC-reID more challenging compared to the Market-1501 dataset for the task of Re-ID.", "mention_start": 157, "mention_end": 181, "dataset_mention": "the Market-1501 dataset"}, {"mentioned_in_paper": "316", "context_id": "168", "dataset_context": "The 767/700 split results in a small training set with only 7365 images against 12, 936/16, 522 training images in Market-1501/DukeMTMC-reID datasets respectively.", "mention_start": 114, "mention_end": 149, "dataset_mention": "Market-1501/DukeMTMC-reID datasets"}, {"mentioned_in_paper": "316", "context_id": "172", "dataset_context": "Table 5 shows the result of our proposed CCAN when trained and evaluated on the new challenging MSMT17 [20] dataset.", "mention_start": 96, "mention_end": 115, "dataset_mention": "MSMT17 [20] dataset"}, {"mentioned_in_paper": "316", "context_id": "179", "dataset_context": "We first evaluate CCAN for different values of d on the Market-1501 [18] dataset.", "mention_start": 52, "mention_end": 80, "dataset_mention": "the Market-1501 [18] dataset"}, {"mentioned_in_paper": "316", "context_id": "193", "dataset_context": "The results, evaluated on Market1501 dataset [18] single query setting, are shown in Table 6.", "mention_start": 25, "mention_end": 44, "dataset_mention": "Market1501 dataset"}, {"mentioned_in_paper": "317", "context_id": "183", "dataset_context": "It is a multi-modal micro-video dataset released in ICME Challenge 2019 3 where a micro-video has the features of caption, audio, and video.", "mention_start": 6, "mention_end": 39, "dataset_mention": "a multi-modal micro-video dataset"}, {"mentioned_in_paper": "317", "context_id": "189", "dataset_context": "This is a news dataset [16] where the title and description of news are exposure features and the news content is treated as content features.", "mention_start": 8, "mention_end": 22, "dataset_mention": "a news dataset"}, {"mentioned_in_paper": "317", "context_id": "267", "dataset_context": "Specifically, we compare CR and NT over filtered datasets with different percentages of clicks that end with dislikes.", "mention_start": 39, "mention_end": 57, "dataset_mention": "filtered datasets"}, {"mentioned_in_paper": "317", "context_id": "269", "dataset_context": "Figure 7 shows the performance with discarding proportion changing from 0 (the original dataset) to 0.8.", "mention_start": 72, "mention_end": 95, "dataset_mention": "0 (the original dataset"}, {"mentioned_in_paper": "318", "context_id": "6", "dataset_context": "We perform extensive experiments on four largescale in-the-wild facial expression datasets -namely AffectNet, FER2013, ExpW, and RAF-DB -and one lab-controlled dataset (CK+) to evaluate our approach.", "mention_start": 36, "mention_end": 90, "dataset_mention": "four largescale in-the-wild facial expression datasets"}, {"mentioned_in_paper": "318", "context_id": "6", "dataset_context": "We perform extensive experiments on four largescale in-the-wild facial expression datasets -namely AffectNet, FER2013, ExpW, and RAF-DB -and one lab-controlled dataset (CK+) to evaluate our approach.", "mention_start": 124, "mention_end": 167, "dataset_mention": " and RAF-DB -and one lab-controlled dataset"}, {"mentioned_in_paper": "318", "context_id": "43", "dataset_context": "We perform rigorous experiments on four in-the-wild large-scale FER datasets, AffectNet [25], FER2013 [26], RAF-DB [27], and ExpW [28], and one lab-controlled dataset, i.e.", "mention_start": 35, "mention_end": 76, "dataset_mention": "four in-the-wild large-scale FER datasets"}, {"mentioned_in_paper": "318", "context_id": "43", "dataset_context": "We perform rigorous experiments on four in-the-wild large-scale FER datasets, AffectNet [25], FER2013 [26], RAF-DB [27], and ExpW [28], and one lab-controlled dataset, i.e.", "mention_start": 135, "mention_end": 166, "dataset_mention": " and one lab-controlled dataset"}, {"mentioned_in_paper": "318", "context_id": "61", "dataset_context": "(v) The effects of the number of extracted facial landmarks is studied (vi) We perform a cross-dataset study utilizing face trees.", "mention_start": 87, "mention_end": 102, "dataset_mention": "a cross-dataset"}, {"mentioned_in_paper": "318", "context_id": "175", "dataset_context": "It is worth mentioning that while CCPSO2 does not promise an optimal solution (it can get stuck in local optima like many other optimizers), what matters is the generalization ability of the converged face tree on unseen data, which will be extensively tested on 5 different FER datasets in the experiment section of the paper.", "mention_start": 262, "mention_end": 287, "dataset_mention": "5 different FER datasets"}, {"mentioned_in_paper": "318", "context_id": "196", "dataset_context": "The patches are then cropped and fed to a pre-trained encoder, such as ResNet-50 pre-trained on the VGG-Face2 dataset [64].", "mention_start": 95, "mention_end": 117, "dataset_mention": "the VGG-Face2 dataset"}, {"mentioned_in_paper": "318", "context_id": "220", "dataset_context": "Generally, FER datasets are highly skewed towards a specific expression, usually happy.", "mention_start": 10, "mention_end": 23, "dataset_mention": " FER datasets"}, {"mentioned_in_paper": "318", "context_id": "235", "dataset_context": "However, for CK+ dataset, we use leave-one-subjectout cross validation [72] Considering CCPSO2 algorithm, the number of iterations is set to 40, as convergence occurs at or before this iteration, after which no changes are observed in the generated trees.", "mention_start": 12, "mention_end": 24, "dataset_mention": "CK+ dataset"}, {"mentioned_in_paper": "318", "context_id": "239", "dataset_context": "We use four well-known public in-the-wild FER datasets to evaluate our method.", "mention_start": 7, "mention_end": 54, "dataset_mention": "four well-known public in-the-wild FER datasets"}, {"mentioned_in_paper": "318", "context_id": "254", "dataset_context": "CK+ [72] : Comprising 327 videos, the extended Cohn-Kanade (CK+) dataset is one of the well-known precursors in the FER datasets.", "mention_start": 33, "mention_end": 72, "dataset_mention": " the extended Cohn-Kanade (CK+) dataset"}, {"mentioned_in_paper": "318", "context_id": "254", "dataset_context": "CK+ [72] : Comprising 327 videos, the extended Cohn-Kanade (CK+) dataset is one of the well-known precursors in the FER datasets.", "mention_start": 111, "mention_end": 128, "dataset_mention": "the FER datasets"}, {"mentioned_in_paper": "318", "context_id": "264", "dataset_context": "The only exception is on the RAF-DB dataset in which GRU exhibits slightly (marginal) better performance than the LSTM.", "mention_start": 25, "mention_end": 43, "dataset_mention": "the RAF-DB dataset"}, {"mentioned_in_paper": "318", "context_id": "272", "dataset_context": "With regards to FER2013 dataset (Table III), FaceTopoNet shows an improvement of 1.13% over the state-of-the-art, BreG-NeXt-50.", "mention_start": 16, "mention_end": 31, "dataset_mention": "FER2013 dataset"}, {"mentioned_in_paper": "318", "context_id": "278", "dataset_context": "A reason for this could be the use of a new loss term by [84], which is equipped to handle class imbalances, which is a common issue in FER datasets, by monitoring the last layers of a CNN.", "mention_start": 135, "mention_end": 148, "dataset_mention": "FER datasets"}, {"mentioned_in_paper": "318", "context_id": "279", "dataset_context": "Finally, regarding CK+ dataset, FaceTopoNet achieves the second best recognition rate.", "mention_start": 18, "mention_end": 30, "dataset_mention": "CK+ dataset"}, {"mentioned_in_paper": "318", "context_id": "281", "dataset_context": "As some of the state-of-the-art methods additionally reported precision and recall values for both AffectNet and FER2013 datasets, we also consider these measures for each emotion class in Tables II and III.", "mention_start": 94, "mention_end": 129, "dataset_mention": "both AffectNet and FER2013 datasets"}, {"mentioned_in_paper": "318", "context_id": "283", "dataset_context": "More specifically, FaceTopoNet obtains the best precision and recall in disgust and contempt expressions, which are considered as more difficult classes due to their small numbers of training samples in the AffectNet dataset.", "mention_start": 202, "mention_end": 224, "dataset_mention": "the AffectNet dataset"}, {"mentioned_in_paper": "318", "context_id": "284", "dataset_context": "Given FER2013 dataset, FaceTopoNet delivers the highest precision in most of the expression classes -i.e.", "mention_start": 6, "mention_end": 21, "dataset_mention": "FER2013 dataset"}, {"mentioned_in_paper": "318", "context_id": "286", "dataset_context": "For a deeper comparison between our proposed model and one of the best performing benchmarks in both AffectNet and FER2013 datasets, BreG-NeXt-50, F1 scores corresponding to [86] pseudo-siamese network 86.50 Wang et al. [35] Region attention 86.90 Wang et al. [87] Attention and relabeling 87.03 Farzaneh et al. [77] CNN and attention 87.78 Li et al. [84] Coarse-fine labeling  the expression classes are presented in Figure 6 using one-vsall method [85].", "mention_start": 96, "mention_end": 131, "dataset_mention": "both AffectNet and FER2013 datasets"}, {"mentioned_in_paper": "318", "context_id": "295", "dataset_context": "Apart from disgust, FaceTopoNet performs well on the FER2013 dataset overall.", "mention_start": 48, "mention_end": 68, "dataset_mention": "the FER2013 dataset"}, {"mentioned_in_paper": "318", "context_id": "296", "dataset_context": "On the RAF-DB dataset, the largest confusions occur in the case of disgust-sad as well as disgustneutral.", "mention_start": 3, "mention_end": 21, "dataset_mention": "the RAF-DB dataset"}, {"mentioned_in_paper": "318", "context_id": "298", "dataset_context": "As for the ExpW dataset, fear is confused with neutral, happy, and sad expressions.", "mention_start": 7, "mention_end": 23, "dataset_mention": "the ExpW dataset"}, {"mentioned_in_paper": "318", "context_id": "299", "dataset_context": "This can be justified as fear only constitutes 1.1% of the ExpW dataset.", "mention_start": 55, "mention_end": 71, "dataset_mention": "the ExpW dataset"}, {"mentioned_in_paper": "318", "context_id": "300", "dataset_context": "Finally, considering CK+ dataset, happy and disgust expressions have the highest accuracy.", "mention_start": 20, "mention_end": 32, "dataset_mention": "CK+ dataset"}, {"mentioned_in_paper": "318", "context_id": "308", "dataset_context": "The resulting curve is shown in Figure 8 for both AffectNet and FER2013 datasets.", "mention_start": 45, "mention_end": 80, "dataset_mention": "both AffectNet and FER2013 datasets"}, {"mentioned_in_paper": "318", "context_id": "309", "dataset_context": "Considering AffectNet dataset, there is a direct relationship between n and the performance, up until 50 facial landmarks, from where the performance starts to drop.", "mention_start": 12, "mention_end": 29, "dataset_mention": "AffectNet dataset"}, {"mentioned_in_paper": "318", "context_id": "311", "dataset_context": "Almost the same behaviour can be seen for the FER2013 dataset.", "mention_start": 42, "mention_end": 61, "dataset_mention": "the FER2013 dataset"}, {"mentioned_in_paper": "318", "context_id": "315", "dataset_context": "To this end, we first consider 4 \u00d7 4 patches for training FaceTopoNet on both AffectNet and FER2013 datasets.", "mention_start": 72, "mention_end": 108, "dataset_mention": "both AffectNet and FER2013 datasets"}, {"mentioned_in_paper": "318", "context_id": "325", "dataset_context": "We then add black patches on these key areas, and subsequently build three new validation sets for each of the AffectNet and FER2013 datasets to test our model against.", "mention_start": 106, "mention_end": 141, "dataset_mention": "the AffectNet and FER2013 datasets"}, {"mentioned_in_paper": "318", "context_id": "333", "dataset_context": "Figure 12 depicts the topological evolution of the minimum-cost spanning trees generated by FaceTopoNet in iterations 1 and 40 for AffectNet, FER2013, RAF-DB, and ExpW datasets.", "mention_start": 158, "mention_end": 176, "dataset_mention": " and ExpW datasets"}, {"mentioned_in_paper": "318", "context_id": "339", "dataset_context": "To this end, we perform a cross-dataset study by using the final tree originally learned using AffectNet for expression recognition on the FER2013 dataset, and vice-versa.", "mention_start": 23, "mention_end": 39, "dataset_mention": "a cross-dataset"}, {"mentioned_in_paper": "318", "context_id": "339", "dataset_context": "To this end, we perform a cross-dataset study by using the final tree originally learned using AffectNet for expression recognition on the FER2013 dataset, and vice-versa.", "mention_start": 134, "mention_end": 154, "dataset_mention": "the FER2013 dataset"}, {"mentioned_in_paper": "318", "context_id": "340", "dataset_context": "Table VII presents the results where we observe an RR drop of 1.54% and 1.26% for AffectNet and FER2013 datasets, respectively.", "mention_start": 82, "mention_end": 112, "dataset_mention": "AffectNet and FER2013 datasets"}, {"mentioned_in_paper": "318", "context_id": "341", "dataset_context": "From this experiment, we conclude that while the learned trees are customized and optimized for individual datasets, given the similarities between the datasets, it is reasonably feasible to perform cross-dataset tree-based transfer learning.", "mention_start": 198, "mention_end": 212, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "318", "context_id": "344", "dataset_context": "Thus, we use this tree to perform FER on AffectNet and FER2013 datasets instead of using the customized trees learned for each individual dataset.", "mention_start": 40, "mention_end": 71, "dataset_mention": "AffectNet and FER2013 datasets"}, {"mentioned_in_paper": "318", "context_id": "345", "dataset_context": "Table VII illustrates the results where we observe a drop of 4.11% and 5.4% for Af-fectNet and FER2013 datasets, respectively.", "mention_start": 80, "mention_end": 111, "dataset_mention": "Af-fectNet and FER2013 datasets"}, {"mentioned_in_paper": "318", "context_id": "350", "dataset_context": "From the results outlined in Table VIII, we observe that the removal of the tree topology learning step  results in substantial performance drop of 5.0% and 7.60%, respectively for AffectNet and FER2013 datasets.", "mention_start": 180, "mention_end": 211, "dataset_mention": "AffectNet and FER2013 datasets"}, {"mentioned_in_paper": "318", "context_id": "366", "dataset_context": "We perform extensive experiments on 4 popular in-the-wild FER datasets, AffectNet, FER2013, RAF-DB, and ExpW.", "mention_start": 36, "mention_end": 70, "dataset_mention": "4 popular in-the-wild FER datasets"}, {"mentioned_in_paper": "318", "context_id": "367", "dataset_context": "Results indicate that not only FaceTopoNet sets new state-of-the-art recognition rate in AffectNet, FER2013, and ExpW datasets, but also shows more robustness against different occlusions.", "mention_start": 108, "mention_end": 126, "dataset_mention": " and ExpW datasets"}, {"mentioned_in_paper": "319", "context_id": "5", "dataset_context": "Using the YouTube-VIS dataset and 30 participants, we experimentally show that MemX significantly improves the attention tracking accuracy over the eye-tracking-alone method, while maintaining high system energy efficiency.", "mention_start": 6, "mention_end": 29, "dataset_mention": "the YouTube-VIS dataset"}, {"mentioned_in_paper": "319", "context_id": "56", "dataset_context": "(3) MemX is evaluated using the YouTube-VIS dataset [49] and 30 participants.", "mention_start": 28, "mention_end": 51, "dataset_mention": "the YouTube-VIS dataset"}, {"mentioned_in_paper": "319", "context_id": "76", "dataset_context": "Thanks to the emerging large-scale data sets and deep learning techniques, the performance of learning-based eye-tracking models has been steadily improving [22, 43, 47].", "mention_start": 10, "mention_end": 44, "dataset_mention": "the emerging large-scale data sets"}, {"mentioned_in_paper": "319", "context_id": "257", "dataset_context": "In this work, we adopt the YouTube-VIS dataset 1 [49] as \"micro-benchmark in a controlled setting\" to quantitatively evaluate such technical capability of MemX.", "mention_start": 22, "mention_end": 46, "dataset_mention": "the YouTube-VIS dataset"}, {"mentioned_in_paper": "319", "context_id": "259", "dataset_context": "Specifically, the YouTube-VIS dataset covers a wide range of complex real-world scenarios.", "mention_start": 13, "mention_end": 37, "dataset_mention": " the YouTube-VIS dataset"}, {"mentioned_in_paper": "319", "context_id": "261", "dataset_context": "Therefore, using the YouTube-VIS dataset, we can design a wide range of interesting testing cases to evaluate the accuracy and efficiency of MemX for eye tracking and attention detection.", "mention_start": 16, "mention_end": 40, "dataset_mention": "the YouTube-VIS dataset"}, {"mentioned_in_paper": "319", "context_id": "263", "dataset_context": "In addition, as a widely used dataset, the YouTube-VIS dataset includes necessary object annotation information regarding object location, classification, and instance segmentation.", "mention_start": 38, "mention_end": 62, "dataset_mention": " the YouTube-VIS dataset"}, {"mentioned_in_paper": "319", "context_id": "265", "dataset_context": "The YouTube-VIS dataset consists of a 40-category label set and 2,238 videos with released annotations.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The YouTube-VIS dataset"}, {"mentioned_in_paper": "319", "context_id": "270", "dataset_context": "Fig. 5 visualizes cases of the 4-class video frames in YouTube-VIS dataset.", "mention_start": 55, "mention_end": 74, "dataset_mention": "YouTube-VIS dataset"}, {"mentioned_in_paper": "319", "context_id": "271", "dataset_context": "Since each video snippet lasts 3 to 5 seconds in the YouTube-VIS dataset, we concatenate multiple video snippets to form videos with a duration of about 7-15 minutes (approximately 100 video snippets).", "mention_start": 49, "mention_end": 72, "dataset_mention": "the YouTube-VIS dataset"}, {"mentioned_in_paper": "319", "context_id": "275", "dataset_context": "Given the 30-recruited participants, we select 1,000 video snippets from the entire YouTube-VIS datasets.", "mention_start": 73, "mention_end": 104, "dataset_mention": "the entire YouTube-VIS datasets"}, {"mentioned_in_paper": "319", "context_id": "282", "dataset_context": "We then obtain the users' eye video dataset that is synchronized with the video dataset.", "mention_start": 0, "mention_end": 43, "dataset_mention": "We then obtain the users' eye video dataset"}, {"mentioned_in_paper": "319", "context_id": "283", "dataset_context": "In the following experiments, the video dataset and the eye dataset are randomly divided into training set, test set, and validation set with a 70%:10%:20% ratio.", "mention_start": 29, "mention_end": 67, "dataset_mention": " the video dataset and the eye dataset"}, {"mentioned_in_paper": "319", "context_id": "430", "dataset_context": "MemX is evaluated using the YouTube-VIS dataset and 30 participants.", "mention_start": 24, "mention_end": 47, "dataset_mention": "the YouTube-VIS dataset"}, {"mentioned_in_paper": "320", "context_id": "109", "dataset_context": "We evaluate our method quantitatively on our own test dataset (1419 scenes) and a test dataset of YCB [3] objects (256 scenes) to demonstrate generalisation.", "mention_start": 41, "mention_end": 94, "dataset_mention": "our own test dataset (1419 scenes) and a test dataset"}, {"mentioned_in_paper": "320", "context_id": "112", "dataset_context": "We demonstrate qualitative result on our synthetic datasets in Figure 9 as well as on real data examples and sequences of the YCB-Video dataset (Figures 1, 10 and 5) and evaluate our method for multi-view reconstruction.", "mention_start": 122, "mention_end": 143, "dataset_mention": "the YCB-Video dataset"}, {"mentioned_in_paper": "320", "context_id": "161", "dataset_context": "For a fixed sequence of 6 viewpoints (see Figure 13) we compare the reconstruction quality for our SQ test dataset.", "mention_start": 95, "mention_end": 114, "dataset_mention": "our SQ test dataset"}, {"mentioned_in_paper": "320", "context_id": "201", "dataset_context": "We provide additional qualitative results on our Superquadric (SQ) shape test dataset and our YCB object test set, comparing SIMstack to our baselines (see Figure 17).", "mention_start": 45, "mention_end": 85, "dataset_mention": "our Superquadric (SQ) shape test dataset"}, {"mentioned_in_paper": "320", "context_id": "242", "dataset_context": "We show example of this on our Superquadrics test dataset in Figure 22.", "mention_start": 27, "mention_end": 57, "dataset_mention": "our Superquadrics test dataset"}, {"mentioned_in_paper": "320", "context_id": "246", "dataset_context": "We demonstrate this on an example from our Superquadrics test dataset in Figure 23.", "mention_start": 39, "mention_end": 69, "dataset_mention": "our Superquadrics test dataset"}, {"mentioned_in_paper": "321", "context_id": "18", "dataset_context": "Overview: We discuss both traditional and deep learning-based related work in Section 2. We present ChArUcoNet, our two-headed custom point detection network, and RefineNet, our corner refinement network in Section 3. Finally, we describe both training and testing ChArUco datasets in Section 4, evaluation results in Section 5, and conclude with a discussion in Section 6. arXiv:1812.03247v2", "mention_start": 252, "mention_end": 281, "dataset_mention": "and testing ChArUco datasets"}, {"mentioned_in_paper": "321", "context_id": "72", "dataset_context": "To train and evaluate our Deep ChArUco Detection system, we created two ChArUco datasets.", "mention_start": 67, "mention_end": 88, "dataset_mention": "two ChArUco datasets"}, {"mentioned_in_paper": "321", "context_id": "77", "dataset_context": "Videos frames are extracted into the positive dataset with the resolution of 320 \u00d7 240, resulting in a total of 7, 955 gray-scale frames.", "mention_start": 33, "mention_end": 53, "dataset_mention": "the positive dataset"}, {"mentioned_in_paper": "321", "context_id": "80", "dataset_context": "The negative dataset contains 91, 406 images in total, including 82, 783 generic images from the MS-COCO dataset 1 and 8, 623 video frames collected in the office.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The negative dataset"}, {"mentioned_in_paper": "321", "context_id": "80", "dataset_context": "The negative dataset contains 91, 406 images in total, including 82, 783 generic images from the MS-COCO dataset 1 and 8, 623 video frames collected in the office.", "mention_start": 92, "mention_end": 112, "dataset_mention": "the MS-COCO dataset"}, {"mentioned_in_paper": "321", "context_id": "128", "dataset_context": "In our large experiment, we evaluate across all 26, 000 frames in the 26-video dataset, without adding synthetic effects.", "mention_start": 65, "mention_end": 86, "dataset_mention": "the 26-video dataset"}, {"mentioned_in_paper": "321", "context_id": "147", "dataset_context": "Our synthetic and realdata experiments show a performance gap favoring our approach and demonstrate the effectiveness of our neural network architecture design and the dataset creation methodology.", "mention_start": 121, "mention_end": 175, "dataset_mention": "our neural network architecture design and the dataset"}, {"mentioned_in_paper": "321", "context_id": "148", "dataset_context": "The key ingredients to our method are the following: ChArUcoNet, a CNN for pattern-specific keypoint detection, RefineNet, a subpixel localization network, a custom ChArUco pattern-specific dataset, comprising extreme data augmentation and proper selection of visually similar patterns as negatives.", "mention_start": 155, "mention_end": 197, "dataset_mention": " a custom ChArUco pattern-specific dataset"}, {"mentioned_in_paper": "322", "context_id": "5", "dataset_context": "Through experimental evaluation on the SiW dataset, we show that OAP improves recognition performance of existing methods on both single video setting and continual setting, where spoof videos are interleaved with live ones to simulate spoofing attacks.", "mention_start": 35, "mention_end": 50, "dataset_mention": "the SiW dataset"}, {"mentioned_in_paper": "322", "context_id": "12", "dataset_context": "While large face anti-spoofing datasets [6, 12, 13] have been collected in recent years to enable the development of deep learning solutions, it is infeasible to capture all the variations in the data that might appear at test time.", "mention_start": 6, "mention_end": 39, "dataset_mention": "large face anti-spoofing datasets"}, {"mentioned_in_paper": "322", "context_id": "68", "dataset_context": "Accordingly, a simple smoothing through majority with a sliding window is used to improve the pseudo-label consistency for samples currently stored in the online dataset:", "mention_start": 150, "mention_end": 169, "dataset_mention": "the online dataset"}, {"mentioned_in_paper": "322", "context_id": "71", "dataset_context": "We select a time window W = 30 frames (equivalent to 1 second in SiW dataset), as we want our solution to work in scenarios with quick transitions between authentic accesses and spoofing attacks.", "mention_start": 65, "mention_end": 76, "dataset_mention": "SiW dataset"}, {"mentioned_in_paper": "322", "context_id": "72", "dataset_context": "During online inference, the classifier is initialized with the pretrained layers \u03d5 0 c = \u03d5c, and the online data is gradually added to an online dataset Do which is initially empty: .", "mention_start": 135, "mention_end": 153, "dataset_mention": "an online dataset"}, {"mentioned_in_paper": "322", "context_id": "83", "dataset_context": "To achieve this, we gradually discard old samples from the online dataset as new frames appear in the online stream.", "mention_start": 54, "mention_end": 73, "dataset_mention": "the online dataset"}, {"mentioned_in_paper": "322", "context_id": "85", "dataset_context": "In our implementation, we discard online samples older than 4 seconds, where full videos in the SiW dataset are up to 30 seconds long.", "mention_start": 91, "mention_end": 107, "dataset_mention": "the SiW dataset"}, {"mentioned_in_paper": "322", "context_id": "91", "dataset_context": "Dataset We evaluate the proposed method on the SiW [6] dataset, which consists of 4620 live and spoof videos from 165 subjects collected under different poses and illumination conditions.", "mention_start": 43, "mention_end": 62, "dataset_mention": "the SiW [6] dataset"}, {"mentioned_in_paper": "322", "context_id": "100", "dataset_context": "As the SiW dataset does not include a development set for calibration, we fix the evaluation threshold at 0.5.", "mention_start": 3, "mention_end": 18, "dataset_mention": "the SiW dataset"}, {"mentioned_in_paper": "322", "context_id": "142", "dataset_context": "We randomly sub-sample the selected number of frames from the videos in the pre-training dataset.", "mention_start": 72, "mention_end": 96, "dataset_mention": "the pre-training dataset"}, {"mentioned_in_paper": "322", "context_id": "147", "dataset_context": "In comparison, the size of the online dataset never exceeds |Do| = 120 (less than 1MB) since we discard online samples older than 4 seconds.", "mention_start": 26, "mention_end": 45, "dataset_mention": "the online dataset"}, {"mentioned_in_paper": "323", "context_id": "1", "dataset_context": "The method has been applied to the problem of approximate nearest neighbor (ANN) search of local and global visual content descriptors, and it has been tested on different datasets: three large scale public datasets of up to one billion descriptors (BIGANN) and, supported by recent progress in convolutional neural networks (CNNs), also on the CIFAR-10 and MNIST datasets.", "mention_start": 340, "mention_end": 372, "dataset_mention": "the CIFAR-10 and MNIST datasets"}, {"mentioned_in_paper": "323", "context_id": "29", "dataset_context": "The method has obtained state-of-the-art results on a large scale SIFT features dataset, improving over methods such as SH [36] and Hamming Embedding [15].", "mention_start": 52, "mention_end": 87, "dataset_mention": "a large scale SIFT features dataset"}, {"mentioned_in_paper": "323", "context_id": "89", "dataset_context": "BIGANN Dataset [17, 18] is a large-scale dataset com-monly used to compare methods for visual feature hashing and approximate nearest neighbor search [2, 9, 17, 18, 21, 29, 31].", "mention_start": 0, "mention_end": 14, "dataset_mention": "BIGANN Dataset"}, {"mentioned_in_paper": "323", "context_id": "92", "dataset_context": "For GIST1M query and base descriptors are from INRIA Holidays and Flickr 1M datasets, while learning vectors are from [34].", "mention_start": 47, "mention_end": 84, "dataset_mention": "INRIA Holidays and Flickr 1M datasets"}, {"mentioned_in_paper": "323", "context_id": "95", "dataset_context": "CIFAR-10 Dataset [22] consists of 60,000 colour images (32 \u00d7 32 pixels) in 10 classes, with 6,000 images per class (see Figure 3).", "mention_start": 0, "mention_end": 16, "dataset_mention": "CIFAR-10 Dataset"}, {"mentioned_in_paper": "323", "context_id": "99", "dataset_context": "MNIST Dataset [26] consists of 70,000 handwritten digits images (28 \u00d7 28 pixels, see Figure 4).", "mention_start": 0, "mention_end": 13, "dataset_mention": "MNIST Dataset"}, {"mentioned_in_paper": "323", "context_id": "101", "dataset_context": "The performance of ANN retrieval in BIGANN dataset is evaluated using recall@R, which is used in most of the results reported in the literature [2, 9, 17, 18, 21, 29] and it is, for varying values of R, the average rate of queries for which the 1-nearest neighbor is retrieved in the top R positions.", "mention_start": 36, "mention_end": 50, "dataset_mention": "BIGANN dataset"}, {"mentioned_in_paper": "323", "context_id": "111", "dataset_context": "The process is carried out in two steps: in the first step a supervised pre-training on the large-scale ImageNet dataset [23] is performed.", "mention_start": 87, "mention_end": 120, "dataset_mention": "the large-scale ImageNet dataset"}, {"mentioned_in_paper": "323", "context_id": "119", "dataset_context": "We perform search with a non-exhaustive approach on both CIFAR-10 and MNIST datasets.", "mention_start": 52, "mention_end": 84, "dataset_mention": "both CIFAR-10 and MNIST datasets"}, {"mentioned_in_paper": "323", "context_id": "122", "dataset_context": "In this set of experiments the proposed approach and its variants are compared on the SIFT1M (Table 2) and GIST1M (Table 3) datasets against four methods discussed in section 2: Product Quantization (ADC and IVFADC) [17], Cartesian k-means [29], LOPQ [21] and a non-exhaustive adaptation of OPQ [9], called I-OPQ [21].", "mention_start": 82, "mention_end": 132, "dataset_mention": "the SIFT1M (Table 2) and GIST1M (Table 3) datasets"}, {"mentioned_in_paper": "323", "context_id": "138", "dataset_context": "In the second component, we add a latent layer to the network and have neurons in this layer learn hashes-like representations while fine-tuning it on the target domain dataset.", "mention_start": 161, "mention_end": 176, "dataset_mention": "domain dataset"}, {"mentioned_in_paper": "323", "context_id": "154", "dataset_context": "To achieve domain adaptation, we fine-tune the proposed network on the target-domain dataset via back propagation.", "mention_start": 66, "mention_end": 92, "dataset_mention": "the target-domain dataset"}, {"mentioned_in_paper": "323", "context_id": "155", "dataset_context": "The initial weights of the deep CNN are set as the weights trained from ImageNet dataset.", "mention_start": 72, "mention_end": 88, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "323", "context_id": "162", "dataset_context": "In the second component, we add a latent layer to the network and have neurons in this layer learn hashes-like representations while fine-tuning it on the target domain dataset.", "mention_start": 161, "mention_end": 176, "dataset_mention": "domain dataset"}, {"mentioned_in_paper": "323", "context_id": "178", "dataset_context": "To achieve domain adaptation, we fine-tune the proposed network on the target-domain dataset via back propagation.", "mention_start": 66, "mention_end": 92, "dataset_mention": "the target-domain dataset"}, {"mentioned_in_paper": "323", "context_id": "179", "dataset_context": "The initial weights of the deep CNN are set as the weights trained from ImageNet dataset.", "mention_start": 72, "mention_end": 88, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "323", "context_id": "186", "dataset_context": "In this experiment we compare our method on the SIFT1B dataset (Table 4) against LOPQ and a sub-optimal variant LOR+PQ [21], a single index approaches IVFADC [17], I-OPQ [9] and a multi-index method Multi-D-ADC [2].", "mention_start": 44, "mention_end": 62, "dataset_mention": "the SIFT1B dataset"}, {"mentioned_in_paper": "323", "context_id": "189", "dataset_context": "In the experiments on CIFAR-10 [22] and MNIST [26] images dataset we use the the following configurations for the proposed method: hash code length of 48 bits (the same Table 4 : Recall@R on SIFT1B -Comparison between our method (m-k-means-t), the Product Quantization method [18], a non-exhaustive adaptation of the Optimized Product Quantization method (I-OPQ), a multi-index method (Multi-D-ADC), a Locally optimized product quantization method (LOPQ) with a sub-optimal variant (LOR+PQ) method R@1 R@10 R@100 IVFADC [18] 0.088 0.372 0.733 IVFADC [17] 0.106 0.379 0.748 ck-means [29] 0.084 0.288 0.637 I-OPQ [21] 0.114 0.399 0.777 Multi-D-ADC [2] 0.165 0.517 0.860 LOR+PQ [21] 0.183 0.565 0.889 LOPQ [21] 0.199 0.586 0.909 m-k-means-t 0.668 0.893 0.926 length used by the compared methods), arithmetic mean for the m-k-means-t variant, n = 24 for m-k-means-n.", "mention_start": 22, "mention_end": 65, "dataset_mention": "CIFAR-10 [22] and MNIST [26] images dataset"}, {"mentioned_in_paper": "323", "context_id": "196", "dataset_context": "On MNIST dataset the best results of our approach are comparable with the second best method [37], and anyway are not far from the best approach [27].", "mention_start": 3, "mention_end": 16, "dataset_mention": "MNIST dataset"}, {"mentioned_in_paper": "324", "context_id": "17", "dataset_context": "Recently, with the availability of powerful deep neural network architectures and large scale datasets, new data-driven approaches have been proposed for automatic captioning of images and have demonstrated intriguing performance [5, 21, 33, 35].", "mention_start": 34, "mention_end": 102, "dataset_mention": "powerful deep neural network architectures and large scale datasets"}, {"mentioned_in_paper": "324", "context_id": "32", "dataset_context": "Based on the Salicon dataset, the SAM model [7] uses an LSTM [12] network, which can attend to different salient regions in the image.", "mention_start": 9, "mention_end": 28, "dataset_mention": "the Salicon dataset"}, {"mentioned_in_paper": "324", "context_id": "66", "dataset_context": "For capgaze1, 1,000 images were selected from the Pascal-50S dataset [32], which provides 50 captions per image by humans and annotated semantic masks with 222 semantic categories (the same images as in sbugaze).", "mention_start": 46, "mention_end": 68, "dataset_mention": "the Pascal-50S dataset"}, {"mentioned_in_paper": "324", "context_id": "93", "dataset_context": "To check the quality of captions in our collected data, we compute the CIDEr [31] and METEOR [9] scores of the collected captions based on the ground truth in Pascal-50S dataset (50 sentences for each image).", "mention_start": 158, "mention_end": 177, "dataset_mention": "Pascal-50S dataset"}, {"mentioned_in_paper": "324", "context_id": "100", "dataset_context": "The sbugaze dataset contains gaze for a maximum of duration of 3s, in free-viewing condition, whereas in our experiments, subjects needed on average 6.79s to look and describe each image.", "mention_start": 0, "mention_end": 19, "dataset_mention": "The sbugaze dataset"}, {"mentioned_in_paper": "326", "context_id": "122", "dataset_context": "Experimental setting on Helen dataset is described in Sec. 4. For celebA dataset, we use the first 18, 000 images for training, and the following 100 images for evaluation.", "mention_start": 24, "mention_end": 37, "dataset_mention": "Helen dataset"}, {"mentioned_in_paper": "326", "context_id": "122", "dataset_context": "Experimental setting on Helen dataset is described in Sec. 4. For celebA dataset, we use the first 18, 000 images for training, and the following 100 images for evaluation.", "mention_start": 66, "mention_end": 80, "dataset_mention": "celebA dataset"}, {"mentioned_in_paper": "326", "context_id": "133", "dataset_context": "Training a basic FSRNet on Helen dataset takes \u223c6 hours on 1 Titan X GPU.", "mention_start": 27, "mention_end": 40, "dataset_mention": "Helen dataset"}, {"mentioned_in_paper": "328", "context_id": "5", "dataset_context": "We experimentally evaluate the 3D ResNets on the ActivityNet and Kinetics datasets.", "mention_start": 45, "mention_end": 82, "dataset_mention": "the ActivityNet and Kinetics datasets"}, {"mentioned_in_paper": "328", "context_id": "17", "dataset_context": "Recent large-scale video datasets, such as Kinetics [12], greatly contribute to improve the recognition performance of the 3D CNNs [2, 12].", "mention_start": 0, "mention_end": 33, "dataset_mention": "Recent large-scale video datasets"}, {"mentioned_in_paper": "328", "context_id": "27", "dataset_context": "We train the networks using the ActivityNet and Kinetics datasets and evaluate their recognition performance.", "mention_start": 28, "mention_end": 65, "dataset_mention": "the ActivityNet and Kinetics datasets"}, {"mentioned_in_paper": "328", "context_id": "37", "dataset_context": "In order to create a successful pretrained model like 2D CNNs trained on ImageNet [3], the Google DeepMind released the Kinetics human action video dataset [12].", "mention_start": 86, "mention_end": 155, "dataset_mention": " the Google DeepMind released the Kinetics human action video dataset"}, {"mentioned_in_paper": "328", "context_id": "38", "dataset_context": "The Kinetics dataset includes 300,000 or over trimmed videos and 400 categories.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The Kinetics dataset"}, {"mentioned_in_paper": "328", "context_id": "50", "dataset_context": "Tran et al. trained 3D CNNs, called C3D, using the Sports-1M dataset [11].", "mention_start": 46, "mention_end": 68, "dataset_mention": "the Sports-1M dataset"}, {"mentioned_in_paper": "328", "context_id": "73", "dataset_context": "Downsampling is performed by conv3 1, conv4 1, conv5 1 with a stride of 2. The dimension of last fully-connected layer is set for the Kinetics dataset (400 categories).", "mention_start": 129, "mention_end": 150, "dataset_mention": "the Kinetics dataset"}, {"mentioned_in_paper": "328", "context_id": "90", "dataset_context": "To train the 3D ResNets on the Kinetics dataset, we use SGD with a mini-batch size of 256 on 4 GPUs (NVIDIA TITAN X) using the training samples described above.", "mention_start": 27, "mention_end": 47, "dataset_mention": "the Kinetics dataset"}, {"mentioned_in_paper": "328", "context_id": "93", "dataset_context": "In preliminary experiments on the ActivityNet dataset, large learning rate and batch size was important to achieve good recognition performance.", "mention_start": 30, "mention_end": 53, "dataset_mention": "the ActivityNet dataset"}, {"mentioned_in_paper": "328", "context_id": "98", "dataset_context": "In the experiments, we used the ActivityNet (v1.3) [4] and Kinetics datasets [12].", "mention_start": 27, "mention_end": 76, "dataset_mention": "the ActivityNet (v1.3) [4] and Kinetics datasets"}, {"mentioned_in_paper": "328", "context_id": "99", "dataset_context": "The ActivityNet dataset provides samples from 200 human action classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The ActivityNet dataset"}, {"mentioned_in_paper": "328", "context_id": "102", "dataset_context": "Figure 2 : Training of the models on the ActivityNet dataset.", "mention_start": 36, "mention_end": 60, "dataset_mention": "the ActivityNet dataset"}, {"mentioned_in_paper": "328", "context_id": "105", "dataset_context": "The Kinetics dataset has 400 human action classes, and consists of 400 or more videos for each class.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The Kinetics dataset"}, {"mentioned_in_paper": "328", "context_id": "111", "dataset_context": "We first describe the preliminary experiment on the Ac-tivityNet dataset.", "mention_start": 48, "mention_end": 72, "dataset_mention": "the Ac-tivityNet dataset"}, {"mentioned_in_paper": "328", "context_id": "117", "dataset_context": "This result indicates that the ActivityNet dataset is too small to train the 3D ResNets from scratch.", "mention_start": 0, "mention_end": 50, "dataset_mention": "This result indicates that the ActivityNet dataset"}, {"mentioned_in_paper": "328", "context_id": "119", "dataset_context": "The relatively shallow architecture of the C3D and pretraining on the Sports-1M dataset prevent the C3D from overfitting.", "mention_start": 66, "mention_end": 87, "dataset_mention": "the Sports-1M dataset"}, {"mentioned_in_paper": "328", "context_id": "120", "dataset_context": "We then show the experiment on the Kinetics dataset.", "mention_start": 31, "mention_end": 51, "dataset_mention": "the Kinetics dataset"}, {"mentioned_in_paper": "328", "context_id": "126", "dataset_context": "In addition, the 3D ResNet is competitive to the C3D without pretraining on the Sports-1M dataset.", "mention_start": 75, "mention_end": 97, "dataset_mention": "the Sports-1M dataset"}, {"mentioned_in_paper": "328", "context_id": "127", "dataset_context": "These results indicate that the C3D is too shallow and the 3D ResNets are effective when using the Kinetics dataset.", "mention_start": 95, "mention_end": 115, "dataset_mention": "the Kinetics dataset"}, {"mentioned_in_paper": "328", "context_id": "138", "dataset_context": "We trained the 3D ResNets using the Kinetics dataset, which is a large-scale video datasets.", "mention_start": 32, "mention_end": 52, "dataset_mention": "the Kinetics dataset"}, {"mentioned_in_paper": "331", "context_id": "214", "dataset_context": "Microsoft Academia (MSA): The MSA dataset contains scientific publication records and the citation relationship between Table 1 : Summary of the datasets used: All the datasets used have both network structure and textual content in each node.", "mention_start": 25, "mention_end": 41, "dataset_mention": " The MSA dataset"}, {"mentioned_in_paper": "331", "context_id": "242", "dataset_context": "Both the plots depict the fast convergence of FSCNMF++ on Wikipedia dataset.", "mention_start": 58, "mention_end": 75, "dataset_mention": "Wikipedia dataset"}, {"mentioned_in_paper": "331", "context_id": "247", "dataset_context": "We use sparse matrix multiplication available in SciPy [12] to compute different powers of A. The runtime for different orders of FSCNMF++ to generate the embeddings on the Wikipedia dataset is shown in Figure 1c.", "mention_start": 169, "mention_end": 190, "dataset_mention": "the Wikipedia dataset"}, {"mentioned_in_paper": "331", "context_id": "263", "dataset_context": "FSCNMF remains to be the second best, except for the Wikipedia dataset where GraphSAGE outperforms FSCNMF.", "mention_start": 48, "mention_end": 70, "dataset_mention": "the Wikipedia dataset"}, {"mentioned_in_paper": "331", "context_id": "269", "dataset_context": "Also the content within a node in MSA dataset contains only the title of the paper and few keywords.", "mention_start": 34, "mention_end": 45, "dataset_mention": "MSA dataset"}, {"mentioned_in_paper": "331", "context_id": "271", "dataset_context": "Interestingly, due to the inherent flexibility of FSCNMF optimization formulation, both FSCNMF and FSCNMF++ are able to outperform all the baselines significantly even for the MSA dataset, when the content is noisy.", "mention_start": 171, "mention_end": 187, "dataset_mention": "the MSA dataset"}, {"mentioned_in_paper": "331", "context_id": "272", "dataset_context": "For example, the clustering accuracy of FSCNMF++ is 65.42% better than that of LINE (best among all the baselines for clustering MSA dataset).", "mention_start": 128, "mention_end": 140, "dataset_mention": "MSA dataset"}, {"mentioned_in_paper": "331", "context_id": "282", "dataset_context": "We have shown the visualization results for Pubmed-Diabetes and the MSA datasets in Figures 3 and 4 respectively.", "mention_start": 44, "mention_end": 80, "dataset_mention": "Pubmed-Diabetes and the MSA datasets"}, {"mentioned_in_paper": "331", "context_id": "287", "dataset_context": "For MSA dataset in Figure 4, FSCNMF++ is a clear winner for visualization, as the overlap between the three communities are minimum and they are well-separated compared to the visualizations by all the baseline approaches.", "mention_start": 4, "mention_end": 15, "dataset_mention": "MSA dataset"}, {"mentioned_in_paper": "331", "context_id": "291", "dataset_context": "We run the experiments on the Pubmed-Diabetes dataset for different training sizes ranging from 10% to 50%, repeating each such experiment 10 times.", "mention_start": 26, "mention_end": 53, "dataset_mention": "the Pubmed-Diabetes dataset"}, {"mentioned_in_paper": "331", "context_id": "312", "dataset_context": "From these plots it is clear that for Citeseer and Wikipedia datasets, content is more informative than structure.", "mention_start": 38, "mention_end": 69, "dataset_mention": "Citeseer and Wikipedia datasets"}, {"mentioned_in_paper": "332", "context_id": "22", "dataset_context": "3 Experiment Setting and Datasets", "mention_start": 0, "mention_end": 33, "dataset_mention": "3 Experiment Setting and Datasets"}, {"mentioned_in_paper": "332", "context_id": "29", "dataset_context": "We perform the experiments on two datasets -Boston house-prices and the MNIST handwritten digit dataset [9].", "mention_start": 30, "mention_end": 103, "dataset_mention": "two datasets -Boston house-prices and the MNIST handwritten digit dataset"}, {"mentioned_in_paper": "332", "context_id": "30", "dataset_context": "The Boston house-prices dataset 3 is available from Scikit-Learn's sklearn.datasets", "mention_start": 0, "mention_end": 31, "dataset_mention": "The Boston house-prices dataset"}, {"mentioned_in_paper": "332", "context_id": "30", "dataset_context": "The Boston house-prices dataset 3 is available from Scikit-Learn's sklearn.datasets", "mention_start": 52, "mention_end": 83, "dataset_mention": "Scikit-Learn's sklearn.datasets"}, {"mentioned_in_paper": "332", "context_id": "32", "dataset_context": "This package contains a few small standard datasets that do not require downloads of any file(s) from an external website.", "mention_start": 0, "mention_end": 51, "dataset_mention": "This package contains a few small standard datasets"}, {"mentioned_in_paper": "332", "context_id": "33", "dataset_context": "The MNIST dataset can be downloaded from http://yann.lecun.com/exdb/mnist/ .", "mention_start": 0, "mention_end": 17, "dataset_mention": "The MNIST dataset"}, {"mentioned_in_paper": "332", "context_id": "36", "dataset_context": "The MLPRegressor is used for predicting housing prices for the Boston dataset and the MLPClassifier is used for classification with the MNIST dataset.", "mention_start": 59, "mention_end": 77, "dataset_mention": "the Boston dataset"}, {"mentioned_in_paper": "332", "context_id": "36", "dataset_context": "The MLPRegressor is used for predicting housing prices for the Boston dataset and the MLPClassifier is used for classification with the MNIST dataset.", "mention_start": 132, "mention_end": 149, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "333", "context_id": "38", "dataset_context": "In particular, in the Wine3vs5 dataset, the recall of HADR is 66.7% better than that of DDAE.", "mention_start": 17, "mention_end": 38, "dataset_mention": "the Wine3vs5 dataset"}, {"mentioned_in_paper": "333", "context_id": "186", "dataset_context": "The other datasets are from the KEEL repository [29], where Wine3vs5 is used for wine quality VOLUME 4, 2016 prediction, and the abalone datasets are used to predict the age of abalone.", "mention_start": 120, "mention_end": 145, "dataset_mention": " and the abalone datasets"}, {"mentioned_in_paper": "334", "context_id": "42", "dataset_context": "The two proposed methods were assessed on a new collected hand tremor dataset, TIM-Tremor, containing both static and dynamic tasks.", "mention_start": 58, "mention_end": 77, "dataset_mention": "hand tremor dataset"}, {"mentioned_in_paper": "334", "context_id": "65", "dataset_context": "The authors evaluate the resulting system in two datasets that are generated from the single-hand datasets Dexter1 [27] and NYU hand pose dataset [33] by concatenating randomly selected left and right hand images.", "mention_start": 82, "mention_end": 106, "dataset_mention": "the single-hand datasets"}, {"mentioned_in_paper": "334", "context_id": "65", "dataset_context": "The authors evaluate the resulting system in two datasets that are generated from the single-hand datasets Dexter1 [27] and NYU hand pose dataset [33] by concatenating randomly selected left and right hand images.", "mention_start": 82, "mention_end": 145, "dataset_mention": "the single-hand datasets Dexter1 [27] and NYU hand pose dataset"}, {"mentioned_in_paper": "334", "context_id": "110", "dataset_context": "The trait \"Application\" is marked for works that solve real-world problems, \"Dataset\" for works that propose a new dataset, \"Egocentric\" for works that assume an egocentric observation of the hand, and \"Gesture\" for works that tackle the problem of recognizing hand gestures.", "mention_start": 75, "mention_end": 84, "dataset_mention": " \"Dataset"}, {"mentioned_in_paper": "336", "context_id": "136", "dataset_context": "These abovementioned observations are also in line with the results evaluated on the LFW dataset as reported in Table 2. Figure 3 shows the faces generated by AnonyGAN and the compared method CIAGAN [23], with the condition (the 1st column) and source (the 2nd column) images.", "mention_start": 81, "mention_end": 96, "dataset_mention": "the LFW dataset"}, {"mentioned_in_paper": "337", "context_id": "6", "dataset_context": "Firstly we have pretrained our model on Diabetic Retinopathy dataset that was published by California HealthCare Publication as a challenge in Kaggle as Diabetic Retinopathy Detection in 2015.", "mention_start": 40, "mention_end": 68, "dataset_mention": "Diabetic Retinopathy dataset"}, {"mentioned_in_paper": "337", "context_id": "41", "dataset_context": "-Firstly we train the model on the Diabetic Retinopathy Dataset as described above with 10000 epocs and in a few shot learning manner.", "mention_start": 31, "mention_end": 63, "dataset_mention": "the Diabetic Retinopathy Dataset"}, {"mentioned_in_paper": "338", "context_id": "9", "dataset_context": "We demonstrate with experiments on the CIFAR-10 dataset that our method, denominated Efficient progressive neural architecture search (EP-NAS), leads to increased search efficiency, while retaining competitiveness of found architectures.", "mention_start": 35, "mention_end": 55, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "338", "context_id": "23", "dataset_context": "Recently, progressive neural architecture search (PNAS) [15], one of the surrogate-based search methods, achieved state-of-the-art results on the CIFAR-10 dataset [11].", "mention_start": 141, "mention_end": 162, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "338", "context_id": "216", "dataset_context": "We demonstrate on the challenging CIFAR-10 dataset that our method is able to find very competitive architectures with results that are not far from the state-of-the-art.", "mention_start": 34, "mention_end": 50, "dataset_mention": "CIFAR-10 dataset"}, {"mentioned_in_paper": "339", "context_id": "46", "dataset_context": "The first column of Figure 1, contains two sample images from the UIUC Texture dataset [24], with their intensity histograms of the corresponding textures appearing directly above or beneath each texture.", "mention_start": 61, "mention_end": 86, "dataset_mention": "the UIUC Texture dataset"}, {"mentioned_in_paper": "339", "context_id": "283", "dataset_context": "The Cambridge hand gesture dataset consists of 900 image sequences of 3 primitive hand shapes (see Figure 11a) where each image sequence consists of around 60 frames of 3 different motions [23].", "mention_start": 0, "mention_end": 34, "dataset_mention": "The Cambridge hand gesture dataset"}, {"mentioned_in_paper": "340", "context_id": "7", "dataset_context": "Experiments demonstrate that GS-Net achieves the state-of-the-art performances on major datasets, 93.3% on ModelNet40, and are more robust to geometric transformations.", "mention_start": 82, "mention_end": 96, "dataset_mention": "major datasets"}, {"mentioned_in_paper": "340", "context_id": "47", "dataset_context": "\u2022 Our GS-Net achieves the state-of-the-art performances on major datasets, ModelNet40, ShapeNet Part.", "mention_start": 59, "mention_end": 73, "dataset_mention": "major datasets"}, {"mentioned_in_paper": "341", "context_id": "46", "dataset_context": "The system accuracy on the state-of-theart dataset such as MNIST and CIFAR-10 are measured.", "mention_start": 23, "mention_end": 50, "dataset_mention": "the state-of-theart dataset"}, {"mentioned_in_paper": "342", "context_id": "191", "dataset_context": "For the real-world tasks, we use 10 semi-supervised classification datasets used in Chien et al. [13] and two visual keypoint matching datasets used in Wang et al. [55].", "mention_start": 83, "mention_end": 143, "dataset_mention": "Chien et al. [13] and two visual keypoint matching datasets"}, {"mentioned_in_paper": "342", "context_id": "206", "dataset_context": "To test EHNN in real-world hypergraph learning, we use 10 transductive semisupervised node classification datasets [13].", "mention_start": 54, "mention_end": 114, "dataset_mention": "10 transductive semisupervised node classification datasets"}, {"mentioned_in_paper": "342", "context_id": "218", "dataset_context": "The Willow dataset consists of 256 images with 5 object categories.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The Willow dataset"}, {"mentioned_in_paper": "342", "context_id": "219", "dataset_context": "The PASCAL-VOC dataset contains 11,530 images with 20 object categories and is considered challenging due to the large variance in illumination and pose.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The PASCAL-VOC dataset"}, {"mentioned_in_paper": "342", "context_id": "426", "dataset_context": "We borrow two real image datasets, Willow ObjectClass [14] and PASCAL-VOC [9, 17] with Berkeley annotations (Table 7c), as well as provided train/test splitting pipelines from the repository.", "mention_start": 0, "mention_end": 33, "dataset_mention": "We borrow two real image datasets"}, {"mentioned_in_paper": "343", "context_id": "5", "dataset_context": "Specifically, we formulate a novel transfer learning based on learning to rank, which effectively transfers a model for automatic annotation of object location from an auxiliary dataset to a target dataset with completely unrelated object categories.", "mention_start": 164, "mention_end": 185, "dataset_mention": "an auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "6", "dataset_context": "We show that our approach outperforms existing state-of-the-art weakly supervised approach to annotating objects in the challenging VOC dataset.", "mention_start": 132, "mention_end": 143, "dataset_mention": "VOC dataset"}, {"mentioned_in_paper": "343", "context_id": "25", "dataset_context": "In this paper we utilise a fourth information cue (Fig. 1 (b)) which is typically neglected by other approaches: an auxiliary fully annotated dataset.", "mention_start": 112, "mention_end": 149, "dataset_mention": " an auxiliary fully annotated dataset"}, {"mentioned_in_paper": "343", "context_id": "27", "dataset_context": "However, these auxiliary datasets seem unhelpful since they often contain object categories that are unrelated to the target object category we wish to annotate.", "mention_start": 8, "mention_end": 33, "dataset_mention": " these auxiliary datasets"}, {"mentioned_in_paper": "343", "context_id": "28", "dataset_context": "For example, an auxiliary dataset might contain annotations of cars, birds, boats and person but a target object category might be cats and buses (Fig. 1(b)).", "mention_start": 12, "mention_end": 33, "dataset_mention": " an auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "36", "dataset_context": "We show that our novel transfer learning model outperforms the state-ofthe-art WSL approaches on the challenging PASCAL VOC 2007 dataset.", "mention_start": 113, "mention_end": 136, "dataset_mention": "PASCAL VOC 2007 dataset"}, {"mentioned_in_paper": "343", "context_id": "49", "dataset_context": "We show that our transfer learning approach using an auxiliary dataset can outperform annotation accuracy of these MIL methods without using either intra or inter-class information from target data.", "mention_start": 50, "mention_end": 70, "dataset_mention": "an auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "54", "dataset_context": "Notice that Deselaers et al. [5] also uses an auxiliary dataset for weakly supervised annotation.", "mention_start": 43, "mention_end": 63, "dataset_mention": "an auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "57", "dataset_context": "In our approach we have an auxiliary dataset, A, with a set of fully annotated images (Fig. 1(b)); each image contains an object with its location manually annotated by a bounding box.", "mention_start": 24, "mention_end": 44, "dataset_mention": "an auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "59", "dataset_context": "As in [5, 15], for all images in the auxiliary dataset A and the target image T we select the top N salient regions returned by the generic object detector proposed in [1] as potential object locations.", "mention_start": 32, "mention_end": 54, "dataset_mention": "the auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "63", "dataset_context": "Given that the object in the target image T is unrelated to the objects in the auxiliary dataset A we need to develop a feature that is independent of object category which can still be used to learn the ranking of salient regions based on the degree of ground truth overlap.", "mention_start": 75, "mention_end": 96, "dataset_mention": "the auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "75", "dataset_context": "That is for the auxiliary dataset we replace Eq. 1 with:", "mention_start": 12, "mention_end": 33, "dataset_mention": "the auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "76", "dataset_context": "Note that the ground truth annotation of the auxiliary dataset is still needed as it is used in Section 2.2 to determine the ranking order of di, j .", "mention_start": 41, "mention_end": 62, "dataset_mention": "the auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "96", "dataset_context": "For each salient region, we obtain the difference vector dT j per Eq. 4. All difference vectors dT j=1...N are then ranked by the RankSVM trained on the auxiliary dataset.", "mention_start": 148, "mention_end": 170, "dataset_mention": "the auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "99", "dataset_context": "Dataset -All experiments are conducted on the challenging VOC 2007 dataset [6].", "mention_start": 58, "mention_end": 74, "dataset_mention": "VOC 2007 dataset"}, {"mentioned_in_paper": "343", "context_id": "100", "dataset_context": "We use the AllView dataset defined in [15] for comparison and it consists of all 20 classes from the VOC 2007 training and validation set with no pose annotation.", "mention_start": 7, "mention_end": 26, "dataset_mention": "the AllView dataset"}, {"mentioned_in_paper": "343", "context_id": "101", "dataset_context": "For our transfer learning model, we randomly choose 10 classes as the auxiliary dataset.", "mention_start": 65, "mention_end": 87, "dataset_mention": "the auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "114", "dataset_context": "From Table 1 we can see that our ranking model outperforms the objectness measure by 8% because it uses the auxiliary dataset to learn the salient region ranking that best overlaps with the ground truth.", "mention_start": 104, "mention_end": 125, "dataset_mention": "the auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "116", "dataset_context": "MI-SVM only uses weak annotation information from the target classes compared to our method which uses a strongly annotated auxiliary dataset.", "mention_start": 103, "mention_end": 141, "dataset_mention": "a strongly annotated auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "117", "dataset_context": "The superior performance of our model suggests that transferrable information learned on the strongly annotated dataset is more useful even though it contains object classes unrelated to the target class.", "mention_start": 89, "mention_end": 119, "dataset_mention": "the strongly annotated dataset"}, {"mentioned_in_paper": "343", "context_id": "148", "dataset_context": "Alternative transfer learning methods -The strongly annotated auxiliary dataset can be used to learn alternative transfer learning models.", "mention_start": 0, "mention_end": 79, "dataset_mention": "Alternative transfer learning methods -The strongly annotated auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "150", "dataset_context": "All salient regions in the auxiliary dataset with greater than 50% overlap with the ground truth are put in the positive class and the rest in the negative class.", "mention_start": 23, "mention_end": 44, "dataset_mention": "the auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "158", "dataset_context": "Comparison to methods using single pose data -As a last comparison we evaluate the performance of our ranking model on the much simpler and smaller SingleView dataset used in [5, 12, 15].", "mention_start": 119, "mention_end": 166, "dataset_mention": "the much simpler and smaller SingleView dataset"}, {"mentioned_in_paper": "343", "context_id": "160", "dataset_context": "For SingleView we use the classes bird, car, cat, cow, dog and sheep as the auxiliary dataset per [5].", "mention_start": 71, "mention_end": 93, "dataset_mention": "the auxiliary dataset"}, {"mentioned_in_paper": "343", "context_id": "163", "dataset_context": "In this paper we presented a novel ranking based transfer learning method for object annotation, which effectively transfers a model for automatic annotation of object location from an auxiliary dataset to a target dataset with completely unrelated object categories.", "mention_start": 181, "mention_end": 202, "dataset_mention": "an auxiliary dataset"}, {"mentioned_in_paper": "345", "context_id": "71", "dataset_context": "I mainly tested this network on screenshots as the dataset was easily made from a simple nodejs application that takes screenshots after several predefined seconds and on the Flickr 8k images dataset (M.", "mention_start": 171, "mention_end": 199, "dataset_mention": "the Flickr 8k images dataset"}, {"mentioned_in_paper": "345", "context_id": "75", "dataset_context": "Some of the pictures of the dataset, test pictures and the generated pictures for encoding matching are given below:-  The dataset contained 4000 different images i.e. images were of no particular category, there were dogs, cats, trees, cars, rocks etc all different images were present.", "mention_start": 116, "mention_end": 130, "dataset_mention": "-  The dataset"}, {"mentioned_in_paper": "347", "context_id": "5", "dataset_context": "Compared to existing reconstruction approaches, our learned event-surface shows good flexibility and expressiveness on optical flow estimation on the MVSEC benchmark and it improves the state-of-the-art of event-based object classification on the N-Cars dataset.", "mention_start": 242, "mention_end": 261, "dataset_mention": "the N-Cars dataset"}, {"mentioned_in_paper": "347", "context_id": "111", "dataset_context": "We evaluated the model on the classification task using two publicly available event-based collections, namely the N-Cars [31] and the N-Caltech101 [23] datasets, which represent to date the most complex benchmarks for event-based classification.", "mention_start": 103, "mention_end": 161, "dataset_mention": " namely the N-Cars [31] and the N-Caltech101 [23] datasets"}, {"mentioned_in_paper": "347", "context_id": "114", "dataset_context": "The N-Caltech101 collection is an event-based conversion of the popular Caltech-101 [16] dataset obtained by moving an event-based camera in front of a still monitor showing one of the original RGB images.", "mention_start": 60, "mention_end": 96, "dataset_mention": "the popular Caltech-101 [16] dataset"}, {"mentioned_in_paper": "347", "context_id": "160", "dataset_context": "In Table 2a we compare Matrix-LSTM with ConvLSTM [30] for different choices of kernel size on the N-Cars [31] dataset using the Ev2Vid-ResNet18 backbone.", "mention_start": 94, "mention_end": 117, "dataset_mention": "the N-Cars [31] dataset"}, {"mentioned_in_paper": "347", "context_id": "188", "dataset_context": "Fusing event-data with lidar, IMU, motion capture and GPS sources, MVSEC is the first event-based dataset to provide a solid benchmark in real urban conditions.", "mention_start": 75, "mention_end": 105, "dataset_mention": "the first event-based dataset"}, {"mentioned_in_paper": "347", "context_id": "224", "dataset_context": "In Figure 3a we analyze the accuracy-vs-latency trade-off on the N-Cars dataset, as proposed in [31], using the ResNet18-Ev2Vid configuration.", "mention_start": 61, "mention_end": 79, "dataset_mention": "the N-Cars dataset"}, {"mentioned_in_paper": "350", "context_id": "62", "dataset_context": "All prototype DNNs are fast trained (20 epochs) directly on the targeted dataset for accuracy results.", "mention_start": 60, "mention_end": 80, "dataset_mention": "the targeted dataset"}, {"mentioned_in_paper": "352", "context_id": "42", "dataset_context": "Finally, thorough experiments are conducted on challenging instructional video datasets [2], [16].", "mention_start": 58, "mention_end": 87, "dataset_mention": "instructional video datasets"}, {"mentioned_in_paper": "352", "context_id": "50", "dataset_context": "Our contributions can be summarized as follows: i) a new fast-forward method based on a reinforcement learning formulation, which is able to accelerate videos according to clip similarity scores with textual data subject to a target speed-up rate; ii) a new state representation and joint reward function that guide our agent to attend the multiple goals, i.e., semantics and speed-up; iii) an extended version of the VDAN, the VDAN+, which can create a joint embedding space for documents and video clips, instead of static frames and; iv) extensive experiments with in-depth discussions, including an ablation study, using the YouCook2 and COIN datasets as opposed to a small subset of YouCook2 used in our previous work.", "mention_start": 624, "mention_end": 655, "dataset_mention": "the YouCook2 and COIN datasets"}, {"mentioned_in_paper": "352", "context_id": "205", "dataset_context": "We conducted our experiments on the YouCook2 and COIN datasets [2], [16].", "mention_start": 32, "mention_end": 62, "dataset_mention": "the YouCook2 and COIN datasets"}, {"mentioned_in_paper": "352", "context_id": "229", "dataset_context": "During experiments in the COIN dataset, the FFNet agent did not converge, acting as a uniform selection method due to the high variability of labels from different domains.", "mention_start": 22, "mention_end": 38, "dataset_mention": "the COIN dataset"}, {"mentioned_in_paper": "352", "context_id": "233", "dataset_context": "Therefore, we first train VDAN+ using pairs of documents and clips from the VaTeX dataset [58].", "mention_start": 71, "mention_end": 89, "dataset_mention": "the VaTeX dataset"}, {"mentioned_in_paper": "352", "context_id": "247", "dataset_context": "We ran a grid search in the YouCook2 dataset to find the best policy learning rate lr \u2208 {1e\u22125, 5e\u22125, 1e\u22124}, \u03c3 \u2208 {0.5, 1, 2}, and \u03b3 \u2208 {0.8, 0.9, 0.99}, with \u03bb = F * , where F * is the desired number of frames.", "mention_start": 24, "mention_end": 44, "dataset_mention": "the YouCook2 dataset"}, {"mentioned_in_paper": "352", "context_id": "265", "dataset_context": "I.e., S * = 12 for the YouCook2 dataset and S * = 16 (on average) for the COIN dataset since S * \u2208 N + .", "mention_start": 18, "mention_end": 39, "dataset_mention": "the YouCook2 dataset"}, {"mentioned_in_paper": "352", "context_id": "265", "dataset_context": "I.e., S * = 12 for the YouCook2 dataset and S * = 16 (on average) for the COIN dataset since S * \u2208 N + .", "mention_start": 69, "mention_end": 86, "dataset_mention": "the COIN dataset"}, {"mentioned_in_paper": "352", "context_id": "398", "dataset_context": "In the YouCook2 dataset, we adapted the SAS and SASv2 methods to use foodrelated content as the semantic input.", "mention_start": 3, "mention_end": 23, "dataset_mention": "the YouCook2 dataset"}, {"mentioned_in_paper": "352", "context_id": "400", "dataset_context": "In the COIN dataset, we used a different filtering strategy.", "mention_start": 3, "mention_end": 19, "dataset_mention": "the COIN dataset"}, {"mentioned_in_paper": "352", "context_id": "411", "dataset_context": "Figure 1 presents extra qualitative results regarding the main text composed of 2 videos in each test set of YouCook2 and COIN datasets.", "mention_start": 109, "mention_end": 135, "dataset_mention": "YouCook2 and COIN datasets"}, {"mentioned_in_paper": "353", "context_id": "105", "dataset_context": "DL is a machine learning method that differs from others in that the trainer no longer has to label the dataset; instead, the machine actively learns the dataset's features.", "mention_start": 121, "mention_end": 161, "dataset_mention": " the machine actively learns the dataset"}, {"mentioned_in_paper": "354", "context_id": "12", "dataset_context": "In KITTI [1] dataset, Velodyne HDL-64E LiDAR has been equipped to collect point clouds of surroundings.", "mention_start": 3, "mention_end": 20, "dataset_mention": "KITTI [1] dataset"}, {"mentioned_in_paper": "354", "context_id": "14", "dataset_context": "On the contrary, person, rider, vehicular, and bicycles are less than 1% in most frames according to SemanticKITTI [2] dataset.", "mention_start": 100, "mention_end": 126, "dataset_mention": "SemanticKITTI [2] dataset"}, {"mentioned_in_paper": "354", "context_id": "45", "dataset_context": "In Section II, we review related work; Section III describes the proposed method and implementation process; Evaluation on semanticKITTI dataset and ablation study are shown in Section IV; Finally, concluding remarks and future work follow in Section V.", "mention_start": 122, "mention_end": 144, "dataset_mention": "semanticKITTI dataset"}, {"mentioned_in_paper": "354", "context_id": "74", "dataset_context": "Paigwar [7] adopts an analogous way but uses a CRF-based elevation map as ground truth to learn each pillar's height, which can estimate and model ground at a speed of 55 Hz on KITTI dataset.", "mention_start": 176, "mention_end": 190, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "354", "context_id": "133", "dataset_context": "KITTI dataset is one of the most popular public segmentation datasets.", "mention_start": 0, "mention_end": 13, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "354", "context_id": "133", "dataset_context": "KITTI dataset is one of the most popular public segmentation datasets.", "mention_start": 24, "mention_end": 69, "dataset_mention": "the most popular public segmentation datasets"}, {"mentioned_in_paper": "354", "context_id": "134", "dataset_context": "On the base of that, SemanticKITTI contains 11 sequences of the KITTI dataset with 28 classes labeled data.", "mention_start": 59, "mention_end": 77, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "354", "context_id": "162", "dataset_context": "Next, we proved that dataset augmentation with normal features permits the better performance of the classification network.", "mention_start": 5, "mention_end": 28, "dataset_mention": " we proved that dataset"}, {"mentioned_in_paper": "355", "context_id": "229", "dataset_context": "The objective function that should be optimized by the tools consists of training a Transformer model [16] on the IMDB sentiment analysis dataset [13].", "mention_start": 110, "mention_end": 145, "dataset_mention": "the IMDB sentiment analysis dataset"}, {"mentioned_in_paper": "355", "context_id": "230", "dataset_context": "Our second experiment concerns the training of an LSTM network on the Walker2D kinematics modelling dataset [11].", "mention_start": 66, "mention_end": 107, "dataset_mention": "the Walker2D kinematics modelling dataset"}, {"mentioned_in_paper": "355", "context_id": "232", "dataset_context": "Random seed was fixed for the training (weight initialization and dataset shuffling).", "mention_start": 39, "mention_end": 73, "dataset_mention": "(weight initialization and dataset"}, {"mentioned_in_paper": "357", "context_id": "54", "dataset_context": "Yom-Tov et al. [36] query different datasets and compute the query's difficulty for each dataset.", "mention_start": 0, "mention_end": 44, "dataset_mention": "Yom-Tov et al. [36] query different datasets"}, {"mentioned_in_paper": "358", "context_id": "117", "dataset_context": "As illustrated in Figure 3b, the observation set ( X, \u0176 ) is a subset of MixUp-augmented dataset (X 0 , Y 0 ) and the latter on has greater cardinality.", "mention_start": 72, "mention_end": 96, "dataset_mention": "MixUp-augmented dataset"}, {"mentioned_in_paper": "358", "context_id": "127", "dataset_context": "As illustrated in Figure 4, given a high-quality diverse HR dataset (e.g.", "mention_start": 33, "mention_end": 67, "dataset_mention": "a high-quality diverse HR dataset"}, {"mentioned_in_paper": "358", "context_id": "160", "dataset_context": "We mainly train our models on the new Real-SR dataset, denoted as RealSR dataset below.", "mention_start": 30, "mention_end": 53, "dataset_mention": "the new Real-SR dataset"}, {"mentioned_in_paper": "358", "context_id": "160", "dataset_context": "We mainly train our models on the new Real-SR dataset, denoted as RealSR dataset below.", "mention_start": 65, "mention_end": 80, "dataset_mention": "RealSR dataset"}, {"mentioned_in_paper": "358", "context_id": "161", "dataset_context": "The default splits of Re-alSR dataset consist of 60 training images, 20 validation images and 20 test images.", "mention_start": 22, "mention_end": 37, "dataset_mention": "Re-alSR dataset"}, {"mentioned_in_paper": "358", "context_id": "164", "dataset_context": "3.4, we also include a prevalent DIV2K dataset [2] as additional training data, since these images cover diverse contents, including objects, environments, animals, natural scenery, etc.", "mention_start": 20, "mention_end": 46, "dataset_mention": "a prevalent DIV2K dataset"}, {"mentioned_in_paper": "358", "context_id": "166", "dataset_context": "To prepare training data, we first crop the HR images into a set of 480 \u00d7 480 sub-images with a stride 240 for DIV2K dataset.", "mention_start": 110, "mention_end": 124, "dataset_mention": "DIV2K dataset"}, {"mentioned_in_paper": "358", "context_id": "167", "dataset_context": "Similarly, we crop HR images into subimages of size 200 \u00d7 200 and stride 100 for RealSR dataset.", "mention_start": 80, "mention_end": 95, "dataset_mention": "RealSR dataset"}, {"mentioned_in_paper": "358", "context_id": "168", "dataset_context": "In this manner we have totally 12, 837 and 32, 208 subimages from RealSR and DIV2K dataset, respectively.", "mention_start": 65, "mention_end": 90, "dataset_mention": "RealSR and DIV2K dataset"}, {"mentioned_in_paper": "358", "context_id": "182", "dataset_context": "In the scope of this section, we mainly use 12, 837 subimage pairs from RealSR dataset as the observation set and 32, 208 HR sub-images from DIV2K dataset for data synthesis.", "mention_start": 71, "mention_end": 86, "dataset_mention": "RealSR dataset"}, {"mentioned_in_paper": "358", "context_id": "182", "dataset_context": "In the scope of this section, we mainly use 12, 837 subimage pairs from RealSR dataset as the observation set and 32, 208 HR sub-images from DIV2K dataset for data synthesis.", "mention_start": 140, "mention_end": 154, "dataset_mention": "DIV2K dataset"}, {"mentioned_in_paper": "358", "context_id": "193", "dataset_context": "We reimplement these state-ofthe-art methods on RealSR dataset.", "mention_start": 48, "mention_end": 62, "dataset_mention": "RealSR dataset"}, {"mentioned_in_paper": "358", "context_id": "210", "dataset_context": "With a sufficiently large dataset comprising \u2022 Add White Gaussian noise with \u03c3 = 25 to HR images.", "mention_start": 5, "mention_end": 33, "dataset_mention": "a sufficiently large dataset"}, {"mentioned_in_paper": "358", "context_id": "213", "dataset_context": "The corresponding data pairs constitute a synthetic dataset, where we will refer to these augmentation set as X1 [13], CARN [3], RRDB [41], RCAN [45] and our method on validation dataset.", "mention_start": 0, "mention_end": 59, "dataset_mention": "The corresponding data pairs constitute a synthetic dataset"}, {"mentioned_in_paper": "359", "context_id": "8", "dataset_context": "We evaluate our AFN on four different types of vascular datasets: X-ray angiography coronary vessel dataset (XCAD), portal vein dataset (PV), digital subtraction angiography cerebrovascular vessel dataset (DSA) and retinal vessel dataset (DRIVE).", "mention_start": 47, "mention_end": 64, "dataset_mention": "vascular datasets"}, {"mentioned_in_paper": "359", "context_id": "8", "dataset_context": "We evaluate our AFN on four different types of vascular datasets: X-ray angiography coronary vessel dataset (XCAD), portal vein dataset (PV), digital subtraction angiography cerebrovascular vessel dataset (DSA) and retinal vessel dataset (DRIVE).", "mention_start": 65, "mention_end": 107, "dataset_mention": " X-ray angiography coronary vessel dataset"}, {"mentioned_in_paper": "359", "context_id": "8", "dataset_context": "We evaluate our AFN on four different types of vascular datasets: X-ray angiography coronary vessel dataset (XCAD), portal vein dataset (PV), digital subtraction angiography cerebrovascular vessel dataset (DSA) and retinal vessel dataset (DRIVE).", "mention_start": 115, "mention_end": 135, "dataset_mention": " portal vein dataset"}, {"mentioned_in_paper": "359", "context_id": "8", "dataset_context": "We evaluate our AFN on four different types of vascular datasets: X-ray angiography coronary vessel dataset (XCAD), portal vein dataset (PV), digital subtraction angiography cerebrovascular vessel dataset (DSA) and retinal vessel dataset (DRIVE).", "mention_start": 141, "mention_end": 204, "dataset_mention": " digital subtraction angiography cerebrovascular vessel dataset"}, {"mentioned_in_paper": "359", "context_id": "8", "dataset_context": "We evaluate our AFN on four different types of vascular datasets: X-ray angiography coronary vessel dataset (XCAD), portal vein dataset (PV), digital subtraction angiography cerebrovascular vessel dataset (DSA) and retinal vessel dataset (DRIVE).", "mention_start": 141, "mention_end": 237, "dataset_mention": " digital subtraction angiography cerebrovascular vessel dataset (DSA) and retinal vessel dataset"}, {"mentioned_in_paper": "359", "context_id": "50", "dataset_context": "We evaluate our method on two public datasets, i.e., the X-ray angiography coronary vessel dataset (XACD) [25], the retinal vessel dataset (DRIVE) [26], and two in-house datasets, i.e., a digital subtraction angiography cerebrovascular vessel dataset (DSA) and a portal vein dataset (PV).", "mention_start": 52, "mention_end": 98, "dataset_mention": " the X-ray angiography coronary vessel dataset"}, {"mentioned_in_paper": "359", "context_id": "50", "dataset_context": "We evaluate our method on two public datasets, i.e., the X-ray angiography coronary vessel dataset (XACD) [25], the retinal vessel dataset (DRIVE) [26], and two in-house datasets, i.e., a digital subtraction angiography cerebrovascular vessel dataset (DSA) and a portal vein dataset (PV).", "mention_start": 111, "mention_end": 138, "dataset_mention": " the retinal vessel dataset"}, {"mentioned_in_paper": "359", "context_id": "50", "dataset_context": "We evaluate our method on two public datasets, i.e., the X-ray angiography coronary vessel dataset (XACD) [25], the retinal vessel dataset (DRIVE) [26], and two in-house datasets, i.e., a digital subtraction angiography cerebrovascular vessel dataset (DSA) and a portal vein dataset (PV).", "mention_start": 152, "mention_end": 178, "dataset_mention": " and two in-house datasets"}, {"mentioned_in_paper": "359", "context_id": "50", "dataset_context": "We evaluate our method on two public datasets, i.e., the X-ray angiography coronary vessel dataset (XACD) [25], the retinal vessel dataset (DRIVE) [26], and two in-house datasets, i.e., a digital subtraction angiography cerebrovascular vessel dataset (DSA) and a portal vein dataset (PV).", "mention_start": 185, "mention_end": 250, "dataset_mention": " a digital subtraction angiography cerebrovascular vessel dataset"}, {"mentioned_in_paper": "359", "context_id": "50", "dataset_context": "We evaluate our method on two public datasets, i.e., the X-ray angiography coronary vessel dataset (XACD) [25], the retinal vessel dataset (DRIVE) [26], and two in-house datasets, i.e., a digital subtraction angiography cerebrovascular vessel dataset (DSA) and a portal vein dataset (PV).", "mention_start": 185, "mention_end": 282, "dataset_mention": " a digital subtraction angiography cerebrovascular vessel dataset (DSA) and a portal vein dataset"}, {"mentioned_in_paper": "359", "context_id": "154", "dataset_context": "XCAD: An X-ray angiography coronary artery disease (XCAD) dataset which include coronary angiography images obtained during stent placement using a General Electric Innova IGS 520 system [25].", "mention_start": 5, "mention_end": 65, "dataset_mention": " An X-ray angiography coronary artery disease (XCAD) dataset"}, {"mentioned_in_paper": "359", "context_id": "159", "dataset_context": "DRIVE: The DRIVE dataset [26] is a retinal vessel segmentation dataset consisting of 40 color retinal images with the same resolution as 565\u00d7584 pixels.", "mention_start": 6, "mention_end": 24, "dataset_mention": " The DRIVE dataset"}, {"mentioned_in_paper": "359", "context_id": "159", "dataset_context": "DRIVE: The DRIVE dataset [26] is a retinal vessel segmentation dataset consisting of 40 color retinal images with the same resolution as 565\u00d7584 pixels.", "mention_start": 32, "mention_end": 70, "dataset_mention": "a retinal vessel segmentation dataset"}, {"mentioned_in_paper": "359", "context_id": "161", "dataset_context": "Portal Vein: Portal vein vessel dataset (PV) is an inhouse dataset containing 32 patient cases.", "mention_start": 12, "mention_end": 39, "dataset_mention": " Portal vein vessel dataset"}, {"mentioned_in_paper": "359", "context_id": "161", "dataset_context": "Portal Vein: Portal vein vessel dataset (PV) is an inhouse dataset containing 32 patient cases.", "mention_start": 47, "mention_end": 66, "dataset_mention": "an inhouse dataset"}, {"mentioned_in_paper": "359", "context_id": "164", "dataset_context": "For a fair evaluation, we randomly divided the dataset into 24 cases for training and 8 cases for testing.", "mention_start": 22, "mention_end": 54, "dataset_mention": " we randomly divided the dataset"}, {"mentioned_in_paper": "359", "context_id": "166", "dataset_context": "DSA vessel: This dataset is an in-house dataset consisting of 20 slices with the same resolution of 512\u00d7512 pixels from different phases in digital subtraction angiography cerebrovascular vessel.", "mention_start": 27, "mention_end": 47, "dataset_mention": "an in-house dataset"}, {"mentioned_in_paper": "359", "context_id": "170", "dataset_context": "The in-house PV and DSA datasets have been reviewed and approved by the institutional review boards of the Medical Ethics Committee of the Union Hospital, Tongji Medical College, Huazhong University of Science and Technology.", "mention_start": 0, "mention_end": 32, "dataset_mention": "The in-house PV and DSA datasets"}, {"mentioned_in_paper": "359", "context_id": "174", "dataset_context": "In our experiments, the threshold is set to 1 for the DRIVE dataset and 2 for other datasets.", "mention_start": 49, "mention_end": 67, "dataset_mention": "the DRIVE dataset"}, {"mentioned_in_paper": "359", "context_id": "179", "dataset_context": "We compare our AFN with various state-of-the-art methods on three-vessel datasets, including XCAD, DRIVE and PV.", "mention_start": 60, "mention_end": 81, "dataset_mention": "three-vessel datasets"}, {"mentioned_in_paper": "359", "context_id": "181", "dataset_context": "Table I shows the performance of AFN and alternatives on the XCAD, DRIVE, and PV datasets respectively.", "mention_start": 73, "mention_end": 89, "dataset_mention": " and PV datasets"}, {"mentioned_in_paper": "359", "context_id": "183", "dataset_context": "Specifically, AFN improves the Quality score by up to 7.03% and the F1 score by up to 3.72% compared to the second-best approach on the XCAD dataset.", "mention_start": 131, "mention_end": 148, "dataset_mention": "the XCAD dataset"}, {"mentioned_in_paper": "359", "context_id": "187", "dataset_context": "To validate the effectiveness of each component in AFN, we conduct ablation studies on the XCAD dataset, including Base:", "mention_start": 86, "mention_end": 103, "dataset_mention": "the XCAD dataset"}, {"mentioned_in_paper": "359", "context_id": "205", "dataset_context": "To this end, we set the vessels thinner than seven pixels as thin vessels and the rest vessels as thick vessels for the XCAD dataset.", "mention_start": 115, "mention_end": 132, "dataset_mention": "the XCAD dataset"}, {"mentioned_in_paper": "359", "context_id": "233", "dataset_context": "To further demonstrate the generalizability of the proposed AFN, we apply the models trained on XCAD to the DSA dataset for testing.", "mention_start": 103, "mention_end": 119, "dataset_mention": "the DSA dataset"}, {"mentioned_in_paper": "359", "context_id": "246", "dataset_context": "Besides, the cross-dataset evaluation also shows the impressive generalizability performance of AFN.", "mention_start": 8, "mention_end": 26, "dataset_mention": " the cross-dataset"}, {"mentioned_in_paper": "361", "context_id": "101", "dataset_context": "Suppose we are generating recommendations for a movies dataset D, containing n attributes A = {a 1 , ..., a n }, such as movie title, creative type, gross, release date, etc.", "mention_start": 46, "mention_end": 62, "dataset_mention": "a movies dataset"}, {"mentioned_in_paper": "361", "context_id": "155", "dataset_context": "Oracles often take as input the user's recent history of visualizations created and interactions performed, as well as statistics about the current dataset.", "mention_start": 135, "mention_end": 155, "dataset_mention": "the current dataset"}, {"mentioned_in_paper": "361", "context_id": "258", "dataset_context": "It shows the name of the current dataset and presents a list of all data fields within the dataset.", "mention_start": 21, "mention_end": 40, "dataset_mention": "the current dataset"}, {"mentioned_in_paper": "361", "context_id": "274", "dataset_context": "The study followed a 4 (recommendation algorithms) \u00d7 2 (dataset) mixed design, thus in total there are 8 designs.", "mention_start": 10, "mention_end": 63, "dataset_mention": "followed a 4 (recommendation algorithms) \u00d7 2 (dataset"}, {"mentioned_in_paper": "361", "context_id": "275", "dataset_context": "We utilized a betweensubjects study design; each participant only conducted one analysis session, with a random combination of recommendation algorithm and dataset.", "mention_start": 126, "mention_end": 163, "dataset_mention": "recommendation algorithm and dataset"}, {"mentioned_in_paper": "361", "context_id": "280", "dataset_context": "We utilized two Voyager [36] datasets for the evaluation: movies and birdstrikes.", "mention_start": 12, "mention_end": 37, "dataset_mention": "two Voyager [36] datasets"}, {"mentioned_in_paper": "361", "context_id": "281", "dataset_context": "The movies 1 dataset contains 3,201 records and 15 attributes (7 nominal, 1 temporal, 8 quantitative).", "mention_start": 0, "mention_end": 20, "dataset_mention": "The movies 1 dataset"}, {"mentioned_in_paper": "361", "context_id": "282", "dataset_context": "The birdstrikes 2 dataset is a redacted version of FAA wildlife airplane strike records with 10,000 records and 14 attributes (9 nominal, 1 temporal, 4 quantitative).", "mention_start": 0, "mention_end": 25, "dataset_mention": "The birdstrikes 2 dataset"}, {"mentioned_in_paper": "361", "context_id": "293", "dataset_context": "\u2022 Confidence in Understanding Data: I am confident in understanding the dataset.", "mention_start": 53, "mention_end": 79, "dataset_mention": "understanding the dataset"}, {"mentioned_in_paper": "361", "context_id": "309", "dataset_context": "T3 provides a particular direction for the data exploration, while T4 asks participants to freely explore the dataset.", "mention_start": 90, "mention_end": 117, "dataset_mention": "freely explore the dataset"}, {"mentioned_in_paper": "361", "context_id": "327", "dataset_context": "We show posterior distributions, 50% and 95% CIs of expected titer thresholds for both the Movies and Birdstrikes dataset.", "mention_start": 81, "mention_end": 121, "dataset_mention": "both the Movies and Birdstrikes dataset"}, {"mentioned_in_paper": "361", "context_id": "336", "dataset_context": "Although not significant, it generally takes less time for participants to complete tasks with the Movies dataset than the Birdstrikes one.", "mention_start": 94, "mention_end": 113, "dataset_mention": "the Movies dataset"}, {"mentioned_in_paper": "361", "context_id": "337", "dataset_context": "On the other hand, the accuracy with the Movies dataset is also slightly higher.", "mention_start": 36, "mention_end": 55, "dataset_mention": "the Movies dataset"}, {"mentioned_in_paper": "361", "context_id": "348", "dataset_context": "We also find that participants were exposed to slightly more unique variable sets and visual designs in the exploration task than in the prediction task, which is reasonable since the exploration task encourages participants to explore the dataset freely while the prediction task restrains a direction for the data exploration.", "mention_start": 227, "mention_end": 247, "dataset_mention": "explore the dataset"}, {"mentioned_in_paper": "361", "context_id": "351", "dataset_context": "In the prediction task, BFS exposed significantly more variable sets with both the Birdstrikes dataset (b = 11.746) and the Movies dataset (b = 14.163).", "mention_start": 73, "mention_end": 102, "dataset_mention": "both the Birdstrikes dataset"}, {"mentioned_in_paper": "361", "context_id": "351", "dataset_context": "In the prediction task, BFS exposed significantly more variable sets with both the Birdstrikes dataset (b = 11.746) and the Movies dataset (b = 14.163).", "mention_start": 73, "mention_end": 138, "dataset_mention": "both the Birdstrikes dataset (b = 11.746) and the Movies dataset"}, {"mentioned_in_paper": "361", "context_id": "355", "dataset_context": "BFS exposed significantly more visual designs with both the Birdstrikes dataset (b = 13.918) and the Movies dataset (b = 16.822) in the prediction task, while in the exploration task, BFS exposed (b = 16.813)", "mention_start": 51, "mention_end": 79, "dataset_mention": "both the Birdstrikes dataset"}, {"mentioned_in_paper": "361", "context_id": "355", "dataset_context": "BFS exposed significantly more visual designs with both the Birdstrikes dataset (b = 13.918) and the Movies dataset (b = 16.822) in the prediction task, while in the exploration task, BFS exposed (b = 16.813)", "mention_start": 51, "mention_end": 115, "dataset_mention": "both the Birdstrikes dataset (b = 13.918) and the Movies dataset"}, {"mentioned_in_paper": "361", "context_id": "356", "dataset_context": "more with the Birdstrikes dataset and (b = 20.316)", "mention_start": 10, "mention_end": 33, "dataset_mention": "the Birdstrikes dataset"}, {"mentioned_in_paper": "361", "context_id": "357", "dataset_context": "more visualizations with the Movies dataset.", "mention_start": 25, "mention_end": 43, "dataset_mention": "the Movies dataset"}, {"mentioned_in_paper": "361", "context_id": "359", "dataset_context": "Dziban exposed significantly more visual designs than CompassQL with both the Movies and the Birdstrikes dataset in both tasks.", "mention_start": 69, "mention_end": 112, "dataset_mention": "both the Movies and the Birdstrikes dataset"}, {"mentioned_in_paper": "361", "context_id": "403", "dataset_context": "One said that \"The tool helps explore datasets and provides useful recommendations in terms of related measures and dimensions to enable getting useful insights.\"", "mention_start": 0, "mention_end": 46, "dataset_mention": "One said that \"The tool helps explore datasets"}, {"mentioned_in_paper": "362", "context_id": "37", "dataset_context": "In the automatic mode (no annotator in the loop), after directly applying our model to automatically re-annotate KITTI dataset [1], re-trained PointPillars [2] and PointRCNN [13] can maintain more than 94% of their original performance.", "mention_start": 86, "mention_end": 126, "dataset_mention": "automatically re-annotate KITTI dataset"}, {"mentioned_in_paper": "362", "context_id": "73", "dataset_context": "However, creating a high-quality 3D object detection dataset is more complex than creating, for example, a 2D object detection dataset.", "mention_start": 17, "mention_end": 60, "dataset_mention": "a high-quality 3D object detection dataset"}, {"mentioned_in_paper": "362", "context_id": "254", "dataset_context": "Extensive experiments on KITTI dataset demonstrate our impressive results, but also illustrate that there is still room for improvement.", "mention_start": 25, "mention_end": 38, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "363", "context_id": "41", "dataset_context": "The two models were trained on the MNIST dataset with varying latent dimensions, while other parameters were kept the same.", "mention_start": 31, "mention_end": 48, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "363", "context_id": "113", "dataset_context": "Taking the complete dataset into consideration, the joint probability of the training data can be expressed as, p(X , Z, \u03b1) = p(X |Z, \u03b1)p(Z|\u03b1)p(\u03b1) and the probability of the training data is p(X ) = N n=1 p(x n ).", "mention_start": 0, "mention_end": 27, "dataset_mention": "Taking the complete dataset"}, {"mentioned_in_paper": "363", "context_id": "133", "dataset_context": "We use the circle and one-moon datasets for proof-of-concept experiments, both of which exhibit an intrinsic dimensionality of one, parameterized by the radius.", "mention_start": 7, "mention_end": 39, "dataset_mention": "the circle and one-moon datasets"}, {"mentioned_in_paper": "363", "context_id": "138", "dataset_context": "Figure 4 shows the results for one-moon and circle datasets with additive Gaussian noise of zero-mean and 10% of the radius of the data manifold as standard deviation.", "mention_start": 31, "mention_end": 59, "dataset_mention": "one-moon and circle datasets"}, {"mentioned_in_paper": "363", "context_id": "202", "dataset_context": "For toy datasets, the decoders were mirrored versions of the encoders.", "mention_start": 4, "mention_end": 16, "dataset_mention": "toy datasets"}, {"mentioned_in_paper": "363", "context_id": "214", "dataset_context": "Figure 10 compares the performance of the various models on the dSprites dataset.", "mention_start": 60, "mention_end": 80, "dataset_mention": "the dSprites dataset"}, {"mentioned_in_paper": "363", "context_id": "215", "dataset_context": "Figure 11 shows the results of R-dpVAE on CELEBA dataset for L = 100 and L = 128.", "mention_start": 42, "mention_end": 56, "dataset_mention": "CELEBA dataset"}, {"mentioned_in_paper": "365", "context_id": "9", "dataset_context": "Such capability enables us to pretrain a vision transformer on a large-scale upstream dataset.", "mention_start": 63, "mention_end": 93, "dataset_mention": "a large-scale upstream dataset"}, {"mentioned_in_paper": "365", "context_id": "95", "dataset_context": "As shown in Figure 2 (a) and (d), in the first stage, vision transformers are pretrained on a large-scale upstream dataset.", "mention_start": 91, "mention_end": 122, "dataset_mention": "a large-scale upstream dataset"}, {"mentioned_in_paper": "366", "context_id": "121", "dataset_context": "We are currently working on optimizing our code, and expanding our data set to thoroughly test our method on more outpatients and surgical cases.", "mention_start": 48, "mention_end": 75, "dataset_mention": " and expanding our data set"}, {"mentioned_in_paper": "367", "context_id": "93", "dataset_context": "We evaluate our knowledge distillation approach on COCO2017 dataset (Lin et al. 2014), a standard and commonly used dataset for object detection, which contains 80 object categories.", "mention_start": 51, "mention_end": 67, "dataset_mention": "COCO2017 dataset"}, {"mentioned_in_paper": "368", "context_id": "134", "dataset_context": "Since there is no ground truth for the instance vector and mask in the KITTI object dataset [32], we calculate the label with the aid of raw LiDAR point cloud and the ground-truth 3D bounding box.", "mention_start": 67, "mention_end": 91, "dataset_mention": "the KITTI object dataset"}, {"mentioned_in_paper": "369", "context_id": "7", "dataset_context": "We demonstrate the effectiveness of our approach on multi-camera person reidentification datasets, to demonstrate the feasibility of learning online classification models in multi-camera big data applications.", "mention_start": 52, "mention_end": 97, "dataset_mention": "multi-camera person reidentification datasets"}, {"mentioned_in_paper": "369", "context_id": "42", "dataset_context": "Using the re-identification datasets allows us to demonstrate the effectiveness of the online representative selection framework where due to a multi-camera setting, large intra-person variation is prevalent.", "mention_start": 6, "mention_end": 36, "dataset_mention": "the re-identification datasets"}, {"mentioned_in_paper": "369", "context_id": "82", "dataset_context": "Time information is either unavailable in person re-identification datasets or is unreliable for a such a scenario over a wide space time horizon.", "mention_start": 42, "mention_end": 75, "dataset_mention": "person re-identification datasets"}, {"mentioned_in_paper": "369", "context_id": "88", "dataset_context": "However, due to the challenges of multi-camera re-identification datasets, we are using some benchmark datasets to evaluate our method.", "mention_start": 33, "mention_end": 73, "dataset_mention": "multi-camera re-identification datasets"}, {"mentioned_in_paper": "369", "context_id": "335", "dataset_context": "The WARD dataset [59] has 4786 images of 70 different people acquired in a real surveillance scenario in 3 non-overlapping cameras.", "mention_start": 0, "mention_end": 16, "dataset_mention": "The WARD dataset"}, {"mentioned_in_paper": "369", "context_id": "363", "dataset_context": "Similar to the WARD dataset, the proficiency of the proposed method is pronounced in the more practical imbalanced scenario.", "mention_start": 11, "mention_end": 27, "dataset_mention": "the WARD dataset"}, {"mentioned_in_paper": "369", "context_id": "380", "dataset_context": "Using the WARD (section 5.1), CAVIAR4REID (section 5.2) and i-LIDS-VID (section 5.3) datasets, we have shown that the proposed method needs less samples to be labeled to attain same accuracy.", "mention_start": 29, "mention_end": 93, "dataset_mention": " CAVIAR4REID (section 5.2) and i-LIDS-VID (section 5.3) datasets"}, {"mentioned_in_paper": "369", "context_id": "383", "dataset_context": "In i-LIDS-VID, the number of persons is comparatively more than the WARD or the CAVIAR4REID dataset which makes it easy to show the need for the redundancy reduction strategy as SMRS (where the redundancy reduction module is not there) is consistently below the proposed method.", "mention_start": 75, "mention_end": 99, "dataset_mention": "the CAVIAR4REID dataset"}, {"mentioned_in_paper": "370", "context_id": "143", "dataset_context": "In Fig. 3 we present the estimated probability density function for the different components of w(x) on the Office31 dataset for the domain shift A\u2192D.", "mention_start": 104, "mention_end": 124, "dataset_mention": "the Office31 dataset"}, {"mentioned_in_paper": "370", "context_id": "154", "dataset_context": "The comparison is done on Office31 dataset and the results appear in Tab. 3. As can be seen, our scoring scheme produces superior results across the entire dataset.", "mention_start": 26, "mention_end": 42, "dataset_mention": "Office31 dataset"}, {"mentioned_in_paper": "370", "context_id": "162", "dataset_context": "We compare the average accuracy on the OfficeHome dataset with the domain shift Ar\u2192Cl and on Office31 with the domain shift A\u2192D.", "mention_start": 35, "mention_end": 57, "dataset_mention": "the OfficeHome dataset"}, {"mentioned_in_paper": "370", "context_id": "170", "dataset_context": "Tab. 5 shows the results on the OfficeHome dataset in the case where pseudo-labels are applied as suggested in our approach and when they are not applied at all.", "mention_start": 28, "mention_end": 50, "dataset_mention": "the OfficeHome dataset"}, {"mentioned_in_paper": "370", "context_id": "176", "dataset_context": "The analysis is done on the Office31 dataset by fixing a set threshold, w \u03b1 = 1.20 (which was found to provide the optimal value).", "mention_start": 24, "mention_end": 44, "dataset_mention": "the Office31 dataset"}, {"mentioned_in_paper": "370", "context_id": "181", "dataset_context": "(a) (b) (c)  Another insight to the advantage of using a dynamic threshold can be seen by looking at Fig. 4. From the figure, one can observe that different domains and datasets give better results when using different thresholds for the pseudo-labels.", "mention_start": 141, "mention_end": 177, "dataset_mention": "that different domains and datasets"}, {"mentioned_in_paper": "370", "context_id": "197", "dataset_context": "Tab. 6 (7) presents the results on the Office31 (Office-Home) dataset in three scenarios: (i) when not applying the diversity loss at all, (ii) when using the diversity loss on both domains and (iii) when using the loss only on the target domain.", "mention_start": 35, "mention_end": 69, "dataset_mention": "the Office31 (Office-Home) dataset"}, {"mentioned_in_paper": "371", "context_id": "139", "dataset_context": "In addition, two publicly available (human and vehicle) datasets are used for evaluation: UCY [14] and NGSIM [15].", "mention_start": 12, "mention_end": 64, "dataset_mention": " two publicly available (human and vehicle) datasets"}, {"mentioned_in_paper": "371", "context_id": "140", "dataset_context": "In particular, for the NGSIM dataset, we only consider the second segment of the road (intersection) in scene Lankershim.", "mention_start": 18, "mention_end": 36, "dataset_mention": "the NGSIM dataset"}, {"mentioned_in_paper": "371", "context_id": "142", "dataset_context": "The UCY dataset contains three scenes: ZARA-01, ZARA-02 and UCY (University of Cyprus).", "mention_start": 0, "mention_end": 15, "dataset_mention": "The UCY dataset"}, {"mentioned_in_paper": "371", "context_id": "146", "dataset_context": "Since no specific knowledge about shapes of objects in UCY or NGSIM datasets is given, we use circles with a radius of 2 pixels to indicate pedestrians and with a radius of 3 pixels to indicate vehicles.", "mention_start": 62, "mention_end": 76, "dataset_mention": "NGSIM datasets"}, {"mentioned_in_paper": "372", "context_id": "8", "dataset_context": "To this end, we gen-erate labeled 2D images from a photo-realistic 3D dataset.", "mention_start": 48, "mention_end": 77, "dataset_mention": "a photo-realistic 3D dataset"}, {"mentioned_in_paper": "372", "context_id": "20", "dataset_context": "The detector for SuperPoint, which we call SuperPoint-COCO to differentiate it from the architecture of the neural network, was trained using homographic adaptation, which is a process DeTone et al. used to adapt a set of detections to be viewpoint invariant on the MS COCO dataset.", "mention_start": 261, "mention_end": 281, "dataset_mention": "the MS COCO dataset"}, {"mentioned_in_paper": "372", "context_id": "22", "dataset_context": "As the MS COCO dataset does not have ground-truth feature detections, the detection labels are provided by the MagicPoint detector.", "mention_start": 3, "mention_end": 22, "dataset_mention": "the MS COCO dataset"}, {"mentioned_in_paper": "372", "context_id": "23", "dataset_context": "The SuperPoint-COCO detector is not pre-trained on the synthetic dataset and is only trained on the augmented MS COCO dataset.", "mention_start": 96, "mention_end": 125, "dataset_mention": "the augmented MS COCO dataset"}, {"mentioned_in_paper": "372", "context_id": "27", "dataset_context": "We generated a 3D data set using the Gibson simulator [29] with the Matterport dataset [5] to tackle the issue of precision and variety in 3D data.", "mention_start": 64, "mention_end": 86, "dataset_mention": "the Matterport dataset"}, {"mentioned_in_paper": "372", "context_id": "28", "dataset_context": "We develop an algorithm for computing a repeatable set of detections given a set of 3D data and a collection of other detections, and we developed a fast, accurate, and powerful way to benchmark algorithm in indoor environments using the Scannet dataset [8].", "mention_start": 233, "mention_end": 253, "dataset_mention": "the Scannet dataset"}, {"mentioned_in_paper": "372", "context_id": "30", "dataset_context": "Because the SuperPoint architecture used for Magic-Point was proven to be effective for learning viewpoint in-variance on non-planar surfaces, we decided to use this network architecture, as well as the loss function, when selecting a network to learn our new dataset.", "mention_start": 245, "mention_end": 267, "dataset_mention": "learn our new dataset"}, {"mentioned_in_paper": "372", "context_id": "37", "dataset_context": "Aanaes [1] utilizes the DTU Robot Dataset, to evaluate several detectors.", "mention_start": 20, "mention_end": 41, "dataset_mention": "the DTU Robot Dataset"}, {"mentioned_in_paper": "372", "context_id": "45", "dataset_context": "Lastly the datasets are not very large as they use the Piccadilly and Roman Forum datasets [27].", "mention_start": 0, "mention_end": 19, "dataset_mention": "Lastly the datasets"}, {"mentioned_in_paper": "372", "context_id": "45", "dataset_context": "Lastly the datasets are not very large as they use the Piccadilly and Roman Forum datasets [27].", "mention_start": 51, "mention_end": 90, "dataset_mention": "the Piccadilly and Roman Forum datasets"}, {"mentioned_in_paper": "372", "context_id": "46", "dataset_context": "Our network utilizes a large dense 3D scene capture dataset.", "mention_start": 21, "mention_end": 59, "dataset_mention": "a large dense 3D scene capture dataset"}, {"mentioned_in_paper": "372", "context_id": "48", "dataset_context": "LF-Net [20] trains using the Scannet Dataset.", "mention_start": 25, "mention_end": 44, "dataset_mention": "the Scannet Dataset"}, {"mentioned_in_paper": "372", "context_id": "49", "dataset_context": "The Scannet dataset [8] is comprised of videos captured by a handheld depth sensor within small indoor environments.", "mention_start": 0, "mention_end": 19, "dataset_mention": "The Scannet dataset"}, {"mentioned_in_paper": "372", "context_id": "55", "dataset_context": "They train on the DTU Robot Image Dataset [1] as well as the NYUv2 dataset [18], both of which are 3D indoor datasets.", "mention_start": 14, "mention_end": 41, "dataset_mention": "the DTU Robot Image Dataset"}, {"mentioned_in_paper": "372", "context_id": "55", "dataset_context": "They train on the DTU Robot Image Dataset [1] as well as the NYUv2 dataset [18], both of which are 3D indoor datasets.", "mention_start": 57, "mention_end": 74, "dataset_mention": "the NYUv2 dataset"}, {"mentioned_in_paper": "372", "context_id": "55", "dataset_context": "They train on the DTU Robot Image Dataset [1] as well as the NYUv2 dataset [18], both of which are 3D indoor datasets.", "mention_start": 98, "mention_end": 117, "dataset_mention": "3D indoor datasets"}, {"mentioned_in_paper": "372", "context_id": "127", "dataset_context": "Figure 2: A room rendered within the Matterport dataset by the Gibson simulator [29].", "mention_start": 32, "mention_end": 55, "dataset_mention": "the Matterport dataset"}, {"mentioned_in_paper": "372", "context_id": "129", "dataset_context": "We did not want to use the Gibson dataset to test algorithms because of issues caused by Goggles [29], the algorithm used to fill holes in the dataset, as it can distort the images.", "mention_start": 23, "mention_end": 41, "dataset_mention": "the Gibson dataset"}, {"mentioned_in_paper": "372", "context_id": "130", "dataset_context": "However, due to our desire to test the ability for a network to handle non planar perspective changes, we needed a corresponding testing dataset that has depth, otherwise the testing method will not be able to check where a feature point lies in 3D space.", "mention_start": 102, "mention_end": 144, "dataset_mention": " we needed a corresponding testing dataset"}, {"mentioned_in_paper": "372", "context_id": "133", "dataset_context": "There two tests designed to check viewpoint invariance in nonplanar surfaces though: the test by Aanaes that utilizes the DTU Robot Dataset [1], and the test by Moreels and Perona [17].Unfortunately, Moreels and Perona [17] do not make their evaluation method available and the test developed by Aanaes et al. [1] takes multiple days to weeks to evaluate a single detector.", "mention_start": 117, "mention_end": 139, "dataset_mention": "the DTU Robot Dataset"}, {"mentioned_in_paper": "372", "context_id": "134", "dataset_context": "As a result, we designed out our detector repeatability test, which is based on the Scannet [8] dataset.", "mention_start": 79, "mention_end": 103, "dataset_mention": "the Scannet [8] dataset"}, {"mentioned_in_paper": "372", "context_id": "135", "dataset_context": "The Scannet dataset is comprised of videos of indoor environments captured by a mobile scanning device.", "mention_start": 0, "mention_end": 19, "dataset_mention": "The Scannet dataset"}, {"mentioned_in_paper": "372", "context_id": "138", "dataset_context": "The Scannet dataset does not fulfill our requirements for the training set because we need many different perspectives of the same objects for our algorithm to work properly.", "mention_start": 0, "mention_end": 19, "dataset_mention": "The Scannet dataset"}, {"mentioned_in_paper": "372", "context_id": "141", "dataset_context": "Angles further than 90 degrees start to introduce issues with occlusion, where the object itself might occlude the visibility of it's corners, which requires the kind of very precise depth that is hard to find in any modern dataset.", "mention_start": 212, "mention_end": 231, "dataset_mention": "any modern dataset"}, {"mentioned_in_paper": "372", "context_id": "142", "dataset_context": "The full Scannet Dataset is very large, so we restrict our usage to the first 200 areas and only use every 30th frame.", "mention_start": 0, "mention_end": 24, "dataset_mention": "The full Scannet Dataset"}, {"mentioned_in_paper": "372", "context_id": "172", "dataset_context": "One of the concerns with using 3D data in feature evaluation is that it has been exceptionally slow, with the DTU Robot Dataset test taking 10 seconds per a pair of images.", "mention_start": 105, "mention_end": 127, "dataset_mention": "the DTU Robot Dataset"}, {"mentioned_in_paper": "372", "context_id": "173", "dataset_context": "If we restrict the dataset to just be comparisons within the same lighting conditions, that would be 424800 pairs, which would take approximately 50 days to evaluate.", "mention_start": 3, "mention_end": 26, "dataset_mention": "we restrict the dataset"}, {"mentioned_in_paper": "372", "context_id": "174", "dataset_context": "The datasets can also be huge, with the DTU Dataset taking 500 GB of storage.", "mention_start": 35, "mention_end": 51, "dataset_mention": "the DTU Dataset"}, {"mentioned_in_paper": "372", "context_id": "200", "dataset_context": "With such a system in place, it might be possible to utilize lower quality datasets to allow for the diversification of data to include more cluttered scenes, outdoor scenes, lighting changes, and more.", "mention_start": 60, "mention_end": 83, "dataset_mention": "lower quality datasets"}, {"mentioned_in_paper": "373", "context_id": "254", "dataset_context": "To illustrate how our method works with real data, we used two subjects from the Human Connectome Project Young Adult (HCP) dataset Glasser et al. (2013).", "mention_start": 76, "mention_end": 131, "dataset_mention": "the Human Connectome Project Young Adult (HCP) dataset"}, {"mentioned_in_paper": "373", "context_id": "263", "dataset_context": "We used six subjects from the Human Connectome Project Young Adult (HCP) dataset Glasser et al. (2013) in this experiment as shown in Figure 6.", "mention_start": 26, "mention_end": 80, "dataset_mention": "the Human Connectome Project Young Adult (HCP) dataset"}, {"mentioned_in_paper": "374", "context_id": "25", "dataset_context": "For example, the largest video scene text dataset 'Text in Videos' [18] only contains 49 videos from 7 different scenarios, which may limit the video text recognition research progress.", "mention_start": 12, "mention_end": 49, "dataset_mention": " the largest video scene text dataset"}, {"mentioned_in_paper": "374", "context_id": "26", "dataset_context": "Therefore, we collect a larger-scale video text dataset (abbr.", "mention_start": 21, "mention_end": 55, "dataset_mention": "a larger-scale video text dataset"}, {"mentioned_in_paper": "374", "context_id": "131", "dataset_context": "The existing video scene text datasets such as IC13 or IC15 (detailed in 1, and mainly characterized by 1) Much larger scale, which is more than twice the scale of IC15.", "mention_start": 0, "mention_end": 38, "dataset_mention": "The existing video scene text datasets"}, {"mentioned_in_paper": "374", "context_id": "169", "dataset_context": "That is, a predict word is considered as a true positive if its IoU over ground truth is larger than 0.5 and the word recognition is correct. 1 Available at https://davar-lab.github.io/opensource/dataset/lsvtd", "mention_start": 163, "mention_end": 203, "dataset_mention": "//davar-lab.github.io/opensource/dataset"}, {"mentioned_in_paper": "374", "context_id": "182", "dataset_context": "The EAST backbone is pre-trained on the 'Incidental Scene Text' dataset [18] and 'COCO-Text' dataset [47] by following [25], and then the model is fine-tuned on corresponding video training set such as IC13 or IC15.", "mention_start": 36, "mention_end": 71, "dataset_mention": "the 'Incidental Scene Text' dataset"}, {"mentioned_in_paper": "374", "context_id": "182", "dataset_context": "The EAST backbone is pre-trained on the 'Incidental Scene Text' dataset [18] and 'COCO-Text' dataset [47] by following [25], and then the model is fine-tuned on corresponding video training set such as IC13 or IC15.", "mention_start": 36, "mention_end": 100, "dataset_mention": "the 'Incidental Scene Text' dataset [18] and 'COCO-Text' dataset"}, {"mentioned_in_paper": "374", "context_id": "215", "dataset_context": "To analyze the contributions of above components, we conduct the end-to-end evaluation on the popular IC15 dataset, as shown in Table.", "mention_start": 89, "mention_end": 114, "dataset_mention": "the popular IC15 dataset"}, {"mentioned_in_paper": "374", "context_id": "250", "dataset_context": "In addition, we release a larger-scale video scene text dataset for better evaluating video text spotting algorithms.", "mention_start": 12, "mention_end": 63, "dataset_mention": " we release a larger-scale video scene text dataset"}, {"mentioned_in_paper": "377", "context_id": "22", "dataset_context": "In an experiment, the ColorChecker dataset prepared by Hemrit et al. [21] is used, and the proposed method is demonstrated to outperform the conventional color balance adjustments.", "mention_start": 17, "mention_end": 42, "dataset_mention": " the ColorChecker dataset"}, {"mentioned_in_paper": "377", "context_id": "84", "dataset_context": "We used the ColorChecker dataset prepared by Hemrit et al. [21], originally published by Gehler et al. in 2008 [24].", "mention_start": 8, "mention_end": 32, "dataset_mention": "the ColorChecker dataset"}, {"mentioned_in_paper": "378", "context_id": "5", "dataset_context": "With this paper, we publish the SEN1-2 dataset to foster deep learning research in SAR-optical data fusion.", "mention_start": 16, "mention_end": 46, "dataset_mention": " we publish the SEN1-2 dataset"}, {"mentioned_in_paper": "378", "context_id": "18", "dataset_context": "Exploiting this novel availability of big remote sensing data, we publish the so-called SEN1-2 dataset with this paper.", "mention_start": 62, "mention_end": 102, "dataset_mention": " we publish the so-called SEN1-2 dataset"}, {"mentioned_in_paper": "378", "context_id": "43", "dataset_context": "In order to generate a multi-sensor SAR-optical patch-pair dataset, a relatively large amount of remote sensing data with very good spatial alignment needs to be acquired.", "mention_start": 21, "mention_end": 66, "dataset_mention": "a multi-sensor SAR-optical patch-pair dataset"}, {"mentioned_in_paper": "378", "context_id": "62", "dataset_context": "Each season is then associated to one of the four sets of random ROIs, thus providing us with the top-level dataset structure (cf.", "mention_start": 93, "mention_end": 115, "dataset_mention": "the top-level dataset"}, {"mentioned_in_paper": "378", "context_id": "63", "dataset_context": "Fig. 3): We structure the final dataset into four distinct sub-groups ROIs1158 spring, ROIs1868 summer, ROIs1970 fall, and ROIs2017 winter.", "mention_start": 8, "mention_end": 39, "dataset_mention": " We structure the final dataset"}, {"mentioned_in_paper": "378", "context_id": "96", "dataset_context": "The SEN1-2 dataset is shared under the open access license CC-BY and available for download at a persistent link provided by the library of the Technical University of Munich: https:// mediatum.ub.tum.de/1436631.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The SEN1-2 dataset"}, {"mentioned_in_paper": "378", "context_id": "102", "dataset_context": "One promising field of application for the SEN1-2 dataset thus is to learn to colorize gray-scale SAR images with color information derived from corresponding optical images, as we have proposed earlier (Schmitt et al., 2018).", "mention_start": 39, "mention_end": 57, "dataset_mention": "the SEN1-2 dataset"}, {"mentioned_in_paper": "378", "context_id": "107", "dataset_context": "The SEN1-2 dataset can assist in creating solutions in the field of multi-modal image matching by providing the large quantities of data required to exploit modern deep matching approaches, such as proposed by (Merkle et al., 2017) or (Hughes et al., 2018) : Using a pseudosiamese convolutional neural network architecture, corresponding SAR-optical image patches of a SEN1-2 test subset can be identified with an accuracy of 93%.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The SEN1-2 dataset"}, {"mentioned_in_paper": "378", "context_id": "109", "dataset_context": "Another possible field of application of the SEN1-2 dataset is to train generative models that allow to predict artificial SAR images from optical input data (Marmanis et al., 2017, Merkle et al., 2018) or artificial optical imagery from SAR inputs (Wang and Patel, 2018, Ley et al., 2018, Grohnfeldt et al., 2018).", "mention_start": 41, "mention_end": 59, "dataset_mention": "the SEN1-2 dataset"}, {"mentioned_in_paper": "378", "context_id": "112", "dataset_context": "The only other existing dataset in this domain is the so-called SARptical dataset published by (Wang and Zhu, 2018).", "mention_start": 50, "mention_end": 81, "dataset_mention": "the so-called SARptical dataset"}, {"mentioned_in_paper": "378", "context_id": "113", "dataset_context": "In contrast to the SEN1-2 dataset, it provides very-high-resolution image patches from TerraSAR-X and aerial photogrammetry, but is restricted to a mere 10,000 patches extracted from a single scene, which is possibly not sufficient for many deep learning applications -especially since many patches show an overlap of more than 50%.", "mention_start": 15, "mention_end": 33, "dataset_mention": "the SEN1-2 dataset"}, {"mentioned_in_paper": "378", "context_id": "117", "dataset_context": "Furthermore, at the time we carried out the dataset preparation, GEE stocked only Level-1C data for Sentinel-2, which basically means that the pixel values represent top-ofatmosphere (TOA) reflectances instead of atmospherically corrected bottom-of-atmosphere (BOA) information.", "mention_start": 15, "mention_end": 51, "dataset_mention": "the time we carried out the dataset"}, {"mentioned_in_paper": "378", "context_id": "119", "dataset_context": "With this paper, we have described and released the SEN1-2 dataset, which contains 282,384 pairs of SAR and optical image patches extracted from versatile Sentinel-1 and Sentinel-2 scenes.", "mention_start": 24, "mention_end": 66, "dataset_mention": "described and released the SEN1-2 dataset"}, {"mentioned_in_paper": "379", "context_id": "4", "dataset_context": "We provide a comprehensive experimentation of our algorithm on two crystal image datasets.", "mention_start": 63, "mention_end": 89, "dataset_mention": "two crystal image datasets"}, {"mentioned_in_paper": "379", "context_id": "5", "dataset_context": "One dataset comprises of images containing objects in multiple shapes overlapping each other and the other dataset contains standard images with objects present in a single shape.", "mention_start": 54, "mention_end": 114, "dataset_mention": "multiple shapes overlapping each other and the other dataset"}, {"mentioned_in_paper": "379", "context_id": "15", "dataset_context": "We test our work on two Datasets, Dataset-I (Fig. 1.a) is a representative of the heterogeneous group of images, whereas Dataset-II (Fig. 1.b) is a standard dataset containing homogeneous images.", "mention_start": 112, "mention_end": 128, "dataset_mention": " whereas Dataset"}, {"mentioned_in_paper": "379", "context_id": "36", "dataset_context": "\u2022 We test our work on both heterogeneous and homogeneous datasets, beating the baselines for both, demonstrating the general applicability of our work.", "mention_start": 22, "mention_end": 65, "dataset_mention": "both heterogeneous and homogeneous datasets"}, {"mentioned_in_paper": "379", "context_id": "37", "dataset_context": "(a) Dataset-I Crystal.", "mention_start": 0, "mention_end": 11, "dataset_mention": "(a) Dataset"}, {"mentioned_in_paper": "379", "context_id": "38", "dataset_context": "(b) Dataset-II Crystal. Figure 1 : Overlapping Crystals Image.", "mention_start": 0, "mention_end": 11, "dataset_mention": "(b) Dataset"}, {"mentioned_in_paper": "379", "context_id": "177", "dataset_context": "The first dataset (Dataset-I, Fig. 1.a), com-", "mention_start": 0, "mention_end": 26, "dataset_mention": "The first dataset (Dataset"}, {"mentioned_in_paper": "380", "context_id": "43", "dataset_context": "DeepSVG performs accurate reconstruction and interpolation of a simple icon dataset and generation of alphabet fonts.", "mention_start": 62, "mention_end": 83, "dataset_mention": "a simple icon dataset"}, {"mentioned_in_paper": "380", "context_id": "116", "dataset_context": "We built a dataset of Chinese characters on Japanese fonts, which consisted of the products of Fontworks Inc. and free fonts from the Web, to evaluate the effectiveness of the proposed method.", "mention_start": 0, "mention_end": 18, "dataset_mention": "We built a dataset"}, {"mentioned_in_paper": "381", "context_id": "10", "dataset_context": "Recently, with the emergence of two curved-text focused datasets, namely Total-Text [24] and CTW1500 [25], the research focus of text detection has shifted from horizontal or multioriented scene texts to more challenging curved or arbitrary-shaped scene texts.", "mention_start": 31, "mention_end": 64, "dataset_mention": "two curved-text focused datasets"}, {"mentioned_in_paper": "381", "context_id": "13", "dataset_context": "Although these approaches, especially the Mask R-CNN [31] based ones [28, 29], have achieved superior performance on some benchmark datasets, they are not robust to nearby long curved text instances which appear often in some commodity images, e.g., the DAST1500 dataset [32].", "mention_start": 249, "mention_end": 270, "dataset_mention": " the DAST1500 dataset"}, {"mentioned_in_paper": "381", "context_id": "60", "dataset_context": "Tang et al. [32] introduced a new dense and arbitrary-shaped text detection dataset, i.e., DAST1500, which is mainly composed of commodity images, to demonstrate this.", "mention_start": 28, "mention_end": 83, "dataset_mention": "a new dense and arbitrary-shaped text detection dataset"}, {"mentioned_in_paper": "381", "context_id": "180", "dataset_context": "To evaluate the performance of our proposed ReLaText, we conduct comprehensive experiments on five scene text detection benchmark datasets, including RCTW-17 [81], MSRA-TD500 [82], Total-Text [24], CTW1500 [25] and DAST1500 [32].", "mention_start": 93, "mention_end": 138, "dataset_mention": "five scene text detection benchmark datasets"}, {"mentioned_in_paper": "381", "context_id": "191", "dataset_context": "For the other four small scale datasets, we first use the SynthText dataset to pre-train a text detection model for 860k iterations with a base learning rate of 0.004, which is divided by 10 at every 387k iterations.", "mention_start": 53, "mention_end": 75, "dataset_mention": "the SynthText dataset"}, {"mentioned_in_paper": "381", "context_id": "209", "dataset_context": "We conduct experiments on two curved-text focused datasets, namely Total-Text and CTW1500, to evaluate the performance of ReLaText on detecting arbitrary-shaped texts in natural scene images.", "mention_start": 26, "mention_end": 58, "dataset_mention": "two curved-text focused datasets"}, {"mentioned_in_paper": "381", "context_id": "215", "dataset_context": "The superior performance achieved on the above five datasets demonstrates the effectiveness and robustness of ReLaText.", "mention_start": 37, "mention_end": 60, "dataset_mention": "the above five datasets"}, {"mentioned_in_paper": "382", "context_id": "308", "dataset_context": "The first 5000 were used for training, and the remaining 5000 for test; Fig. 4(a) shows the training data for the SRU dataset, the peaks are clearly visible.", "mention_start": 109, "mention_end": 125, "dataset_mention": "the SRU dataset"}, {"mentioned_in_paper": "384", "context_id": "55", "dataset_context": "\u2022 The results on four clinical datasets show that our method achieves good performance in three different organ segmentation tasks and demonstrates good cross-domain performance through external validation.", "mention_start": 17, "mention_end": 39, "dataset_mention": "four clinical datasets"}, {"mentioned_in_paper": "384", "context_id": "62", "dataset_context": "The liver dataset contains 245 CT volumes collected from the First Affiliated Hospital, Zhejiang University School of Medicine.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The liver dataset"}, {"mentioned_in_paper": "384", "context_id": "67", "dataset_context": "The spleen dataset is from Medical Segmentation Decathlon (MSD) [22] Challenge.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The spleen dataset"}, {"mentioned_in_paper": "384", "context_id": "70", "dataset_context": "The kidneys dataset is from 2019 Kidney Tumor Segmentation (KiTS) [23] Challenge and 210 abdominal CT images in the late-arterial phase are available.", "mention_start": 0, "mention_end": 19, "dataset_mention": "The kidneys dataset"}, {"mentioned_in_paper": "384", "context_id": "204", "dataset_context": "Thus, we can only ensure that the dataset used in the selected comparative article intersects as much as possible with the one we use, given that it discusses the same organs (liver, spleen and kidneys) under the same modality (CT).", "mention_start": 17, "mention_end": 41, "dataset_mention": "ensure that the dataset"}, {"mentioned_in_paper": "384", "context_id": "205", "dataset_context": "We compared our method with the one proposed by Ma et al. [30], which presented an abdominal CT organ segmentation dataset including liver, kidney, spleen and pancreas, and provided benchmarks for fully supervised, semi-supervised and weakly supervised multi-organ segmentation models.", "mention_start": 69, "mention_end": 122, "dataset_mention": "presented an abdominal CT organ segmentation dataset"}, {"mentioned_in_paper": "384", "context_id": "212", "dataset_context": "The total number of cases in the MSD spleen dataset (41 cases) is much smaller than that in our liver (245 cases) and kidney (210 cases) datasets, and the organs in the images contain a lot of noise, which affects the final segmentation performance.", "mention_start": 29, "mention_end": 51, "dataset_mention": "the MSD spleen dataset"}, {"mentioned_in_paper": "384", "context_id": "212", "dataset_context": "The total number of cases in the MSD spleen dataset (41 cases) is much smaller than that in our liver (245 cases) and kidney (210 cases) datasets, and the organs in the images contain a lot of noise, which affects the final segmentation performance.", "mention_start": 92, "mention_end": 145, "dataset_mention": "our liver (245 cases) and kidney (210 cases) datasets"}, {"mentioned_in_paper": "385", "context_id": "20", "dataset_context": "Typically, a new model, the calibrator, is trained on the outputs of the classifier evaluated on a held-out dataset.", "mention_start": 96, "mention_end": 115, "dataset_mention": "a held-out dataset"}, {"mentioned_in_paper": "385", "context_id": "104", "dataset_context": "The standard practice is to fit this calibration map or calibrator in a held-out data set D val , or validation data, that is supposed to resemble the data on which the model will make predictions.", "mention_start": 70, "mention_end": 89, "dataset_mention": "a held-out data set"}, {"mentioned_in_paper": "385", "context_id": "220", "dataset_context": "We refer to model-dataset pairs as calibration tasks.", "mention_start": 12, "mention_end": 25, "dataset_mention": "model-dataset"}, {"mentioned_in_paper": "385", "context_id": "242", "dataset_context": "From Figure 1 we notice that the classifier does produce more overconfident predictions for some classes than for others, even in a curated and well-balanced dataset such as CIFAR-10.", "mention_start": 139, "mention_end": 165, "dataset_mention": "and well-balanced dataset"}, {"mentioned_in_paper": "385", "context_id": "296", "dataset_context": "In this section, we provide in tables the results for each model-dataset task.", "mention_start": 53, "mention_end": 72, "dataset_mention": "each model-dataset"}, {"mentioned_in_paper": "386", "context_id": "3", "dataset_context": "We experimented using a monsoonaffected precipitation dataset from southwest India, which should give a roughly stable pattern of rainy days and ease our investigation.", "mention_start": 22, "mention_end": 61, "dataset_mention": "a monsoonaffected precipitation dataset"}, {"mentioned_in_paper": "386", "context_id": "35", "dataset_context": "We used the Climate Hazards group Infrared Precipitation with Stations v2.0 (CHIRPS) dataset (Funk et al., 2015), a global interpolated dataset of daily precipitation providing a spatial resolution of 0.05 degrees.", "mention_start": 62, "mention_end": 92, "dataset_mention": "Stations v2.0 (CHIRPS) dataset"}, {"mentioned_in_paper": "386", "context_id": "35", "dataset_context": "We used the Climate Hazards group Infrared Precipitation with Stations v2.0 (CHIRPS) dataset (Funk et al., 2015), a global interpolated dataset of daily precipitation providing a spatial resolution of 0.05 degrees.", "mention_start": 113, "mention_end": 143, "dataset_mention": " a global interpolated dataset"}, {"mentioned_in_paper": "386", "context_id": "78", "dataset_context": "We reported that controlling the sampling from the known latent distribution is effectively related to synthesizing samples with more extreme scenarios in the precipitation dataset experimented in our tests.", "mention_start": 155, "mention_end": 180, "dataset_mention": "the precipitation dataset"}, {"mentioned_in_paper": "387", "context_id": "4", "dataset_context": "In order to demonstrate the effectiveness of the proposed method, two common data set are used for training while tracking by localization application is considered to demonstrate its final performance.", "mention_start": 65, "mention_end": 85, "dataset_mention": " two common data set"}, {"mentioned_in_paper": "387", "context_id": "47", "dataset_context": "For instance, it has major limitations over the COCO data set [14], which contains a wide range of objects on different scales.", "mention_start": 43, "mention_end": 61, "dataset_mention": "the COCO data set"}, {"mentioned_in_paper": "387", "context_id": "65", "dataset_context": "In section three, the method for the training process will be elaborated as well as the issue of producing the desired data set.", "mention_start": 96, "mention_end": 127, "dataset_mention": "producing the desired data set"}, {"mentioned_in_paper": "387", "context_id": "142", "dataset_context": "At this stage, let us elaborate on the proper dataset used to train the proposed network.", "mention_start": 34, "mention_end": 53, "dataset_mention": "the proper dataset"}, {"mentioned_in_paper": "387", "context_id": "154", "dataset_context": "To further illustrate this important point, in Remark 6: A point to ponder is that the annotations of the NFS dataset have to be changed according to the new images indicated in Fig. 5.", "mention_start": 101, "mention_end": 117, "dataset_mention": "the NFS dataset"}, {"mentioned_in_paper": "387", "context_id": "156", "dataset_context": "Figure 5 : Some representative samples of the produced dataset using higher resolution ROI matrices such as (28 \u00d7 28).", "mention_start": 41, "mention_end": 62, "dataset_mention": "the produced dataset"}, {"mentioned_in_paper": "387", "context_id": "192", "dataset_context": "Then, it is trained on the NFS dataset for 100 epochs.", "mention_start": 22, "mention_end": 38, "dataset_mention": "the NFS dataset"}, {"mentioned_in_paper": "387", "context_id": "272", "dataset_context": "Furthermore, to avoid worries related to implementing this tracker on another dataset which may downgrade its performance, both trackers are trained on the LaSOT dataset where SPLT has been trained and tested on it.", "mention_start": 151, "mention_end": 169, "dataset_mention": "the LaSOT dataset"}, {"mentioned_in_paper": "388", "context_id": "5", "dataset_context": "Experiments on PASCAL VOC 2012 segmentation dataset and four saliency benchmark datasets show the performance of our method compares favorably against state-of-the-art weakly supervised segmentation methods and fully supervised saliency detection methods.", "mention_start": 15, "mention_end": 51, "dataset_mention": "PASCAL VOC 2012 segmentation dataset"}, {"mentioned_in_paper": "388", "context_id": "5", "dataset_context": "Experiments on PASCAL VOC 2012 segmentation dataset and four saliency benchmark datasets show the performance of our method compares favorably against state-of-the-art weakly supervised segmentation methods and fully supervised saliency detection methods.", "mention_start": 15, "mention_end": 88, "dataset_mention": "PASCAL VOC 2012 segmentation dataset and four saliency benchmark datasets"}, {"mentioned_in_paper": "388", "context_id": "118", "dataset_context": "We use two training sets to train the proposed SSNet: the saliency dataset with pixel-level saliency annotations, and the classification dataset with image-level semantic category labels.", "mention_start": 53, "mention_end": 74, "dataset_mention": " the saliency dataset"}, {"mentioned_in_paper": "388", "context_id": "119", "dataset_context": "Let D s = {(X n , Y n )} Ns n=1 denote the saliency dataset, in which X n is the image, and Y n is the ground truth.", "mention_start": 38, "mention_end": 59, "dataset_mention": "the saliency dataset"}, {"mentioned_in_paper": "388", "context_id": "153", "dataset_context": "PASCAL-S stems from the validation set of PASCAL VOC 2010 segmentation dataset and contains 850 natural images.", "mention_start": 42, "mention_end": 78, "dataset_mention": "PASCAL VOC 2010 segmentation dataset"}, {"mentioned_in_paper": "388", "context_id": "188", "dataset_context": "In this setting L c + L s1 is used as loss function, with both the image-level category labels and the saliency dataset are used as training data.", "mention_start": 57, "mention_end": 119, "dataset_mention": "both the image-level category labels and the saliency dataset"}, {"mentioned_in_paper": "388", "context_id": "191", "dataset_context": "In the second training the one is the predictions of SSNet-1, and the other is the saliency dataset.", "mention_start": 78, "mention_end": 99, "dataset_mention": "the saliency dataset"}, {"mentioned_in_paper": "388", "context_id": "192", "dataset_context": "In order to verify the contribution of each split, we train SSNet-2 in three settings: 1) train with only the predictions of SSNet-S using the L s2 as loss function, 2) train with only the predictions of SSNet-M using the L s2 as loss function, and 3) train with the predictions of SSNet-M and the saliency dataset using L s1 + L s2 as loss function.", "mention_start": 281, "mention_end": 314, "dataset_mention": "SSNet-M and the saliency dataset"}, {"mentioned_in_paper": "388", "context_id": "196", "dataset_context": "Then we run the model SSNet-MM mentioned above on saliency dataset, and the resulted F-measure and MAE are shown in the second column of Table 6.", "mention_start": 50, "mention_end": 66, "dataset_mention": "saliency dataset"}, {"mentioned_in_paper": "389", "context_id": "69", "dataset_context": "Holdout TS One way is to partition the training dataset into two parts D = D0 D1 and use D k = D0 for calculating the TS according to (5) and D1 for training (e.g., applied in [8] for Criteo dataset).", "mention_start": 183, "mention_end": 198, "dataset_mention": "Criteo dataset"}, {"mentioned_in_paper": "389", "context_id": "91", "dataset_context": "This theorem means that the trained model is an unbiased estimate of the true dependence y = f * (x), when we use independent datasets at each gradient step. 6", "mention_start": 113, "mention_end": 134, "dataset_mention": "independent datasets"}, {"mentioned_in_paper": "389", "context_id": "175", "dataset_context": "We also empirically analyzed the running times of the algorithms on Epsilon dataset.", "mention_start": 68, "mention_end": 83, "dataset_mention": "Epsilon dataset"}, {"mentioned_in_paper": "389", "context_id": "181", "dataset_context": "Indeed, the largest benefit from Ordered is observed on Adult and Internet datasets, which are relatively small (less than 40K training examples), which supports our hypothesis that a higher bias negatively affects the performance.", "mention_start": 55, "mention_end": 83, "dataset_mention": "Adult and Internet datasets"}, {"mentioned_in_paper": "389", "context_id": "183", "dataset_context": "In order to further validate this hypothesis, we make the following experiment: we train CatBoost in Ordered and Plain modes on randomly filtered datasets and compare the obtained losses, see Figure 2. As we expected, for smaller datasets the relative performance of Plain mode becomes worse.", "mention_start": 127, "mention_end": 154, "dataset_mention": "randomly filtered datasets"}, {"mentioned_in_paper": "389", "context_id": "278", "dataset_context": "Train-test splits Each dataset was randomly split into training set (80%) and test set (20%).", "mention_start": 0, "mention_end": 30, "dataset_mention": "Train-test splits Each dataset"}, {"mentioned_in_paper": "390", "context_id": "175", "dataset_context": "This global model is also implemented and compared in the experimental part, known as CNN-LSTM a) Data preprocessing: For VOC 2007 [48] and MS-COCO dataset [49], the ground-truth of bounding-boxes for the single objects are provided.", "mention_start": 121, "mention_end": 155, "dataset_mention": "VOC 2007 [48] and MS-COCO dataset"}, {"mentioned_in_paper": "390", "context_id": "220", "dataset_context": "Microsoft COCO dataset [49] is a large scale benchmark dataset for several vision tasks.", "mention_start": 0, "mention_end": 22, "dataset_mention": "Microsoft COCO dataset"}, {"mentioned_in_paper": "390", "context_id": "232", "dataset_context": "Fig. 8 shows the precision-recall curves of three selected classes (bird, fire hydrant and kite) in the MS COCO dataset, these three kinds of objects are visually small in the images of this dataset.", "mention_start": 99, "mention_end": 119, "dataset_mention": "the MS COCO dataset"}, {"mentioned_in_paper": "390", "context_id": "237", "dataset_context": "Fig. 9 analyzes the recall along with the object ground-truth bounding-box size on the MS COCO dataset, which shows that our model achieves much higher recall than the CNN+LSTM model on those labels that corresponding bounding-boxes are smaller.", "mention_start": 83, "mention_end": 102, "dataset_mention": "the MS COCO dataset"}, {"mentioned_in_paper": "390", "context_id": "240", "dataset_context": " [50] is a web image dataset associated with user tags.", "mention_start": 8, "mention_end": 28, "dataset_mention": "a web image dataset"}, {"mentioned_in_paper": "390", "context_id": "244", "dataset_context": "Tab. IV shows the comparison results on the NUS-WIDE dataset on 81 concepts.", "mention_start": 40, "mention_end": 60, "dataset_mention": "the NUS-WIDE dataset"}, {"mentioned_in_paper": "390", "context_id": "249", "dataset_context": "Fig. 8 : Precision-Recall curves for the 'bird', 'fire hydrant' and 'kite' classes in the MS COCO dataset, for our RLSD models and the baseline models.", "mention_start": 85, "mention_end": 105, "dataset_mention": "the MS COCO dataset"}, {"mentioned_in_paper": "390", "context_id": "251", "dataset_context": "Because the bounding-boxes are not provided in the NUS-WIDE dataset, there is no upper bound for this dataset.", "mention_start": 47, "mention_end": 67, "dataset_mention": "the NUS-WIDE dataset"}, {"mentioned_in_paper": "391", "context_id": "100", "dataset_context": "To highlight the characteristics of feature maps, we train a VGGNet model on the CIFAR10 dataset and obtain the statistical information of feature maps.", "mention_start": 76, "mention_end": 96, "dataset_mention": "the CIFAR10 dataset"}, {"mentioned_in_paper": "391", "context_id": "148", "dataset_context": "We train a PreResNet-164 on the CIFAR10 dataset and compare the statistical information of feature maps between those in the first two layers of sequential branches (f1+f2) and those after the additional operation (f-last).", "mention_start": 28, "mention_end": 47, "dataset_mention": "the CIFAR10 dataset"}, {"mentioned_in_paper": "391", "context_id": "152", "dataset_context": "Besides, the distribution of M-std  Table 1 : Results of pruned VGGNet on CIFAR dataset.", "mention_start": 73, "mention_end": 87, "dataset_mention": "CIFAR dataset"}, {"mentioned_in_paper": "391", "context_id": "200", "dataset_context": "To further illustrate the generalization of our method, we prune MobileNet on both CI-FAR and ILSVRC-2012 datasets.", "mention_start": 77, "mention_end": 114, "dataset_mention": "both CI-FAR and ILSVRC-2012 datasets"}, {"mentioned_in_paper": "392", "context_id": "100", "dataset_context": "We thus employ crop size to be 513 during both training and test on PASCAL VOC 2012 dataset.", "mention_start": 68, "mention_end": 91, "dataset_mention": "PASCAL VOC 2012 dataset"}, {"mentioned_in_paper": "392", "context_id": "136", "dataset_context": "Pretrained on COCO: For comparison with other stateof-art models, we further pretrain our best ASPP model on MS-COCO dataset [57].", "mention_start": 108, "mention_end": 124, "dataset_mention": "MS-COCO dataset"}, {"mentioned_in_paper": "392", "context_id": "139", "dataset_context": "After pretraining on MS-COCO dataset, our proposed model attains performance of 82.7% on val set when using output stride = 8, multi-scale inputs and adding left-right flipped images during inference.", "mention_start": 21, "mention_end": 36, "dataset_mention": "MS-COCO dataset"}, {"mentioned_in_paper": "392", "context_id": "141", "dataset_context": "4.1 when fine-tuning on PASCAL VOC 2012 dataset.", "mention_start": 24, "mention_end": 47, "dataset_mention": "PASCAL VOC 2012 dataset"}, {"mentioned_in_paper": "392", "context_id": "142", "dataset_context": "Test set result and an effective bootstrapping method: We notice that PASCAL VOC 2012 dataset provides higher quality of annotations than the augmented dataset [29], especially for the bicycle class.", "mention_start": 54, "mention_end": 93, "dataset_mention": " We notice that PASCAL VOC 2012 dataset"}, {"mentioned_in_paper": "392", "context_id": "142", "dataset_context": "Test set result and an effective bootstrapping method: We notice that PASCAL VOC 2012 dataset provides higher quality of annotations than the augmented dataset [29], especially for the bicycle class.", "mention_start": 137, "mention_end": 159, "dataset_mention": "the augmented dataset"}, {"mentioned_in_paper": "392", "context_id": "149", "dataset_context": "Model pretrained on JFT-300M: Motivated by the recent work of [79], we further employ the ResNet-101 model which has been pretraind on both ImageNet and the JFT-300M dataset [34, 13, 79], resulting in a performance of", "mention_start": 134, "mention_end": 173, "dataset_mention": "both ImageNet and the JFT-300M dataset"}, {"mentioned_in_paper": "392", "context_id": "162", "dataset_context": "We measure the effect of training the model with multiple replicas on PASCAL VOC 2012 semantic segmentation dataset.", "mention_start": 70, "mention_end": 115, "dataset_mention": "PASCAL VOC 2012 semantic segmentation dataset"}, {"mentioned_in_paper": "393", "context_id": "58", "dataset_context": "For instance, in some domains, there exist multiple perspectives on the same phenomenon, resulting in distinct datasets for the same problem.", "mention_start": 101, "mention_end": 119, "dataset_mention": "distinct datasets"}, {"mentioned_in_paper": "394", "context_id": "0", "dataset_context": "We propose Batch-Expansion Training (BET), a framework for running a batch optimizer on a gradually expanding dataset.", "mention_start": 87, "mention_end": 117, "dataset_mention": "a gradually expanding dataset"}, {"mentioned_in_paper": "394", "context_id": "5", "dataset_context": "State-of-the-art optimization algorithms used in machine learning broadly tend to fall into two main categories: batch methods, which visit the entire dataset once before performing an expensive parameter update, and stochastic methods, which rely on a small subset of training data, to apply quick parameter updates at a much greater frequency.", "mention_start": 133, "mention_end": 158, "dataset_mention": "visit the entire dataset"}, {"mentioned_in_paper": "394", "context_id": "8", "dataset_context": "On the other hand, by accessing the full dataset at each step, batch algorithms can better utilize second-order information about the loss function, while taking advantage of parallel and distributed architectures.", "mention_start": 21, "mention_end": 48, "dataset_mention": "accessing the full dataset"}, {"mentioned_in_paper": "394", "context_id": "13", "dataset_context": "When running a batch optimizer, which requires loading the entire dataset, one has to wait until all the machines become available before beginning the computation.", "mention_start": 46, "mention_end": 73, "dataset_mention": "loading the entire dataset"}, {"mentioned_in_paper": "394", "context_id": "76", "dataset_context": "In this paper, we propose that instead of finding the optimal dataset size at the beginning, we first load a small subset of data, train the model until we reach the corresponding effective optimization error tolerance, then we load more data, and optimize further, etc. Vertical lines in Figure 1b illustrate how optimization error is reduced in multiple stages working with increasing data sizes.", "mention_start": 41, "mention_end": 69, "dataset_mention": "finding the optimal dataset"}, {"mentioned_in_paper": "394", "context_id": "78", "dataset_context": "Thus, our approach is able to reach smaller effective optimization error tolerance within the same time budget compared to fixing the dataset size at the beginning (see Figures 1a and 1b).", "mention_start": 122, "mention_end": 141, "dataset_mention": "fixing the dataset"}, {"mentioned_in_paper": "394", "context_id": "112", "dataset_context": "Note that to establish convergence of the proposed algorithm, it is only required that the dataset is randomly permuted, i.e. that each subset {z i } nt i=1 represents a random portion of the data.", "mention_start": 72, "mention_end": 98, "dataset_mention": "required that the dataset"}, {"mentioned_in_paper": "394", "context_id": "174", "dataset_context": "As the optimization problem we use logistic loss with \u2113 2 -norm regularization trained on several standard LIBSVM datasets (see Table 1).", "mention_start": 90, "mention_end": 122, "dataset_mention": "several standard LIBSVM datasets"}, {"mentioned_in_paper": "394", "context_id": "187", "dataset_context": "Figure 2 shows the performance comparison of using BET with the two inner optimizers on webspam dataset, contrasted with both of them ran in regular batch mode.", "mention_start": 88, "mention_end": 103, "dataset_mention": "webspam dataset"}, {"mentioned_in_paper": "394", "context_id": "200", "dataset_context": "This is shown on a sample plot for the covtype dataset in Figure 3, where we use Parallel SGD [28] as the stochastic algorithm and L-BFGS as the Batch method, with all algorithms running on a 16 core machine (for the sake of clarity, only a portion of the full convergence time is shown).", "mention_start": 35, "mention_end": 54, "dataset_mention": "the covtype dataset"}, {"mentioned_in_paper": "394", "context_id": "205", "dataset_context": "More plots are presented in Appendix B, showing datasets which favor either batch or stochastic methods.", "mention_start": 39, "mention_end": 56, "dataset_mention": " showing datasets"}, {"mentioned_in_paper": "394", "context_id": "209", "dataset_context": "For this experiment we selected the url dataset to demonstrate a stark contrast with Parallel SGD.", "mention_start": 32, "mention_end": 47, "dataset_mention": "the url dataset"}, {"mentioned_in_paper": "394", "context_id": "231", "dataset_context": "Taking the limit of N \u2192 \u221e, the relationship between any subset Z t and the full dataset Z becomes statistically equivalent to i.i.d.", "mention_start": 51, "mention_end": 87, "dataset_mention": "any subset Z t and the full dataset"}, {"mentioned_in_paper": "394", "context_id": "276", "dataset_context": "In the case of HIGGS, BET simply converges to the optimum accuracy before it even reaches full dataset, thus saving on expensive iterations.", "mention_start": 81, "mention_end": 102, "dataset_mention": "reaches full dataset"}, {"mentioned_in_paper": "395", "context_id": "165", "dataset_context": "We consider two indoor datasets NYUDv2 [29], Taskonomy [34], and an outdoor dataset CityScapes [8].", "mention_start": 12, "mention_end": 31, "dataset_mention": "two indoor datasets"}, {"mentioned_in_paper": "395", "context_id": "165", "dataset_context": "We consider two indoor datasets NYUDv2 [29], Taskonomy [34], and an outdoor dataset CityScapes [8].", "mention_start": 60, "mention_end": 83, "dataset_mention": " and an outdoor dataset"}, {"mentioned_in_paper": "395", "context_id": "166", "dataset_context": "NYUDv2 is one of the most popular RGB-D datasets in the literature for indoor scene labeling, containing 1449 densely labeled pairs of RGB and depth frames.", "mention_start": 17, "mention_end": 48, "dataset_mention": "the most popular RGB-D datasets"}, {"mentioned_in_paper": "395", "context_id": "168", "dataset_context": "Cityscapes is a RGB-D dataset for street scene understanding which becomes one of the standard benchmarks.", "mention_start": 14, "mention_end": 29, "dataset_mention": "a RGB-D dataset"}, {"mentioned_in_paper": "395", "context_id": "173", "dataset_context": "We adopt its official sub-dataset containing 9000 samples.", "mention_start": 9, "mention_end": 33, "dataset_mention": "its official sub-dataset"}, {"mentioned_in_paper": "395", "context_id": "195", "dataset_context": "We report results on both NYUDv2 and Cityscapes datasets in Table 1 and Table 2 respectively.", "mention_start": 21, "mention_end": 56, "dataset_mention": "both NYUDv2 and Cityscapes datasets"}, {"mentioned_in_paper": "395", "context_id": "204", "dataset_context": "In Table 2, we compare our fusion method to state-of-the-art methods on Cityscapes validation dataset, and our fusion method outperforms previous methods.", "mention_start": 71, "mention_end": 101, "dataset_mention": "Cityscapes validation dataset"}, {"mentioned_in_paper": "395", "context_id": "212", "dataset_context": "we Table 1 : Semantic segmentation results comparison on NYUDv2 dataset.", "mention_start": 56, "mention_end": 71, "dataset_mention": "NYUDv2 dataset"}, {"mentioned_in_paper": "395", "context_id": "254", "dataset_context": "Regarding fusion methods, to make advantage of the bidirectional fusion scheme, we propose channel shuffle and pixel shift operations that are asymmetric with Table 4 : Exploration of different parameter sharing strategies for the encoder, based on NYUDv2 and Cityscapes datasets.", "mention_start": 248, "mention_end": 279, "dataset_mention": "NYUDv2 and Cityscapes datasets"}, {"mentioned_in_paper": "397", "context_id": "7", "dataset_context": "To obtain steady improvement, models would be occasionally upgraded by training on larger or cleaner datasets, adopting more powerful network structures and training losses, or applying techniques like network architecture search [58, 51], knowledge distillation [18] and network pruning [24, 11].", "mention_start": 92, "mention_end": 109, "dataset_mention": "cleaner datasets"}, {"mentioned_in_paper": "397", "context_id": "152", "dataset_context": "The original MS-Celeb-1M dataset [15] contains about 10 million images of 100k identities.", "mention_start": 0, "mention_end": 32, "dataset_mention": "The original MS-Celeb-1M dataset"}, {"mentioned_in_paper": "397", "context_id": "155", "dataset_context": "For change of training datasets, we also collect faces from the first 50% of identities from MS1Mv2 and call the dataset as MS1Mv2(1/2).", "mention_start": 92, "mention_end": 120, "dataset_mention": "MS1Mv2 and call the dataset"}, {"mentioned_in_paper": "397", "context_id": "209", "dataset_context": "Here MS1Mv0 is the original MS-Celeb-1M dataset [15] while the other two datasets are cleaned versions by authors of ArcFace [9].", "mention_start": 15, "mention_end": 47, "dataset_mention": "the original MS-Celeb-1M dataset"}, {"mentioned_in_paper": "397", "context_id": "211", "dataset_context": "For CMC, we adopt three pretrained models provided by the InsightFace project 1 , which are of different network architectures and trained on different version of MS1M datasets as well as different losses.", "mention_start": 162, "mention_end": 176, "dataset_mention": "MS1M datasets"}, {"mentioned_in_paper": "397", "context_id": "265", "dataset_context": "-Table 9 : 1:1 verification TAR (%@FAR=1e-4) on the IJB-C dataset [26] with decreasing feature dimensions.", "mention_start": 48, "mention_end": 65, "dataset_mention": "the IJB-C dataset"}, {"mentioned_in_paper": "397", "context_id": "285", "dataset_context": "We use the market-1501 dataset [55] consisting of 1501 identities and 32217 images.", "mention_start": 7, "mention_end": 30, "dataset_mention": "the market-1501 dataset"}, {"mentioned_in_paper": "397", "context_id": "287", "dataset_context": "We use the first 11 : 1:1 verification TAR (%@FAR=1e-4) on the IJB-C dataset [26] with different compatible directions.", "mention_start": 59, "mention_end": 76, "dataset_mention": "the IJB-C dataset"}, {"mentioned_in_paper": "397", "context_id": "288", "dataset_context": "half of identities to train the old model and the full dataset to train the new model.", "mention_start": 28, "mention_end": 62, "dataset_mention": "the old model and the full dataset"}, {"mentioned_in_paper": "398", "context_id": "112", "dataset_context": "The experimental results on the TREC Contextual Suggestion Track dataset demonstrated our system effectiveness compared to the state of the art.", "mention_start": 28, "mention_end": 72, "dataset_mention": "the TREC Contextual Suggestion Track dataset"}, {"mentioned_in_paper": "400", "context_id": "5", "dataset_context": "Extensive experiments on existing benchmarks especially non-idealized datasets verify the effectiveness of DNDFN and DNDFN achieves the state of the arts.", "mention_start": 25, "mention_end": 78, "dataset_mention": "existing benchmarks especially non-idealized datasets"}, {"mentioned_in_paper": "400", "context_id": "85", "dataset_context": "First of all, DNDFN is evaluated on idealized datasets ModelNet40 [15] and ShapeNet [16].", "mention_start": 35, "mention_end": 54, "dataset_mention": "idealized datasets"}, {"mentioned_in_paper": "400", "context_id": "86", "dataset_context": "Then the nonidealized dataset ScanObjectNN [17] is employed to the experiments.", "mention_start": 0, "mention_end": 29, "dataset_mention": "Then the nonidealized dataset"}, {"mentioned_in_paper": "400", "context_id": "117", "dataset_context": "ScanObjectNN is a recent real-world point cloud object dataset based on scanned indoor scene data.", "mention_start": 16, "mention_end": 62, "dataset_mention": "a recent real-world point cloud object dataset"}, {"mentioned_in_paper": "400", "context_id": "124", "dataset_context": "First of all, the performance of TN-Learning and IT-Conv on the HARDEST dataset is evaluated in Table 4. IT-Conv only based on TN-Learning, ball query and kNN are competitive with accuracies of 79.4%, 80.3% and 80.4%, respectively.", "mention_start": 59, "mention_end": 79, "dataset_mention": "the HARDEST dataset"}, {"mentioned_in_paper": "400", "context_id": "149", "dataset_context": "More importantly, DNDFN not only shows strong competitiveness in idealized data but also performs well in real-world dataset, as shown in the experiments on two idealized datasets and one non-idealized datasets.", "mention_start": 156, "mention_end": 179, "dataset_mention": "two idealized datasets"}, {"mentioned_in_paper": "400", "context_id": "149", "dataset_context": "More importantly, DNDFN not only shows strong competitiveness in idealized data but also performs well in real-world dataset, as shown in the experiments on two idealized datasets and one non-idealized datasets.", "mention_start": 156, "mention_end": 210, "dataset_mention": "two idealized datasets and one non-idealized datasets"}, {"mentioned_in_paper": "403", "context_id": "2", "dataset_context": "As a result, our method achieves state-of-the-art performance among all other single-image APR methods on the 7-Scenes benchmark and the LLFF dataset.", "mention_start": 105, "mention_end": 149, "dataset_mention": "the 7-Scenes benchmark and the LLFF dataset"}, {"mentioned_in_paper": "403", "context_id": "33", "dataset_context": "With contributions above, our method achieves state-of-theart performance in single-image APR on 7-Scenes benchmark and LLFF dataset.", "mention_start": 96, "mention_end": 132, "dataset_mention": "7-Scenes benchmark and LLFF dataset"}, {"mentioned_in_paper": "403", "context_id": "119", "dataset_context": "We perform a thorough evaluation of the proposed method in Section 4.2 on the 7-Scenes dataset.", "mention_start": 74, "mention_end": 94, "dataset_mention": "the 7-Scenes dataset"}, {"mentioned_in_paper": "403", "context_id": "120", "dataset_context": "We further evaluate our method on the LLFF dataset to demonstrate that the proposed method benefits from both the traditional pose regression method and the direct matching method (Section 4.3).", "mention_start": 34, "mention_end": 50, "dataset_mention": "the LLFF dataset"}, {"mentioned_in_paper": "403", "context_id": "136", "dataset_context": "We evaluate our method on a well-known camera localization dataset 7-Scenes [15, 51].", "mention_start": 26, "mention_end": 66, "dataset_mention": "a well-known camera localization dataset"}, {"mentioned_in_paper": "403", "context_id": "141", "dataset_context": "We further evaluate our method on another real-world complex scene dataset, the LLFF dataset [33].", "mention_start": 34, "mention_end": 74, "dataset_mention": "another real-world complex scene dataset"}, {"mentioned_in_paper": "403", "context_id": "141", "dataset_context": "We further evaluate our method on another real-world complex scene dataset, the LLFF dataset [33].", "mention_start": 75, "mention_end": 92, "dataset_mention": " the LLFF dataset"}, {"mentioned_in_paper": "403", "context_id": "145", "dataset_context": "Table 2 : We report the percentage of correctly re-localized frames below an error threshold of 5cm and percentage of re-localized frames below an error threshold 5\u00b0on the Fern, Fortress, Horns, Room scenes of LLFF dataset [33] al. [63].", "mention_start": 209, "mention_end": 222, "dataset_mention": "LLFF dataset"}, {"mentioned_in_paper": "403", "context_id": "152", "dataset_context": "Effectiveness of Modified Positional Encoding In reallife datasets such as 7-Scenes, there are multiple sources to keep NeRF from rendering a high-quality, photorealistic view of the scene.", "mention_start": 49, "mention_end": 66, "dataset_mention": "reallife datasets"}, {"mentioned_in_paper": "403", "context_id": "155", "dataset_context": "We build a toy example to demonstrate the phenomenon in the 7-Scenes dataset, and the effectiveness of our modification in NeRF's positional encoding.", "mention_start": 56, "mention_end": 76, "dataset_mention": "the 7-Scenes dataset"}, {"mentioned_in_paper": "403", "context_id": "158", "dataset_context": "We report the peak signal-to-noise ratio (PSNR) on this toy dataset for fixed full encoding (m = 10) in the original NeRF paper, fixed half encoding (m = 5), and the coarse-(a) Fixed P.E.", "mention_start": 51, "mention_end": 67, "dataset_mention": "this toy dataset"}, {"mentioned_in_paper": "403", "context_id": "161", "dataset_context": "We show a qualitative comparison between the original NeRF embedding scheme and our modified coarse-to-fine scheme in Fig. 3. Overall, we find that using a coarse-to-fine positional encoding approach generally obtains a higher quality NeRF model throughout the 7-Scenes dataset in our experiments.", "mention_start": 153, "mention_end": 277, "dataset_mention": "a coarse-to-fine positional encoding approach generally obtains a higher quality NeRF model throughout the 7-Scenes dataset"}, {"mentioned_in_paper": "403", "context_id": "191", "dataset_context": "present a relocalization pipeline that outperforms previous single-image APR methods on the 7-Scenes benchmark and achieves state-of-the-art performance on the LLFF dataset, with two main contributions.", "mention_start": 156, "mention_end": 172, "dataset_mention": "the LLFF dataset"}, {"mentioned_in_paper": "403", "context_id": "194", "dataset_context": "To adapt with outwardlooking relocalization datasets, which violates assumptions in NeRF, we employ a coarse-to-fine positional encoding strategy to improve rendering qualities.", "mention_start": 14, "mention_end": 52, "dataset_mention": "outwardlooking relocalization datasets"}, {"mentioned_in_paper": "403", "context_id": "225", "dataset_context": "For scenes with low texture or flat background, i.e., Lego scene from NeRF Synthetic dataset [34], pose loss ensures the regressed pose is regularized in relevant positions.", "mention_start": 69, "mention_end": 92, "dataset_mention": "NeRF Synthetic dataset"}, {"mentioned_in_paper": "403", "context_id": "230", "dataset_context": "able 9: Result of using different \u03bb 1 and \u03bb 2 values on Heads, Fire, and Pumpkin in 7-Scenes dataset (first 3 rows).", "mention_start": 83, "mention_end": 100, "dataset_mention": "7-Scenes dataset"}, {"mentioned_in_paper": "403", "context_id": "231", "dataset_context": "We also tested in Lego scene (bottom row) on the NeRF synthesis datasets [34], which contains large areas of textureless background.", "mention_start": 45, "mention_end": 72, "dataset_mention": "the NeRF synthesis datasets"}, {"mentioned_in_paper": "405", "context_id": "5", "dataset_context": "Experiments show that our method achieves the state of the arts in symmetry detection on LDRS and DENDI datasets.", "mention_start": 89, "mention_end": 112, "dataset_mention": "LDRS and DENDI datasets"}, {"mentioned_in_paper": "405", "context_id": "28", "dataset_context": "Finally, the number of images is 1.7x and 2.0x larger than the second-largest reflection and rotation symmetry detection datasets, respectively.", "mention_start": 58, "mention_end": 129, "dataset_mention": "the second-largest reflection and rotation symmetry detection datasets"}, {"mentioned_in_paper": "405", "context_id": "31", "dataset_context": "\u2022 We present a new dataset, DENse and DIverse symmetry dataset (DENDI), containing images of reflection and rotation symmetries annotated in a broader range of typical real-world objects.", "mention_start": 27, "mention_end": 62, "dataset_mention": " DENse and DIverse symmetry dataset"}, {"mentioned_in_paper": "405", "context_id": "125", "dataset_context": "We present a new dataset for symmetry detection named DENse and DIverse symmetry dataset (DENDI) in the following.", "mention_start": 29, "mention_end": 88, "dataset_mention": "symmetry detection named DENse and DIverse symmetry dataset"}, {"mentioned_in_paper": "405", "context_id": "129", "dataset_context": "The early reflection symmetry datasets [4, 27] contain small number of images with few reflection axis and rotation center.", "mention_start": 0, "mention_end": 38, "dataset_mention": "The early reflection symmetry datasets"}, {"mentioned_in_paper": "405", "context_id": "132", "dataset_context": "Furthermore, no existing reflection symmetry datasets take into account the continuous symmetry group, such as a circle with an infinite number of reflection symmetry axes.", "mention_start": 12, "mention_end": 53, "dataset_mention": " no existing reflection symmetry datasets"}, {"mentioned_in_paper": "405", "context_id": "136", "dataset_context": "We integrate 239 images of NYU [4] and 181 images of SDRW [27], and collect 2,080 images from COCO [26] dataset.", "mention_start": 94, "mention_end": 111, "dataset_mention": "COCO [26] dataset"}, {"mentioned_in_paper": "405", "context_id": "139", "dataset_context": "The sizes of the symmetry detection datasets are compared in Tab. 1.", "mention_start": 13, "mention_end": 44, "dataset_mention": "the symmetry detection datasets"}, {"mentioned_in_paper": "405", "context_id": "168", "dataset_context": "The complexity of our rotation symmetry dataset is high, as shown in Fig. 5 (d).", "mention_start": 18, "mention_end": 47, "dataset_mention": "our rotation symmetry dataset"}, {"mentioned_in_paper": "405", "context_id": "174", "dataset_context": "For rotation, we only use DENDI which contains the images of SDRW [27] rotation dataset.", "mention_start": 60, "mention_end": 87, "dataset_mention": "SDRW [27] rotation dataset"}, {"mentioned_in_paper": "405", "context_id": "202", "dataset_context": "Note that the real dataset consists of SDRW, LDRS, and NYU [4] dataset.", "mention_start": 50, "mention_end": 70, "dataset_mention": " and NYU [4] dataset"}, {"mentioned_in_paper": "405", "context_id": "252", "dataset_context": "The details of data annotation for the DENDI dataset are described in this section.", "mention_start": 35, "mention_end": 52, "dataset_mention": "the DENDI dataset"}, {"mentioned_in_paper": "405", "context_id": "258", "dataset_context": "A reflection symmetry axis is drawn as a line following the previous datasets [4, 12, 13, 27, 38].", "mention_start": 39, "mention_end": 77, "dataset_mention": "a line following the previous datasets"}, {"mentioned_in_paper": "405", "context_id": "264", "dataset_context": "We again employ  wise, in the reflection symmetry dataset, symmetry in characters such as H, I, N, O, S, X, and Z, as well as the numbers 0 and 8, are taken into account.", "mention_start": 25, "mention_end": 57, "dataset_mention": "the reflection symmetry dataset"}, {"mentioned_in_paper": "406", "context_id": "28", "dataset_context": "Furthermore, we establish a new dataset FS13 with several heterogeneous domains.", "mention_start": 12, "mention_end": 39, "dataset_mention": " we establish a new dataset"}, {"mentioned_in_paper": "406", "context_id": "49", "dataset_context": "NIR-VIS datasets include CASIA NIR-VIS 2.0 [23], Oulu-CASIA NIR-VIS [3] and the BUAA-VisNir Face [13] databases.", "mention_start": 0, "mention_end": 16, "dataset_mention": "NIR-VIS datasets"}, {"mentioned_in_paper": "406", "context_id": "50", "dataset_context": "Sketch-VIS datasets include CUHK Face Sketch FERET (CUFSF) [44].", "mention_start": 0, "mention_end": 19, "dataset_mention": "Sketch-VIS datasets"}, {"mentioned_in_paper": "406", "context_id": "51", "dataset_context": "More sketch datasets focus on sketch-photo generation and sketch face recognition, e.g., IIIT-D Viewed Sketch database [1], XM2VTS database [27].", "mention_start": 0, "mention_end": 20, "dataset_mention": "More sketch datasets"}, {"mentioned_in_paper": "406", "context_id": "52", "dataset_context": " [18] proposes selfie2anime dataset with diverse poses.", "mention_start": 14, "mention_end": 35, "dataset_mention": "selfie2anime dataset"}, {"mentioned_in_paper": "406", "context_id": "54", "dataset_context": "Different from above datasets, we establish a novel heterogeneous dataset including more diverse styles to study the mutual IDS among a variety of styles.", "mention_start": 15, "mention_end": 29, "dataset_mention": "above datasets"}, {"mentioned_in_paper": "406", "context_id": "54", "dataset_context": "Different from above datasets, we establish a novel heterogeneous dataset including more diverse styles to study the mutual IDS among a variety of styles.", "mention_start": 30, "mention_end": 73, "dataset_mention": " we establish a novel heterogeneous dataset"}, {"mentioned_in_paper": "406", "context_id": "68", "dataset_context": "To study the challenging IDS problem, we propose FS13 dataset, where there are 13 PSU heterogeneous domains considering abundant lighting conditions, art representations and life dimensions, as shown in Figure 3.", "mention_start": 48, "mention_end": 61, "dataset_mention": "FS13 dataset"}, {"mentioned_in_paper": "406", "context_id": "78", "dataset_context": "We select some girl faces in selfie2anime dataset [18], and crawl some boy faces.", "mention_start": 29, "mention_end": 49, "dataset_mention": "selfie2anime dataset"}, {"mentioned_in_paper": "406", "context_id": "135", "dataset_context": "Styleverse is trained with the proposed FS13 dataset where N = 13, M = 100.", "mention_start": 27, "mention_end": 52, "dataset_mention": "the proposed FS13 dataset"}, {"mentioned_in_paper": "406", "context_id": "199", "dataset_context": "We have proposed Styleverse to implement the topologyaware identity stylization based on the new established FS13 dataset, achieving high fidelity content and diverse styles.", "mention_start": 89, "mention_end": 121, "dataset_mention": "the new established FS13 dataset"}, {"mentioned_in_paper": "408", "context_id": "3", "dataset_context": "Most existing methods are not fast enough to analyze large data sets for motif finding or achieve low accuracy for motif clustering.", "mention_start": 45, "mention_end": 68, "dataset_mention": "analyze large data sets"}, {"mentioned_in_paper": "408", "context_id": "17", "dataset_context": "For example, IGC data set in human gut microbiome (Qin et al., 2010) has about 10 million protein sequences with total length 2.4 billion amino acids.", "mention_start": 12, "mention_end": 25, "dataset_mention": " IGC data set"}, {"mentioned_in_paper": "408", "context_id": "100", "dataset_context": "The length of k-mers is 25, and all of them are randomly selected from IGC data set (Qin et al., 2010).", "mention_start": 70, "mention_end": 83, "dataset_mention": "IGC data set"}, {"mentioned_in_paper": "408", "context_id": "103", "dataset_context": "The center of each motif sequences are treated as queries for searching k-mers from IGC data set, and a k-mer with a distance less than threshold T to the center point of a motif, then the k-mer is identified as a sequence of that motif.", "mention_start": 84, "mention_end": 96, "dataset_mention": "IGC data set"}, {"mentioned_in_paper": "408", "context_id": "124", "dataset_context": "We assessed the performance of HSERACH for clustering protein sequences on a ground truth data set selected from Pfam database.", "mention_start": 75, "mention_end": 98, "dataset_mention": "a ground truth data set"}, {"mentioned_in_paper": "408", "context_id": "131", "dataset_context": "We investigated the ground truth data set by plotting pairwise distances for data points inside motifs, and between motifs, as shown in Figure 5. Since HSEARCH is a heuristic algorithm, if two data points have distance to the center less than T , it dose not guarantee that the distance between these two data points is less than T .", "mention_start": 0, "mention_end": 41, "dataset_mention": "We investigated the ground truth data set"}, {"mentioned_in_paper": "408", "context_id": "137", "dataset_context": "All four programs ran very fast in the ground truth data set.", "mention_start": 35, "mention_end": 60, "dataset_mention": "the ground truth data set"}, {"mentioned_in_paper": "409", "context_id": "113", "dataset_context": "The network is trained on three currently prevalent large scale video datasets of GOT-10k Huang et al. (2019), LaSOT Fan et al. (2019) and TrackingNet Mueller et al. (2018) in form of template-search image pair, the deviation between which is restrict within an interval of less than 100 frames.", "mention_start": 26, "mention_end": 78, "dataset_mention": "three currently prevalent large scale video datasets"}, {"mentioned_in_paper": "409", "context_id": "167", "dataset_context": "Extensive experiments show that PCDHV achieves better or comparable results than SOTA algorithm on several mainstream datasets.", "mention_start": 99, "mention_end": 126, "dataset_mention": "several mainstream datasets"}, {"mentioned_in_paper": "411", "context_id": "163", "dataset_context": "To investigate the bounds for convergence more rigorously, we compute the ratios f IRLS (k gt )/f opt (k \u03b4 ) on two publicly avaiable datasets: Levin et al.'s [11] and Sun et al.'s [19] (Fig. 4).", "mention_start": 111, "mention_end": 142, "dataset_mention": "two publicly avaiable datasets"}, {"mentioned_in_paper": "411", "context_id": "164", "dataset_context": "Levin et al.'s dataset consists of 32 real blurred images generated from four images and eight blur kernels.", "mention_start": 0, "mention_end": 22, "dataset_mention": "Levin et al.'s dataset"}, {"mentioned_in_paper": "411", "context_id": "204", "dataset_context": "We conducted performance comparison of the multi-scale version using Levin et al.'s dataset [11] (Fig. 8).", "mention_start": 69, "mention_end": 91, "dataset_mention": "Levin et al.'s dataset"}, {"mentioned_in_paper": "411", "context_id": "208", "dataset_context": "Besides estimation of blur caused by camera shakes, there are many problems related to image blur, such as de- [19] using the cumulative error ratio histogram proposed by [11] and Levin et al.'s dataset [11].", "mention_start": 170, "mention_end": 202, "dataset_mention": "[11] and Levin et al.'s dataset"}, {"mentioned_in_paper": "412", "context_id": "5", "dataset_context": "Cross-view gait recognition experiments are conducted on OU-ISIR large population dataset.", "mention_start": 57, "mention_end": 89, "dataset_mention": "OU-ISIR large population dataset"}, {"mentioned_in_paper": "412", "context_id": "63", "dataset_context": "We test it on OU-ISIR large population dataset [11], and obtain promising performance which is comparable with the state of art.", "mention_start": 14, "mention_end": 46, "dataset_mention": "OU-ISIR large population dataset"}, {"mentioned_in_paper": "412", "context_id": "112", "dataset_context": "We test our method on OU-ISIR large population dataset [11], as it is the largest gait dataset suitable for train-ing deep neural networks.", "mention_start": 22, "mention_end": 54, "dataset_mention": "OU-ISIR large population dataset"}, {"mentioned_in_paper": "412", "context_id": "112", "dataset_context": "We test our method on OU-ISIR large population dataset [11], as it is the largest gait dataset suitable for train-ing deep neural networks.", "mention_start": 69, "mention_end": 94, "dataset_mention": "the largest gait dataset"}, {"mentioned_in_paper": "412", "context_id": "113", "dataset_context": "There are two versions for OU-ISIR large population dataset: OULP-C1V1 and OULP-C1V2 1.", "mention_start": 27, "mention_end": 59, "dataset_mention": "OU-ISIR large population dataset"}, {"mentioned_in_paper": "412", "context_id": "158", "dataset_context": "From these two tables, we can see that the proposed method shows promising results on OULP-C1V1 gait dataset.", "mention_start": 85, "mention_end": 108, "dataset_mention": "OULP-C1V1 gait dataset"}, {"mentioned_in_paper": "412", "context_id": "171", "dataset_context": "Experiments for cross-view gait recognition on OU-ISIR large popula-tion dataset are conducted.", "mention_start": 47, "mention_end": 80, "dataset_mention": "OU-ISIR large popula-tion dataset"}, {"mentioned_in_paper": "413", "context_id": "1", "dataset_context": "The performance improvements they have provided in the so called in-the-wild datasets are significant, however, their performance under image quality degradations have not been assessed, yet.", "mention_start": 51, "mention_end": 85, "dataset_mention": "the so called in-the-wild datasets"}, {"mentioned_in_paper": "413", "context_id": "7", "dataset_context": "With the recent advances in deep convolutional neural networks, researchers have reached encouraging improvements in face recognition accuracy on the LFW [Hu07] and YouTube Faces [WHM11] datasets [Su15, PVZ15, SKP15].", "mention_start": 145, "mention_end": 195, "dataset_mention": "the LFW [Hu07] and YouTube Faces [WHM11] datasets"}, {"mentioned_in_paper": "414", "context_id": "180", "dataset_context": "To show the superiority of our method, we create an unprecedented continual learning experiment with 100 tasks in which the model learns to classify 300 classes from the Omniglot dataset.", "mention_start": 165, "mention_end": 186, "dataset_mention": "the Omniglot dataset"}, {"mentioned_in_paper": "414", "context_id": "188", "dataset_context": "To construct a CL task with the Omniglot dataset, we follow the approach of Rao et al. [2019] and use alphabets as classes, then split the whole dataset into ten tasks of 5 classes each.", "mention_start": 28, "mention_end": 48, "dataset_mention": "the Omniglot dataset"}, {"mentioned_in_paper": "414", "context_id": "194", "dataset_context": "To the best of our knowledge, LogCL is the first generativebased model which achieves a satisfactory performance on the CIFAR-100 dataset in a scenario with 20 splits.", "mention_start": 115, "mention_end": 137, "dataset_mention": "the CIFAR-100 dataset"}, {"mentioned_in_paper": "414", "context_id": "201", "dataset_context": "Table 2: Average accuracy after the final task in the class incremental scenario (in % \u00b1 SEM) for CIFAR-100 dataset with 20 splits.", "mention_start": 97, "mention_end": 115, "dataset_mention": "CIFAR-100 dataset"}, {"mentioned_in_paper": "414", "context_id": "214", "dataset_context": "In the experimental study, we apply our approach to obtain state-of-the-art performance of continual learning on MNIST, Omniglot, and CIFAR-100 datasets.", "mention_start": 129, "mention_end": 152, "dataset_mention": " and CIFAR-100 datasets"}, {"mentioned_in_paper": "415", "context_id": "11", "dataset_context": "Recognition of human actions from RGB-D data has generated renewed interest in the computer vision com-Figure 1 : Samples of variants of SFAM for action \"Bounce Basketball\" from M 2 I Dataset [21].", "mention_start": 177, "mention_end": 191, "dataset_mention": "M 2 I Dataset"}, {"mentioned_in_paper": "415", "context_id": "75", "dataset_context": "This category has achieved state-of-the-art results in action recognition on many RGB and depth/skeleton datasets.", "mention_start": 77, "mention_end": 113, "dataset_mention": "many RGB and depth/skeleton datasets"}, {"mentioned_in_paper": "415", "context_id": "103", "dataset_context": "For the RGB-D datasets with spatial misalignment, we propose an effective self-calibration method to perform spatial alignment without knowledge of the cameras parameters.", "mention_start": 4, "mention_end": 22, "dataset_mention": "the RGB-D datasets"}, {"mentioned_in_paper": "415", "context_id": "156", "dataset_context": "A few examples of the SFAM variants are shown in Figure 1 Dataset [21].", "mention_start": 49, "mention_end": 65, "dataset_mention": "Figure 1 Dataset"}, {"mentioned_in_paper": "415", "context_id": "170", "dataset_context": "According to the survey of RGB-D datasets [63], we chose two public benchmark datasets, which contain both RGB+depth modalities and have relatively large training samples to evaluate the proposed method.", "mention_start": 27, "mention_end": 41, "dataset_mention": "RGB-D datasets"}, {"mentioned_in_paper": "415", "context_id": "171", "dataset_context": "Specifically we chose ChaLearn LAP IsoGD Dataset [46] and M 2 I Dataset [21].", "mention_start": 22, "mention_end": 48, "dataset_mention": "ChaLearn LAP IsoGD Dataset"}, {"mentioned_in_paper": "415", "context_id": "171", "dataset_context": "Specifically we chose ChaLearn LAP IsoGD Dataset [46] and M 2 I Dataset [21].", "mention_start": 22, "mention_end": 71, "dataset_mention": "ChaLearn LAP IsoGD Dataset [46] and M 2 I Dataset"}, {"mentioned_in_paper": "415", "context_id": "176", "dataset_context": "For ChaLearn LAP IsoGD Dataset, in order to minimize the interference of the background, it is assumed that the background in the histogram of depth maps occupies the last peak representing far distances.", "mention_start": 4, "mention_end": 30, "dataset_mention": "ChaLearn LAP IsoGD Dataset"}, {"mentioned_in_paper": "415", "context_id": "193", "dataset_context": "The ChaLearn LAP IsoGD Dataset [46] includes 47933 RGB-D depth sequences, each RGB-D video representing one gesture instance.", "mention_start": 0, "mention_end": 30, "dataset_mention": "The ChaLearn LAP IsoGD Dataset"}, {"mentioned_in_paper": "415", "context_id": "196", "dataset_context": "To use this dataset for scene flow calculation, we estimate the depth values using the average minimum and maximum values provided for CGD dataset.", "mention_start": 134, "mention_end": 146, "dataset_mention": "CGD dataset"}, {"mentioned_in_paper": "415", "context_id": "211", "dataset_context": "Even though this dataset is large, on average 144 video clips per class, it is still much smaller compared with 1200 images per class in ImageNet.", "mention_start": 5, "mention_end": 24, "dataset_mention": "though this dataset"}, {"mentioned_in_paper": "415", "context_id": "219", "dataset_context": "Multi-modal & Multi-view & Interactive (M 2 I) Dataset [21] provides person-person interaction actions and person-object interaction actions.", "mention_start": 0, "mention_end": 54, "dataset_mention": "Multi-modal & Multi-view & Interactive (M 2 I) Dataset"}, {"mentioned_in_paper": "415", "context_id": "223", "dataset_context": "In total, M 2 I dataset contains 1760 samples (22 actions \u00d7 20 groups \u00d7 2 views \u00d7 2 run).", "mention_start": 9, "mention_end": 23, "dataset_mention": " M 2 I dataset"}, {"mentioned_in_paper": "415", "context_id": "232", "dataset_context": "Table 2 shows the comparisons on the M 2 I Dataset for single task scenario, that is, learning and testing in the same view while Table 3 presents the comparisons for cross-view scenario.", "mention_start": 33, "mention_end": 50, "dataset_mention": "the M 2 I Dataset"}, {"mentioned_in_paper": "416", "context_id": "117", "dataset_context": "LM dataset consists of 13 object instances, each of which is labeled around the center of a sequence of \u22481.2k cluttered images.", "mention_start": 0, "mention_end": 10, "dataset_mention": "LM dataset"}, {"mentioned_in_paper": "416", "context_id": "120", "dataset_context": "YCB-V dataset is very challenging due to severe occlusions and various lighting conditions.", "mention_start": 0, "mention_end": 13, "dataset_mention": "YCB-V dataset"}, {"mentioned_in_paper": "416", "context_id": "129", "dataset_context": "It clearly shows that our method surpasses the baseline by a large margin, especially on the strict 5\u00b02 cm metric, achieving an absolute improvement of 26.7% on the challenging REAL275 dataset and 21.3% on the CAMERA25 dataset.", "mention_start": 176, "mention_end": 192, "dataset_mention": "REAL275 dataset"}, {"mentioned_in_paper": "416", "context_id": "129", "dataset_context": "It clearly shows that our method surpasses the baseline by a large margin, especially on the strict 5\u00b02 cm metric, achieving an absolute improvement of 26.7% on the challenging REAL275 dataset and 21.3% on the CAMERA25 dataset.", "mention_start": 205, "mention_end": 226, "dataset_mention": "the CAMERA25 dataset"}, {"mentioned_in_paper": "416", "context_id": "176", "dataset_context": "Table 5 shows the experimental results on LM dataset.", "mention_start": 42, "mention_end": 52, "dataset_mention": "LM dataset"}, {"mentioned_in_paper": "416", "context_id": "181", "dataset_context": "We verify this assumption in two instances on the challenging YCB-V dataset, i.e., master chef can and cracker box, and present qualitative and quantitative results in Table 6.", "mention_start": 62, "mention_end": 75, "dataset_mention": "YCB-V dataset"}, {"mentioned_in_paper": "416", "context_id": "201", "dataset_context": "Since rare occlusion exists in REAL275 dataset, we cropped a 25% block around a random corner of predicted bounding boxes to imitate occlusion.", "mention_start": 31, "mention_end": 46, "dataset_mention": "REAL275 dataset"}, {"mentioned_in_paper": "417", "context_id": "7", "dataset_context": "Experimental results on four groups, totally twelve public color image datasets show that the proposed method outperforms most of the LBP variants for color image recognition in terms of dimension of features, recognition accuracy under noise-free, noisy and illumination variation conditions.", "mention_start": 36, "mention_end": 79, "dataset_mention": " totally twelve public color image datasets"}, {"mentioned_in_paper": "417", "context_id": "124", "dataset_context": "The experimental setting and color mage datasets used for validation are introduced in subsection A and B. In subsection C, the color image recognition ability affected by the dimension of feature vector by the proposed cLBP is discussed.", "mention_start": 0, "mention_end": 48, "dataset_mention": "The experimental setting and color mage datasets"}, {"mentioned_in_paper": "417", "context_id": "135", "dataset_context": "To thoroughly test the performance of the proposed cLBP for color image recognition, totally twelve public color datasets that can be divided into four groups: 1) KTH-TIPS", "mention_start": 84, "mention_end": 121, "dataset_mention": " totally twelve public color datasets"}, {"mentioned_in_paper": "417", "context_id": "136", "dataset_context": "[30], STex-512S [31] and Colored Brodatz [32] which belong to the color texture datasets; 2) Wang or SIMPLIcity [33], Corel-10k [34], FTVL [35] and Coil-100 [36] which belong to the color object datasets; 3) Color FERET [37] and AR face [38] which belong to the color face datasets were utilized for validating the recognition accuracy and noise robustness.", "mention_start": 61, "mention_end": 88, "dataset_mention": "the color texture datasets"}, {"mentioned_in_paper": "417", "context_id": "136", "dataset_context": "[30], STex-512S [31] and Colored Brodatz [32] which belong to the color texture datasets; 2) Wang or SIMPLIcity [33], Corel-10k [34], FTVL [35] and Coil-100 [36] which belong to the color object datasets; 3) Color FERET [37] and AR face [38] which belong to the color face datasets were utilized for validating the recognition accuracy and noise robustness.", "mention_start": 177, "mention_end": 203, "dataset_mention": "the color object datasets"}, {"mentioned_in_paper": "417", "context_id": "136", "dataset_context": "[30], STex-512S [31] and Colored Brodatz [32] which belong to the color texture datasets; 2) Wang or SIMPLIcity [33], Corel-10k [34], FTVL [35] and Coil-100 [36] which belong to the color object datasets; 3) Color FERET [37] and AR face [38] which belong to the color face datasets were utilized for validating the recognition accuracy and noise robustness.", "mention_start": 257, "mention_end": 281, "dataset_mention": "the color face datasets"}, {"mentioned_in_paper": "417", "context_id": "138", "dataset_context": "The detail description about these twelve color image datasets is summarized in Table I.", "mention_start": 29, "mention_end": 62, "dataset_mention": "these twelve color image datasets"}, {"mentioned_in_paper": "417", "context_id": "139", "dataset_context": "In this experiment, the classification accuracy of the proposed cLBP with different dimension of feature, is evaluated on three of the above twelve image datasets (from the texture, object and face groups), to study the effectiveness of the proposed pattern learning framework.", "mention_start": 130, "mention_end": 162, "dataset_mention": "the above twelve image datasets"}, {"mentioned_in_paper": "417", "context_id": "143", "dataset_context": "It can be seen from this figure that, for both cLBP-RGB and cLBP-RSS, the recognition accuracy increase firstly and then decreases with the increment of feature dimension especially for the \"Corel-FERET\" dataset, while for the cLBP, the recognition accuracy increases firstly and then keeps stable.", "mention_start": 185, "mention_end": 211, "dataset_mention": "the \"Corel-FERET\" dataset"}, {"mentioned_in_paper": "417", "context_id": "150", "dataset_context": "Firstly, we conduct the color texture image recognition on \"STex-512S\", \"Colored Brodatz\" and \"KTH-TIPS\" image datasets by the proposed cLBP and other LBP variants.", "mention_start": 71, "mention_end": 119, "dataset_mention": " \"Colored Brodatz\" and \"KTH-TIPS\" image datasets"}, {"mentioned_in_paper": "417", "context_id": "151", "dataset_context": "Table II shows the recognition accuracy on those three color texture image datasets by all the comparison methods.", "mention_start": 43, "mention_end": 83, "dataset_mention": "those three color texture image datasets"}, {"mentioned_in_paper": "417", "context_id": "155", "dataset_context": "Secondly, we test the color object recognition ability of the proposed cLBP and comparison methods on \"Wang or SIMPLIcity\", \"Corel-10k\", \"FTVL\" and \"Coil-100\" datasets.", "mention_start": 136, "mention_end": 167, "dataset_mention": " \"FTVL\" and \"Coil-100\" datasets"}, {"mentioned_in_paper": "417", "context_id": "158", "dataset_context": "When D is up to 900, the recognition of our proposed method on \"Corel-10k\" dataset is up to 74.66%, while the highest recognition accuracy of the state of the art methods on this dataset is only 63.71%.", "mention_start": 62, "mention_end": 82, "dataset_mention": "\"Corel-10k\" dataset"}, {"mentioned_in_paper": "417", "context_id": "159", "dataset_context": "Thirdly, the color face recognition ability of all comparison methods are evaluated on the \"Color FERET\" and \"AR face\" datasets.", "mention_start": 86, "mention_end": 127, "dataset_mention": "the \"Color FERET\" and \"AR face\" datasets"}, {"mentioned_in_paper": "417", "context_id": "169", "dataset_context": "Fig. 7 shows the recognition accuracy achieved by different methods on the first three groups (nine) image datasets.", "mention_start": 71, "mention_end": 115, "dataset_mention": "the first three groups (nine) image datasets"}, {"mentioned_in_paper": "417", "context_id": "171", "dataset_context": "When the SNR is decreased to 0 dB (i.e., half noise and half signal), the recognition accuracy of the proposed cLBP on \"KTH-TIPS\" image dataset is still up to 80%.", "mention_start": 118, "mention_end": 143, "dataset_mention": "\"KTH-TIPS\" image dataset"}, {"mentioned_in_paper": "417", "context_id": "177", "dataset_context": "D, some images in the \"KTH-TIPS\", \"color FERET\" and \"AR face\" image datasets are obtained under illumination variation.", "mention_start": 33, "mention_end": 76, "dataset_mention": " \"color FERET\" and \"AR face\" image datasets"}, {"mentioned_in_paper": "417", "context_id": "179", "dataset_context": "In this experiment, three color image datasets, i.e., \"Outex-14\" [39], \"ALOI\" [40] and \"CUReT\" [41] which are specifically constructed for illumination invariant image recognition as well as have been widely used in existing methods, are utilized for further validating the recognition ability of the proposed cLBP under illumination variation.", "mention_start": 19, "mention_end": 46, "dataset_mention": " three color image datasets"}, {"mentioned_in_paper": "417", "context_id": "181", "dataset_context": "Table V shows the recognition accuracy achieved on \"Outex-14\", \"ALOI\" and \"CUReT\" image datasets by different methods.", "mention_start": 62, "mention_end": 96, "dataset_mention": " \"ALOI\" and \"CUReT\" image datasets"}, {"mentioned_in_paper": "418", "context_id": "160", "dataset_context": "Yosemite Flickr Dataset [41] consists of 1,200 winter photos and 1,540 summer photos of Yosemite National Park.", "mention_start": 0, "mention_end": 23, "dataset_mention": "Yosemite Flickr Dataset"}, {"mentioned_in_paper": "418", "context_id": "163", "dataset_context": "Edge2Photo Dataset [5] contains photos of 250 categories of objects and the corresponding edges.", "mention_start": 0, "mention_end": 18, "dataset_mention": "Edge2Photo Dataset"}, {"mentioned_in_paper": "420", "context_id": "52", "dataset_context": "Our choice of parameters was based on the validation error (root mean squared difference between input and reconstruction) on a 17,000 example dataset (15,000 examples for training and 2,000 for validation).", "mention_start": 131, "mention_end": 150, "dataset_mention": "000 example dataset"}, {"mentioned_in_paper": "421", "context_id": "7", "dataset_context": "We evaluate results of the proposed approach on two new datasets: (i) HW-SQuAD: a synthetic, handwritten document image counterpart of SQuAD1.0 dataset, and (ii) BenthamQA: a smaller set of QA pairs defined on documents from the popular Bentham manuscripts collection.", "mention_start": 134, "mention_end": 151, "dataset_mention": "SQuAD1.0 dataset"}, {"mentioned_in_paper": "421", "context_id": "20", "dataset_context": "The questions in this space are usually semantically richer, compared to questions in VQA datasets.", "mention_start": 85, "mention_end": 98, "dataset_mention": "VQA datasets"}, {"mentioned_in_paper": "421", "context_id": "35", "dataset_context": "For example, in the STE VQA dataset [60], in addition to the textual answer, each question is provided with a bounding box representing the image region where the question is grounded on.", "mention_start": 15, "mention_end": 35, "dataset_mention": "the STE VQA dataset"}, {"mentioned_in_paper": "421", "context_id": "45", "dataset_context": "-Introduce two new datasets: HW-SQuAD and Ben-thamQA, for QA on handwritten document collection.", "mention_start": 0, "mention_end": 27, "dataset_mention": "-Introduce two new datasets"}, {"mentioned_in_paper": "421", "context_id": "55", "dataset_context": "Introduction of large scale datasets like the Stanford Question Answering Dataset (SQuAD1.0)", "mention_start": 16, "mention_end": 81, "dataset_mention": "large scale datasets like the Stanford Question Answering Dataset"}, {"mentioned_in_paper": "421", "context_id": "56", "dataset_context": " [47], Mi-croSoft MAchine Reading COmprehension dataset (MS MACRO) [43] and Natural Questions [38] have led to the development of deep learning based QA/MRC systems [21, 50, 59, 14] that can answer questions about a given a corpus of text or passage.", "mention_start": 6, "mention_end": 55, "dataset_mention": " Mi-croSoft MAchine Reading COmprehension dataset"}, {"mentioned_in_paper": "421", "context_id": "59", "dataset_context": "Most of the early VQA datasets and methods disregard text present in the images, and the problem is often modelled as a multi-class classification where the set of output answers is fixed.", "mention_start": 8, "mention_end": 30, "dataset_mention": "the early VQA datasets"}, {"mentioned_in_paper": "421", "context_id": "63", "dataset_context": "Our work is different from these tasks on two accounts: (i) these datasets contain images \"in the wild\" which are drawn from popular scene text datasets or datasets like Open-Images [35], which predominantly have scattered text tokens compared to the handwritten document images we consider, and (ii) almost all VQA problems including the ones involving text on the images are formulated as QA on a single image, while the proposed QA task is for a collection of document images.", "mention_start": 55, "mention_end": 74, "dataset_mention": " (i) these datasets"}, {"mentioned_in_paper": "421", "context_id": "63", "dataset_context": "Our work is different from these tasks on two accounts: (i) these datasets contain images \"in the wild\" which are drawn from popular scene text datasets or datasets like Open-Images [35], which predominantly have scattered text tokens compared to the handwritten document images we consider, and (ii) almost all VQA problems including the ones involving text on the images are formulated as QA on a single image, while the proposed QA task is for a collection of document images.", "mention_start": 124, "mention_end": 152, "dataset_mention": "popular scene text datasets"}, {"mentioned_in_paper": "421", "context_id": "65", "dataset_context": "The TQA dataset aims at answering questions given a context of text, diagrams and images.", "mention_start": 0, "mention_end": 15, "dataset_mention": "The TQA dataset"}, {"mentioned_in_paper": "421", "context_id": "144", "dataset_context": "The annotation of a QA dataset on a handwritten document collection requires considerable human effort.", "mention_start": 18, "mention_end": 30, "dataset_mention": "a QA dataset"}, {"mentioned_in_paper": "421", "context_id": "146", "dataset_context": "As an alternative to costly human annotation, we build a dataset using an existing QA dataset for electronic text by reusing the questions and answers.", "mention_start": 70, "mention_end": 93, "dataset_mention": "an existing QA dataset"}, {"mentioned_in_paper": "421", "context_id": "161", "dataset_context": "data to create the new HW-SQuAD dataset.", "mention_start": 15, "mention_end": 39, "dataset_mention": "the new HW-SQuAD dataset"}, {"mentioned_in_paper": "421", "context_id": "190", "dataset_context": "To build a handwritten QA dataset containing real images, we use manuscripts from historical collections since OCR is typically poorer on such images.", "mention_start": 9, "mention_end": 33, "dataset_mention": "a handwritten QA dataset"}, {"mentioned_in_paper": "421", "context_id": "192", "dataset_context": "We use the ImageCLEF 2016 Bentham Handwritten Retrieval dataset [57], which has images from the Bentham Transcriptorium project [10].", "mention_start": 7, "mention_end": 63, "dataset_mention": "the ImageCLEF 2016 Bentham Handwritten Retrieval dataset"}, {"mentioned_in_paper": "421", "context_id": "199", "dataset_context": "Hence we tested an approach similar to the annotation of SQuAD-like datasets.", "mention_start": 57, "mention_end": 76, "dataset_mention": "SQuAD-like datasets"}, {"mentioned_in_paper": "421", "context_id": "244", "dataset_context": "Note that the HW-SQuAD and BenthamQA datasets can also be used for recognitionbased QA where textual answers are expected.", "mention_start": 5, "mention_end": 45, "dataset_mention": "that the HW-SQuAD and BenthamQA datasets"}, {"mentioned_in_paper": "421", "context_id": "256", "dataset_context": "The classifier is trained using synthetic, handwritten word images in HW-SYNTH dataset [37] and word images from IAM dataset [40].", "mention_start": 69, "mention_end": 86, "dataset_mention": "HW-SYNTH dataset"}, {"mentioned_in_paper": "421", "context_id": "256", "dataset_context": "The classifier is trained using synthetic, handwritten word images in HW-SYNTH dataset [37] and word images from IAM dataset [40].", "mention_start": 112, "mention_end": 124, "dataset_mention": "IAM dataset"}, {"mentioned_in_paper": "421", "context_id": "259", "dataset_context": "The pretrained model is trained on HW-SYNTH and IAM datasets.", "mention_start": 35, "mention_end": 60, "dataset_mention": "HW-SYNTH and IAM datasets"}, {"mentioned_in_paper": "421", "context_id": "319", "dataset_context": "Chen et al. [11] note that open domain QA performance on SQuAD1.0 dataset is significantly affected by the nature of questions.", "mention_start": 57, "mention_end": 73, "dataset_mention": "SQuAD1.0 dataset"}, {"mentioned_in_paper": "421", "context_id": "328", "dataset_context": "In our experiments two aspects make the experimental setting (see Section 5.1) advantageous for HW-SQuAD, compared to the BenthamQA dataset.", "mention_start": 117, "mention_end": 139, "dataset_mention": "the BenthamQA dataset"}, {"mentioned_in_paper": "421", "context_id": "329", "dataset_context": "Firstly, the off-the-shelf, end-to-end embedding model which we use for word embeddings is trained on HW-SYNTH dataset whose synthetic word images are similar to the word images in HW-SQuAD dataset.", "mention_start": 101, "mention_end": 118, "dataset_mention": "HW-SYNTH dataset"}, {"mentioned_in_paper": "421", "context_id": "329", "dataset_context": "Firstly, the off-the-shelf, end-to-end embedding model which we use for word embeddings is trained on HW-SYNTH dataset whose synthetic word images are similar to the word images in HW-SQuAD dataset.", "mention_start": 180, "mention_end": 197, "dataset_mention": "HW-SQuAD dataset"}, {"mentioned_in_paper": "421", "context_id": "331", "dataset_context": "Secondly, PCA rotation matrices and GMMs used for the FV aggregation scheme are learnt on the train split of the HW-SQuAD dataset alone.", "mention_start": 108, "mention_end": 129, "dataset_mention": "the HW-SQuAD dataset"}, {"mentioned_in_paper": "421", "context_id": "334", "dataset_context": "To study how recognition-based QA models work on the newly introduced Handwritten QA datasets, we evaluate a full pipeline IR/NLP QA framework on the text transcriptions of the datasets.", "mention_start": 70, "mention_end": 93, "dataset_mention": "Handwritten QA datasets"}, {"mentioned_in_paper": "421", "context_id": "343", "dataset_context": "The specific model we use is a BERT LARGE model fine-tuned for QA on SQuAD1.0 dataset.", "mention_start": 69, "mention_end": 85, "dataset_mention": "SQuAD1.0 dataset"}, {"mentioned_in_paper": "421", "context_id": "350", "dataset_context": "We trained two OCRs: i) SynthIam -trained on 9 million synthetic handwritten word images in HW-SYNTH [37], train split of IAM dataset with real handwritten images, and ii) SynthI-amHwSq -trained on HW-SYNTH, IAM train split and train split of HW-SQuAD.", "mention_start": 121, "mention_end": 133, "dataset_mention": "IAM dataset"}, {"mentioned_in_paper": "421", "context_id": "370", "dataset_context": "In this paper, we have introduced the problem of QA on handwritten document collections and presented two new datasets -HW-SQuAD and BenthamQA.", "mention_start": 54, "mention_end": 118, "dataset_mention": "handwritten document collections and presented two new datasets"}, {"mentioned_in_paper": "423", "context_id": "25", "dataset_context": "However, the boundary error caused by shot segmentation will affect the scene segmentation quality, and no shot boundary information is given on the 2021 TAAC data set, making it difficult to develop a convincing shot segmentation method.", "mention_start": 144, "mention_end": 167, "dataset_mention": "the 2021 TAAC data set"}, {"mentioned_in_paper": "423", "context_id": "54", "dataset_context": "In addition, the 2D CNN models are pretrained on ImageNet and the 3D CNN model are pretrained on Kinetics-400, and then all the models are finetuned on the ads dataset.", "mention_start": 151, "mention_end": 167, "dataset_mention": "the ads dataset"}, {"mentioned_in_paper": "423", "context_id": "92", "dataset_context": "The 2021 TAAC video ads dataset contains 5,000 training videos and 5000 testing videos, which are collected from online video advertisements.", "mention_start": 0, "mention_end": 31, "dataset_mention": "The 2021 TAAC video ads dataset"}, {"mentioned_in_paper": "424", "context_id": "14", "dataset_context": "Code, model and dataset are publicly available 1.", "mention_start": 5, "mention_end": 23, "dataset_mention": " model and dataset"}, {"mentioned_in_paper": "424", "context_id": "222", "dataset_context": "We first show comparison with state-of-the-art learning-based fluid animation method on both Holynski and self-collected datasets to discuss the effectiveness of proposed Surface-based Layered Representation (Sec.", "mention_start": 88, "mention_end": 129, "dataset_mention": "both Holynski and self-collected datasets"}, {"mentioned_in_paper": "425", "context_id": "127", "dataset_context": "As a result, we discard their labels and obtain an unlabeled dataset composed of approximately 300k real samples, termed Real-300K 1 .", "mention_start": 12, "mention_end": 68, "dataset_mention": " we discard their labels and obtain an unlabeled dataset"}, {"mentioned_in_paper": "425", "context_id": "142", "dataset_context": "The probes are trained on the same labeled SynthText dataset.", "mention_start": 26, "mention_end": 60, "dataset_mention": "the same labeled SynthText dataset"}, {"mentioned_in_paper": "425", "context_id": "159", "dataset_context": "We perform selfsupervised learning of the backbone using the Real-300K dataset.", "mention_start": 57, "mention_end": 78, "dataset_mention": "the Real-300K dataset"}, {"mentioned_in_paper": "428", "context_id": "6", "dataset_context": "Our method is evaluated in two tasks on four popular realistic datasets: action recognition on YouTube, UCF50, HMDB51 databases, and action similarity labeling on ASLAN database.", "mention_start": 40, "mention_end": 71, "dataset_mention": "four popular realistic datasets"}, {"mentioned_in_paper": "428", "context_id": "9", "dataset_context": "Recent research mainly focuses on the realistic datasets collected from web videos or digital movies [22, 23, 27].", "mention_start": 34, "mention_end": 56, "dataset_mention": "the realistic datasets"}, {"mentioned_in_paper": "428", "context_id": "30", "dataset_context": "Our method is evaluated in two tasks on four realistic datasets: action recognition on YouTube [27], UCF50 [34] and HMDB51 [23] databases, and action similarity labeling on ASLAN database [22].", "mention_start": 40, "mention_end": 63, "dataset_mention": "four realistic datasets"}, {"mentioned_in_paper": "428", "context_id": "119", "dataset_context": "Our method is evaluated in two tasks on four popular realistic datasets: action recognition task on YouTube, UCF50, and HMDB51 databases; action similarity labeling task on ASLAN database.", "mention_start": 40, "mention_end": 71, "dataset_mention": "four popular realistic datasets"}, {"mentioned_in_paper": "428", "context_id": "126", "dataset_context": "We apply the same LOGO protocol as for the YouTube dataset and report average accuracy over all categories.", "mention_start": 39, "mention_end": 58, "dataset_mention": "the YouTube dataset"}, {"mentioned_in_paper": "428", "context_id": "183", "dataset_context": "Our method is evaluated in two tasks on four popular realistic datasets and has achieved the state-of-the-art performance in all cases.", "mention_start": 40, "mention_end": 71, "dataset_mention": "four popular realistic datasets"}, {"mentioned_in_paper": "429", "context_id": "42", "dataset_context": "-We generate large and realistic synthetic data and collect a mid-size real dataset of deformed T-shirts which we annotated with edge labels and grasping points.", "mention_start": 60, "mention_end": 83, "dataset_mention": "a mid-size real dataset"}, {"mentioned_in_paper": "429", "context_id": "161", "dataset_context": "Note that the results for [2] \u2022 , [2] * and [1] are for different and much larger datasets in the original papers, while [2] + and DA are for our dataset.", "mention_start": 55, "mention_end": 90, "dataset_mention": "different and much larger datasets"}, {"mentioned_in_paper": "430", "context_id": "12", "dataset_context": "Inception V3 network is trained by using Mount Tai tree image dataset.", "mention_start": 41, "mention_end": 69, "dataset_mention": "Mount Tai tree image dataset"}, {"mentioned_in_paper": "431", "context_id": "29", "dataset_context": "We analyze the performance of the anchor-based module and the anchor-free module on THUMOS14 dataset, shown in Table I and Figure 1 1 .", "mention_start": 84, "mention_end": 100, "dataset_mention": "THUMOS14 dataset"}, {"mentioned_in_paper": "431", "context_id": "32", "dataset_context": "Considering the complementarity of the anchor-free module and the anchor-based module, we propose integrating these 1 The dataset is equally divided into 5 sub-datasets according to the action duration d, including extremely short (ES) d \u2208 (0, 1.5s), short (S)", "mention_start": 97, "mention_end": 129, "dataset_mention": "integrating these 1 The dataset"}, {"mentioned_in_paper": "431", "context_id": "191", "dataset_context": "Challenges for ActivityNet v1.3 come from the large variety of action scales, intra-category differences and inter-category similarity, etc. Metric We adopt the official evaluation metric mAP on both THUMOS14 [16] and ActivityNet v1.3 [17] datasets.", "mention_start": 194, "mention_end": 248, "dataset_mention": "both THUMOS14 [16] and ActivityNet v1.3 [17] datasets"}, {"mentioned_in_paper": "431", "context_id": "197", "dataset_context": "Raw videos are first decomposed into frames with frame rate 30f ps for both THUMOS14 and AcitivityNet v1.3 datasets.", "mention_start": 71, "mention_end": 115, "dataset_mention": "both THUMOS14 and AcitivityNet v1.3 datasets"}, {"mentioned_in_paper": "431", "context_id": "239", "dataset_context": "Table IV reports the performance on ActivityNet v1.3 dataset.", "mention_start": 36, "mention_end": 60, "dataset_mention": "ActivityNet v1.3 dataset"}, {"mentioned_in_paper": "431", "context_id": "310", "dataset_context": "We compare A2Net with recent state-of-the-art methods on THUMOS14 and ActivityNet v1.3 datasets.", "mention_start": 57, "mention_end": 95, "dataset_mention": "THUMOS14 and ActivityNet v1.3 datasets"}, {"mentioned_in_paper": "431", "context_id": "311", "dataset_context": "Table VIII summarizes the comparison performances on the THUMOS14 dataset.", "mention_start": 53, "mention_end": 73, "dataset_mention": "the THUMOS14 dataset"}, {"mentioned_in_paper": "431", "context_id": "344", "dataset_context": "Although TGM [50] achieves mAP@0.5=53.5, it finetunes the I3D [34] model on the THUMOS dataset before extracting features and it specifically adjusts parameters for each category 2.", "mention_start": 75, "mention_end": 94, "dataset_mention": "the THUMOS dataset"}, {"mentioned_in_paper": "431", "context_id": "350", "dataset_context": "To make a comprehensive evaluation of the performance, we follow the evaluation metric on ActivityNet dataset [17] and report the average mAP.", "mention_start": 89, "mention_end": 109, "dataset_mention": "ActivityNet dataset"}, {"mentioned_in_paper": "431", "context_id": "366", "dataset_context": "The classification labels are obtained from a well-performed video recognition method [52] on the ActivityNet dataset, which is a heavy and complicated model for video recognition.", "mention_start": 94, "mention_end": 117, "dataset_mention": "the ActivityNet dataset"}, {"mentioned_in_paper": "431", "context_id": "371", "dataset_context": "In Figure 7, we visualize two action localization results on videos from the THUMOS14 dataset.", "mention_start": 72, "mention_end": 93, "dataset_mention": "the THUMOS14 dataset"}, {"mentioned_in_paper": "431", "context_id": "404", "dataset_context": "It achieves promising performance on THUMOS14 dataset, providing a simple and effective baseline for subsequent researches.", "mention_start": 37, "mention_end": 53, "dataset_mention": "THUMOS14 dataset"}, {"mentioned_in_paper": "432", "context_id": "61", "dataset_context": "BAGAN [6] restores the dataset balance by generating minority-class samples.", "mention_start": 0, "mention_end": 30, "dataset_mention": "BAGAN [6] restores the dataset"}, {"mentioned_in_paper": "432", "context_id": "166", "dataset_context": "We evaluate our method on UTD-MHAD [20] and AID [21] datasets, both composed of skeleton-based single-person daily interactive actions.", "mention_start": 26, "mention_end": 61, "dataset_mention": "UTD-MHAD [20] and AID [21] datasets"}, {"mentioned_in_paper": "433", "context_id": "96", "dataset_context": "The proposed method is tested on two public datasets including ShanghaiTech dataset and UCF CC 50 dataset.", "mention_start": 63, "mention_end": 83, "dataset_mention": "ShanghaiTech dataset"}, {"mentioned_in_paper": "433", "context_id": "96", "dataset_context": "The proposed method is tested on two public datasets including ShanghaiTech dataset and UCF CC 50 dataset.", "mention_start": 63, "mention_end": 105, "dataset_mention": "ShanghaiTech dataset and UCF CC 50 dataset"}, {"mentioned_in_paper": "433", "context_id": "98", "dataset_context": "The ShanghaiTech dataset [1] is divided into two parts: Part A and Part B. In the Part A sub-dataset, there are 482 images crawled from the internet, and in the Part B subdataset, there are 716 images collected from the busy street.", "mention_start": 0, "mention_end": 24, "dataset_mention": "The ShanghaiTech dataset"}, {"mentioned_in_paper": "433", "context_id": "98", "dataset_context": "The ShanghaiTech dataset [1] is divided into two parts: Part A and Part B. In the Part A sub-dataset, there are 482 images crawled from the internet, and in the Part B subdataset, there are 716 images collected from the busy street.", "mention_start": 77, "mention_end": 100, "dataset_mention": "the Part A sub-dataset"}, {"mentioned_in_paper": "433", "context_id": "98", "dataset_context": "The ShanghaiTech dataset [1] is divided into two parts: Part A and Part B. In the Part A sub-dataset, there are 482 images crawled from the internet, and in the Part B subdataset, there are 716 images collected from the busy street.", "mention_start": 156, "mention_end": 178, "dataset_mention": "the Part B subdataset"}, {"mentioned_in_paper": "433", "context_id": "101", "dataset_context": "The UCF CC 50 dataset [21] consists of 50 images collected from the publicly available web.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The UCF CC 50 dataset"}, {"mentioned_in_paper": "433", "context_id": "105", "dataset_context": "Table 2. Comparison of CRDNet with other state-of-the-art methods on the UCF CC 50 dataset.", "mention_start": 69, "mention_end": 90, "dataset_mention": "the UCF CC 50 dataset"}, {"mentioned_in_paper": "434", "context_id": "16", "dataset_context": "Partially supervised discovery of discriminative parts from category labels is also a compelling approach [14], especially given the effectiveness of training with web-scale datasets [17].", "mention_start": 163, "mention_end": 182, "dataset_mention": "web-scale datasets"}, {"mentioned_in_paper": "434", "context_id": "20", "dataset_context": "They can be trained end-to-end using only classification loss and have achieved state-of-the-art performance on the very challenging CUB bird dataset [32], but the resulting models are large and stable optimization is non-trivial.", "mention_start": 133, "mention_end": 149, "dataset_mention": "CUB bird dataset"}, {"mentioned_in_paper": "434", "context_id": "75", "dataset_context": "To demonstrate this lowrank hypothesis empirically, we plot in Figure 2 the sorted average eigenvalues with standard deviation of the 200 classifiers trained on bilinear pooled features from the CUB Bird dataset [32].", "mention_start": 190, "mention_end": 211, "dataset_mention": "the CUB Bird dataset"}, {"mentioned_in_paper": "434", "context_id": "121", "dataset_context": "In particular, we consider this comparison for the CUB200-2011 bird dataset [32] which has K = 200 classes.", "mention_start": 46, "mention_end": 75, "dataset_mention": "the CUB200-2011 bird dataset"}, {"mentioned_in_paper": "434", "context_id": "138", "dataset_context": "Numbers in brackets indicate typical values when bilinear pooling is applied after the last convolutional layer of VGG16 model over the CUB200-2011 bird dataset [32] where K = 200.", "mention_start": 132, "mention_end": 160, "dataset_mention": "the CUB200-2011 bird dataset"}, {"mentioned_in_paper": "434", "context_id": "151", "dataset_context": ", K. To investigate these two parameters in our model, we conduct an experiment on CUB-200-2011 bird dataset [32], which contains 11, 788 images of 200 bird species, with a standard training and testing set split.", "mention_start": 82, "mention_end": 108, "dataset_mention": "CUB-200-2011 bird dataset"}, {"mentioned_in_paper": "434", "context_id": "172", "dataset_context": "We compare state-the-art methods on four widely used fine-grained classification benchmark datasets, CUB-200-2011 Bird dataset [32], Aircrafts [22], Cars [18], and describing texture dataset (DTD) [4].", "mention_start": 100, "mention_end": 126, "dataset_mention": " CUB-200-2011 Bird dataset"}, {"mentioned_in_paper": "434", "context_id": "172", "dataset_context": "We compare state-the-art methods on four widely used fine-grained classification benchmark datasets, CUB-200-2011 Bird dataset [32], Aircrafts [22], Cars [18], and describing texture dataset (DTD) [4].", "mention_start": 174, "mention_end": 190, "dataset_mention": "texture dataset"}, {"mentioned_in_paper": "434", "context_id": "176", "dataset_context": "We list the performance of these methods in Table 2 and highlight the parameter size of the models trained on CUB-200 dataset in the last row.", "mention_start": 110, "mention_end": 125, "dataset_mention": "CUB-200 dataset"}, {"mentioned_in_paper": "434", "context_id": "194", "dataset_context": "In Figure 8, we show some randomly selected images from four different classes in CUB-200-2011 dataset and their corresponding visualizations.", "mention_start": 81, "mention_end": 102, "dataset_mention": "CUB-200-2011 dataset"}, {"mentioned_in_paper": "434", "context_id": "208", "dataset_context": "We expect these results will form a basis for future experiments such as training on weakly supervised web-scale datasets [17], pooling multiple feature modalities and further compression of models for use on mobile devices.", "mention_start": 85, "mention_end": 121, "dataset_mention": "weakly supervised web-scale datasets"}, {"mentioned_in_paper": "436", "context_id": "24", "dataset_context": "When the network is pre-trained on data sets of different sizes and fine-tuned on the same Cifar data set, the problem of excessive long-range attention exposes: as the amount of data decreases, the accuracy gradually degrades to below convolutional neural networks, and the network begins to lack local attention heads.", "mention_start": 82, "mention_end": 105, "dataset_mention": "the same Cifar data set"}, {"mentioned_in_paper": "436", "context_id": "184", "dataset_context": "We compare recent approaches trained from scratch on Cifar data sets [14] in Table 4, to investigate data efficiency.", "mention_start": 53, "mention_end": 68, "dataset_mention": "Cifar data sets"}, {"mentioned_in_paper": "436", "context_id": "203", "dataset_context": "And MRFA-W, as a suppression method with comparable performance, enables the vision transformer to be applied to visual data sets of various sizes.", "mention_start": 112, "mention_end": 129, "dataset_mention": "visual data sets"}, {"mentioned_in_paper": "437", "context_id": "55", "dataset_context": "We adopted COWC dataset [8] as source domain dataset.", "mention_start": 11, "mention_end": 23, "dataset_mention": "COWC dataset"}, {"mentioned_in_paper": "437", "context_id": "55", "dataset_context": "We adopted COWC dataset [8] as source domain dataset.", "mention_start": 31, "mention_end": 52, "dataset_mention": "source domain dataset"}, {"mentioned_in_paper": "437", "context_id": "58", "dataset_context": "The images are processed by orthorectifications that are the same as the COWC dataset.", "mention_start": 69, "mention_end": 85, "dataset_mention": "the COWC dataset"}, {"mentioned_in_paper": "438", "context_id": "4", "dataset_context": "In this paper, we introduce a separated domain generalization task with separated source datasets that can only be accessed locally for data privacy protection.", "mention_start": 71, "mention_end": 97, "dataset_mention": "separated source datasets"}, {"mentioned_in_paper": "438", "context_id": "8", "dataset_context": "We unify multi-source semantic learning and alignment in a collaborative way by repeating the semantic aggregation and calibration alternately, keeping each dataset localized, and the data privacy is thus carefully protected.", "mention_start": 143, "mention_end": 164, "dataset_mention": " keeping each dataset"}, {"mentioned_in_paper": "438", "context_id": "12", "dataset_context": "Since the adopted datasets could be very distinct in many real-world applications, the performance of deep models learned from one training (source) dataset may drop rapidly on another test (target) dataset.", "mention_start": 139, "mention_end": 156, "dataset_mention": "(source) dataset"}, {"mentioned_in_paper": "438", "context_id": "12", "dataset_context": "Since the adopted datasets could be very distinct in many real-world applications, the performance of deep models learned from one training (source) dataset may drop rapidly on another test (target) dataset.", "mention_start": 176, "mention_end": 206, "dataset_mention": "another test (target) dataset"}, {"mentioned_in_paper": "438", "context_id": "25", "dataset_context": "For example, some alignment-based methods [24], [27], [64] match source data distributions in latent space for generating domain-invariant feature representations; and some meta-learning based strategies [9], [23], [30] utilize meta-train and meta-test datasets built by sampling from multi-source data for training a stable model to unknown domains.", "mention_start": 227, "mention_end": 261, "dataset_mention": "meta-train and meta-test datasets"}, {"mentioned_in_paper": "438", "context_id": "27", "dataset_context": "Therefore, a dilemma is encountered: The requirements of learning from shared multi-source data for training a highly generalizable model may hard to be met in many real scenarios due to the privacy issues; While without simultaneous access to the source datasets for obtaining adequate information of multisource distribution, identifying and learning domain invariance for improving model generalization might be led astray.", "mention_start": 243, "mention_end": 263, "dataset_mention": "the source datasets"}, {"mentioned_in_paper": "438", "context_id": "28", "dataset_context": "In this paper, we introduce separated domain generalization (see Fig. 1), where the source datasets are separated and can only be accessed locally.", "mention_start": 79, "mention_end": 99, "dataset_mention": "the source datasets"}, {"mentioned_in_paper": "438", "context_id": "29", "dataset_context": "It enables privacy preserving of sensitive data when employing them for improving model generalization, however, is much more challenging as: The separated source datasets are private and may not be directly fused, hence the simultaneous learning of the multi-source semantic information is greatly hindered, making the identification of domain invariance tricky; While the heterogeneous source datasets with distinct data distributions may constitute enormous obstacles for training a generalizable model as the model is allowed to access only one local dataset each time, while the accessed dataset could contain particularly unusual bias and even bring negative gain for model generalization.", "mention_start": 141, "mention_end": 171, "dataset_mention": " The separated source datasets"}, {"mentioned_in_paper": "438", "context_id": "29", "dataset_context": "It enables privacy preserving of sensitive data when employing them for improving model generalization, however, is much more challenging as: The separated source datasets are private and may not be directly fused, hence the simultaneous learning of the multi-source semantic information is greatly hindered, making the identification of domain invariance tricky; While the heterogeneous source datasets with distinct data distributions may constitute enormous obstacles for training a generalizable model as the model is allowed to access only one local dataset each time, while the accessed dataset could contain particularly unusual bias and even bring negative gain for model generalization.", "mention_start": 369, "mention_end": 403, "dataset_mention": "the heterogeneous source datasets"}, {"mentioned_in_paper": "438", "context_id": "29", "dataset_context": "It enables privacy preserving of sensitive data when employing them for improving model generalization, however, is much more challenging as: The separated source datasets are private and may not be directly fused, hence the simultaneous learning of the multi-source semantic information is greatly hindered, making the identification of domain invariance tricky; While the heterogeneous source datasets with distinct data distributions may constitute enormous obstacles for training a generalizable model as the model is allowed to access only one local dataset each time, while the accessed dataset could contain particularly unusual bias and even bring negative gain for model generalization.", "mention_start": 579, "mention_end": 600, "dataset_mention": "the accessed dataset"}, {"mentioned_in_paper": "438", "context_id": "36", "dataset_context": "Each source dataset contributes semantic information locally for boosting model generalization during this process, resulting in a high-quality generalizable model under effective data privacy protection.", "mention_start": 0, "mention_end": 19, "dataset_mention": "Each source dataset"}, {"mentioned_in_paper": "438", "context_id": "46", "dataset_context": "Another set of works [2], [9], [21], [23], [30] are based on meta-learning, they employ an episodic training paradigm that trains the model and improves its out-of-distribution generalization ability on meta-train and meta-test datasets, respectively, which are built by the shared multi-source data.", "mention_start": 202, "mention_end": 236, "dataset_mention": "meta-train and meta-test datasets"}, {"mentioned_in_paper": "438", "context_id": "52", "dataset_context": "In comparison, we introduce separated domain generalization towards privacy-preserving generalizable model training by accessing and learning each source dataset locally.", "mention_start": 118, "mention_end": 161, "dataset_mention": "accessing and learning each source dataset"}, {"mentioned_in_paper": "438", "context_id": "69", "dataset_context": "In separated domain generalization, we have source datasets {D 1 , ..., D H } from H separated domains.", "mention_start": 43, "mention_end": 59, "dataset_mention": "source datasets"}, {"mentioned_in_paper": "438", "context_id": "71", "dataset_context": "The goal is to utilize the separated source datasets for training a generalizable model, which can perform well on unknown target domains.", "mention_start": 23, "mention_end": 52, "dataset_mention": "the separated source datasets"}, {"mentioned_in_paper": "438", "context_id": "72", "dataset_context": "Note that FedDG [34] may consider thousands of heterogeneous clients for model training, while our separated domain generalization task mainly focuses on multiple homogeneous source datasets with the same data spaces but different data distributions.", "mention_start": 153, "mention_end": 190, "dataset_mention": "multiple homogeneous source datasets"}, {"mentioned_in_paper": "438", "context_id": "73", "dataset_context": "The challenges of this task are: (1) The source datasets are separated and can only be utilized locally, which greatly hinders the simultaneous learning of the multi-source semantic information and even leads to invalid domain invariance identification; (2) The heterogeneous source datasets with distinct data distributions constitute enormous obstacles for generalizable model training, as the model can only access one local data each time.", "mention_start": 32, "mention_end": 56, "dataset_mention": " (1) The source datasets"}, {"mentioned_in_paper": "438", "context_id": "73", "dataset_context": "The challenges of this task are: (1) The source datasets are separated and can only be utilized locally, which greatly hinders the simultaneous learning of the multi-source semantic information and even leads to invalid domain invariance identification; (2) The heterogeneous source datasets with distinct data distributions constitute enormous obstacles for generalizable model training, as the model can only access one local data each time.", "mention_start": 253, "mention_end": 291, "dataset_mention": " (2) The heterogeneous source datasets"}, {"mentioned_in_paper": "438", "context_id": "74", "dataset_context": "That is, if one exploited local dataset contains particularly unusual domain-specific bias, the trained model may even exhibit a negative generalization gain.", "mention_start": 11, "mention_end": 39, "dataset_mention": "one exploited local dataset"}, {"mentioned_in_paper": "438", "context_id": "78", "dataset_context": "Before gathering and aligning the multi-source semantic information, we need to fully absorb the data distribution information of the separated source datasets.", "mention_start": 129, "mention_end": 159, "dataset_mention": "the separated source datasets"}, {"mentioned_in_paper": "438", "context_id": "80", "dataset_context": "Given H separated source datasets, y h with C categories is the groud-truth label of the image x h in dataset D h , where h \u2208 {1, ..., H}.", "mention_start": 6, "mention_end": 33, "dataset_mention": "H separated source datasets"}, {"mentioned_in_paper": "438", "context_id": "93", "dataset_context": "To fairly absorb the information of the source datasets for precise semantic calibration, we assign weight to each model based on its semantic divergence to the average distribution G AV G l :", "mention_start": 36, "mention_end": 55, "dataset_mention": "the source datasets"}, {"mentioned_in_paper": "438", "context_id": "97", "dataset_context": "Due to the different data distributions of the source datasets, i.e., domain shift, the same level of semantic information from different domains could be distributed across the layers of the fused model M during the aggregation process, which we call the semantic dislocation problem.", "mention_start": 43, "mention_end": 62, "dataset_mention": "the source datasets"}, {"mentioned_in_paper": "438", "context_id": "122", "dataset_context": "We thus employ an auxiliary retraining loss L h AR for the model M on each source dataset D h , that is,", "mention_start": 70, "mention_end": 89, "dataset_mention": "each source dataset"}, {"mentioned_in_paper": "438", "context_id": "132", "dataset_context": "In this section, we empirically evaluate the proposed CSAC method for the separated domain generalization task on multi-ple datasets, and give in-depth ablation studies and discussions.", "mention_start": 113, "mention_end": 132, "dataset_mention": "multi-ple datasets"}, {"mentioned_in_paper": "438", "context_id": "138", "dataset_context": "A simulated digit dataset Rotated MNIST [11] is then employed.", "mention_start": 0, "mention_end": 25, "dataset_mention": "A simulated digit dataset"}, {"mentioned_in_paper": "438", "context_id": "140", "dataset_context": "We use 100 images per class for Rotated MNIST dataset by following [11], [64].", "mention_start": 32, "mention_end": 53, "dataset_mention": "Rotated MNIST dataset"}, {"mentioned_in_paper": "438", "context_id": "142", "dataset_context": "To obtain more domains, we construct a new dataset Office-Caltech-Home by choosing the common classes from Office-Caltech [14] and Office-Home [46] datasets, and merge them to get 7 domains (DSLR is discarded due to few images), i.e., Amazon (Am), Webcam (We), Caltech (Ca), Art (Ar), Clipart (Cl), Product (Pr), and Real-World (Rw).", "mention_start": 106, "mention_end": 156, "dataset_mention": "Office-Caltech [14] and Office-Home [46] datasets"}, {"mentioned_in_paper": "438", "context_id": "150", "dataset_context": "Following [11], [64], we use standard MNIST CNN architecture with two convolution layers and two fully-connected layers for Rotated MNIST dataset.", "mention_start": 123, "mention_end": 145, "dataset_mention": "Rotated MNIST dataset"}, {"mentioned_in_paper": "438", "context_id": "151", "dataset_context": "We use the pretrained ResNet-18 network [17] for PACS, VLCS, and Office-Caltech-Home datasets and also use the AlexNet network [19] for VLCS, following [4], [9], [18].", "mention_start": 60, "mention_end": 93, "dataset_mention": " and Office-Caltech-Home datasets"}, {"mentioned_in_paper": "438", "context_id": "154", "dataset_context": "We randomly run 50 models on each domain of Digits-DG [65] and VLCS datasets, and calculate intra-domain distance, i.e., the distance between the models trained on the same domain, and inter-domain distance for the models trained on the different domains.", "mention_start": 44, "mention_end": 76, "dataset_mention": "Digits-DG [65] and VLCS datasets"}, {"mentioned_in_paper": "438", "context_id": "158", "dataset_context": "Table I, II, and III show the results PACS, VLCS, and Rotated MNIST datasets, respectively.", "mention_start": 49, "mention_end": 76, "dataset_mention": " and Rotated MNIST datasets"}, {"mentioned_in_paper": "438", "context_id": "160", "dataset_context": "We then notice that CSAC can even exhibit comparable or even better model generalization performance than the state-of-the-art DG methods with shared data, especially with larger networks (ResNet-18) and larger datasets (PACS), which indicates that the generalizable model can be effectively trained with separated source domains for privacy protection.", "mention_start": 171, "mention_end": 219, "dataset_mention": "larger networks (ResNet-18) and larger datasets"}, {"mentioned_in_paper": "438", "context_id": "184", "dataset_context": "We then present insights on the proposed CSAC via showing the accuracy curves of the models on all the source datasets during training in Fig. 9 (note that the target dataset is only used for testing the model performance).", "mention_start": 95, "mention_end": 118, "dataset_mention": "all the source datasets"}, {"mentioned_in_paper": "438", "context_id": "195", "dataset_context": "In future, one may be demanded to collaboratively train a generalizable model by exploiting thousands of separated source datasets, which shows the significance of our work in shedding the first light on this promising direction for many privacy-sensitive scenarios like finance and medical care.", "mention_start": 104, "mention_end": 130, "dataset_mention": "separated source datasets"}, {"mentioned_in_paper": "440", "context_id": "79", "dataset_context": "In Table 1 we report dataset statistics.", "mention_start": 3, "mention_end": 28, "dataset_mention": "Table 1 we report dataset"}, {"mentioned_in_paper": "441", "context_id": "99", "dataset_context": "All experiments were conducted on a subset of 32 CT scans from the DLCST [8] dataset.", "mention_start": 63, "mention_end": 84, "dataset_mention": "the DLCST [8] dataset"}, {"mentioned_in_paper": "442", "context_id": "76", "dataset_context": " Hu et al. (2019) recently described a dataset that contains pairs of closely similar images, that can be used as hard-negatives for evaluating image retrieval tasks.", "mention_start": 26, "mention_end": 46, "dataset_mention": "described a dataset"}, {"mentioned_in_paper": "442", "context_id": "178", "dataset_context": "We evaluate our approach two image captioning benchmark datasets: COCO (Lin et al., 2014), and Flickr30k (Young et al., 2014).", "mention_start": 12, "mention_end": 64, "dataset_mention": "our approach two image captioning benchmark datasets"}, {"mentioned_in_paper": "442", "context_id": "179", "dataset_context": "The COCO dataset has \u223c123K images, where each image is annotated with 5 human-generated captions.", "mention_start": 0, "mention_end": 16, "dataset_mention": "The COCO dataset"}, {"mentioned_in_paper": "442", "context_id": "182", "dataset_context": "The Flickr30K dataset has \u223c31K images, annotated with 5 human-generated captions for a total of \u223c159,000 captions.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The Flickr30K dataset"}, {"mentioned_in_paper": "442", "context_id": "253", "dataset_context": "We first evaluate the naturalness and discriminability of PSST and the competing methods on the COCO dataset.", "mention_start": 92, "mention_end": 108, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "442", "context_id": "309", "dataset_context": "We also repeated evaluations on the Flickr30K dataset.", "mention_start": 32, "mention_end": 53, "dataset_mention": "the Flickr30K dataset"}, {"mentioned_in_paper": "443", "context_id": "5", "dataset_context": "Experimental results show that MCML is scalable and outperforms metric-based meta-learning and optimization-based meta-learning on all 1-shot, 5-shot, 10shot, and 20-shot scenarios of the SNIPS dataset.", "mention_start": 183, "mention_end": 201, "dataset_mention": "the SNIPS dataset"}, {"mentioned_in_paper": "443", "context_id": "96", "dataset_context": "B-person and B-city almost appear in every slot tagging dataset.", "mention_start": 37, "mention_end": 63, "dataset_mention": "every slot tagging dataset"}, {"mentioned_in_paper": "443", "context_id": "107", "dataset_context": "It is in the episode data setting (Vinyals et al., 2016), where each episode contains a support set (1-shot or 5-shot) and a batch of labeled samples.", "mention_start": 9, "mention_end": 29, "dataset_mention": "the episode data set"}, {"mentioned_in_paper": "443", "context_id": "108", "dataset_context": "For slot tagging, the SNIPS dataset consists of 7 domains with different label sets: Weather (We), Music (Mu), PlayList (Pl), Book (Bo), Search Screen (Se), Restaurant (Re) and Creative Work (Cr).", "mention_start": 17, "mention_end": 35, "dataset_mention": " the SNIPS dataset"}, {"mentioned_in_paper": "443", "context_id": "124", "dataset_context": "Table 1 shows the results of both 1-shot and 5-shot slot tagging of SNIPS dataset.", "mention_start": 68, "mention_end": 81, "dataset_mention": "SNIPS dataset"}, {"mentioned_in_paper": "443", "context_id": "137", "dataset_context": "Table 3 shows the result of 10-shot and 20-shot on SNIPS dataset which is generated follow the method proposed by Hou et al. (2020) 4 .", "mention_start": 51, "mention_end": 64, "dataset_mention": "SNIPS dataset"}, {"mentioned_in_paper": "443", "context_id": "138", "dataset_context": "It is noted there may be distribution variation since we do not strictly require the 10-shot/20-shot dataset must contain the original 1-shot/5-shot.", "mention_start": 81, "mention_end": 108, "dataset_mention": "the 10-shot/20-shot dataset"}, {"mentioned_in_paper": "443", "context_id": "155", "dataset_context": "In addition to that, we conduct extensive experiments on all 1shot, 5-shot, 10-shot and 20-shot scenarios of widely used SNIPS dataset.", "mention_start": 120, "mention_end": 134, "dataset_mention": "SNIPS dataset"}, {"mentioned_in_paper": "444", "context_id": "29", "dataset_context": "Our Mask R-CNN model, trained on these ground truth masks, achieves reasonable performance on our limited expert-labeled dataset, and can be used to process in-field images of leaves with tar spot.", "mention_start": 105, "mention_end": 128, "dataset_mention": "expert-labeled dataset"}, {"mentioned_in_paper": "444", "context_id": "94", "dataset_context": "We also have a manually labeled dataset consisting of 100 similar images.", "mention_start": 13, "mention_end": 39, "dataset_mention": "a manually labeled dataset"}, {"mentioned_in_paper": "444", "context_id": "100", "dataset_context": "Our trained Mask R-CNN achieves an F1-score of 0.76 on the 80 images in the manually ground truth testing dataset, as well as an average error of 10.4 in counting the number of tar spots.", "mention_start": 72, "mention_end": 113, "dataset_mention": "the manually ground truth testing dataset"}, {"mentioned_in_paper": "445", "context_id": "20", "dataset_context": "We use the subjectverb agreement dataset (Linzen et al., 2016).", "mention_start": 7, "mention_end": 40, "dataset_mention": "the subjectverb agreement dataset"}, {"mentioned_in_paper": "445", "context_id": "21", "dataset_context": "This task and dataset are convenient choices, as they offer a clear hypothesis about what part of the input is essential to get the right solution.", "mention_start": 0, "mention_end": 21, "dataset_mention": "This task and dataset"}, {"mentioned_in_paper": "446", "context_id": "27", "dataset_context": "To identify masses on PCMs, we adopt transfer learning with Mask R-CNN due to the limited size of publicly available mammographic datasets.", "mention_start": 97, "mention_end": 138, "dataset_mention": "publicly available mammographic datasets"}, {"mentioned_in_paper": "446", "context_id": "30", "dataset_context": "This work is evaluated on the publicly available INbreast dataset [8] and outperforms the state-of-the-art methods on identical evaluation sets.", "mention_start": 26, "mention_end": 65, "dataset_mention": "the publicly available INbreast dataset"}, {"mentioned_in_paper": "446", "context_id": "32", "dataset_context": "The INbreast dataset [8] is used for evaluating the proposed method.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The INbreast dataset"}, {"mentioned_in_paper": "446", "context_id": "33", "dataset_context": "This dataset is currently the largest publicly available full-digital mammographic dataset with mammograms precisely annotated [5].", "mention_start": 16, "mention_end": 90, "dataset_mention": "currently the largest publicly available full-digital mammographic dataset"}, {"mentioned_in_paper": "446", "context_id": "56", "dataset_context": "In this work, we adopt transfer learning with a pre-trained Mask R-CNN model, since the mammographic dataset is limited in size.", "mention_start": 83, "mention_end": 108, "dataset_mention": "the mammographic dataset"}, {"mentioned_in_paper": "446", "context_id": "106", "dataset_context": "In future work, we would like to evaluate our method on larger mammographic datasets, and investigate the method's capability of identifying different types of lesions.", "mention_start": 55, "mention_end": 84, "dataset_mention": "larger mammographic datasets"}, {"mentioned_in_paper": "447", "context_id": "26", "dataset_context": "\"Best practices\" from the data stewardship perspective need to be balanced with a constellation of other motivations, incentives, and needs, such that the resulting \"good enough\" practices enable the research team to complete all of their goals, including the curation and dissemination of accessible and (re)usable datasets.", "mention_start": 289, "mention_end": 324, "dataset_mention": "accessible and (re)usable datasets"}, {"mentioned_in_paper": "447", "context_id": "62", "dataset_context": "Data should be structured in a standard way so it can be easily combined with other similarly structured datasets.", "mention_start": 78, "mention_end": 113, "dataset_mention": "other similarly structured datasets"}, {"mentioned_in_paper": "447", "context_id": "140", "dataset_context": "Depending on the nature of the research effort and the conclusions being drawn, it is possible that only the raw data, only the \"final\" fully processed dataset, or a selection of intermediate data products need to be shared to establish reproducibility.", "mention_start": 123, "mention_end": 159, "dataset_mention": "the \"final\" fully processed dataset"}, {"mentioned_in_paper": "447", "context_id": "184", "dataset_context": "We have a standardized set of practices related to saving datasets and other project materials while we are working with them (e.g.", "mention_start": 51, "mention_end": 66, "dataset_mention": "saving datasets"}, {"mentioned_in_paper": "447", "context_id": "187", "dataset_context": "We have standardized conventions for naming project-related objects and files (including datasets) that enable us to quickly identify the materials we are looking for.", "mention_start": 37, "mention_end": 97, "dataset_mention": "naming project-related objects and files (including datasets"}, {"mentioned_in_paper": "447", "context_id": "194", "dataset_context": "We maintain documentation that describes how we keep datasets and other materials organized while we are working with them (e.g.", "mention_start": 41, "mention_end": 61, "dataset_mention": "how we keep datasets"}, {"mentioned_in_paper": "448", "context_id": "6", "dataset_context": "We evaluate our MatteFormer on the commonly used image matting datasets: Composition-1k and Distinctions-646.", "mention_start": 49, "mention_end": 71, "dataset_mention": "image matting datasets"}, {"mentioned_in_paper": "448", "context_id": "54", "dataset_context": "For the trimap-based methods, [51] proposed a two-stage architecture and released the Composition-1K dataset.", "mention_start": 29, "mention_end": 108, "dataset_mention": " [51] proposed a two-stage architecture and released the Composition-1K dataset"}, {"mentioned_in_paper": "448", "context_id": "64", "dataset_context": " [34] designed hierarchical attention structure and proposed Distinctions-646 dataset.", "mention_start": 0, "mention_end": 85, "dataset_mention": " [34] designed hierarchical attention structure and proposed Distinctions-646 dataset"}, {"mentioned_in_paper": "448", "context_id": "161", "dataset_context": "Our encoder is first initialized with the Tiny model of Swin Transformer pretrained on ImageNet [9], then trained on the image matting dataset in an end-to-end manner.", "mention_start": 116, "mention_end": 142, "dataset_mention": "the image matting dataset"}, {"mentioned_in_paper": "448", "context_id": "174", "dataset_context": "We first compare our MatteFormer with state-of-the-art models on Composition-1k dataset.", "mention_start": 65, "mention_end": 87, "dataset_mention": "Composition-1k dataset"}, {"mentioned_in_paper": "448", "context_id": "244", "dataset_context": "We evaluate MatteFormer on the common datasets of the image matting problem.", "mention_start": 27, "mention_end": 46, "dataset_mention": "the common datasets"}, {"mentioned_in_paper": "449", "context_id": "10", "dataset_context": "In particular, based on ResNet-18, SFANet respectively obtains 78.1% and 74.7% mean of class-wise Intersection-over-Union (mIoU) at inference speeds of 37 FPS and 96 FPS on the challenging Cityscapes and CamVid test datasets by using only a single GTX 1080Ti GPU.", "mention_start": 188, "mention_end": 224, "dataset_mention": "Cityscapes and CamVid test datasets"}, {"mentioned_in_paper": "449", "context_id": "18", "dataset_context": "We provide a detailed analysis of prediction accuracy and processing time on Cityscapes and CamVid datasets for models based on ResNet-18 and MobileNetv2.", "mention_start": 77, "mention_end": 107, "dataset_mention": "Cityscapes and CamVid datasets"}, {"mentioned_in_paper": "449", "context_id": "33", "dataset_context": "Benefiting from the progress of Deep Convolutional Neural Network (DCNN), a large number of accuracy-oriented semantic segmentation methods [6] - [9] have been developed and achieved promising performance on a variety of datasets, including street scene datasets (such as Cityscapes [10] and CamVid [11]) and natural scene datasets (such as PASCAL VOC 2012 [12]).", "mention_start": 240, "mention_end": 262, "dataset_mention": "street scene datasets"}, {"mentioned_in_paper": "449", "context_id": "33", "dataset_context": "Benefiting from the progress of Deep Convolutional Neural Network (DCNN), a large number of accuracy-oriented semantic segmentation methods [6] - [9] have been developed and achieved promising performance on a variety of datasets, including street scene datasets (such as Cityscapes [10] and CamVid [11]) and natural scene datasets (such as PASCAL VOC 2012 [12]).", "mention_start": 271, "mention_end": 331, "dataset_mention": "Cityscapes [10] and CamVid [11]) and natural scene datasets"}, {"mentioned_in_paper": "449", "context_id": "76", "dataset_context": "More specifically, based on ResNet-18, our method obtains 78.1% mIoU and 74.7% mIoU on the Cityscapes and CamVid test datasets at inference speeds of 37 FPS and 96 FPS, respectively, with a single GTX 1080Ti GPU.", "mention_start": 86, "mention_end": 126, "dataset_mention": "the Cityscapes and CamVid test datasets"}, {"mentioned_in_paper": "449", "context_id": "288", "dataset_context": "In Section IV-A, we first introduce two representative street scene benchmark datasets and evaluation metrics.", "mention_start": 35, "mention_end": 86, "dataset_mention": "two representative street scene benchmark datasets"}, {"mentioned_in_paper": "449", "context_id": "292", "dataset_context": "To show the superiority of the proposed SFANet for semantic segmentation of street scenes, we conduct experiments on the Cityscapes dataset [10] and the CamVid dataset [11].", "mention_start": 116, "mention_end": 139, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "449", "context_id": "292", "dataset_context": "To show the superiority of the proposed SFANet for semantic segmentation of street scenes, we conduct experiments on the Cityscapes dataset [10] and the CamVid dataset [11].", "mention_start": 116, "mention_end": 167, "dataset_mention": "the Cityscapes dataset [10] and the CamVid dataset"}, {"mentioned_in_paper": "449", "context_id": "293", "dataset_context": "The Cityscapes dataset is a large-scale urban-scene dataset collected from 50 different cities in Germany.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The Cityscapes dataset"}, {"mentioned_in_paper": "449", "context_id": "293", "dataset_context": "The Cityscapes dataset is a large-scale urban-scene dataset collected from 50 different cities in Germany.", "mention_start": 26, "mention_end": 59, "dataset_mention": "a large-scale urban-scene dataset"}, {"mentioned_in_paper": "449", "context_id": "298", "dataset_context": "For a fair comparison, the annotations of the test set are not publicly released for the Cityscapes dataset.", "mention_start": 84, "mention_end": 107, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "449", "context_id": "299", "dataset_context": "The CamVid dataset is another challenging street scene dataset extracted from five video sequences.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The CamVid dataset"}, {"mentioned_in_paper": "449", "context_id": "299", "dataset_context": "The CamVid dataset is another challenging street scene dataset extracted from five video sequences.", "mention_start": 42, "mention_end": 62, "dataset_mention": "street scene dataset"}, {"mentioned_in_paper": "449", "context_id": "306", "dataset_context": "Instead of training from scratch, we use the publicly available ResNet-18 model pretrained on the Ima-geNet dataset [43] to initialize our backbone network.", "mention_start": 93, "mention_end": 115, "dataset_mention": "the Ima-geNet dataset"}, {"mentioned_in_paper": "449", "context_id": "309", "dataset_context": "The mini-batch size is set to 12 for the Cityscapes dataset and 4 for the CamVid dataset.", "mention_start": 37, "mention_end": 59, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "449", "context_id": "309", "dataset_context": "The mini-batch size is set to 12 for the Cityscapes dataset and 4 for the CamVid dataset.", "mention_start": 70, "mention_end": 88, "dataset_mention": "the CamVid dataset"}, {"mentioned_in_paper": "449", "context_id": "310", "dataset_context": "Moreover, the \"poly\" learning rate strategy is employed to decay the initial learning rate, where the initial learning rate is multiplied by (1\u2212 iter total iters ) power at each iteration with the power of 0.9, and it is set to 0.005 and 0.001 for the Cityscapes and CamVid datasets, respectively.", "mention_start": 247, "mention_end": 282, "dataset_mention": "the Cityscapes and CamVid datasets"}, {"mentioned_in_paper": "449", "context_id": "312", "dataset_context": "We train our model for 120K and 80K iterations for the Cityscapes and CamVid datasets, respectively.", "mention_start": 51, "mention_end": 85, "dataset_mention": "the Cityscapes and CamVid datasets"}, {"mentioned_in_paper": "449", "context_id": "318", "dataset_context": "In this subsection, we conduct ablation studies to investigate the effectiveness of each component of our proposed method for real-time semantic segmentation on the Cityscapes validation dataset.", "mention_start": 160, "mention_end": 194, "dataset_mention": "the Cityscapes validation dataset"}, {"mentioned_in_paper": "449", "context_id": "329", "dataset_context": "Next, we further analyze the segmentation performance obtained by different backbone networks on the Cityscapes validation dataset.", "mention_start": 96, "mention_end": 130, "dataset_mention": "the Cityscapes validation dataset"}, {"mentioned_in_paper": "449", "context_id": "350", "dataset_context": "From Table II, we can find that the Baseline+EA method achieves the accuracy of 74.4% mIoU on the Cityscapes validation dataset, and it significantly outperforms FCN+ResNet-18 (SCA) by about 10.2% mIoU.", "mention_start": 93, "mention_end": 127, "dataset_mention": "the Cityscapes validation dataset"}, {"mentioned_in_paper": "449", "context_id": "397", "dataset_context": "In this subsection, we compare our proposed SFANet method with several state-of-the-art semantic segmentation methods on the Cityscapes test dataset and the CamVid test dataset, respectively.", "mention_start": 120, "mention_end": 148, "dataset_mention": "the Cityscapes test dataset"}, {"mentioned_in_paper": "449", "context_id": "397", "dataset_context": "In this subsection, we compare our proposed SFANet method with several state-of-the-art semantic segmentation methods on the Cityscapes test dataset and the CamVid test dataset, respectively.", "mention_start": 120, "mention_end": 176, "dataset_mention": "the Cityscapes test dataset and the CamVid test dataset"}, {"mentioned_in_paper": "449", "context_id": "402", "dataset_context": "1) Results on Cityscapes: Table V shows the comparison results obtained by the proposed SFANet and representative semantic segmentation methods (including several state-of-theart real-time methods and some accuracy-oriented methods) on the Cityscapes test dataset.", "mention_start": 235, "mention_end": 263, "dataset_mention": "the Cityscapes test dataset"}, {"mentioned_in_paper": "449", "context_id": "425", "dataset_context": "2) Results on CamVid: Table VI gives the performance obtained by SFANet and several state-of-the-art semantic segmentation methods on the CamVid test dataset.", "mention_start": 133, "mention_end": 157, "dataset_mention": "the CamVid test dataset"}, {"mentioned_in_paper": "449", "context_id": "426", "dataset_context": "The proposed SFANet (ResNet-18) and SFANet (DF2) respectively SFANet (DF2) achieves much better performance (about 4.0% mIoU higher) and faster inference speed than SFNet (DF2) for the CamVid dataset.", "mention_start": 181, "mention_end": 199, "dataset_mention": "the CamVid dataset"}, {"mentioned_in_paper": "449", "context_id": "428", "dataset_context": "Therefore, the mIoU improvement is more evident on the CamVid dataset involving smaller image sizes than on the Cityscapes dataset.", "mention_start": 50, "mention_end": 69, "dataset_mention": "the CamVid dataset"}, {"mentioned_in_paper": "449", "context_id": "428", "dataset_context": "Therefore, the mIoU improvement is more evident on the CamVid dataset involving smaller image sizes than on the Cityscapes dataset.", "mention_start": 107, "mention_end": 130, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "449", "context_id": "431", "dataset_context": "We can see that SFANet (ResNet-18) and SFANet (DF2) achieve good performance on both the Cityscapes and CamVid datasets.", "mention_start": 80, "mention_end": 119, "dataset_mention": "both the Cityscapes and CamVid datasets"}, {"mentioned_in_paper": "450", "context_id": "0", "dataset_context": "Train Static scene, moving camera MannequinChallenge (MC) Dataset Predicted depth Human Mask Initial depth from flow RGB Image MVS Depth (supervison) Inference Our depth predictions Moving people, moving camera Figure 1.", "mention_start": 19, "mention_end": 65, "dataset_mention": " moving camera MannequinChallenge (MC) Dataset"}, {"mentioned_in_paper": "450", "context_id": "2", "dataset_context": "We train our model on our new MannequinChallenge dataset-a collection of Internet videos of people imitating mannequins, i.e., freezing in diverse, natural poses, while a camera tours the scene (left).", "mention_start": 22, "mention_end": 56, "dataset_mention": "our new MannequinChallenge dataset"}, {"mentioned_in_paper": "450", "context_id": "16", "dataset_context": "These videos comprise our new MannequinChallenge (MC) dataset, which we plan to release for the research community.", "mention_start": 0, "mention_end": 61, "dataset_mention": "These videos comprise our new MannequinChallenge (MC) dataset"}, {"mentioned_in_paper": "450", "context_id": "36", "dataset_context": "There are a number of RGBD datasets of indoor scenes, captured using depth sensors [35, 2, 4, 45] or synthetically rendered [37].", "mention_start": 22, "mention_end": 35, "dataset_mention": "RGBD datasets"}, {"mentioned_in_paper": "450", "context_id": "39", "dataset_context": "REFRESH [20] is a recent semi-synthetic scene flow dataset created by overlaying animated people on NYUv2 images.", "mention_start": 16, "mention_end": 58, "dataset_mention": "a recent semi-synthetic scene flow dataset"}, {"mentioned_in_paper": "450", "context_id": "50", "dataset_context": "These videos comprise our new MannequinChallenge (MC) Dataset, which spans a wide range of scenes with people of different ages, naturally posing in different group configurations.", "mention_start": 0, "mention_end": 61, "dataset_mention": "These videos comprise our new MannequinChallenge (MC) Dataset"}, {"mentioned_in_paper": "450", "context_id": "80", "dataset_context": "We split our MC dataset into training, validation and testing sets with a 80:3:17 split over clips.", "mention_start": 9, "mention_end": 23, "dataset_mention": "our MC dataset"}, {"mentioned_in_paper": "450", "context_id": "81", "dataset_context": "We train our depth prediction model on the Mannequin-Challenge dataset in a supervised manner, i.e., by regressing to the depth generated by the MVS pipeline.", "mention_start": 39, "mention_end": 70, "dataset_mention": "the Mannequin-Challenge dataset"}, {"mentioned_in_paper": "450", "context_id": "153", "dataset_context": "To quantify the importance of our designed model's input, we compare the performance of several models, each trained on our MC dataset with a different input configuration.", "mention_start": 119, "mention_end": 134, "dataset_mention": "our MC dataset"}, {"mentioned_in_paper": "450", "context_id": "165", "dataset_context": "We used a subset of the TUM RGBD dataset [38], which contains indoor scenes of people performing complex actions, captured from different camera poses.", "mention_start": 20, "mention_end": 40, "dataset_mention": "the TUM RGBD dataset"}, {"mentioned_in_paper": "450", "context_id": "171", "dataset_context": "We compare our depth predictions (using our MC trained models) with several state-of-the-art monocular depth prediction methods trained on indoor NYUv2 [17, 46, 7] in the Wild (DIW) datasets [3], and the recent two-frame stereo model DeMoN [39], which assumes a static scene.", "mention_start": 166, "mention_end": 190, "dataset_mention": "the Wild (DIW) datasets"}, {"mentioned_in_paper": "450", "context_id": "175", "dataset_context": "Our single-view model already outperforms the other singleview models,demonstrating the benefit of the MC dataset for training.", "mention_start": 99, "mention_end": 113, "dataset_mention": "the MC dataset"}, {"mentioned_in_paper": "450", "context_id": "188", "dataset_context": "For all of our predictions, we use a single model trained from scratch on our MC dataset.", "mention_start": 73, "mention_end": 88, "dataset_mention": "our MC dataset"}, {"mentioned_in_paper": "451", "context_id": "6", "dataset_context": "For reproducibility we make our dataset and source code publicly available.", "mention_start": 4, "mention_end": 39, "dataset_mention": "reproducibility we make our dataset"}, {"mentioned_in_paper": "451", "context_id": "20", "dataset_context": "Python code alongside with a captured dataset is made publicly available * .", "mention_start": 27, "mention_end": 45, "dataset_mention": "a captured dataset"}, {"mentioned_in_paper": "452", "context_id": "58", "dataset_context": "The Minimally Invasive Surgical Training and Innovation Center -Science of Learning (MISTIC-SL) dataset focuses on minimally-invasive, robot-assisted surgery using a da Vinci surgical system, in which trainees perform a structured set of tasks (see Fig. 4).", "mention_start": 75, "mention_end": 103, "dataset_mention": "Learning (MISTIC-SL) dataset"}, {"mentioned_in_paper": "453", "context_id": "2", "dataset_context": "We demonstrate a framework to accomplish this using a mock data set with SDSS ugriz photometry and Gaia astrometric information.", "mention_start": 52, "mention_end": 67, "dataset_mention": "a mock data set"}, {"mentioned_in_paper": "453", "context_id": "19", "dataset_context": "With the enormous size of the Gaia data set, there is great potential for inferring properties of the WD population using photometry and astrometry, rather than the smaller data set of spectroscopically observed WDs.", "mention_start": 26, "mention_end": 43, "dataset_mention": "the Gaia data set"}, {"mentioned_in_paper": "455", "context_id": "141", "dataset_context": "We evaluate our method on the D-NeRF dataset [39], which contains eight dynamic scenes with 360 \u2022 viewpoint settings.", "mention_start": 26, "mention_end": 44, "dataset_mention": "the D-NeRF dataset"}, {"mentioned_in_paper": "455", "context_id": "267", "dataset_context": "First, we show some of the test images of real scenes dataset synthesised by our method in Figure 7.", "mention_start": 41, "mention_end": 61, "dataset_mention": "real scenes dataset"}, {"mentioned_in_paper": "457", "context_id": "95", "dataset_context": "We use images from the ILSVRC challenge (Russakovsky et al., 2015) (a large dataset of natural images from 1000 categories) and three DCNNs: the AlexNet (Krizhevsky et al., 2012), the GoogLeNet (Szegedy et al., 2015) and the (16-layer) VGG network (Simonyan & Zisserman, 2014).", "mention_start": 60, "mention_end": 83, "dataset_mention": " 2015) (a large dataset"}, {"mentioned_in_paper": "458", "context_id": "70", "dataset_context": "The Morpho-MNIST [2] data set contains morphologically perturbed digits.", "mention_start": 0, "mention_end": 29, "dataset_mention": "The Morpho-MNIST [2] data set"}, {"mentioned_in_paper": "458", "context_id": "92", "dataset_context": "The main objectives for reporting this analysis is (a) to investigate how the proposed framework performs with and without attention mechanism on a controlled (Morpho-MNIST) and a clinical MM (WB-MRI) dataset, (b) how different task reweighting approaches could provide gradient directions that yield additional supervision to the main task.", "mention_start": 146, "mention_end": 208, "dataset_mention": "a controlled (Morpho-MNIST) and a clinical MM (WB-MRI) dataset"}, {"mentioned_in_paper": "459", "context_id": "10", "dataset_context": "INTRODUCTION For most practitioners of information visualization, the concept of data seems unproblematic and trivial: what can a data set be, other than a file with columns and values, keys and attributes?", "mention_start": 118, "mention_end": 138, "dataset_mention": " what can a data set"}, {"mentioned_in_paper": "459", "context_id": "32", "dataset_context": "Any number of digital data sets could be generated from the sample, depending on which of the items' characteristics are of interest.", "mention_start": 14, "mention_end": 31, "dataset_mention": "digital data sets"}, {"mentioned_in_paper": "459", "context_id": "74", "dataset_context": "It is not a large leap to compare this process to the interpretation of a physicalized data set.", "mention_start": 72, "mention_end": 95, "dataset_mention": "a physicalized data set"}, {"mentioned_in_paper": "459", "context_id": "76", "dataset_context": "The situation is different for the observer -the object embodies an absent and unfamiliar data set in the same way the incision in stone embodies the abstract measure.", "mention_start": 31, "mention_end": 98, "dataset_mention": "the observer -the object embodies an absent and unfamiliar data set"}, {"mentioned_in_paper": "459", "context_id": "135", "dataset_context": "But, as Leonelli points out, also the representational model has practical advantages; it accounts, for example, for the expectation that a data set remains the same when copied or converted into different formats [12].", "mention_start": 116, "mention_end": 148, "dataset_mention": "the expectation that a data set"}, {"mentioned_in_paper": "459", "context_id": "162", "dataset_context": "Projects on the left focus on the epistemic interpretation of a given digital data set, while projects on the right side interrogate the material nature of data.", "mention_start": 70, "mention_end": 86, "dataset_mention": "digital data set"}, {"mentioned_in_paper": "459", "context_id": "167", "dataset_context": "Perpetual Plastic, for example, involves a symbolic data set as well as a material data source; includes representational as well as relational aspects.", "mention_start": 40, "mention_end": 60, "dataset_mention": "a symbolic data set"}, {"mentioned_in_paper": "459", "context_id": "169", "dataset_context": "They are based on digital data sets and map their values to material variables.", "mention_start": 18, "mention_end": 35, "dataset_mention": "digital data sets"}, {"mentioned_in_paper": "459", "context_id": "176", "dataset_context": "Examples in the second quadrant are based on symbolic data sets, but express these data by establishing contextual relationships with a specific situation, the recipient, or the process of data collection.", "mention_start": 45, "mention_end": 63, "dataset_mention": "symbolic data sets"}, {"mentioned_in_paper": "459", "context_id": "184", "dataset_context": "The sculpture consists of a 3x3x3m cube made from lignite coal briquette, equivalent to the volume of coal that was burnt for the creation of the electrical energy used to serve, transmit and view the online-video-trailer 1 million times, Other examples include the multi-year Data Cuisine project, inviting workshop participants to express a data set through cooking.", "mention_start": 332, "mention_end": 351, "dataset_mention": "express a data set"}, {"mentioned_in_paper": "459", "context_id": "199", "dataset_context": "Examples in this last quadrant operate without traditional data sets and don't use visual lan- They may use traditional forms of media, but deconstruct and defamiliarize them to reveal their inner physical workings.", "mention_start": 12, "mention_end": 68, "dataset_mention": "this last quadrant operate without traditional data sets"}, {"mentioned_in_paper": "459", "context_id": "217", "dataset_context": "Conversely, also born-digital projects can take ontological or relational perspectives; after all, also digital data sets are physical artifacts.", "mention_start": 103, "mention_end": 121, "dataset_mention": "digital data sets"}, {"mentioned_in_paper": "460", "context_id": "6", "dataset_context": "We are not only able to construct and apply query models in real-time, but with the help of a simple re-ranking scheme, we also outperform state-of-theart automatic retrieval methods by a significant margin on TRECVID MED13 (3.5%), MED14 (1.3%) and CCV datasets (5.2%).", "mention_start": 231, "mention_end": 261, "dataset_mention": " MED14 (1.3%) and CCV datasets"}, {"mentioned_in_paper": "460", "context_id": "75", "dataset_context": "We show that measuring similarity using inner products between Fisher Vectors of web images and dataset videos performs significantly better than training a linear SVM on web images or computing inner product between average pooled CNN features.", "mention_start": 81, "mention_end": 103, "dataset_mention": "web images and dataset"}, {"mentioned_in_paper": "460", "context_id": "99", "dataset_context": "Singh et al. [45] use the top ranked videos as positives, train a detector, and use it to re-rank dataset videos.", "mention_start": 89, "mention_end": 105, "dataset_mention": "re-rank dataset"}, {"mentioned_in_paper": "460", "context_id": "230", "dataset_context": "We evaluate our method on three event detection datasets.", "mention_start": 26, "mention_end": 56, "dataset_mention": "three event detection datasets"}, {"mentioned_in_paper": "460", "context_id": "232", "dataset_context": "The TRECVID MED 2013/2014 dataset consist of unconstrained Internet videos collected by the Linguistic Data Consortium from various Internet video web sites.", "mention_start": 0, "mention_end": 33, "dataset_mention": "The TRECVID MED 2013/2014 dataset"}, {"mentioned_in_paper": "460", "context_id": "236", "dataset_context": "We evaluate using the EK0 setting on the TRECVID dataset.", "mention_start": 37, "mention_end": 56, "dataset_mention": "the TRECVID dataset"}, {"mentioned_in_paper": "460", "context_id": "237", "dataset_context": "EK0 is a setting for the TRECVID dataset, in which no training videos are provided for the event query.", "mention_start": 21, "mention_end": 40, "dataset_mention": "the TRECVID dataset"}, {"mentioned_in_paper": "460", "context_id": "248", "dataset_context": "The CCV dataset contains 9,317 videos collected from YouTube with annotations of 20 semantic categories: \"E1: basketball\", \"E2: baseball\", \"E3: soccer\", \"E4: ice skating\", \"E5: skiing\", \"E6: swimming\", \"E7: biking\", \"E8: cat\", \"E9: dog\", \"E10: bird\", \"E11: graduation\", \"E12: birthday\",\"E13: wedding reception\", \"E14: wedding ceremony\", \"E15: wedding dance\", \"E16: music performance\", \"E17: non-music performance\", \"E18: parade\", \"E19: beach\", \"E20: playground\".", "mention_start": 0, "mention_end": 15, "dataset_mention": "The CCV dataset"}, {"mentioned_in_paper": "460", "context_id": "249", "dataset_context": "This dataset is evenly split into 4,659 training videos and 4,658 dataset videos.", "mention_start": 62, "mention_end": 73, "dataset_mention": "658 dataset"}, {"mentioned_in_paper": "460", "context_id": "254", "dataset_context": "We sample one frame every 2 seconds for TRECVID MED and CCV dataset.", "mention_start": 40, "mention_end": 67, "dataset_mention": "TRECVID MED and CCV dataset"}, {"mentioned_in_paper": "460", "context_id": "258", "dataset_context": "The TRECVID dataset contains 4,992 background videos, which do not contain any test event.", "mention_start": 0, "mention_end": 19, "dataset_mention": "The TRECVID dataset"}, {"mentioned_in_paper": "460", "context_id": "279", "dataset_context": "Wherever SVM is not mentioned (i.e., Max Pooling, Avg Pooling, VLAD and Fisher Vectors in Table II), we first generate the representations of web images and dataset videos using the corresponding method, and then use cosine similarity to measure distance between the representations of web images and dataset videos.", "mention_start": 141, "mention_end": 164, "dataset_mention": "web images and dataset"}, {"mentioned_in_paper": "460", "context_id": "279", "dataset_context": "Wherever SVM is not mentioned (i.e., Max Pooling, Avg Pooling, VLAD and Fisher Vectors in Table II), we first generate the representations of web images and dataset videos using the corresponding method, and then use cosine similarity to measure distance between the representations of web images and dataset videos.", "mention_start": 285, "mention_end": 308, "dataset_mention": "web images and dataset"}, {"mentioned_in_paper": "460", "context_id": "286", "dataset_context": "However, on the CCV dataset, SVM using average pooled features performs better.", "mention_start": 11, "mention_end": 27, "dataset_mention": "the CCV dataset"}, {"mentioned_in_paper": "460", "context_id": "287", "dataset_context": "This is because the CCV dataset contains videos with only a few frames that are all very similar.", "mention_start": 16, "mention_end": 31, "dataset_mention": "the CCV dataset"}, {"mentioned_in_paper": "460", "context_id": "289", "dataset_context": "Nevertheless, in section V-E, we will show that the video retrieval performance of Fisher Vector outperforms SVM on the CCV dataset after reranking.", "mention_start": 115, "mention_end": 131, "dataset_mention": "the CCV dataset"}, {"mentioned_in_paper": "460", "context_id": "290", "dataset_context": "We conducted experiments on the TRECVID MED13 dataset to show the advantages of the Fisher Vector representation compared to VLAD, average pooling and SVM for matching noisy web image collection with video frames.", "mention_start": 28, "mention_end": 53, "dataset_mention": "the TRECVID MED13 dataset"}, {"mentioned_in_paper": "460", "context_id": "295", "dataset_context": "We show the mAP on MED13 test dataset for each case.", "mention_start": 19, "mention_end": 37, "dataset_mention": "MED13 test dataset"}, {"mentioned_in_paper": "460", "context_id": "334", "dataset_context": "Performance (measured by mAP) vs. matching time on TRECVID MED13 dataset.", "mention_start": 51, "mention_end": 72, "dataset_mention": "TRECVID MED13 dataset"}, {"mentioned_in_paper": "460", "context_id": "340", "dataset_context": "For the CCV dataset, re-ranking the initial results generated by FV cosine similarity improves mAP to 40.81%.", "mention_start": 4, "mention_end": 19, "dataset_mention": "the CCV dataset"}, {"mentioned_in_paper": "460", "context_id": "343", "dataset_context": "Thus, FV is generally a good choice to match web images and video frames and to re-rank the dataset videos.", "mention_start": 79, "mention_end": 99, "dataset_mention": "re-rank the dataset"}, {"mentioned_in_paper": "460", "context_id": "371", "dataset_context": "Table VIII shows the results of VRFP on TRECVID MED13/14 dataset and CCV datasets.", "mention_start": 40, "mention_end": 64, "dataset_mention": "TRECVID MED13/14 dataset"}, {"mentioned_in_paper": "460", "context_id": "371", "dataset_context": "Table VIII shows the results of VRFP on TRECVID MED13/14 dataset and CCV datasets.", "mention_start": 40, "mention_end": 81, "dataset_mention": "TRECVID MED13/14 dataset and CCV datasets"}, {"mentioned_in_paper": "460", "context_id": "385", "dataset_context": "With the help of web video thumbnails, the mAP for TRECVID MED13 dataset improve from 16.44% to 16.83%, and mAP of MED14 improve from 9.67% to 9.99%.", "mention_start": 50, "mention_end": 72, "dataset_mention": "TRECVID MED13 dataset"}, {"mentioned_in_paper": "460", "context_id": "386", "dataset_context": "Since it would not be fair to use YouTube thumbnails for CCV dataset (as all videos in these datasets are from YouTube), we do not conduct this experiment on CCV.", "mention_start": 57, "mention_end": 68, "dataset_mention": "CCV dataset"}, {"mentioned_in_paper": "460", "context_id": "387", "dataset_context": "In Table V, VI and VII we also show the AP scores of all events in TRECVID MED13/14 and CCV dataset, where VRFP with and without re-ranking are shown.", "mention_start": 66, "mention_end": 99, "dataset_mention": "TRECVID MED13/14 and CCV dataset"}, {"mentioned_in_paper": "460", "context_id": "389", "dataset_context": "Fig. 8 shows visual results of VRFP on three events from TRECVID MED13 dataset.", "mention_start": 57, "mention_end": 78, "dataset_mention": "TRECVID MED13 dataset"}, {"mentioned_in_paper": "460", "context_id": "396", "dataset_context": "State-of-the-art results on three popular event retrieval datasets demonstrated the effectiveness of our approach.", "mention_start": 28, "mention_end": 66, "dataset_mention": "three popular event retrieval datasets"}, {"mentioned_in_paper": "464", "context_id": "116", "dataset_context": "During the learning phase, in order to get a representative dataset for training, we sampled a static environment selecting targets for the controller distributed in a rectangular area, while changing the times for gazing.", "mention_start": 42, "mention_end": 67, "dataset_mention": "a representative dataset"}, {"mentioned_in_paper": "465", "context_id": "101", "dataset_context": "Parameters of the network are initialized with those pre-trained on the ILSVRC-12 dataset [22].", "mention_start": 68, "mention_end": 89, "dataset_mention": "the ILSVRC-12 dataset"}, {"mentioned_in_paper": "465", "context_id": "146", "dataset_context": "Again the 16-layers model from VGG [21] is employed, pretrained on the ILSVRC-2012 [22] dataset.", "mention_start": 66, "mention_end": 95, "dataset_mention": "the ILSVRC-2012 [22] dataset"}, {"mentioned_in_paper": "465", "context_id": "182", "dataset_context": "The amount of regularization and number of neurons were selected with a grid search on the BBC Planet Earth dataset, the most challenging we used.", "mention_start": 87, "mention_end": 115, "dataset_mention": "the BBC Planet Earth dataset"}, {"mentioned_in_paper": "465", "context_id": "276", "dataset_context": "To test the temporal segmentation capabilities of our model, we run a series of experimental tests on the Ally McBeal dataset released in [39], which contains the temporal segmentation into stories of four episodes of the first season of Ally McBeal.", "mention_start": 101, "mention_end": 125, "dataset_mention": "the Ally McBeal dataset"}, {"mentioned_in_paper": "465", "context_id": "279", "dataset_context": "We also employ the BBC Planet Earth dataset [10], which contains the segmentation into stories of eleven episodes from the BBC documentary series Planet Earth [40].", "mention_start": 15, "mention_end": 43, "dataset_mention": "the BBC Planet Earth dataset"}, {"mentioned_in_paper": "465", "context_id": "283", "dataset_context": "It is worth to mention that the aforementioned datasets are considerably different, both because of the nature of the videos they contain, and because of the kind of annotation.", "mention_start": 15, "mention_end": 55, "dataset_mention": "mention that the aforementioned datasets"}, {"mentioned_in_paper": "465", "context_id": "287", "dataset_context": "Figure 3 reports an example of the variation of intersection over union with respect to C for the two different videos of the BBC Planet Earth dataset.", "mention_start": 122, "mention_end": 150, "dataset_mention": "the BBC Planet Earth dataset"}, {"mentioned_in_paper": "465", "context_id": "301", "dataset_context": "To test the generality of the learned embedding, we also perform a second experiment, in which we train a model on the entire BBC Planet Earth dataset, and test it on the Ally McBeal series.", "mention_start": 114, "mention_end": 150, "dataset_mention": "the entire BBC Planet Earth dataset"}, {"mentioned_in_paper": "465", "context_id": "334", "dataset_context": "It is also worth to notice that when the annotation to be learned is challenging, like in the BBC Planet Earth dataset, every feature becomes relevant, thus confirming the effectiveness of the proposed features.", "mention_start": 89, "mention_end": 118, "dataset_mention": "the BBC Planet Earth dataset"}, {"mentioned_in_paper": "465", "context_id": "346", "dataset_context": "As stated at the beginning of this section, we extended the BBC Planet Earth dataset by collecting four more annotations.", "mention_start": 43, "mention_end": 84, "dataset_mention": " we extended the BBC Planet Earth dataset"}, {"mentioned_in_paper": "466", "context_id": "65", "dataset_context": "The networks are pre-trained on a large synthetic FlyingChairs dataset but can surprisingly capture the motion of fast moving objects on the Sintel dataset.", "mention_start": 32, "mention_end": 70, "dataset_mention": "a large synthetic FlyingChairs dataset"}, {"mentioned_in_paper": "466", "context_id": "65", "dataset_context": "The networks are pre-trained on a large synthetic FlyingChairs dataset but can surprisingly capture the motion of fast moving objects on the Sintel dataset.", "mention_start": 137, "mention_end": 155, "dataset_mention": "the Sintel dataset"}, {"mentioned_in_paper": "466", "context_id": "165", "dataset_context": "We first train the models using the FlyingChairs dataset in Caffe [28] using the S long learning rate schedule introduced in [24], i.e., starting from 0.0001 and reducing the learning rate by half at 0.4M, 0.6M, 0.8M, and 1M iterations.", "mention_start": 32, "mention_end": 56, "dataset_mention": "the FlyingChairs dataset"}, {"mentioned_in_paper": "466", "context_id": "167", "dataset_context": "We crop 448 \u00d7 384 patches during data augmentation and use a batch size of 8. We then fine-tune the models on the FlyingThings3D dataset using the S f ine schedule [24] while excluding image pairs with extreme motion (magnitude larger than 1000 pixels).", "mention_start": 110, "mention_end": 136, "dataset_mention": "the FlyingThings3D dataset"}, {"mentioned_in_paper": "466", "context_id": "194", "dataset_context": "The large patches can capture the large motion in the KITTI dataset.", "mention_start": 50, "mention_end": 67, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "466", "context_id": "225", "dataset_context": "A larger range has lower EPE on KITTI, likely because the images from the KITTI dataset have larger displacements than those from Sintel.", "mention_start": 69, "mention_end": 87, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "467", "context_id": "86", "dataset_context": "Table I shows four different network configurations (1-4) and their validation accuracies on the Indian Pines data set (see Section III-A) in an attempt to demonstrate the effect of different architectural design choices on the performance of the network.", "mention_start": 93, "mention_end": 118, "dataset_mention": "the Indian Pines data set"}, {"mentioned_in_paper": "467", "context_id": "115", "dataset_context": "Some classes in the Indian Pines data set have very few samples.", "mention_start": 16, "mention_end": 41, "dataset_mention": "the Indian Pines data set"}, {"mentioned_in_paper": "468", "context_id": "6", "dataset_context": "To evaluate the method, we fit the SMPL model to our network output and show state-of-the-art results on the SURREAL and Unite the People datasets, outperforming recent approaches.", "mention_start": 104, "mention_end": 146, "dataset_mention": "the SURREAL and Unite the People datasets"}, {"mentioned_in_paper": "468", "context_id": "32", "dataset_context": "To evaluate our method, we fit the SMPL model [13] to the BodyNet output and measure single-view 3D human shape estimation performance in the recent SURREAL [33] and Unite the People [34] datasets.", "mention_start": 137, "mention_end": 196, "dataset_mention": "the recent SURREAL [33] and Unite the People [34] datasets"}, {"mentioned_in_paper": "468", "context_id": "163", "dataset_context": "We set these weights on the SURREAL dataset and use the same values in all experiments.", "mention_start": 24, "mention_end": 43, "dataset_mention": "the SURREAL dataset"}, {"mentioned_in_paper": "468", "context_id": "168", "dataset_context": "We take the first two stacks of the 2D pose network trained on the MPII dataset [56] with 8 stacks [1].", "mention_start": 63, "mention_end": 79, "dataset_mention": "the MPII dataset"}, {"mentioned_in_paper": "468", "context_id": "169", "dataset_context": "Similarly, the segmentation network is trained on the SURREAL dataset with 8 stacks [33] and the first two stacks are used.", "mention_start": 49, "mention_end": 69, "dataset_mention": "the SURREAL dataset"}, {"mentioned_in_paper": "468", "context_id": "172", "dataset_context": "Architectural details are given in Appendix B. SURREAL [33], being a large-scale dataset, provides pretraining for the UP dataset [34] where the networks converge relatively faster.", "mention_start": 60, "mention_end": 88, "dataset_mention": " being a large-scale dataset"}, {"mentioned_in_paper": "468", "context_id": "172", "dataset_context": "Architectural details are given in Appendix B. SURREAL [33], being a large-scale dataset, provides pretraining for the UP dataset [34] where the networks converge relatively faster.", "mention_start": 114, "mention_end": 129, "dataset_mention": "the UP dataset"}, {"mentioned_in_paper": "468", "context_id": "200", "dataset_context": "Next, we report performance on the UP dataset (Sec.", "mention_start": 30, "mention_end": 45, "dataset_mention": "the UP dataset"}, {"mentioned_in_paper": "468", "context_id": "204", "dataset_context": "SURREAL dataset [33] is a large-scale synthetic dataset for 3D human body shapes with ground truth labels for segmentation, 2D/3D pose, and SMPL body parameters.", "mention_start": 0, "mention_end": 15, "dataset_mention": "SURREAL dataset"}, {"mentioned_in_paper": "468", "context_id": "214", "dataset_context": "Unite the People dataset (UP) [34] is a recent collection of multiple datasets (e.g., MPII [56], LSP [66]) providing additional annotations for each image.", "mention_start": 0, "mention_end": 24, "dataset_mention": "Unite the People dataset"}, {"mentioned_in_paper": "468", "context_id": "226", "dataset_context": "We also report the average error between the corresponding 91 landmarks defined for the UP dataset [34].", "mention_start": 84, "mention_end": 98, "dataset_mention": "the UP dataset"}, {"mentioned_in_paper": "468", "context_id": "243", "dataset_context": "We first motivate our proposed architecture by evaluating performance of 3D shape estimation in the SURREAL dataset using alternative inputs (see Tab. 1).", "mention_start": 96, "mention_end": 115, "dataset_mention": "the SURREAL dataset"}, {"mentioned_in_paper": "468", "context_id": "277", "dataset_context": "Other works do not report results on the recent SURREAL dataset.", "mention_start": 37, "mention_end": 63, "dataset_mention": "the recent SURREAL dataset"}, {"mentioned_in_paper": "468", "context_id": "278", "dataset_context": "Table 3 : Body shape performance and comparison to the state of the art on the UP dataset.", "mention_start": 74, "mention_end": 89, "dataset_mention": "the UP dataset"}, {"mentioned_in_paper": "468", "context_id": "283", "dataset_context": "For the networks trained on the UP dataset, we initialize the weights pre-trained on SURREAL and fine-tune with the complete training set of UP-3D where the 2D segmentations are obtained from the provided 3D SMPL fits [34].", "mention_start": 28, "mention_end": 42, "dataset_mention": "the UP dataset"}, {"mentioned_in_paper": "468", "context_id": "298", "dataset_context": "We provide qualitative results in Fig. 6 on the UP dataset where the parts network is only trained on SURREAL.", "mention_start": 44, "mention_end": 58, "dataset_mention": "the UP dataset"}, {"mentioned_in_paper": "468", "context_id": "314", "dataset_context": "Results are shown for static images from the Unite the People dataset [34] and on a few real videos from YouTube.", "mention_start": 41, "mention_end": 69, "dataset_mention": "the Unite the People dataset"}, {"mentioned_in_paper": "468", "context_id": "319", "dataset_context": "We note that we never use manual segmentation during training as such annotations are not available for the full UP-3D dataset.", "mention_start": 104, "mention_end": 126, "dataset_mention": "the full UP-3D dataset"}, {"mentioned_in_paper": "468", "context_id": "322", "dataset_context": "Due to difficulties with the quantitative evaluation, we mostly rely on qualitative results for the UP dataset.", "mention_start": 95, "mention_end": 110, "dataset_mention": "the UP dataset"}, {"mentioned_in_paper": "468", "context_id": "358", "dataset_context": "We give details on how the segmentation network that is pre-trained on SUR-REAL is fine-tuned on the UP dataset.", "mention_start": 97, "mention_end": 111, "dataset_mention": "the UP dataset"}, {"mentioned_in_paper": "468", "context_id": "361", "dataset_context": "On the UP dataset, there are several types of segmentation annotations.", "mention_start": 3, "mention_end": 17, "dataset_mention": "the UP dataset"}, {"mentioned_in_paper": "468", "context_id": "400", "dataset_context": "As stated in Appendix A.2, experiments in our main paper do not use the manual segmentation of the UP dataset for training, although the evaluation on 2D metrics is performed against this ground truth.", "mention_start": 94, "mention_end": 109, "dataset_mention": "the UP dataset"}, {"mentioned_in_paper": "469", "context_id": "128", "dataset_context": "As a second application, we compare our approach to a variety of other state-ofthe-art RGB and RGB-D methods by conducting experiments in pose refinement on 'Hinterstoisser', the 'Occlusion' dataset and 'Tejani'.", "mention_start": 174, "mention_end": 198, "dataset_mention": " the 'Occlusion' dataset"}, {"mentioned_in_paper": "469", "context_id": "131", "dataset_context": "To this end, we use the 'Hinterstoisser' dataset since it provides a lot of variety in terms of both colors and shapes.", "mention_start": 19, "mention_end": 48, "dataset_mention": "the 'Hinterstoisser' dataset"}, {"mentioned_in_paper": "469", "context_id": "153", "dataset_context": "This RGB-D dataset consists of four synthetic sequences and we present detailed numbers in Figure 5.", "mention_start": 0, "mention_end": 18, "dataset_mention": "This RGB-D dataset"}, {"mentioned_in_paper": "469", "context_id": "179", "dataset_context": "Tables 1, 2 (a) and (b) depict our results for the 'Hinterstoisser', 'Occlusion' and the 'Tejani' dataset using different metrics.", "mention_start": 68, "mention_end": 105, "dataset_mention": " 'Occlusion' and the 'Tejani' dataset"}, {"mentioned_in_paper": "469", "context_id": "210", "dataset_context": "Eventually, referring to the 'Occlusion' dataset, we can report a strong improvement compared to the original numbers from SSD-6D, despite the presence of strong occlusion.", "mention_start": 24, "mention_end": 48, "dataset_mention": "the 'Occlusion' dataset"}, {"mentioned_in_paper": "470", "context_id": "7", "dataset_context": "Considerable progresses and obvious improvements have been achieved, mainly driven by the competitions and public datasets in this area, such as the ICDAR Rubust Reading competitions [9], [10], [11], [12], MSRA-TD500 [4], SVT [3], Chars74K [13] and IIIT-5K Word [14].", "mention_start": 85, "mention_end": 122, "dataset_mention": "the competitions and public datasets"}, {"mentioned_in_paper": "470", "context_id": "8", "dataset_context": "However, upon close observation and investigation, we found that most of the previous systems and datasets fall short in at least two aspects: (1) Though there are more than 100 kinds of languages that are widely used all over the world, majority of these algorithms can only handle texts of English (or other Latin-root languages).", "mention_start": 72, "mention_end": 106, "dataset_mention": "the previous systems and datasets"}, {"mentioned_in_paper": "471", "context_id": "10", "dataset_context": "We validate the proposed model on the public MIMIC data set, and the experimental results show that the proposed model can outperform state-of-the-art approaches.", "mention_start": 34, "mention_end": 59, "dataset_mention": "the public MIMIC data set"}, {"mentioned_in_paper": "471", "context_id": "27", "dataset_context": "As shown in Figure 1, we conduct a statistical analysis on the MIMIC-III dataset.", "mention_start": 58, "mention_end": 80, "dataset_mention": "the MIMIC-III dataset"}, {"mentioned_in_paper": "471", "context_id": "145", "dataset_context": "We use the Medical Information Mart for Intensive Care (MIMIC-III) 2 [10] dataset released on PhysioNet.", "mention_start": 40, "mention_end": 81, "dataset_mention": "Intensive Care (MIMIC-III) 2 [10] dataset"}, {"mentioned_in_paper": "471", "context_id": "200", "dataset_context": "Experiment results on the publicly available MIMIC-III dataset demonstrate that COGNet outperforms existing medication recommendation methods.", "mention_start": 22, "mention_end": 62, "dataset_mention": "the publicly available MIMIC-III dataset"}, {"mentioned_in_paper": "473", "context_id": "213", "dataset_context": "Specifically, the PHOTO-SKETCH and the LABEL-FACADE datasets include the paired images between the corresponding two domains.", "mention_start": 13, "mention_end": 60, "dataset_mention": " the PHOTO-SKETCH and the LABEL-FACADE datasets"}, {"mentioned_in_paper": "473", "context_id": "236", "dataset_context": "We guess a possible reason is that CycleGAN-VGG relied on the pretrained network parameters from the ImageNet dataset,   which is designed on the classification task with specific categories.", "mention_start": 97, "mention_end": 117, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "473", "context_id": "240", "dataset_context": "The pretrained ImageNet dataset does not contain the label category.", "mention_start": 0, "mention_end": 31, "dataset_mention": "The pretrained ImageNet dataset"}, {"mentioned_in_paper": "473", "context_id": "268", "dataset_context": "Since manual scoring is time-consuming and expensive, researchers often run small datasets with a random selection to approximate human perception, we randomly select a third of all test image of four tasks.", "mention_start": 53, "mention_end": 90, "dataset_mention": " researchers often run small datasets"}, {"mentioned_in_paper": "474", "context_id": "7", "dataset_context": "Experimental results on CT and MRI datasets demonstrate that the proposed PCL method can substantially improve the segmentation performance compared to existing methods in both semi-supervised setting and transfer learning setting. 1", "mention_start": 24, "mention_end": 43, "dataset_mention": "CT and MRI datasets"}, {"mentioned_in_paper": "474", "context_id": "10", "dataset_context": "However, due to the extensive annotation effort and the requirement of expertise in the medical domain, acquiring such large labeled datasets is usually prohibitive.", "mention_start": 103, "mention_end": 141, "dataset_mention": " acquiring such large labeled datasets"}, {"mentioned_in_paper": "474", "context_id": "28", "dataset_context": "We evaluate the proposed PCL framework on two CT datasets and two MRI datasets.", "mention_start": 42, "mention_end": 57, "dataset_mention": "two CT datasets"}, {"mentioned_in_paper": "474", "context_id": "28", "dataset_context": "We evaluate the proposed PCL framework on two CT datasets and two MRI datasets.", "mention_start": 42, "mention_end": 78, "dataset_mention": "two CT datasets and two MRI datasets"}, {"mentioned_in_paper": "474", "context_id": "40", "dataset_context": "For example, in ACDC MICCAI 2017 dataset [1], the target structures such as the left ventricle and the right ventricle appear in almost every slice of the volumetric image for all patients.", "mention_start": 15, "mention_end": 40, "dataset_mention": "ACDC MICCAI 2017 dataset"}, {"mentioned_in_paper": "474", "context_id": "68", "dataset_context": "The threshold t is a hyper-parameter that is different for different medical datasets.", "mention_start": 59, "mention_end": 85, "dataset_mention": "different medical datasets"}, {"mentioned_in_paper": "474", "context_id": "82", "dataset_context": "Datasets: We evaluate the performance of the proposed PCL on four publicly available medical image datasets.", "mention_start": 60, "mention_end": 107, "dataset_mention": "four publicly available medical image datasets"}, {"mentioned_in_paper": "474", "context_id": "83", "dataset_context": "(1) The CHD dataset is a CT dataset that consists of 68 3D cardiac images captured by a Simens biograph 64 machine [23].", "mention_start": 0, "mention_end": 19, "dataset_mention": "(1) The CHD dataset"}, {"mentioned_in_paper": "474", "context_id": "83", "dataset_context": "(1) The CHD dataset is a CT dataset that consists of 68 3D cardiac images captured by a Simens biograph 64 machine [23].", "mention_start": 23, "mention_end": 35, "dataset_mention": "a CT dataset"}, {"mentioned_in_paper": "474", "context_id": "85", "dataset_context": "(2) The MMWHS dataset was hosted in STACOM and MICCAI 2017 [25, 24].", "mention_start": 0, "mention_end": 21, "dataset_mention": "(2) The MMWHS dataset"}, {"mentioned_in_paper": "474", "context_id": "86", "dataset_context": "It consists of 20 cardiac CT and 20 MRI images and the annotations include the same seven substructures as the CHD dataset.", "mention_start": 107, "mention_end": 122, "dataset_mention": "the CHD dataset"}, {"mentioned_in_paper": "474", "context_id": "87", "dataset_context": "(3) The ACDC dataset was hosted in MICCAI 2017 challenge [1].", "mention_start": 0, "mention_end": 20, "dataset_mention": "(3) The ACDC dataset"}, {"mentioned_in_paper": "474", "context_id": "91", "dataset_context": "(4) The HVSMR dataset was hosted in MICCAI 2016 challenge [16].", "mention_start": 0, "mention_end": 21, "dataset_mention": "(4) The HVSMR dataset"}, {"mentioned_in_paper": "474", "context_id": "96", "dataset_context": "The f r and f s for each dataset are defined as follows (1) CHD dataset: f r = 1.0 \u00d7 1.0mm 2 and f s = 512 \u00d7 512, (2) MMWHS dataset: f r = 1.0 \u00d7 1.0mm 2 and f s = 256\u00d7256, (3) ACDC dataset: f r = 1.25\u00d71.25mm 2 and f s = 352\u00d7352, (4) HVSMR dataset: f r = 0.7 \u00d7 0.7mm 2 and f s = 352 \u00d7 352.", "mention_start": 48, "mention_end": 71, "dataset_mention": "follows (1) CHD dataset"}, {"mentioned_in_paper": "474", "context_id": "96", "dataset_context": "The f r and f s for each dataset are defined as follows (1) CHD dataset: f r = 1.0 \u00d7 1.0mm 2 and f s = 512 \u00d7 512, (2) MMWHS dataset: f r = 1.0 \u00d7 1.0mm 2 and f s = 256\u00d7256, (3) ACDC dataset: f r = 1.25\u00d71.25mm 2 and f s = 352\u00d7352, (4) HVSMR dataset: f r = 0.7 \u00d7 0.7mm 2 and f s = 352 \u00d7 352.", "mention_start": 113, "mention_end": 131, "dataset_mention": " (2) MMWHS dataset"}, {"mentioned_in_paper": "474", "context_id": "96", "dataset_context": "The f r and f s for each dataset are defined as follows (1) CHD dataset: f r = 1.0 \u00d7 1.0mm 2 and f s = 512 \u00d7 512, (2) MMWHS dataset: f r = 1.0 \u00d7 1.0mm 2 and f s = 256\u00d7256, (3) ACDC dataset: f r = 1.25\u00d71.25mm 2 and f s = 352\u00d7352, (4) HVSMR dataset: f r = 0.7 \u00d7 0.7mm 2 and f s = 352 \u00d7 352.", "mention_start": 171, "mention_end": 188, "dataset_mention": " (3) ACDC dataset"}, {"mentioned_in_paper": "474", "context_id": "96", "dataset_context": "The f r and f s for each dataset are defined as follows (1) CHD dataset: f r = 1.0 \u00d7 1.0mm 2 and f s = 512 \u00d7 512, (2) MMWHS dataset: f r = 1.0 \u00d7 1.0mm 2 and f s = 256\u00d7256, (3) ACDC dataset: f r = 1.25\u00d71.25mm 2 and f s = 352\u00d7352, (4) HVSMR dataset: f r = 0.7 \u00d7 0.7mm 2 and f s = 352 \u00d7 352.", "mention_start": 228, "mention_end": 246, "dataset_mention": " (4) HVSMR dataset"}, {"mentioned_in_paper": "474", "context_id": "97", "dataset_context": "No additional alignment technique is used for CHD and ACDC datasets because they are already roughly aligned as they are acquired.", "mention_start": 46, "mention_end": 67, "dataset_mention": "CHD and ACDC datasets"}, {"mentioned_in_paper": "474", "context_id": "121", "dataset_context": "Baselines: We compare the performance of PCL with a random approach that does not use any pre-training as well as the following state-of-the-art baselines, all of which use the same unlabeled dataset in the pre-training and labeled dataset in the fine-tuning as PCL: (1) Rotation [6] : a pretext-based method that uses image rotation prediction to pre-train the encoder; (2) PIRL [14] : adopted from a contrastive learning scheme for natural image classification, which uses contrastive loss to learn pretext-invariant representations.", "mention_start": 202, "mention_end": 239, "dataset_mention": "the pre-training and labeled dataset"}, {"mentioned_in_paper": "474", "context_id": "135", "dataset_context": "To assess whether the learned representations by PCL are transferrable, we use the encoder pre-trained on CHD and ACDC without labels as the initialization of a U-Net to fine-tune on MMWHS and HVSMR datasets respectively.", "mention_start": 182, "mention_end": 207, "dataset_mention": "MMWHS and HVSMR datasets"}, {"mentioned_in_paper": "474", "context_id": "145", "dataset_context": "Experimental results on four 3D medical image datasets show that PCL significantly improves the segmentation performance in both semi-supervised setting and transfer learning setting.", "mention_start": 24, "mention_end": 54, "dataset_mention": "four 3D medical image datasets"}, {"mentioned_in_paper": "476", "context_id": "127", "dataset_context": "Experiments on Solomon benchmarks [75] and synthetic datasets demonstrated that the model of Chen et al. [73] performed better than the adaptive LNS (ALNS) algorithm in terms of the solution quality and computational efficiency.", "mention_start": 15, "mention_end": 61, "dataset_mention": "Solomon benchmarks [75] and synthetic datasets"}, {"mentioned_in_paper": "477", "context_id": "4", "dataset_context": "Employing these techniques, we extract correct governing equations from oscillator, pendulum, two-body, and three-body gravitational systems with noisy and extremely small datasets.", "mention_start": 145, "mention_end": 180, "dataset_mention": "noisy and extremely small datasets"}, {"mentioned_in_paper": "477", "context_id": "9", "dataset_context": "When compared to traditional machine learning tasks, governing equation discovery offers additional changes in that observation datasets tend to be relatively small and are inevitably tainted with noise or measurement error.", "mention_start": 110, "mention_end": 136, "dataset_mention": "that observation datasets"}, {"mentioned_in_paper": "477", "context_id": "158", "dataset_context": "For each experiment, we generated 3 different time series datasets via fourth-order symplectic integration with different initial conditions, which are available in Appendix B.2.", "mention_start": 33, "mention_end": 66, "dataset_mention": "3 different time series datasets"}, {"mentioned_in_paper": "478", "context_id": "3", "dataset_context": "A total of 1,341 car-following events extracted from the Next Generation Simulation (NGSIM) dataset were used to train the model.", "mention_start": 53, "mention_end": 99, "dataset_mention": "the Next Generation Simulation (NGSIM) dataset"}, {"mentioned_in_paper": "478", "context_id": "172", "dataset_context": "To give an illustration of the safe driving of the DDPG model, a car-following event was randomly chosen from the NGSIM dataset.", "mention_start": 109, "mention_end": 127, "dataset_mention": "the NGSIM dataset"}, {"mentioned_in_paper": "478", "context_id": "188", "dataset_context": "To give an illustration of the comfortable driving of the DDPG model, a car-following event was randomly chosen in the NGSIM dataset.", "mention_start": 114, "mention_end": 132, "dataset_mention": "the NGSIM dataset"}, {"mentioned_in_paper": "479", "context_id": "24", "dataset_context": "(vi) Achieving the best results on the KITTI 2012 and KITTI 2015 stereo data sets with an error rates of 2.29 and 3.42, respectively, improving the 2.43 and 3.89 of the MC-CNN [36] baseline.", "mention_start": 35, "mention_end": 81, "dataset_mention": "the KITTI 2012 and KITTI 2015 stereo data sets"}, {"mentioned_in_paper": "479", "context_id": "119", "dataset_context": "For the KITTI data set, we are interested in 3-pixel error metric and use:", "mention_start": 4, "mention_end": 22, "dataset_mention": "the KITTI data set"}, {"mentioned_in_paper": "479", "context_id": "127", "dataset_context": "Note that although the KITTI data set requires an error less than three pixels, we notice that training the confidence allowing a three pixel error (and not just one) causes too many positive samples and is not effective.", "mention_start": 19, "mention_end": 37, "dataset_mention": "the KITTI data set"}, {"mentioned_in_paper": "479", "context_id": "147", "dataset_context": "We evaluated our pipeline on the three largest and most competitive stereo data sets: KITTI The S in the settings indicates the use of semantic segmentation.", "mention_start": 29, "mention_end": 84, "dataset_mention": "the three largest and most competitive stereo data sets"}, {"mentioned_in_paper": "479", "context_id": "153", "dataset_context": "KITTI stereo data sets: The KITTI 2012 [7] data set contains 194 training and 195 testing images, and the KITTI 2015 [24] data set contains 200 training and 200 testing images.", "mention_start": 0, "mention_end": 22, "dataset_mention": "KITTI stereo data sets"}, {"mentioned_in_paper": "479", "context_id": "153", "dataset_context": "KITTI stereo data sets: The KITTI 2012 [7] data set contains 194 training and 195 testing images, and the KITTI 2015 [24] data set contains 200 training and 200 testing images.", "mention_start": 23, "mention_end": 51, "dataset_mention": " The KITTI 2012 [7] data set"}, {"mentioned_in_paper": "479", "context_id": "153", "dataset_context": "KITTI stereo data sets: The KITTI 2012 [7] data set contains 194 training and 195 testing images, and the KITTI 2015 [24] data set contains 200 training and 200 testing images.", "mention_start": 97, "mention_end": 130, "dataset_mention": " and the KITTI 2015 [24] data set"}, {"mentioned_in_paper": "479", "context_id": "159", "dataset_context": "Middlebury stereo data set: The Middlebury stereo data set contains five separate works in the years 2001 [3], 2003 [30], 2005 [28], 2006 [14], and 2014 [4].", "mention_start": 0, "mention_end": 26, "dataset_mention": "Middlebury stereo data set"}, {"mentioned_in_paper": "479", "context_id": "159", "dataset_context": "Middlebury stereo data set: The Middlebury stereo data set contains five separate works in the years 2001 [3], 2003 [30], 2005 [28], 2006 [14], and 2014 [4].", "mention_start": 27, "mention_end": 58, "dataset_mention": " The Middlebury stereo data set"}, {"mentioned_in_paper": "479", "context_id": "170", "dataset_context": "In order to demonstrate the effectiveness of our novelties in each stage of the pipeline, we gradually tested them on the above data sets.", "mention_start": 117, "mention_end": 137, "dataset_mention": "the above data sets"}, {"mentioned_in_paper": "479", "context_id": "172", "dataset_context": "One can see that on the KITTI 2015 data set, the greatest improvement comes from employing the global disparity network, while on KITTI 2012 it is the novel constant highway network.", "mention_start": 20, "mention_end": 43, "dataset_mention": "the KITTI 2015 data set"}, {"mentioned_in_paper": "479", "context_id": "204", "dataset_context": "The images are taken from the KITTI 2012 data set and their sizes are 1242 \u00d7 350, with 228 possible disparities.", "mention_start": 26, "mention_end": 49, "dataset_mention": "the KITTI 2012 data set"}, {"mentioned_in_paper": "479", "context_id": "215", "dataset_context": "In order to compute the matching cost map on the KITTI data set, the description network has to be run twice: once to create the left image descriptors and once to create the right image descriptors.", "mention_start": 45, "mention_end": 63, "dataset_mention": "the KITTI data set"}, {"mentioned_in_paper": "480", "context_id": "166", "dataset_context": "In our experimental evaluation we use the fine-grained Caltech UCSD Birds-2011 (CUB) [45] dataset that contains 200 classes of different North-American bird species populated with \u224860 images each.", "mention_start": 38, "mention_end": 97, "dataset_mention": "the fine-grained Caltech UCSD Birds-2011 (CUB) [45] dataset"}, {"mentioned_in_paper": "480", "context_id": "190", "dataset_context": "This result is important as we aim to increase the zero-shot learning performance on the CUB dataset for this unsupervised setting.", "mention_start": 85, "mention_end": 100, "dataset_mention": "the CUB dataset"}, {"mentioned_in_paper": "480", "context_id": "207", "dataset_context": "We present our results in Tab 3. The state-of-the-art [4] retrieval accuracy reported on unseen classes without human supervision on the CUB dataset is 13.0% mAUC.", "mention_start": 133, "mention_end": 148, "dataset_mention": "the CUB dataset"}, {"mentioned_in_paper": "480", "context_id": "256", "dataset_context": "With strong visual supervision and humanannotated attributes we improve the state-of-the-art on the CUB dataset to 56.5% (from 50.2%) in the supervised setting.", "mention_start": 96, "mention_end": 111, "dataset_mention": "the CUB dataset"}, {"mentioned_in_paper": "481", "context_id": "115", "dataset_context": "Do-mainNet is a large-scale domain adaptation benchmark dataset, which contains six domains.", "mention_start": 14, "mention_end": 63, "dataset_mention": "a large-scale domain adaptation benchmark dataset"}, {"mentioned_in_paper": "481", "context_id": "118", "dataset_context": "Office-Home is another domain adaptation dataset in office and home settings, which is more difficult than Office-31.", "mention_start": 15, "mention_end": 48, "dataset_mention": "another domain adaptation dataset"}, {"mentioned_in_paper": "481", "context_id": "165", "dataset_context": "The results of our main experiments on the DomainNet dataset are shown in Table 2.", "mention_start": 39, "mention_end": 60, "dataset_mention": "the DomainNet dataset"}, {"mentioned_in_paper": "482", "context_id": "36", "dataset_context": "Fig. 1 (Left) illustrates how the same dataset D could be streamed according to different schedules.", "mention_start": 0, "mention_end": 46, "dataset_mention": "Fig. 1 (Left) illustrates how the same dataset"}, {"mentioned_in_paper": "482", "context_id": "92", "dataset_context": "We note that the prototype for each class is invariant to any ordering of D once the dataset has been fully observed.", "mention_start": 74, "mention_end": 92, "dataset_mention": "D once the dataset"}, {"mentioned_in_paper": "482", "context_id": "120", "dataset_context": "4.1 we will use f 0 trained on Meta-Dataset (Triantafillou et al., 2019) to perform CL on CIFAR datasets, which do not overlap.", "mention_start": 31, "mention_end": 43, "dataset_mention": "Meta-Dataset"}, {"mentioned_in_paper": "482", "context_id": "120", "dataset_context": "4.1 we will use f 0 trained on Meta-Dataset (Triantafillou et al., 2019) to perform CL on CIFAR datasets, which do not overlap.", "mention_start": 89, "mention_end": 104, "dataset_mention": "CIFAR datasets"}, {"mentioned_in_paper": "482", "context_id": "171", "dataset_context": "We use a ResNet18 initial predictor trained on Meta-Dataset using multi-class classification (Li et al., 2021).", "mention_start": 47, "mention_end": 59, "dataset_mention": "Meta-Dataset"}, {"mentioned_in_paper": "482", "context_id": "172", "dataset_context": "Following the convention of meta-learning, Meta-Dataset has no class overlap with CIFAR datasets.", "mention_start": 42, "mention_end": 55, "dataset_mention": " Meta-Dataset"}, {"mentioned_in_paper": "482", "context_id": "172", "dataset_context": "Following the convention of meta-learning, Meta-Dataset has no class overlap with CIFAR datasets.", "mention_start": 81, "mention_end": 96, "dataset_mention": "CIFAR datasets"}, {"mentioned_in_paper": "482", "context_id": "180", "dataset_context": "For CIFAR datasets, we first follow a standard schedule from Buzzega et al. (2020); Wu et al. (2019) : CIFAR-10 is divided into 5 splits of 2 classes each while CIFAR-100 into 10 splits of 10 classes each.", "mention_start": 4, "mention_end": 18, "dataset_mention": "CIFAR datasets"}, {"mentioned_in_paper": "482", "context_id": "200", "dataset_context": "This suggests that our method does not require pre-training on massive datasets and could work well with limited data.", "mention_start": 63, "mention_end": 79, "dataset_mention": "massive datasets"}, {"mentioned_in_paper": "482", "context_id": "247", "dataset_context": "4.2 that pre-training with even a modest dataset is very beneficial, which is a promising result for applying this strategy to other domains.", "mention_start": 32, "mention_end": 48, "dataset_mention": "a modest dataset"}, {"mentioned_in_paper": "483", "context_id": "56", "dataset_context": "While we have used this approach to obtain partial patient specific B-reps, full CT/PET/MRI scans are not performed on the patients due to additional radiation exposure and associated costs, hence such datasets will not provide complete patient B-reps.", "mention_start": 190, "mention_end": 210, "dataset_mention": " hence such datasets"}, {"mentioned_in_paper": "485", "context_id": "9", "dataset_context": "Especially, on the challenging MSMT17 dataset, we gain 14.3% Rank-1 and 10.2% mAP improvements when compared to the second place.", "mention_start": 30, "mention_end": 45, "dataset_mention": "MSMT17 dataset"}, {"mentioned_in_paper": "485", "context_id": "42", "dataset_context": "Especially, on the challenging MSMT17 dataset, we gain 14.3% Rank-1 and 10.2% mAP improvements when compared to the second place.", "mention_start": 30, "mention_end": 45, "dataset_mention": "MSMT17 dataset"}, {"mentioned_in_paper": "485", "context_id": "53", "dataset_context": "Unsupervised domain adaptation (UDA) based person Re-ID requires some source datasets that are fully annotated, but leaves the target dataset unlabeled.", "mention_start": 65, "mention_end": 85, "dataset_mention": "some source datasets"}, {"mentioned_in_paper": "485", "context_id": "109", "dataset_context": "We denote the newly labeled dataset of the c-th camera by", "mention_start": 10, "mention_end": 35, "dataset_mention": "the newly labeled dataset"}, {"mentioned_in_paper": "485", "context_id": "174", "dataset_context": "The MSMT17 dataset is a lot more challenging than the other two datasets, containing complex scenarios and appearance variations.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The MSMT17 dataset"}, {"mentioned_in_paper": "485", "context_id": "217", "dataset_context": "On the challenging MSMT17 dataset, our approach surpasses all methods by a great margin, achieving 14.3% Rank-1 and 10.4% mAP gain when compared to SpCL.", "mention_start": 19, "mention_end": 33, "dataset_mention": "MSMT17 dataset"}, {"mentioned_in_paper": "486", "context_id": "21", "dataset_context": "In contrast, after training on a more realistic dataset (as detailed in Sec.", "mention_start": 30, "mention_end": 55, "dataset_mention": "a more realistic dataset"}, {"mentioned_in_paper": "486", "context_id": "30", "dataset_context": "In contrast, our proposed implicit subspace prior learning (ISPL) algorithm produces superior results even when trained on the same synthetic dataset (ISPL synth), and is further empowered after training with our new FaceRenov dataset (ISPL real).", "mention_start": 208, "mention_end": 234, "dataset_mention": "our new FaceRenov dataset"}, {"mentioned_in_paper": "486", "context_id": "35", "dataset_context": "In contrast, our method well preserves the ethnicity of input faces, albeit trained upon the same biased dataset.", "mention_start": 88, "mention_end": 112, "dataset_mention": "the same biased dataset"}, {"mentioned_in_paper": "486", "context_id": "198", "dataset_context": "We adopt the challenging FFHQ [25] dataset to fully reflect the challenge in real-world face restoration, which are resized to 512 \u00d7 512 and perturbed under various degradation settings for different tasks.", "mention_start": 25, "mention_end": 42, "dataset_mention": "FFHQ [25] dataset"}, {"mentioned_in_paper": "486", "context_id": "199", "dataset_context": "Besides synthetic degradation tests in [27], we further develop a real-world degradation benchmark dataset \"FaceRenov\" to help quantify the degradation domain gap and generalization ability of different restoration methods.", "mention_start": 63, "mention_end": 106, "dataset_mention": "a real-world degradation benchmark dataset"}, {"mentioned_in_paper": "486", "context_id": "207", "dataset_context": "For the new FaceRenov dataset, the domain gap between real-world and synthetic degradation is quantified by comparing the training/testing performance across datasets.", "mention_start": 4, "mention_end": 29, "dataset_mention": "the new FaceRenov dataset"}, {"mentioned_in_paper": "486", "context_id": "239", "dataset_context": "Furthermore, ISPL is capable of capturing the holistic prior of natural images instead of just focusing on facial details, leading to even better natural image statistics (NIQE) than the original FFHQ dataset.", "mention_start": 182, "mention_end": 208, "dataset_mention": "the original FFHQ dataset"}, {"mentioned_in_paper": "486", "context_id": "245", "dataset_context": "Overall, all methods suffer from intensive performance drop when transferring to the FaceRenov dataset (S2R), which is relieved after training with the same dataset (R2R).", "mention_start": 80, "mention_end": 102, "dataset_mention": "the FaceRenov dataset"}, {"mentioned_in_paper": "486", "context_id": "247", "dataset_context": "Similarly, GAN-based methods also better adapt to realistic degradation in FaceRenov dataset, with an average of 43.6% performance gain after retraining on FaceRenov.", "mention_start": 74, "mention_end": 92, "dataset_mention": "FaceRenov dataset"}, {"mentioned_in_paper": "487", "context_id": "1", "dataset_context": "In these methods the features are extracted using a deep convolutional neural network, which was preliminarily trained with an external very-large dataset.", "mention_start": 123, "mention_end": 154, "dataset_mention": "an external very-large dataset"}, {"mentioned_in_paper": "487", "context_id": "61", "dataset_context": "In Section 4 we present the experimental results in visual object category recognition [51] and unconstrained face recognition [52], [53] using the wellknown datasets (Caltech101, Caltech256, Stanford Dogs, PubFig83 and CASIA-WebFace) and the feature extraction with the popular deep neural networks architectures, e.g., GoogLeNet [8] and VGGNet [7].", "mention_start": 143, "mention_end": 166, "dataset_mention": "the wellknown datasets"}, {"mentioned_in_paper": "487", "context_id": "88", "dataset_context": "Though this classification algorithm (1)-(3) can be learned very fast just by memorizing the whole dataset, its run-time complexity is equal to O(DR), and the space (memory) complexity also linearly depends on the size of the training set.", "mention_start": 78, "mention_end": 106, "dataset_mention": "memorizing the whole dataset"}, {"mentioned_in_paper": "487", "context_id": "197", "dataset_context": "Thus, we decided to use the extra datasets (Caltech-UCSD Birds 200 and the Labeled Faces in the Wild [61]) for parameters tuning in image categorization and face recognition tasks, respectively.", "mention_start": 23, "mention_end": 42, "dataset_mention": "the extra datasets"}, {"mentioned_in_paper": "487", "context_id": "202", "dataset_context": "1. Caltech 101 Object Category dataset [62], which contains T = 8677 images of C=101 classes, i.e., we ignored the distractor background class.", "mention_start": 0, "mention_end": 38, "dataset_mention": "1. Caltech 101 Object Category dataset"}, {"mentioned_in_paper": "487", "context_id": "203", "dataset_context": "2. Caltech 256 dataset [63] with T = 29780 images of C=256 classes (without clutter class).", "mention_start": 0, "mention_end": 22, "dataset_mention": "2. Caltech 256 dataset"}, {"mentioned_in_paper": "487", "context_id": "204", "dataset_context": "3. Stanford Dogs dataset [64] with T = 20580 images of C = 120 classes.", "mention_start": 0, "mention_end": 24, "dataset_mention": "3. Stanford Dogs dataset"}, {"mentioned_in_paper": "487", "context_id": "207", "dataset_context": "These CNNs have already been trained to recognize images of 1000 classes from ImageNet dataset [54].", "mention_start": 78, "mention_end": 94, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "487", "context_id": "210", "dataset_context": "The dependence of the accuracy and classification time (in the format mean \u00b1 standard deviation) on the average number of instances per class for the Caltech-101 dataset and the GoogleNet features is presented in Table II and Table III, respectively.", "mention_start": 146, "mention_end": 169, "dataset_mention": "the Caltech-101 dataset"}, {"mentioned_in_paper": "487", "context_id": "241", "dataset_context": "In the next experiment much more complex Caltech 256 dataset is explored.", "mention_start": 3, "mention_end": 60, "dataset_mention": "the next experiment much more complex Caltech 256 dataset"}, {"mentioned_in_paper": "487", "context_id": "249", "dataset_context": "What is important, performance of our classifier ( 15)-( 20) does not decrease linearly with the increase of the size of the training set R.  In the next experiment we present several results obtained for the Stanford Dogs dataset (Fig. 6, 7).", "mention_start": 203, "mention_end": 230, "dataset_mention": "the Stanford Dogs dataset"}, {"mentioned_in_paper": "487", "context_id": "265", "dataset_context": "For example, 10 images per class allow traditional classifiers (SVM, RF) to achieve relatively high accuracy for the Stanford Dogs dataset, but such amount of reference instances is absolutely not enough for these methods to recognize images from complex Caltech-256 dataset.", "mention_start": 112, "mention_end": 138, "dataset_mention": "the Stanford Dogs dataset"}, {"mentioned_in_paper": "487", "context_id": "265", "dataset_context": "For example, 10 images per class allow traditional classifiers (SVM, RF) to achieve relatively high accuracy for the Stanford Dogs dataset, but such amount of reference instances is absolutely not enough for these methods to recognize images from complex Caltech-256 dataset.", "mention_start": 246, "mention_end": 274, "dataset_mention": "complex Caltech-256 dataset"}, {"mentioned_in_paper": "487", "context_id": "266", "dataset_context": "Finally, these results are presented to demonstrate that our algorithm (Table I) is not the best classifier in all cases (see, e.g., Stanford Dogs dataset).", "mention_start": 132, "mention_end": 154, "dataset_mention": " Stanford Dogs dataset"}, {"mentioned_in_paper": "487", "context_id": "288", "dataset_context": "In the last experiment we considered T = 66000 images from C = 1000 first classes of the CASIA-WebFace facial dataset [55].", "mention_start": 85, "mention_end": 117, "dataset_mention": "the CASIA-WebFace facial dataset"}, {"mentioned_in_paper": "487", "context_id": "313", "dataset_context": "Finally, though our results for Caltech-101 dataset are rather inspiring, it is worth emphasizing that the fusion of classifiers [69] or simple stacking of descriptors for multi-scale features can improve the accuracy in 6-7% [7].", "mention_start": 31, "mention_end": 51, "dataset_mention": "Caltech-101 dataset"}, {"mentioned_in_paper": "488", "context_id": "113", "dataset_context": "The results are shown in In addition, we also present a comparative analysis of the performance of the Twin NN on UCI benchmark datasets w.r.t.", "mention_start": 113, "mention_end": 136, "dataset_mention": "UCI benchmark datasets"}, {"mentioned_in_paper": "488", "context_id": "119", "dataset_context": "To verify that the results on the UCI datasets are independent of the randomization due to the data distribution across folds, we also performed the Friedman's test [8].", "mention_start": 30, "mention_end": 46, "dataset_mention": "the UCI datasets"}, {"mentioned_in_paper": "488", "context_id": "159", "dataset_context": "For other cases, we have employed the same procedure as for the UCI datasets to split the data into train, validation or test sets and those entries are marked by a '-' in Table 13 We also illustrate the benefit of using the Twin NN in cases where there is high imbalance for a multi-class dataset.", "mention_start": 59, "mention_end": 76, "dataset_mention": "the UCI datasets"}, {"mentioned_in_paper": "488", "context_id": "160", "dataset_context": "We consider the Connect4 dataset, which has 3 classes and 126 features.", "mention_start": 12, "mention_end": 32, "dataset_mention": "the Connect4 dataset"}, {"mentioned_in_paper": "490", "context_id": "7", "dataset_context": "Extensive experiments on the DeepFashion benchmark dataset have verified the power of proposed benchmark against start-of-the-art works, with 12%-14% gain on top-10 retrieval recall, 5% higher joint localization accuracy, and near 40% gain on face identity preservation.", "mention_start": 25, "mention_end": 58, "dataset_mention": "the DeepFashion benchmark dataset"}, {"mentioned_in_paper": "490", "context_id": "31", "dataset_context": "Extensive experiments carried on the DeepFashion [22] dataset verify the efficacy of our proposed DRN in preserving semantic attributes in the source image, as well as replenishing fine-grained appearance details in a styleconsistent fashion.", "mention_start": 33, "mention_end": 61, "dataset_mention": "the DeepFashion [22] dataset"}, {"mentioned_in_paper": "490", "context_id": "81", "dataset_context": "Several works [24] [32] [43] have utilized the generated images for data augmentation on person Re-ID, but over the low-resolution (128 \u00d7 64) Market1501 [42] benchmark dataset, making it unreliable for measuring the visual quality of high-resolution and detailrich images.", "mention_start": 111, "mention_end": 175, "dataset_mention": "the low-resolution (128 \u00d7 64) Market1501 [42] benchmark dataset"}, {"mentioned_in_paper": "490", "context_id": "193", "dataset_context": "Datasets We carry all experiments on the In-shop Clothes Retrieval Benchmark of the DeepFashion dataset [22], which contains more than 50,000 editorial images of fashion models under varying poses with texture-rich garments.", "mention_start": 80, "mention_end": 103, "dataset_mention": "the DeepFashion dataset"}, {"mentioned_in_paper": "490", "context_id": "222", "dataset_context": "We present the scores evaluated on the DeepFashion dataset in Table II.", "mention_start": 35, "mention_end": 58, "dataset_mention": "the DeepFashion dataset"}, {"mentioned_in_paper": "490", "context_id": "279", "dataset_context": "The DeepFashion benchmark dataset features an uneven gender distribution (7248 male vs 41426 female), which often causes \"gender bias\" [32] in existing HPT methods to falsely producing female faces against a male input.", "mention_start": 0, "mention_end": 33, "dataset_mention": "The DeepFashion benchmark dataset"}, {"mentioned_in_paper": "492", "context_id": "35", "dataset_context": "Joint-training via EBMs for SSL has been explored with very encouraging results [11, 14, 15], which show state-of-the-art SSL performance across different data modalities (images, natural languages, an protein structure prediction and year prediction from the UCI dataset repository) and in different data settings (fix-dimensional and sequence data).", "mention_start": 255, "mention_end": 271, "dataset_mention": "the UCI dataset"}, {"mentioned_in_paper": "492", "context_id": "53", "dataset_context": "It is also shown in [14] that joint-training via EBMs outperforms VAT on tabular data from the UCI dataset repository other than images.", "mention_start": 91, "mention_end": 106, "dataset_mention": "the UCI dataset"}, {"mentioned_in_paper": "492", "context_id": "110", "dataset_context": "SSL experiments are conducted on standard benchmark datasets in different domains, including the CIFAR-10 and SVHN datasets [11] for image classification and the POS, chunking and NER datasets [29, 15] for natural language labeling.", "mention_start": 92, "mention_end": 123, "dataset_mention": "the CIFAR-10 and SVHN datasets"}, {"mentioned_in_paper": "492", "context_id": "110", "dataset_context": "SSL experiments are conducted on standard benchmark datasets in different domains, including the CIFAR-10 and SVHN datasets [11] for image classification and the POS, chunking and NER datasets [29, 15] for natural language labeling.", "mention_start": 166, "mention_end": 192, "dataset_mention": " chunking and NER datasets"}, {"mentioned_in_paper": "492", "context_id": "132", "dataset_context": "We sample varying proportions of labels as labeled training data and use the Google one-billion-word dataset [36] as the large pool of unlabeled sentences.", "mention_start": 73, "mention_end": 108, "dataset_mention": "the Google one-billion-word dataset"}, {"mentioned_in_paper": "493", "context_id": "7", "dataset_context": "Extensive experiments and analysis on three challenging video anomaly datasets demonstrate the effectiveness of our approach to improve the basic AEs in achieving superiority against several existing stateof-the-art models.", "mention_start": 56, "mention_end": 78, "dataset_mention": "video anomaly datasets"}, {"mentioned_in_paper": "493", "context_id": "35", "dataset_context": "Our extensive experiments and analysis demonstrate the superior capability of our approach in three challenging anomaly detection datasets, i.e.", "mention_start": 112, "mention_end": 138, "dataset_mention": "anomaly detection datasets"}, {"mentioned_in_paper": "493", "context_id": "107", "dataset_context": "We evaluate our approach on three widely popular video anomaly detection datasets, i.e.", "mention_start": 28, "mention_end": 81, "dataset_mention": "three widely popular video anomaly detection datasets"}, {"mentioned_in_paper": "493", "context_id": "118", "dataset_context": "This is by far the largest one-class anomaly detection dataset [21].", "mention_start": 11, "mention_end": 62, "dataset_mention": "far the largest one-class anomaly detection dataset"}, {"mentioned_in_paper": "493", "context_id": "139", "dataset_context": "Table 1 shows the AUC comparisons of our proposed STEAL Net on Ped2 [16], Avenue [19], and ShanghaiTech [21] datasets.", "mention_start": 86, "mention_end": 117, "dataset_mention": " and ShanghaiTech [21] datasets"}, {"mentioned_in_paper": "493", "context_id": "141", "dataset_context": "Specifically, our approach demonstrates an absolute gain of 5.9%, 5.6%, and 2.4% AUC on Ped2, Avenue, and ShanghaiTech datasets respectively.", "mention_start": 101, "mention_end": 127, "dataset_mention": " and ShanghaiTech datasets"}, {"mentioned_in_paper": "493", "context_id": "158", "dataset_context": "The effectiveness of our approach in complementing the AEs to achieve superiority against several existing state-of-the-art models is extensively analyzed on three challenging video anomaly datasets.", "mention_start": 176, "mention_end": 198, "dataset_mention": "video anomaly datasets"}, {"mentioned_in_paper": "494", "context_id": "49", "dataset_context": "Ahmadabad-e Mostowfi dataset contains 209 images that were acquired over a test area specifically prepared for this study.", "mention_start": 0, "mention_end": 28, "dataset_mention": "Ahmadabad-e Mostowfi dataset"}, {"mentioned_in_paper": "495", "context_id": "18", "dataset_context": "This property is used in transfer learning technique [1], [18], where the model pretrained on a large dataset is later fine-tuned on a typically smaller dataset.", "mention_start": 132, "mention_end": 160, "dataset_mention": "a typically smaller dataset"}, {"mentioned_in_paper": "495", "context_id": "126", "dataset_context": "4) What is the performance of our algorithm in a multidataset environment?", "mention_start": 47, "mention_end": 61, "dataset_mention": "a multidataset"}, {"mentioned_in_paper": "495", "context_id": "129", "dataset_context": "\u2022 Permuted MNIST [17] created from original MNIST [22] dataset.", "mention_start": 35, "mention_end": 62, "dataset_mention": "original MNIST [22] dataset"}, {"mentioned_in_paper": "495", "context_id": "132", "dataset_context": "\u2022 Split CIFAR100 [19] created by splitting CIFAR100 dataset [20] into 20 different tasks, 5 classes for each task.", "mention_start": 43, "mention_end": 59, "dataset_mention": "CIFAR100 dataset"}, {"mentioned_in_paper": "495", "context_id": "133", "dataset_context": "\u2022 Tiny ImageNet dataset [21] with 20 tasks and 5 classes for each task.", "mention_start": 0, "mention_end": 23, "dataset_mention": "\u2022 Tiny ImageNet dataset"}, {"mentioned_in_paper": "495", "context_id": "136", "dataset_context": "In the case of Cifar and Tiny ImageNet datasets, horizontal flip transform was used.", "mention_start": 15, "mention_end": 47, "dataset_mention": "Cifar and Tiny ImageNet datasets"}, {"mentioned_in_paper": "495", "context_id": "167", "dataset_context": "For Cifar100 dataset proposed method has accuracy close to replay.", "mention_start": 4, "mention_end": 20, "dataset_mention": "Cifar100 dataset"}, {"mentioned_in_paper": "495", "context_id": "176", "dataset_context": "To estimate what parts of our solution contribute most to obtained results, we performed ablation studies on the Cifar100 dataset.", "mention_start": 108, "mention_end": 129, "dataset_mention": "the Cifar100 dataset"}, {"mentioned_in_paper": "495", "context_id": "189", "dataset_context": "We utilized Cifar10, Fashion MNIST, and MNIST datasets.", "mention_start": 35, "mention_end": 54, "dataset_mention": " and MNIST datasets"}, {"mentioned_in_paper": "495", "context_id": "213", "dataset_context": "This could be noticed in the first experiment with Permuted MNIST and in a multi-dataset setup, where each task contains 50000 examples for training.", "mention_start": 73, "mention_end": 88, "dataset_mention": "a multi-dataset"}, {"mentioned_in_paper": "495", "context_id": "224", "dataset_context": "In a more challenging setup with a single computer vision dataset as a separate task, our method outperforms Experience Replay.", "mention_start": 33, "mention_end": 65, "dataset_mention": "a single computer vision dataset"}, {"mentioned_in_paper": "496", "context_id": "4", "dataset_context": "Experiments were performed using benchmarks (CIFAR-10, CIFAR-100, Tiny-ImageNet) and real-world datasets (ANIMAL-10N, Clothing1M) to evaluate the proposed criteria in various scenarios with different noise rates.", "mention_start": 65, "mention_end": 104, "dataset_mention": " Tiny-ImageNet) and real-world datasets"}, {"mentioned_in_paper": "496", "context_id": "68", "dataset_context": "In this section, we mainly describe the effectiveness of Criteria ALL based on empirical evidence on CIFAR-10 dataset (Krizhevsky et al., 2009).", "mention_start": 100, "mention_end": 117, "dataset_mention": "CIFAR-10 dataset"}, {"mentioned_in_paper": "496", "context_id": "160", "dataset_context": "Thereafter, the ANIMAL-10N dataset (Song et al., 2019) and Clothing1M (Xiao et al., 2015), whose data are collected from the real world, were employed to examine the performance of Criteria ALL.", "mention_start": 11, "mention_end": 34, "dataset_mention": " the ANIMAL-10N dataset"}, {"mentioned_in_paper": "496", "context_id": "161", "dataset_context": "ANIMAL-10N dataset, whose labels were corrupted by human errors, consists of images from 10 animals of similar appearance.", "mention_start": 0, "mention_end": 18, "dataset_mention": "ANIMAL-10N dataset"}, {"mentioned_in_paper": "496", "context_id": "164", "dataset_context": "The Clothing1M dataset, which contains 1 million images of clothing, was obtained from online shopping websites with 14 classes.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The Clothing1M dataset"}, {"mentioned_in_paper": "496", "context_id": "181", "dataset_context": "From (Song et al., 2019), the ANIMAL-10N dataset was influenced by human errors during the image labeling process, because the classes consist of five pairs of \"confusing\" animals.", "mention_start": 25, "mention_end": 48, "dataset_mention": " the ANIMAL-10N dataset"}, {"mentioned_in_paper": "497", "context_id": "57", "dataset_context": "An empirical observation: By considering an FL setup with 50 clients out of which 25% are attackers, we experiment on Fashion-MNIST [14] dataset with non-IID data, and the correlations between the clients are shown in Fig. 2. It is easy to see that the correlation between the attackers is always greater than that between two normal clients, which in turn is greater than the correlation between an attacker and a normal client.", "mention_start": 117, "mention_end": 144, "dataset_mention": "Fashion-MNIST [14] dataset"}, {"mentioned_in_paper": "497", "context_id": "102", "dataset_context": "We simulate the attacker as follows: for the MNIST dataset, the labels of all images with '0' and '1' are flipped and for the FMNIST dataset, labels of all images of \"T-Shirt\" and \"Trouser\" are flipped.", "mention_start": 40, "mention_end": 58, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "497", "context_id": "102", "dataset_context": "We simulate the attacker as follows: for the MNIST dataset, the labels of all images with '0' and '1' are flipped and for the FMNIST dataset, labels of all images of \"T-Shirt\" and \"Trouser\" are flipped.", "mention_start": 121, "mention_end": 140, "dataset_mention": "the FMNIST dataset"}, {"mentioned_in_paper": "497", "context_id": "121", "dataset_context": "This is evident in the case of the FoolsGold algorithm for the MNIST dataset where a steep jump in the loss appears after initial rounds of training.", "mention_start": 59, "mention_end": 76, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "497", "context_id": "128", "dataset_context": "3) Analyzing ASR: The ASR and the earliest detection round (ED) of all the attackers are presented in Tables I and II for MNIST and FMNIST datasets, respectively.", "mention_start": 121, "mention_end": 147, "dataset_mention": "MNIST and FMNIST datasets"}, {"mentioned_in_paper": "500", "context_id": "245", "dataset_context": "The experiments are conducted on real sequences from public datasets such as the I2R [24], flowerwall [50], SABS [51], and BMC datasets [52].", "mention_start": 118, "mention_end": 135, "dataset_mention": " and BMC datasets"}, {"mentioned_in_paper": "500", "context_id": "253", "dataset_context": "First, we provide the details of our experiment on the \"Bootstrap\" sequence of the I2R dataset.", "mention_start": 78, "mention_end": 94, "dataset_mention": "the I2R dataset"}, {"mentioned_in_paper": "500", "context_id": "263", "dataset_context": "Next, we repeat the operations on some additional video clips, i.e., the \"Escalator\", \"hall\", and \"ShoppingMall\" sequences in the I2R dataset, and a real video sequence in the BMC dataset.", "mention_start": 125, "mention_end": 141, "dataset_mention": "the I2R dataset"}, {"mentioned_in_paper": "500", "context_id": "263", "dataset_context": "Next, we repeat the operations on some additional video clips, i.e., the \"Escalator\", \"hall\", and \"ShoppingMall\" sequences in the I2R dataset, and a real video sequence in the BMC dataset.", "mention_start": 171, "mention_end": 187, "dataset_mention": "the BMC dataset"}, {"mentioned_in_paper": "500", "context_id": "295", "dataset_context": "We use four video sequences, the first of which is from the BMC dataset, and the other three are intersection monitoring video sequences from a public resource.", "mention_start": 55, "mention_end": 71, "dataset_mention": "the BMC dataset"}, {"mentioned_in_paper": "500", "context_id": "298", "dataset_context": "The man-made video from the BMC dataset consumes the least amount of time.", "mention_start": 24, "mention_end": 39, "dataset_mention": "the BMC dataset"}, {"mentioned_in_paper": "500", "context_id": "308", "dataset_context": "In the experiments, we use the \"MovedObject\" and \"Bootstrap\" sequences from the flowerwall dataset, and the \"hall\" and \"Campus\" sequences from the I2R dataset.", "mention_start": 75, "mention_end": 98, "dataset_mention": "the flowerwall dataset"}, {"mentioned_in_paper": "500", "context_id": "308", "dataset_context": "In the experiments, we use the \"MovedObject\" and \"Bootstrap\" sequences from the flowerwall dataset, and the \"hall\" and \"Campus\" sequences from the I2R dataset.", "mention_start": 142, "mention_end": 158, "dataset_mention": "the I2R dataset"}, {"mentioned_in_paper": "500", "context_id": "316", "dataset_context": "In this section, we provide the evaluation results obtained using the SABS dataset.", "mention_start": 65, "mention_end": 82, "dataset_mention": "the SABS dataset"}, {"mentioned_in_paper": "500", "context_id": "317", "dataset_context": "Some approaches in the literatures [52], [53] have been evaluated on the SABS dataset, and their recall-precision curves have been given.", "mention_start": 68, "mention_end": 85, "dataset_mention": "the SABS dataset"}, {"mentioned_in_paper": "500", "context_id": "318", "dataset_context": "For a comparison of these curves, we also evaluate our algorithm on the SABS dataset, and give the corresponding recall-precision curves of our algorithm.", "mention_start": 67, "mention_end": 84, "dataset_mention": "the SABS dataset"}, {"mentioned_in_paper": "500", "context_id": "319", "dataset_context": "In Fig. 9, the curves are evaluated on different scenes in the SABS dataset.", "mention_start": 58, "mention_end": 75, "dataset_mention": "the SABS dataset"}, {"mentioned_in_paper": "500", "context_id": "326", "dataset_context": "We use the video sequences from the I2R and flowerwall datasets and compare the detected foreground region with the given hand-segmented foreground region.", "mention_start": 32, "mention_end": 63, "dataset_mention": "the I2R and flowerwall datasets"}, {"mentioned_in_paper": "503", "context_id": "4", "dataset_context": "We apply our approach to the challenging problem of optical flow estimation and empirically validate it against state-of-the-art CNN-based solutions trained from scratch and tested on large optical flow datasets.", "mention_start": 184, "mention_end": 211, "dataset_mention": "large optical flow datasets"}, {"mentioned_in_paper": "503", "context_id": "25", "dataset_context": "We demonstrate the benefits of our proposal in state-of-the-art optical flow datasets.", "mention_start": 47, "mention_end": 85, "dataset_mention": "state-of-the-art optical flow datasets"}, {"mentioned_in_paper": "503", "context_id": "92", "dataset_context": "We further evaluate the generalization capacities of our approach by testing the models trained on FlyingChairs over the unseen Sintel dataset without any finetuning.", "mention_start": 117, "mention_end": 142, "dataset_mention": "the unseen Sintel dataset"}, {"mentioned_in_paper": "504", "context_id": "126", "dataset_context": "\u2022 NYU v2 dataset [41] : This dataset provides 1449 RGBD pairs of indoor scenes captured by Microsoft Kinect [59] using structural light.", "mention_start": 0, "mention_end": 16, "dataset_mention": "\u2022 NYU v2 dataset"}, {"mentioned_in_paper": "504", "context_id": "128", "dataset_context": "\u2022 Middlebury dataset [16, 40] : we use a subset of 30 RGBD pairs from the 2001-2006 datasets provided by Lu et al. [30] for testing.", "mention_start": 0, "mention_end": 20, "dataset_mention": "\u2022 Middlebury dataset"}, {"mentioned_in_paper": "504", "context_id": "128", "dataset_context": "\u2022 Middlebury dataset [16, 40] : we use a subset of 30 RGBD pairs from the 2001-2006 datasets provided by Lu et al. [30] for testing.", "mention_start": 69, "mention_end": 92, "dataset_mention": "the 2001-2006 datasets"}, {"mentioned_in_paper": "504", "context_id": "129", "dataset_context": "\u2022 Lu dataset [30] : This dataset consists of 6 RGBD pairs acquired by ASUS Xtion Pro camera.", "mention_start": 0, "mention_end": 12, "dataset_mention": "\u2022 Lu dataset"}, {"mentioned_in_paper": "504", "context_id": "131", "dataset_context": "Following Kim et al. [20], we train our model on the NYU v2 dataset, and test it on all the three datasets.", "mention_start": 48, "mention_end": 67, "dataset_mention": "the NYU v2 dataset"}, {"mentioned_in_paper": "504", "context_id": "132", "dataset_context": "We do not fine-tune the model on Middlebury dataset or Lu dataset in order to test the generalization ability of the model.", "mention_start": 33, "mention_end": 51, "dataset_mention": "Middlebury dataset"}, {"mentioned_in_paper": "504", "context_id": "132", "dataset_context": "We do not fine-tune the model on Middlebury dataset or Lu dataset in order to test the generalization ability of the model.", "mention_start": 55, "mention_end": 65, "dataset_mention": "Lu dataset"}, {"mentioned_in_paper": "504", "context_id": "143", "dataset_context": "For the Middlebury dataset and the Lu dataset, we interpret the provided disparity map as the depth map according to [20].", "mention_start": 4, "mention_end": 26, "dataset_mention": "the Middlebury dataset"}, {"mentioned_in_paper": "504", "context_id": "143", "dataset_context": "For the Middlebury dataset and the Lu dataset, we interpret the provided disparity map as the depth map according to [20].", "mention_start": 4, "mention_end": 45, "dataset_mention": "the Middlebury dataset and the Lu dataset"}, {"mentioned_in_paper": "504", "context_id": "154", "dataset_context": "For the NYU v2 dataset, the average RMSE is measured in centimeters.", "mention_start": 4, "mention_end": 22, "dataset_mention": "the NYU v2 dataset"}, {"mentioned_in_paper": "504", "context_id": "155", "dataset_context": "For the Middlebury dataset and the Lu dataset, the average RMSE is measured in the original scale of the provided disparity.", "mention_start": 4, "mention_end": 26, "dataset_mention": "the Middlebury dataset"}, {"mentioned_in_paper": "504", "context_id": "155", "dataset_context": "For the Middlebury dataset and the Lu dataset, the average RMSE is measured in the original scale of the provided disparity.", "mention_start": 4, "mention_end": 45, "dataset_mention": "the Middlebury dataset and the Lu dataset"}, {"mentioned_in_paper": "504", "context_id": "161", "dataset_context": "For training, we use the NYU v2 dataset with the same type of noise added to the input images, and do not fine-tune the model on the Noisy Middlebury dataset.", "mention_start": 20, "mention_end": 39, "dataset_mention": "the NYU v2 dataset"}, {"mentioned_in_paper": "504", "context_id": "161", "dataset_context": "For training, we use the NYU v2 dataset with the same type of noise added to the input images, and do not fine-tune the model on the Noisy Middlebury dataset.", "mention_start": 128, "mention_end": 157, "dataset_mention": "the Noisy Middlebury dataset"}, {"mentioned_in_paper": "504", "context_id": "162", "dataset_context": "In particular, the  is set to 651 for the Noisy Middlebury dataset following [37], and 0.04 for the NYU v2 dataset to simulate similar magnitude of noise.", "mention_start": 36, "mention_end": 66, "dataset_mention": "the Noisy Middlebury dataset"}, {"mentioned_in_paper": "504", "context_id": "162", "dataset_context": "In particular, the  is set to 651 for the Noisy Middlebury dataset following [37], and 0.04 for the NYU v2 dataset to simulate similar magnitude of noise.", "mention_start": 95, "mention_end": 114, "dataset_mention": "the NYU v2 dataset"}, {"mentioned_in_paper": "504", "context_id": "165", "dataset_context": "Although trained on depth maps from the NYU v2 dataset, our method generalizes well to the disparity maps from the Noisy Middlebury dataset.", "mention_start": 36, "mention_end": 54, "dataset_mention": "the NYU v2 dataset"}, {"mentioned_in_paper": "504", "context_id": "165", "dataset_context": "Although trained on depth maps from the NYU v2 dataset, our method generalizes well to the disparity maps from the Noisy Middlebury dataset.", "mention_start": 110, "mention_end": 139, "dataset_mention": "the Noisy Middlebury dataset"}, {"mentioned_in_paper": "504", "context_id": "170", "dataset_context": "We conduct ablation studies on different proposed modules in our method, and verify the effect of these modules for the \u00d78 guided depth super-resolution task on the NYU v2 dataset.", "mention_start": 160, "mention_end": 179, "dataset_mention": "the NYU v2 dataset"}, {"mentioned_in_paper": "505", "context_id": "38", "dataset_context": "Our method outperforms the state-of-the-art methods without using 6D object pose, 3D hand joint locations, and depth information in the sophisticated hand action dataset.", "mention_start": 131, "mention_end": 169, "dataset_mention": "the sophisticated hand action dataset"}, {"mentioned_in_paper": "505", "context_id": "155", "dataset_context": "We use the First-Person Hand Action (FPHA) dataset [3] since this dataset is the only public dataset in 3D hand-object action recognition task with 3D-fitted MANO hand model.", "mention_start": 7, "mention_end": 50, "dataset_mention": "the First-Person Hand Action (FPHA) dataset"}, {"mentioned_in_paper": "505", "context_id": "162", "dataset_context": "We further analyze the dataset by plotting the distribution of hand type for each action class as a scatter plot (see Fig. 6).", "mention_start": 0, "mention_end": 30, "dataset_mention": "We further analyze the dataset"}, {"mentioned_in_paper": "505", "context_id": "217", "dataset_context": "We find that hand grasp type estimation improves the accuracy by 7.7% and adding mean curvature feature enhances the accuracy by 1.47 % in TinyFPHA dataset.", "mention_start": 139, "mention_end": 155, "dataset_mention": "TinyFPHA dataset"}, {"mentioned_in_paper": "505", "context_id": "222", "dataset_context": "In full FPHA dataset, adding mean curvature improves the accuracy by 2.37%.", "mention_start": 3, "mention_end": 20, "dataset_mention": "full FPHA dataset"}, {"mentioned_in_paper": "505", "context_id": "223", "dataset_context": "Adding the temporal model enhances the performance by 1.9% in full FPHA dataset.", "mention_start": 62, "mention_end": 79, "dataset_mention": "full FPHA dataset"}, {"mentioned_in_paper": "505", "context_id": "225", "dataset_context": "Our method shows the accuracy of 95.62% on the full FPHA dataset and 97.54% on the TinyFPHA dataset, outperforming the other methods by a large margin.", "mention_start": 43, "mention_end": 64, "dataset_mention": "the full FPHA dataset"}, {"mentioned_in_paper": "505", "context_id": "225", "dataset_context": "Our method shows the accuracy of 95.62% on the full FPHA dataset and 97.54% on the TinyFPHA dataset, outperforming the other methods by a large margin.", "mention_start": 79, "mention_end": 99, "dataset_mention": "the TinyFPHA dataset"}, {"mentioned_in_paper": "506", "context_id": "106", "dataset_context": "The CIFAR datasets, CIFAR-10 [18] and CIFAR-100 [37], are subsets of the 80 million tiny images.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The CIFAR datasets"}, {"mentioned_in_paper": "506", "context_id": "114", "dataset_context": "The ILSVRC 2012 classification dataset [4] contains over 1.2 million training images and 50,000 validation images, and each image is labeled from 1000 categories.", "mention_start": 0, "mention_end": 38, "dataset_mention": "The ILSVRC 2012 classification dataset"}, {"mentioned_in_paper": "506", "context_id": "120", "dataset_context": "We evaluate and compare the performance of IGCV3 and MobileNetV2 for object detection on COCO dataset [24], which are used as a backbone for SSDLite and SSDLite2 respectively.", "mention_start": 89, "mention_end": 101, "dataset_mention": "COCO dataset"}, {"mentioned_in_paper": "506", "context_id": "128", "dataset_context": "IGCV3 outperforms the prior works slightly on CIFAR datasets, and achieves significant improvement about 1.5% on ImageNet.", "mention_start": 46, "mention_end": 60, "dataset_mention": "CIFAR datasets"}, {"mentioned_in_paper": "507", "context_id": "9", "dataset_context": "We evaluate the proposed approach with an actual well logging dataset from the oil field and a public 3W dataset.", "mention_start": 39, "mention_end": 69, "dataset_mention": "an actual well logging dataset"}, {"mentioned_in_paper": "507", "context_id": "9", "dataset_context": "We evaluate the proposed approach with an actual well logging dataset from the oil field and a public 3W dataset.", "mention_start": 75, "mention_end": 112, "dataset_mention": "the oil field and a public 3W dataset"}, {"mentioned_in_paper": "507", "context_id": "142", "dataset_context": "In this section, we evaluate the model performance on an actual well logging dataset and a 3w public dataset [32].", "mention_start": 53, "mention_end": 84, "dataset_mention": "an actual well logging dataset"}, {"mentioned_in_paper": "507", "context_id": "142", "dataset_context": "In this section, we evaluate the model performance on an actual well logging dataset and a 3w public dataset [32].", "mention_start": 53, "mention_end": 108, "dataset_mention": "an actual well logging dataset and a 3w public dataset"}, {"mentioned_in_paper": "507", "context_id": "153", "dataset_context": "On the other hand, since it is numerical data collected from the sensor, the 3W dataset is similar to well logging data on both source and type.", "mention_start": 72, "mention_end": 87, "dataset_mention": " the 3W dataset"}, {"mentioned_in_paper": "507", "context_id": "162", "dataset_context": "Similarly, four wells(well 2,5,11,14) in 3W dataset are selected as clients data, 80 percent of which are used for training and the rest are used for testing.", "mention_start": 41, "mention_end": 51, "dataset_mention": "3W dataset"}, {"mentioned_in_paper": "507", "context_id": "185", "dataset_context": "Similar processing in 3W dataset to demonstrate the effectiveness of our approach.", "mention_start": 22, "mention_end": 32, "dataset_mention": "3W dataset"}, {"mentioned_in_paper": "507", "context_id": "191", "dataset_context": "Similar conclusions can conclude in 3W dataset.", "mention_start": 36, "mention_end": 46, "dataset_mention": "3W dataset"}, {"mentioned_in_paper": "508", "context_id": "18", "dataset_context": "Since this dataset is one of the few Iranian datasets that has been compiled with its users' specifications, it can be of great value.", "mention_start": 29, "mention_end": 53, "dataset_mention": "the few Iranian datasets"}, {"mentioned_in_paper": "509", "context_id": "69", "dataset_context": "Our goal is to learn a classifier for query set Q by leveraging unlabeled sample set Q , support set S, and base class dataset D base .", "mention_start": 103, "mention_end": 126, "dataset_mention": " and base class dataset"}, {"mentioned_in_paper": "509", "context_id": "86", "dataset_context": "As shown in Figure 2, following the episodic training paradigm (Vinyals et al. 2016), we first mimic the test setting and construct a number of N -way K-shot tasks (called episodes) from base class dataset D base .", "mention_start": 186, "mention_end": 205, "dataset_mention": "base class dataset"}, {"mentioned_in_paper": "510", "context_id": "16", "dataset_context": "Number of parameters and peak signal-to-noise ratio (PSNR) values of the state-of-the-art and the proposed methods for an upscaling factor of 2 on the Urban100 dataset [11].", "mention_start": 147, "mention_end": 167, "dataset_mention": "the Urban100 dataset"}, {"mentioned_in_paper": "510", "context_id": "109", "dataset_context": "We employ the DIV2K dataset [2] for training the BSRN models, which is widely used for training the recent super-resolution models [3, 15].", "mention_start": 10, "mention_end": 27, "dataset_mention": "the DIV2K dataset"}, {"mentioned_in_paper": "510", "context_id": "132", "dataset_context": "Fig. 4 compares the performance of the trained BSRN models in terms of the number of parameters and the PSNR values measured for the BSD100 dataset [20].", "mention_start": 129, "mention_end": 147, "dataset_mention": "the BSD100 dataset"}, {"mentioned_in_paper": "510", "context_id": "150", "dataset_context": " 2. Performance comparison of the state-of-the-art methods and our model evaluated on the Set5 [5], Set14 [29], BSD100 [20], and Urban100 [11] datasets.", "mention_start": 124, "mention_end": 151, "dataset_mention": " and Urban100 [11] datasets"}, {"mentioned_in_paper": "510", "context_id": "152", "dataset_context": "Table 1 shows the average processing time spent on upscaling an image by a factor of 4, PSNR values, and SSIM values for the BSD100 dataset [20] for various values of r.", "mention_start": 120, "mention_end": 139, "dataset_mention": "the BSD100 dataset"}, {"mentioned_in_paper": "510", "context_id": "161", "dataset_context": "For example, our method achieves a quality gain of 0.31 dB for a scale factor of 2 on the BSD100 dataset over the LapSRN model.", "mention_start": 85, "mention_end": 104, "dataset_mention": "the BSD100 dataset"}, {"mentioned_in_paper": "510", "context_id": "170", "dataset_context": "For example, our method successfully upscales fine details of the structures in the Urban100 dataset, which results in clearer outputs, while the other methods produce highly blurred images or images containing large amounts of artifacts.", "mention_start": 79, "mention_end": 100, "dataset_mention": "the Urban100 dataset"}, {"mentioned_in_paper": "511", "context_id": "227", "dataset_context": "Figure 5 shows exemplary the result for the elevators dataset.", "mention_start": 40, "mention_end": 61, "dataset_mention": "the elevators dataset"}, {"mentioned_in_paper": "511", "context_id": "237", "dataset_context": "However, approximation can also hinder fast convergence as Figure 7 reveals on for the metro dataset.", "mention_start": 82, "mention_end": 100, "dataset_mention": "the metro dataset"}, {"mentioned_in_paper": "512", "context_id": "4", "dataset_context": "The TinyFace dataset is released publicly at: https://qmul-tinyface.github.io/.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The TinyFace dataset"}, {"mentioned_in_paper": "512", "context_id": "7", "dataset_context": "In particular, when tested against native low-resolution face images from a newly constructed tiny face dataset, we reveal that the performances of current state-of-the-art deep learning FR models degrade significantly.", "mention_start": 73, "mention_end": 111, "dataset_mention": "a newly constructed tiny face dataset"}, {"mentioned_in_paper": "512", "context_id": "11", "dataset_context": "To collect sufficient data for deep learning, it requires to process a large amount of public domain (e.g. from the web) video and image data generated from a wide range of sources such as social-media, e.g. the MegaFace dataset [22, 28].", "mention_start": 207, "mention_end": 228, "dataset_mention": "the MegaFace dataset"}, {"mentioned_in_paper": "512", "context_id": "21", "dataset_context": "In the experiments, we benchmark the performance of four state-of-theart deep learning FR models [30, 26, 36, 42] and three super-resolution methods [11, 37, 23] on the TinyFace dataset.", "mention_start": 164, "mention_end": 185, "dataset_mention": "the TinyFace dataset"}, {"mentioned_in_paper": "512", "context_id": "26", "dataset_context": "One main driving force behind recent advances is the availability of large sized FR benchmarks and datasets.", "mention_start": 69, "mention_end": 107, "dataset_mention": "large sized FR benchmarks and datasets"}, {"mentioned_in_paper": "512", "context_id": "93", "dataset_context": "As the auxiliary and native LR data sets are highly imbalanced in size, we further propose to train the CSRI in two steps for improving the model convergence stability: (1) We first pre-train the synthetic LR SR-FR branch on a large auxiliary face data (CelebA [27]).", "mention_start": 3, "mention_end": 40, "dataset_mention": "the auxiliary and native LR data sets"}, {"mentioned_in_paper": "512", "context_id": "97", "dataset_context": "To create a native LR face dataset, we need an explicit LR criterion.", "mention_start": 10, "mention_end": 34, "dataset_mention": "a native LR face dataset"}, {"mentioned_in_paper": "512", "context_id": "99", "dataset_context": "Existing FR datasets are all >100\u00d7100 pixels (Table 1).", "mention_start": 0, "mention_end": 20, "dataset_mention": "Existing FR datasets"}, {"mentioned_in_paper": "512", "context_id": "101", "dataset_context": "The TinyFace dataset contains two parts, face images with labelled and unlabelled identities.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The TinyFace dataset"}, {"mentioned_in_paper": "512", "context_id": "102", "dataset_context": "The labelled TinyFace images were collected from the publicly available PIPA [46] and MegaFace2 [28] datasets, both of which provide unconstrained social-media web face images with large variety in facial expression/pose and imaging conditions.", "mention_start": 49, "mention_end": 109, "dataset_mention": "the publicly available PIPA [46] and MegaFace2 [28] datasets"}, {"mentioned_in_paper": "512", "context_id": "119", "dataset_context": "To establish an evaluation protocol on the TinyFace dataset, it is necessary to first define the training and test data partition.", "mention_start": 39, "mention_end": 59, "dataset_mention": "the TinyFace dataset"}, {"mentioned_in_paper": "512", "context_id": "156", "dataset_context": "For this purpose, we created a synthetic LR face dataset, which we call SynLR-MF2, using 169,403 HR MegaFace2 images [28].", "mention_start": 28, "mention_end": 56, "dataset_mention": "a synthetic LR face dataset"}, {"mentioned_in_paper": "512", "context_id": "191", "dataset_context": "Compared to previous FR datasets that focus on high-resolution face images, TinyFace is uniquely characterised by natively low-resolution and unconstrained face images, both for model training and testing.", "mention_start": 12, "mention_end": 32, "dataset_mention": "previous FR datasets"}, {"mentioned_in_paper": "515", "context_id": "7", "dataset_context": "Our approach is evaluated on a synthetic dataset, the KITTI dataset and a dataset recorded with our experimental vehicle.", "mention_start": 49, "mention_end": 67, "dataset_mention": " the KITTI dataset"}, {"mentioned_in_paper": "515", "context_id": "7", "dataset_context": "Our approach is evaluated on a synthetic dataset, the KITTI dataset and a dataset recorded with our experimental vehicle.", "mention_start": 49, "mention_end": 81, "dataset_mention": " the KITTI dataset and a dataset"}, {"mentioned_in_paper": "515", "context_id": "42", "dataset_context": "We evaluate our approach on a synthetic dataset generated by LGSVL [9], the real world KITTI dataset [10] and also a real world dataset recorded with our experimental vehicle Bertha One [11].", "mention_start": 71, "mention_end": 100, "dataset_mention": " the real world KITTI dataset"}, {"mentioned_in_paper": "515", "context_id": "42", "dataset_context": "We evaluate our approach on a synthetic dataset generated by LGSVL [9], the real world KITTI dataset [10] and also a real world dataset recorded with our experimental vehicle Bertha One [11].", "mention_start": 114, "mention_end": 135, "dataset_mention": "a real world dataset"}, {"mentioned_in_paper": "515", "context_id": "139", "dataset_context": "We evaluate our 3D reconstruction, texturing and semantic mapping system on: a synthetic dataset from LGSVL [9], the real world KITTI dataset [10] and a dataset recorded with our experimental vehicle Bertha One [11] equipped with a Velodyne VLS-128 LiDAR and cameras.", "mention_start": 112, "mention_end": 141, "dataset_mention": " the real world KITTI dataset"}, {"mentioned_in_paper": "515", "context_id": "139", "dataset_context": "We evaluate our 3D reconstruction, texturing and semantic mapping system on: a synthetic dataset from LGSVL [9], the real world KITTI dataset [10] and a dataset recorded with our experimental vehicle Bertha One [11] equipped with a Velodyne VLS-128 LiDAR and cameras.", "mention_start": 112, "mention_end": 160, "dataset_mention": " the real world KITTI dataset [10] and a dataset"}, {"mentioned_in_paper": "515", "context_id": "149", "dataset_context": "Besides that, we also evaluate our adaptive TSDF-based 3D reconstruction on the real world KITTI dataset.", "mention_start": 75, "mention_end": 104, "dataset_mention": "the real world KITTI dataset"}, {"mentioned_in_paper": "516", "context_id": "29", "dataset_context": "However, in many cases, supervised training is not feasible, first and foremost because it requires an appropriate dataset that contains labeled ground truth data.", "mention_start": 99, "mention_end": 122, "dataset_mention": "an appropriate dataset"}, {"mentioned_in_paper": "516", "context_id": "31", "dataset_context": "Acquiring such a dataset is cumbersome and costly.", "mention_start": 0, "mention_end": 24, "dataset_mention": "Acquiring such a dataset"}, {"mentioned_in_paper": "516", "context_id": "160", "dataset_context": "Both the rural and the urban dataset were captured with a DJI Phantom 3 Professional.", "mention_start": 0, "mention_end": 36, "dataset_mention": "Both the rural and the urban dataset"}, {"mentioned_in_paper": "516", "context_id": "195", "dataset_context": "The best accuracy is achieved on the rural dataset, followed by the synthetic dataset.", "mention_start": 33, "mention_end": 50, "dataset_mention": "the rural dataset"}, {"mentioned_in_paper": "516", "context_id": "196", "dataset_context": "The results achieved on the urban dataset are considerably worse, which we assume to be caused by the bad picture quality and the faulty ground truth.", "mention_start": 24, "mention_end": 41, "dataset_mention": "the urban dataset"}, {"mentioned_in_paper": "516", "context_id": "212", "dataset_context": "We have performed the comparison on the rural dataset, since it is the real-world dataset on which the best results are achieved by the SMDE.", "mention_start": 36, "mention_end": 53, "dataset_mention": "the rural dataset"}, {"mentioned_in_paper": "518", "context_id": "56", "dataset_context": "Assume that we are given a 3D CT volume dataset S containing K organs.", "mention_start": 25, "mention_end": 47, "dataset_mention": "a 3D CT volume dataset"}, {"mentioned_in_paper": "518", "context_id": "87", "dataset_context": "The student model is trained on this augmented dataset S the same way we train the teacher model as described in Sec.", "mention_start": 32, "mention_end": 54, "dataset_mention": "this augmented dataset"}, {"mentioned_in_paper": "518", "context_id": "96", "dataset_context": "To the best of our knowledge, this is the largest abdominal CT dataset with the most number of organs segmented.", "mention_start": 37, "mention_end": 70, "dataset_mention": "the largest abdominal CT dataset"}, {"mentioned_in_paper": "518", "context_id": "106", "dataset_context": "Similar to [48, 45, 41], we initialize the network parameters \u03b8 by using the FCN-8s model [24] pre-trained on the PascalVOC image segmentation dataset.", "mention_start": 109, "mention_end": 150, "dataset_mention": "the PascalVOC image segmentation dataset"}, {"mentioned_in_paper": "518", "context_id": "145", "dataset_context": "We apply our trained DMPCT model (50 labeled data + 100 unlabeled data) and baseline FCN model (50 labeled data + 0 unlabeled data) on a public available abdominal CT datasets 1 with 13 anatomical structures labeled without any further re-training on new data cases.", "mention_start": 135, "mention_end": 175, "dataset_mention": "a public available abdominal CT datasets"}, {"mentioned_in_paper": "518", "context_id": "148", "dataset_context": "We also directly test our models on the NIH pancreas segmentation dataset of 82 cases 2 and observe that our DMPCT model achieves an average DSC of 66.16%, outperforming the fully supervised method, with an average DSC of 58.73%, by more than 7%.", "mention_start": 36, "mention_end": 73, "dataset_mention": "the NIH pancreas segmentation dataset"}, {"mentioned_in_paper": "518", "context_id": "155", "dataset_context": "We evaluate our approach on our own large newly collected high-quality dataset.", "mention_start": 58, "mention_end": 78, "dataset_mention": "high-quality dataset"}, {"mentioned_in_paper": "519", "context_id": "239", "dataset_context": "For example, the movie rating dataset ML 1M (integer ratings in {1 . . .", "mention_start": 12, "mention_end": 37, "dataset_mention": " the movie rating dataset"}, {"mentioned_in_paper": "519", "context_id": "240", "dataset_context": "5}) is the densest dataset.", "mention_start": 7, "mention_end": 26, "dataset_mention": "the densest dataset"}, {"mentioned_in_paper": "519", "context_id": "243", "dataset_context": "5}) are movie rating datasets.", "mention_start": 8, "mention_end": 29, "dataset_mention": "movie rating datasets"}, {"mentioned_in_paper": "519", "context_id": "248", "dataset_context": "The book rating dataset Goodreads (integer ratings in {1 . . .", "mention_start": 0, "mention_end": 23, "dataset_mention": "The book rating dataset"}, {"mentioned_in_paper": "519", "context_id": "307", "dataset_context": "This way, Gain and Gain+Reuse create a neighborhood for a given target user with sufficient rating data even in sparse datasets.", "mention_start": 111, "mention_end": 127, "dataset_mention": "sparse datasets"}, {"mentioned_in_paper": "519", "context_id": "308", "dataset_context": "Plus, we highlight that Gain and Gain+Reuse significantly increase recommendation accuracy for Goodreads, despite the dataset's low density.", "mention_start": 105, "mention_end": 125, "dataset_mention": " despite the dataset"}, {"mentioned_in_paper": "519", "context_id": "320", "dataset_context": "For example, for the ML 1M dataset, UserKNN leads to 80.39% of users that are vulnerable, since their privacy risk exceeds threshold  = 92.89", "mention_start": 16, "mention_end": 34, "dataset_mention": "the ML 1M dataset"}, {"mentioned_in_paper": "519", "context_id": "336", "dataset_context": "For sparse datasets, personalized neighborhood reuse seems to be a better solution than unpersonalized neighborhood reuse.", "mention_start": 4, "mention_end": 19, "dataset_mention": "sparse datasets"}, {"mentioned_in_paper": "520", "context_id": "22", "dataset_context": "For example, consider the example of \"pizza-tossing event\" from the UCF50 1 unconstrained actions dataset.", "mention_start": 63, "mention_end": 105, "dataset_mention": "the UCF50 1 unconstrained actions dataset"}, {"mentioned_in_paper": "520", "context_id": "50", "dataset_context": "A priori information about the geometry of manifold is integrated in a Logitboost algorithm to achieve impressive detection results on two challenging pedestrian datasets.", "mention_start": 151, "mention_end": 170, "dataset_mention": "pedestrian datasets"}, {"mentioned_in_paper": "520", "context_id": "93", "dataset_context": "Figures 3(a) and 3(b) visualize the appearance and motion features respectively for a sample frame from the UCF50 dataset, where a person is exercising a \"benchpress\".", "mention_start": 104, "mention_end": 121, "dataset_mention": "the UCF50 dataset"}, {"mentioned_in_paper": "520", "context_id": "167", "dataset_context": "To systematically study the behavior of our proposed descriptor and the associated classification methods, we conduct preliminary experiments on a relatively simple, well recognized, human actions dataset [28] to validate our hypothesis and then proceed towards the unconstrained case.", "mention_start": 182, "mention_end": 204, "dataset_mention": " human actions dataset"}, {"mentioned_in_paper": "520", "context_id": "171", "dataset_context": "UCF50: The UCF50, human actions dataset consists of video clips that are sourced from YouTube videos (unedited) respectively.", "mention_start": 17, "mention_end": 39, "dataset_mention": " human actions dataset"}, {"mentioned_in_paper": "520", "context_id": "178", "dataset_context": "The videos in the dataset are characterized by significant background clutter, camera jitter and to some extent the other challenges observed in the UCF50 dataset.", "mention_start": 144, "mention_end": 162, "dataset_mention": "the UCF50 dataset"}, {"mentioned_in_paper": "520", "context_id": "180", "dataset_context": " (8) to perform evaluations on the KTH dataset, as not much contextual information is available in this case.", "mention_start": 30, "mention_end": 46, "dataset_mention": "the KTH dataset"}, {"mentioned_in_paper": "520", "context_id": "207", "dataset_context": "Although our proposed method does not show significant improvement over the state of the art on the KTH dataset, we observe definite increase in performance over the two other challenging action recognition datasets.", "mention_start": 96, "mention_end": 111, "dataset_mention": "the KTH dataset"}, {"mentioned_in_paper": "520", "context_id": "207", "dataset_context": "Although our proposed method does not show significant improvement over the state of the art on the KTH dataset, we observe definite increase in performance over the two other challenging action recognition datasets.", "mention_start": 187, "mention_end": 215, "dataset_mention": "action recognition datasets"}, {"mentioned_in_paper": "520", "context_id": "214", "dataset_context": "Note that the performance reflected in case of UCF50 and HMDB51 datasets are significantly high as compared to other approaches as a lot of contextual information is available from the RGB channels of the video.", "mention_start": 47, "mention_end": 72, "dataset_mention": "UCF50 and HMDB51 datasets"}, {"mentioned_in_paper": "520", "context_id": "228", "dataset_context": "Fig. 8 indicates F-measures derived from precision and recall for 8 different classes of unconstrained actions from UCF50 dataset.", "mention_start": 116, "mention_end": 129, "dataset_mention": "UCF50 dataset"}, {"mentioned_in_paper": "520", "context_id": "269", "dataset_context": "Finally, in Fig. 11, we show the respective confusion matrices obtained after applying the proposed method on first 10 of the development batches from the CGD 2011 dataset.", "mention_start": 150, "mention_end": 171, "dataset_mention": "the CGD 2011 dataset"}, {"mentioned_in_paper": "522", "context_id": "3", "dataset_context": "This is the goal of our work, where we introduce Lesion-Harvestera powerful system to harvest missing annotations from lesion datasets at high precision.", "mention_start": 118, "mention_end": 134, "dataset_mention": "lesion datasets"}, {"mentioned_in_paper": "522", "context_id": "19", "dataset_context": "As a result, popular large-scale medical imaging datasets suffer from uncertainties, mislabelings [3], [8], [9] and incomplete annotations [5], a trend that promises to increase as more and more PACS data is exploited.", "mention_start": 12, "mention_end": 57, "dataset_mention": " popular large-scale medical imaging datasets"}, {"mentioned_in_paper": "522", "context_id": "22", "dataset_context": "This is the focus of our work, where we articulate a powerful and effective label completion framework for lesion datasets, applying it to harvest unlabeled lesions from the recent DeepLesion dataset [5].", "mention_start": 106, "mention_end": 122, "dataset_mention": "lesion datasets"}, {"mentioned_in_paper": "522", "context_id": "22", "dataset_context": "This is the focus of our work, where we articulate a powerful and effective label completion framework for lesion datasets, applying it to harvest unlabeled lesions from the recent DeepLesion dataset [5].", "mention_start": 169, "mention_end": 199, "dataset_mention": "the recent DeepLesion dataset"}, {"mentioned_in_paper": "522", "context_id": "29", "dataset_context": "As such, the DeepLesion dataset is an important source of data for medical imaging analysis tasks, including training and characterizing lesion detectors and for developing radiomics-based biomarkers for tumor assessment and tracking.", "mention_start": 8, "mention_end": 31, "dataset_mention": " the DeepLesion dataset"}, {"mentioned_in_paper": "522", "context_id": "74", "dataset_context": "We also tackle the large-scale and noisy DeepLesion dataset, but we process each CT volume as a whole, rather than focus on post-processing a given region of interest.", "mention_start": 15, "mention_end": 59, "dataset_mention": "the large-scale and noisy DeepLesion dataset"}, {"mentioned_in_paper": "522", "context_id": "76", "dataset_context": "Recently, Yan et al., [4] introduced the DeepLesion dataset, which is a large-scale clinical database for whole body lesion detection, with follow-up work focusing on incorporating 3D context into 2D two-stage region-proposal CNNs [13], [15].", "mention_start": 36, "mention_end": 59, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "522", "context_id": "92", "dataset_context": "As motivated above, we aim to harvest missing annotations from the incomplete DeepLesion dataset [4], [5].", "mention_start": 62, "mention_end": 96, "dataset_mention": "the incomplete DeepLesion dataset"}, {"mentioned_in_paper": "522", "context_id": "226", "dataset_context": "To harvest lesions from the DeepLesion dataset, we randomly selected 844 volumes from the original 14 075 training CTs 2.", "mention_start": 24, "mention_end": 46, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "522", "context_id": "237", "dataset_context": "Accordingly, we trained state-of-the art detection methods (the same 12 outlined in Table IV 's later experiments) on the DeepLesion dataset and measured their performance using 3D IoU, incomplete RECIST [4], and the proposed P3D IoU metrics.", "mention_start": 117, "mention_end": 140, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "522", "context_id": "287", "dataset_context": "In summary, these results indicate that the harvested lesions and hard negatives can significantly boost how many lesions can be recovered from the DeepLesion dataset.", "mention_start": 143, "mention_end": 166, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "522", "context_id": "348", "dataset_context": "We test our system on the DeepLesion dataset and show that after only annotating 5% of the volumes we can successfully harvest 9, 805 additional lesions, which corresponds to 47.9% recall at 90% precision, which is a boost of 11.2% in recall over the original RECIST marks.", "mention_start": 22, "mention_end": 44, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "522", "context_id": "357", "dataset_context": "As we demonstrated, training off-the-shelf detectors on our harvested lesions allows them to outperform the current best performance on the DeepLesion dataset by margins as high as 10% AP, which is a significant boost in performance.", "mention_start": 135, "mention_end": 158, "dataset_mention": "the DeepLesion dataset"}, {"mentioned_in_paper": "522", "context_id": "359", "dataset_context": "More broadly, our results indicate that the lesion harvesting framework is a powerful means to complete PACS-derived datasets, which we anticipate will be an increasingly important topic.", "mention_start": 94, "mention_end": 125, "dataset_mention": "complete PACS-derived datasets"}, {"mentioned_in_paper": "524", "context_id": "7", "dataset_context": "Extensive experiments on BraTS 2019 and CHAOS datasets show that our MISSU achieves the best performance over previous state-of-the-art methods.", "mention_start": 25, "mention_end": 54, "dataset_mention": "BraTS 2019 and CHAOS datasets"}, {"mentioned_in_paper": "524", "context_id": "143", "dataset_context": "We experiment on two widely-used medical image segmentation benchmarks: BraTS 2019 dataset [42] for brain tumor segmentation and CHAOS dataset [43] for liver segmentation.", "mention_start": 71, "mention_end": 90, "dataset_mention": " BraTS 2019 dataset"}, {"mentioned_in_paper": "524", "context_id": "143", "dataset_context": "We experiment on two widely-used medical image segmentation benchmarks: BraTS 2019 dataset [42] for brain tumor segmentation and CHAOS dataset [43] for liver segmentation.", "mention_start": 99, "mention_end": 142, "dataset_mention": "brain tumor segmentation and CHAOS dataset"}, {"mentioned_in_paper": "524", "context_id": "151", "dataset_context": "CHAOS dataset contains 40 two-modality (i.e., T1 and T2) image sequences, where 20 image sequences are used for training and the remaining for testing without annotations.", "mention_start": 0, "mention_end": 13, "dataset_mention": "CHAOS dataset"}, {"mentioned_in_paper": "524", "context_id": "177", "dataset_context": "For CHAOS dataset, we randomly cropped its sequences into patches from 256\u00d7256\u00d730 to 128\u00d7 B. Quantitative and Qualitative Results", "mention_start": 4, "mention_end": 17, "dataset_mention": "CHAOS dataset"}, {"mentioned_in_paper": "524", "context_id": "204", "dataset_context": "We further evaluate the performance of liver segmentation on the CHAOS dataset, which is summarized in Tab.", "mention_start": 61, "mention_end": 78, "dataset_mention": "the CHAOS dataset"}, {"mentioned_in_paper": "524", "context_id": "238", "dataset_context": "We also provide qualitative comparison results on the BraTS dataset to effectively demonstrate the effects of different modules, as shown in Fig. 4. Similarly, to validate the performance under different settings of Transformer, a variety of ablation studies were performed, including 1) the number of skip-connections (Fig. 5); 2) Transformer layers L (Tab.", "mention_start": 50, "mention_end": 67, "dataset_mention": "the BraTS dataset"}, {"mentioned_in_paper": "524", "context_id": "244", "dataset_context": "IV, compared to the base model, Transformer added after it (i.e., the second row) achieves significant performance improvement, especially with the increase of 8.16% and 11.65% ACC on BraTS 2019 and CHAOS dataset, respectively.", "mention_start": 183, "mention_end": 212, "dataset_mention": "BraTS 2019 and CHAOS dataset"}, {"mentioned_in_paper": "524", "context_id": "269", "dataset_context": "By varying the number of skip-connections to be 0/1/2/3, the segmentation performance in Dice on BraTS dataset are summarized in Fig. 5.", "mention_start": 96, "mention_end": 110, "dataset_mention": "BraTS dataset"}, {"mentioned_in_paper": "524", "context_id": "294", "dataset_context": "We have comprehensively evaluated the performance of MISSU on BraTS 2019 and CHAOS datasets, which demonstrates the superior performance gains over the state-of-the-art methods.", "mention_start": 62, "mention_end": 91, "dataset_mention": "BraTS 2019 and CHAOS datasets"}, {"mentioned_in_paper": "525", "context_id": "146", "dataset_context": "However, for many data clustering problems, there exists complex geometry within the data points and typically the clusters cannot be distinguished by the centroid of each cluster, for example in the three-circles synthetic data set.", "mention_start": 195, "mention_end": 232, "dataset_mention": "the three-circles synthetic data set"}, {"mentioned_in_paper": "525", "context_id": "230", "dataset_context": "The COIL data set is downloaded from the supplementary ma- 14), (16), and (20), respectively.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The COIL data set"}, {"mentioned_in_paper": "526", "context_id": "186", "dataset_context": "In this subsection, we compare the classification accuracy of our method and several strong baseline methods under four datasets, and report the results in Table 2.", "mention_start": 61, "mention_end": 128, "dataset_mention": "our method and several strong baseline methods under four datasets"}, {"mentioned_in_paper": "526", "context_id": "200", "dataset_context": "The superiority of performance when combining SupGCon and StruInf is more significant in PTC dataset.", "mention_start": 89, "mention_end": 100, "dataset_mention": "PTC dataset"}, {"mentioned_in_paper": "529", "context_id": "71", "dataset_context": "We used the dTSC objective with SWAPR to train 5 different brain parcellation models corresponding to 5 spatial scales (300, 400, 666, 800, 1000 cortical parcels) using 2457 BOLD images from the Human Connectome Project (HCP) dataset [20].", "mention_start": 190, "mention_end": 233, "dataset_mention": "the Human Connectome Project (HCP) dataset"}, {"mentioned_in_paper": "529", "context_id": "103", "dataset_context": "In a held-out sample of 351 BOLD images from the HCP dataset (Figure 4), the RFNN model outperformed both the top-performing a priori one-confound model (GSR, global signal regression) and a 36-confound model (36P, a superset of GSR) that has previously been demonstrated to give SOTA performance [29].", "mention_start": 45, "mention_end": 60, "dataset_mention": "the HCP dataset"}, {"mentioned_in_paper": "529", "context_id": "114", "dataset_context": "Relaxing this constraint to require only a nonnegative support and a mean of 1, satisfied inter alia by noise sampled from an appropriately chosen gamma or truncated normal distribution, leads to a simple method for augmenting covariance datasets that complements random windowing of the input time series (Figure 5, Left).", "mention_start": 226, "mention_end": 246, "dataset_mention": "covariance datasets"}, {"mentioned_in_paper": "529", "context_id": "126", "dataset_context": "The results of the clustering experiment in subsamples of the HCP dataset are shown in Figure 5. Clustering t \u00d7 t covariances into c = 2 subnetworks divides the brain along a unimodal (blue) to higher-order (red) axis (Figure 5, Centre); this clustering is replicable across data splits.", "mention_start": 58, "mention_end": 73, "dataset_mention": "the HCP dataset"}, {"mentioned_in_paper": "529", "context_id": "151", "dataset_context": "Training this unsupervised model on 30 parcellated time series from the Midnight Scan Club (MSC) dataset [18], we identify a set of dynamic communities that accord well with previously characterised brain subnetworks.", "mention_start": 68, "mention_end": 104, "dataset_mention": "the Midnight Scan Club (MSC) dataset"}, {"mentioned_in_paper": "529", "context_id": "169", "dataset_context": "Dataset, preprocessing, and computational resources The openly available Human Connectome Project (HCP) dataset comprises scans from 1200 participants performing a number of in-scanner tasks.", "mention_start": 23, "mention_end": 111, "dataset_mention": " and computational resources The openly available Human Connectome Project (HCP) dataset"}, {"mentioned_in_paper": "529", "context_id": "183", "dataset_context": "Data were stored as tar shards for compatibility with the webdataset format [8].", "mention_start": 54, "mention_end": 68, "dataset_mention": "the webdataset"}, {"mentioned_in_paper": "529", "context_id": "229", "dataset_context": "Additionally, of the reference parcellations, only the MMP parcellation [11] was defined using the HCP dataset; because of differences between datasets, acquisition protocols, and coordinate spaces in which the different parcellations were defined, comparison results should be interpreted with caution.", "mention_start": 94, "mention_end": 110, "dataset_mention": "the HCP dataset"}, {"mentioned_in_paper": "529", "context_id": "242", "dataset_context": "Dataset, preprocessing, and computational resources We again use the minimally preprocessed Human Connectome Project dataset detailed in the parcellation section above; however, we make a few adjustments.", "mention_start": 64, "mention_end": 124, "dataset_mention": "the minimally preprocessed Human Connectome Project dataset"}, {"mentioned_in_paper": "529", "context_id": "248", "dataset_context": "Additionally, because the HCP dataset includes only gross motion estimates and ICA decompositions, we ran additional preprocessing steps to obtain a more complete complement of confound time series.", "mention_start": 21, "mention_end": 37, "dataset_mention": "the HCP dataset"}, {"mentioned_in_paper": "529", "context_id": "251", "dataset_context": "We then computed the mean time series across voxels in binary masks indicating membership in white matter (WM) and cerebrospinal fluid (CSF) compartments; these masks were included with the HCP dataset.", "mention_start": 185, "mention_end": 201, "dataset_mention": "the HCP dataset"}, {"mentioned_in_paper": "529", "context_id": "254", "dataset_context": "Following reports that head motion estimates can be contaminated by respiratory artefact [14], we applied a notch filter to the gross estimates of head motion included with the HCP dataset.", "mention_start": 172, "mention_end": 188, "dataset_mention": "the HCP dataset"}, {"mentioned_in_paper": "529", "context_id": "291", "dataset_context": "Benchmarks are computed on a held-out dataset of 351 BOLD time series.", "mention_start": 27, "mention_end": 45, "dataset_mention": "a held-out dataset"}, {"mentioned_in_paper": "529", "context_id": "299", "dataset_context": "Time-by-time covariance (subnetwork detection) We again used the HCP dataset for the timeby-time covariance clustering experiment.", "mention_start": 61, "mention_end": 76, "dataset_mention": "the HCP dataset"}, {"mentioned_in_paper": "529", "context_id": "300", "dataset_context": "Of our 12 HCP dataset splits, we selected 4 for training and 4 for evaluation.", "mention_start": 3, "mention_end": 21, "dataset_mention": "our 12 HCP dataset"}, {"mentioned_in_paper": "529", "context_id": "348", "dataset_context": "The templates are initialised by first computing slidingwindow correlations (as in [22]) with a window size of 50 and a sliding step size of 25 for each of 1180 total images that constitute 4 splits of the HCP dataset.", "mention_start": 202, "mention_end": 217, "dataset_mention": "the HCP dataset"}, {"mentioned_in_paper": "529", "context_id": "370", "dataset_context": "We also repeated the state detection experiment in 4 additional subsamples of the HCP dataset, each comprising between 50 and 80 instances, which were used in computing the k-means initialisation, and 5 similar subsamples that were not used in computing the initialisation.", "mention_start": 78, "mention_end": 93, "dataset_mention": "the HCP dataset"}, {"mentioned_in_paper": "529", "context_id": "372", "dataset_context": "Dataset and preprocessing We use the Midnight Scan Club dataset (MSC) for the community detection analysis.", "mention_start": 33, "mention_end": 63, "dataset_mention": "the Midnight Scan Club dataset"}, {"mentioned_in_paper": "529", "context_id": "373", "dataset_context": "The MSC dataset includes a total 10 subjects each densely scanned over 10 sessions, both at rest and performing a number of directed cognitive tasks [23].", "mention_start": 0, "mention_end": 15, "dataset_mention": "The MSC dataset"}, {"mentioned_in_paper": "530", "context_id": "169", "dataset_context": "In this section, we provide a description of our Reddit dataset ( \u00a73.1), our methods for identifying users of interest to our study ( \u00a73.2) and communities belonging to the Manosphere ( \u00a73.3), and the influence of social engagement with the Manosphere on the traits associated with radicalization ( \u00a73.4).", "mention_start": 44, "mention_end": 63, "dataset_mention": "our Reddit dataset"}, {"mentioned_in_paper": "530", "context_id": "177", "dataset_context": "We use the Pushshift Reddit dataset to gather all user submitted comments and posts, along with their attached metadata for a 68 month period between 01/2015 and 08/2020 as the basis for our analysis.", "mention_start": 7, "mention_end": 35, "dataset_mention": "the Pushshift Reddit dataset"}, {"mentioned_in_paper": "530", "context_id": "211", "dataset_context": "In an effort to reduce the false-positive rates of our feminist discourse dataset, we clustered all identified comments and posts by the context in which they were used.", "mention_start": 51, "mention_end": 81, "dataset_mention": "our feminist discourse dataset"}, {"mentioned_in_paper": "530", "context_id": "547", "dataset_context": "Some of these limitations arise simply because of the observational and textual nature of the data that we engage with and others due to possibly incomplete datasets.", "mention_start": 137, "mention_end": 165, "dataset_mention": "possibly incomplete datasets"}, {"mentioned_in_paper": "531", "context_id": "136", "dataset_context": "This section aims to demonstrate the superiority of the proposed spatial deduction algorithm on the monitoring dataset obtained from SHMS of Dinghuaimen tunnel.", "mention_start": 96, "mention_end": 118, "dataset_mention": "the monitoring dataset"}, {"mentioned_in_paper": "532", "context_id": "51", "dataset_context": "For demonstrative purposes, we have included in sequeval a down-sampled version of the playlists dataset originally collected by Shuo Chen from the Yes.com website [2].", "mention_start": 82, "mention_end": 104, "dataset_mention": "the playlists dataset"}, {"mentioned_in_paper": "533", "context_id": "29", "dataset_context": "To improve the availability of datasets for vehicle reidentification, we collected and annotated a new vehicle reidentification dataset called CarsReId74k.", "mention_start": 82, "mention_end": 135, "dataset_mention": "and annotated a new vehicle reidentification dataset"}, {"mentioned_in_paper": "533", "context_id": "34", "dataset_context": "We make the dataset publicly available 3 for future comparison and research.", "mention_start": 0, "mention_end": 19, "dataset_mention": "We make the dataset"}, {"mentioned_in_paper": "533", "context_id": "101", "dataset_context": "When it comes to genuine vehicle re-identification, Liu et al. (2016c) constructed a rather small VeRi-776 dataset containing 50,000 images of 776 vehicles.", "mention_start": 51, "mention_end": 114, "dataset_mention": " Liu et al. (2016c) constructed a rather small VeRi-776 dataset"}, {"mentioned_in_paper": "533", "context_id": "102", "dataset_context": " Liu et al. (2016a) collected VehicleID dataset containing 26,267 vehicles in 220k images taken from a frontal/rear viewpoint above road.", "mention_start": 29, "mention_end": 47, "dataset_mention": "VehicleID dataset"}, {"mentioned_in_paper": "533", "context_id": "103", "dataset_context": "Recently, Yan et al. (2017) published two datasets VD1 and VD2 for vehicle re-identification and fine-grained classification with over 220k of vehicles in total, with make, model, and year annotation.", "mention_start": 9, "mention_end": 50, "dataset_mention": " Yan et al. (2017) published two datasets"}, {"mentioned_in_paper": "533", "context_id": "162", "dataset_context": "High weights Fig. 4. Middle: Distribution of mean weights for test images in iLIDS-VID dataset (Wang et al., 2014).", "mention_start": 76, "mention_end": 94, "dataset_mention": "iLIDS-VID dataset"}, {"mentioned_in_paper": "533", "context_id": "174", "dataset_context": "Furthermore, on iLIDS-VID dataset (Wang et al., 2014), we tested how important different parts of the network are.", "mention_start": 15, "mention_end": 33, "dataset_mention": "iLIDS-VID dataset"}, {"mentioned_in_paper": "533", "context_id": "189", "dataset_context": "Therefore, we cannot use fine-grained vehicle recognition datasets (Sochor et al., 2016 (Sochor et al., , 2017;; Yang et al., 2015; Krause et al., 2013) for the task.", "mention_start": 24, "mention_end": 66, "dataset_mention": "fine-grained vehicle recognition datasets"}, {"mentioned_in_paper": "533", "context_id": "190", "dataset_context": "As other existing vehicle re-identification datasets VeRi-776 (Liu et al., 2016c), VehicleID (Liu et al., 2016a) and PKU-VDs (Yan et al., 2017) are either small (VeRi-776) or limited to frontal/rear viewpoints (VehicleID, PKU-VDs).", "mention_start": 3, "mention_end": 52, "dataset_mention": "other existing vehicle re-identification datasets"}, {"mentioned_in_paper": "533", "context_id": "192", "dataset_context": "The data were collected by 66 cameras from various angles and the dataset contains almost 74 k of vehicle tracks with precise identity annotation (acquired from license plates).", "mention_start": 43, "mention_end": 73, "dataset_mention": "various angles and the dataset"}, {"mentioned_in_paper": "533", "context_id": "193", "dataset_context": "More detailed comparison of different available vehicle re-identification datasets can be found in Table 2.", "mention_start": 28, "mention_end": 82, "dataset_mention": "different available vehicle re-identification datasets"}, {"mentioned_in_paper": "533", "context_id": "210", "dataset_context": "We divided the dataset into the training, the testing and the validation part by sessions (five sessions for training, five sessions for testing and one validation).", "mention_start": 0, "mention_end": 22, "dataset_mention": "We divided the dataset"}, {"mentioned_in_paper": "533", "context_id": "212", "dataset_context": "The table shows that our dataset is significantly larger than VeRi-776 (Liu et al., 2016c) dataset with only 776 unique vehicles.", "mention_start": 83, "mention_end": 98, "dataset_mention": " 2016c) dataset"}, {"mentioned_in_paper": "533", "context_id": "213", "dataset_context": "And compared to VehicleID, VD1 and VD2 datasets (Liu et al., 2016a; Yan et al., 2017), our dataset is not limited to frontal/rear viewpoints.", "mention_start": 26, "mention_end": 47, "dataset_mention": " VD1 and VD2 datasets"}, {"mentioned_in_paper": "533", "context_id": "214", "dataset_context": "Compared to VehicleID dataset, CarsReId74k dataset has fewer unique vehicles (17,681 vs. 26,267), however far more image (3,242,713 vs. 221,763) as vehicles are seen from more viewpoints.", "mention_start": 12, "mention_end": 29, "dataset_mention": "VehicleID dataset"}, {"mentioned_in_paper": "533", "context_id": "214", "dataset_context": "Compared to VehicleID dataset, CarsReId74k dataset has fewer unique vehicles (17,681 vs. 26,267), however far more image (3,242,713 vs. 221,763) as vehicles are seen from more viewpoints.", "mention_start": 30, "mention_end": 50, "dataset_mention": " CarsReId74k dataset"}, {"mentioned_in_paper": "533", "context_id": "226", "dataset_context": "Currently available datasets does not fit conditions described before at least in one condition (see Sec. 4), thus vehicle reidentification task was evaluated on our novel CarsReId74k only.", "mention_start": 0, "mention_end": 28, "dataset_mention": "Currently available datasets"}, {"mentioned_in_paper": "533", "context_id": "229", "dataset_context": "The feature extractor was fine-tuned on the identification task using the training part of the CarsReId74k dataset.", "mention_start": 91, "mention_end": 114, "dataset_mention": "the CarsReId74k dataset"}, {"mentioned_in_paper": "533", "context_id": "265", "dataset_context": "We use two common datasets: iLIDS-VID (Wang et al., 2014) and PRID-2011 (Hirzer et al., 2011) as they are usually used by other methods for feature aggregation in temporal domain (Yan et al., 2016; McLaughlin et al., 2016; Gao et al., 2016; Xu et al., 2017; Zhang et al., 2017b; Chen et al., 2017b; Zhou et al., 2017b).", "mention_start": 7, "mention_end": 26, "dataset_mention": "two common datasets"}, {"mentioned_in_paper": "533", "context_id": "266", "dataset_context": "Futrhermore, for fair comparison of proposed method, our work was also evaluated on the MARS dataset by Zheng et al. (2016).", "mention_start": 83, "mention_end": 100, "dataset_mention": "the MARS dataset"}, {"mentioned_in_paper": "533", "context_id": "273", "dataset_context": "Therefore, the evaluation is done on 100 tracks (150 tracks) with PRID-2011 (iLIDS-VID) dataset.", "mention_start": 65, "mention_end": 95, "dataset_mention": "PRID-2011 (iLIDS-VID) dataset"}, {"mentioned_in_paper": "533", "context_id": "274", "dataset_context": "We used 10 random splits in the case of the PRID-2011 dataset, and the 10 published splits in the case of iLIDS-VID.", "mention_start": 40, "mention_end": 61, "dataset_mention": "the PRID-2011 dataset"}, {"mentioned_in_paper": "533", "context_id": "284", "dataset_context": "Person re-identification results on MARS dataset.", "mention_start": 36, "mention_end": 48, "dataset_mention": "MARS dataset"}, {"mentioned_in_paper": "533", "context_id": "299", "dataset_context": "The results show that the increase of HIT@1 by using the LFTD was 7.3 percentage points for the vehicle reidentification task compared to average pooling, and 17.4 percentage points for the person re-identification with the iLIDS-VID dataset and up to 6.4 percentage points on the MARS dataset.", "mention_start": 219, "mention_end": 241, "dataset_mention": "the iLIDS-VID dataset"}, {"mentioned_in_paper": "533", "context_id": "299", "dataset_context": "The results show that the increase of HIT@1 by using the LFTD was 7.3 percentage points for the vehicle reidentification task compared to average pooling, and 17.4 percentage points for the person re-identification with the iLIDS-VID dataset and up to 6.4 percentage points on the MARS dataset.", "mention_start": 276, "mention_end": 293, "dataset_mention": "the MARS dataset"}, {"mentioned_in_paper": "533", "context_id": "301", "dataset_context": "We collected and annotated a vehicle re-identification dataset CarsReId74k for development and evaluation of vehicle reidentification systems and we make it public.", "mention_start": 13, "mention_end": 62, "dataset_mention": "and annotated a vehicle re-identification dataset"}, {"mentioned_in_paper": "534", "context_id": "29", "dataset_context": "This many-to-one correspondence leads to noisy training signals due to the improper matching of outlier points, which are common in sparse and noisy Li-DAR point clouds collected for popular autonomous driving datasets.", "mention_start": 182, "mention_end": 218, "dataset_mention": "popular autonomous driving datasets"}, {"mentioned_in_paper": "534", "context_id": "134", "dataset_context": "FlyingThings3D (FT3D): The dataset (Mayer et al. 2016) is the first large-scale synthetic dataset designed for scene flow estimation where each scene contains multiple randomlymoving objects taken from the ShapeNet dataset (Chang et al. 2015).", "mention_start": 201, "mention_end": 222, "dataset_mention": "the ShapeNet dataset"}, {"mentioned_in_paper": "534", "context_id": "143", "dataset_context": "Because the KITTI  dataset is collected from autonomous vehicles across real-world environments with varied conditions, it tends to include more noise and outliers when compared to synthetic datasets.", "mention_start": 7, "mention_end": 26, "dataset_mention": "the KITTI  dataset"}, {"mentioned_in_paper": "534", "context_id": "147", "dataset_context": "It shows that CS is much more robust when conducting self-supervised learning on the real-world KITTI  dataset.", "mention_start": 80, "mention_end": 110, "dataset_mention": "the real-world KITTI  dataset"}, {"mentioned_in_paper": "534", "context_id": "160", "dataset_context": "We evaluate all trained models on the KITTI Scene Flow dataset.", "mention_start": 34, "mention_end": 62, "dataset_mention": "the KITTI Scene Flow dataset"}, {"mentioned_in_paper": "534", "context_id": "161", "dataset_context": "We find that CS is more robust to noise in LiDAR datasets than CD and EMD.", "mention_start": 43, "mention_end": 57, "dataset_mention": "LiDAR datasets"}, {"mentioned_in_paper": "534", "context_id": "166", "dataset_context": "We demonstrate its effectiveness by training on the FT3D dataset without using any ground truth annotations.", "mention_start": 48, "mention_end": 64, "dataset_mention": "the FT3D dataset"}, {"mentioned_in_paper": "534", "context_id": "167", "dataset_context": "Table 3 summarizes the evaluation results on the test split of the FT3D dataset.", "mention_start": 63, "mention_end": 79, "dataset_mention": "the FT3D dataset"}, {"mentioned_in_paper": "534", "context_id": "169", "dataset_context": "We apply the trained model from the FT3D dataset to the unseen KSF dataset.", "mention_start": 32, "mention_end": 48, "dataset_mention": "the FT3D dataset"}, {"mentioned_in_paper": "534", "context_id": "169", "dataset_context": "We apply the trained model from the FT3D dataset to the unseen KSF dataset.", "mention_start": 52, "mention_end": 74, "dataset_mention": "the unseen KSF dataset"}, {"mentioned_in_paper": "535", "context_id": "12", "dataset_context": "By taking the advantage of low-level pooling features and probability distributions of neighboring contents, recent studies [10, 11, 12] have further improved segmentation accuracy on the PASCAL VOC dataset [4].", "mention_start": 183, "mention_end": 206, "dataset_mention": "the PASCAL VOC dataset"}, {"mentioned_in_paper": "535", "context_id": "14", "dataset_context": "The ground truth labels provided by the PASCAL VOC dataset have already offered the object contour information.", "mention_start": 36, "mention_end": 58, "dataset_mention": "the PASCAL VOC dataset"}, {"mentioned_in_paper": "535", "context_id": "26", "dataset_context": "We evaluate the performance of the proposed OBG-FCN method on the PASCAL VOC 2011 and 2012 semantic segmentation datasets.", "mention_start": 62, "mention_end": 121, "dataset_mention": "the PASCAL VOC 2011 and 2012 semantic segmentation datasets"}, {"mentioned_in_paper": "535", "context_id": "55", "dataset_context": "The FCN in [10] is trained using the PASCAL VOC dataset for the recognition of 20 object classes, which offers good performance since it recognizes patterns of desired classes by examining both coarse-level and fine-level visual features.", "mention_start": 33, "mention_end": 55, "dataset_mention": "the PASCAL VOC dataset"}, {"mentioned_in_paper": "535", "context_id": "60", "dataset_context": "We first process the existing labels in the PASCAL-VOC dataset and convert them into a set of new labels.", "mention_start": 40, "mention_end": 62, "dataset_mention": "the PASCAL-VOC dataset"}, {"mentioned_in_paper": "535", "context_id": "62", "dataset_context": "The PASCAL VOC dataset provides labels for object classes and instances as the ground truth.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The PASCAL VOC dataset"}, {"mentioned_in_paper": "535", "context_id": "76", "dataset_context": "To initialize the OBP-FCN, we begin with the VGG network [2] pre-trained on the ImageNet dataset [6].", "mention_start": 75, "mention_end": 96, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "535", "context_id": "132", "dataset_context": "In this section, we evaluate the performance of the proposed OBG-FCN on the PASCAL VOC dataset.", "mention_start": 71, "mention_end": 94, "dataset_mention": "the PASCAL VOC dataset"}, {"mentioned_in_paper": "535", "context_id": "133", "dataset_context": "We first evaluate the contributions of edge labeling on object inference with the PASCAL VOC 11 dataset.", "mention_start": 78, "mention_end": 103, "dataset_mention": "the PASCAL VOC 11 dataset"}, {"mentioned_in_paper": "536", "context_id": "55", "dataset_context": "Indeed, the currently available UCCA datasets span across English, French, German and Hebrew.", "mention_start": 7, "mention_end": 45, "dataset_mention": " the currently available UCCA datasets"}, {"mentioned_in_paper": "536", "context_id": "57", "dataset_context": "However, while small proof of concept datasets exist for some of these, there is no parallel corpus between any of the UCCA extension layers and other semantic frameworks, such as AMR.", "mention_start": 29, "mention_end": 46, "dataset_mention": "concept datasets"}, {"mentioned_in_paper": "536", "context_id": "68", "dataset_context": "These are (1) punctuation is not annotated in the guidelines, but is in the MRP dataset and (2) the root node from the UCCA guidelines would not be the same as the one in the MRP dataset.", "mention_start": 71, "mention_end": 87, "dataset_mention": "the MRP dataset"}, {"mentioned_in_paper": "536", "context_id": "68", "dataset_context": "These are (1) punctuation is not annotated in the guidelines, but is in the MRP dataset and (2) the root node from the UCCA guidelines would not be the same as the one in the MRP dataset.", "mention_start": 170, "mention_end": 186, "dataset_mention": "the MRP dataset"}, {"mentioned_in_paper": "536", "context_id": "195", "dataset_context": "In section 4, we saw that there were a number of adjustments we had to make to the gold dataset in order to get a better idea of how our system performs on the task we set to tackle.", "mention_start": 78, "mention_end": 95, "dataset_mention": "the gold dataset"}, {"mentioned_in_paper": "538", "context_id": "61", "dataset_context": "The combined values of both hyperparameters resulted in a total of twenty models that were trained using the soccer ball data set, described on section 4.4.", "mention_start": 105, "mention_end": 129, "dataset_mention": "the soccer ball data set"}, {"mentioned_in_paper": "538", "context_id": "65", "dataset_context": "All models used pretrained weights learned in the COCO dataset [13]. 1", "mention_start": 46, "mention_end": 62, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "539", "context_id": "223", "dataset_context": "In Digits dataset, the model architecture is conv-pool-convpool-fc-fc-softmax.", "mention_start": 3, "mention_end": 17, "dataset_mention": "Digits dataset"}, {"mentioned_in_paper": "539", "context_id": "239", "dataset_context": "In Digits dataset, the encoder and decoder are built with FC layers.", "mention_start": 3, "mention_end": 17, "dataset_mention": "Digits dataset"}, {"mentioned_in_paper": "540", "context_id": "29", "dataset_context": "(a) Weakly visible classes: In this study we use an in-house dataset which is is also publicly available in [8] and published in [9].", "mention_start": 48, "mention_end": 68, "dataset_mention": "an in-house dataset"}, {"mentioned_in_paper": "540", "context_id": "181", "dataset_context": "In order to reduce such errors and increase segmentation performance more, we apply augmentation on all weakly visible dataset and their corresponding GT images.", "mention_start": 99, "mention_end": 126, "dataset_mention": "all weakly visible dataset"}, {"mentioned_in_paper": "540", "context_id": "196", "dataset_context": "In the future work, we will extend our scope to testing our novel method on microorganim dataset with more than two microorganisms, clusters, and biofilms.", "mention_start": 75, "mention_end": 96, "dataset_mention": "microorganim dataset"}, {"mentioned_in_paper": "542", "context_id": "0", "dataset_context": "Interactive analytics increasingly involves querying for quantiles over sub-populations of high cardinality datasets.", "mention_start": 91, "mention_end": 116, "dataset_mention": "high cardinality datasets"}, {"mentioned_in_paper": "542", "context_id": "23", "dataset_context": "Many quantile summaries support the merge operation [3, 28, 34], but their runtime overheads can lead to severe performance penalties on high-cardinality datasets.", "mention_start": 136, "mention_end": 162, "dataset_mention": "high-cardinality datasets"}, {"mentioned_in_paper": "542", "context_id": "33", "dataset_context": "The statistics in a moments sketch provide only loose constraints on the distribution of values in the original dataset: many distributions might match the moments of a moments sketch but fail to capture the dataset structure.", "mention_start": 195, "mention_end": 215, "dataset_mention": "capture the dataset"}, {"mentioned_in_paper": "542", "context_id": "35", "dataset_context": "On continuous real-valued datasets, we find that this approach yields more accurate estimates than alternative methods, achieving \u03f5 \u2264 1% error with 200 bytes of memory.", "mention_start": 3, "mention_end": 34, "dataset_mention": "continuous real-valued datasets"}, {"mentioned_in_paper": "542", "context_id": "104", "dataset_context": "An \u03f5approximate quantile summary provides \u03f5 approximate \u03d5-quantiles, where \u03f5 can be a function of space usage and the dataset [18, 23, 34, 71].", "mention_start": 97, "mention_end": 125, "dataset_mention": "space usage and the dataset"}, {"mentioned_in_paper": "542", "context_id": "112", "dataset_context": "In the MapReduce paradigm, a \"map\" function can construct summaries over shards while a \"reduce\" function merges them to summarize a complete dataset [3].", "mention_start": 120, "mention_end": 149, "dataset_mention": "summarize a complete dataset"}, {"mentioned_in_paper": "542", "context_id": "114", "dataset_context": "As described in Section 1, we focus on improving the performance of quantile queries over aggregations on high cardinality datasets.", "mention_start": 105, "mention_end": 131, "dataset_mention": "high cardinality datasets"}, {"mentioned_in_paper": "542", "context_id": "147", "dataset_context": "The moments sketch records logarithmic moments (log-moments) in order to recover better quantile estimates for long-tailed datasets.", "mention_start": 111, "mention_end": 131, "dataset_mention": "long-tailed datasets"}, {"mentioned_in_paper": "542", "context_id": "158", "dataset_context": "This suits our goal of estimating quantiles of a finite dataset, rather than an underlying distribution.", "mention_start": 47, "mention_end": 63, "dataset_mention": "a finite dataset"}, {"mentioned_in_paper": "542", "context_id": "173", "dataset_context": "In practice, we find that the use of maximum entropy distributions yields quantile estimates with comparable accuracy to alternative methods on a range of real-world datasets, unless the datasets are more discrete than continuous.", "mention_start": 175, "mention_end": 195, "dataset_mention": " unless the datasets"}, {"mentioned_in_paper": "542", "context_id": "210", "dataset_context": "On certain datasets, if ill-conditioned matrices are still an issue at query time we further limit ourselves to using the first k 1 \u2264 k moments and k 2 \u2264 k log moments by selecting k 1 , k 2 such that the condition number of the Hessian is less than a threshold \u03ba max .", "mention_start": 3, "mention_end": 19, "dataset_mention": "certain datasets"}, {"mentioned_in_paper": "542", "context_id": "231", "dataset_context": "In practice, error on non-adversarial datasets is lower than these bounds suggest.", "mention_start": 21, "mention_end": 46, "dataset_mention": "non-adversarial datasets"}, {"mentioned_in_paper": "542", "context_id": "237", "dataset_context": "If f is the true dataset distribution, we estimate q\u03d5 by calculating the \u03d5-quantile of the maximum entropy distribution f .", "mention_start": 8, "mention_end": 24, "dataset_mention": "the true dataset"}, {"mentioned_in_paper": "542", "context_id": "254", "dataset_context": "Given a non-negative dataset D with moments \u00b5 i Markov's inequality tells us that for any value t, rank(t) \u2265 n 1 \u2212 \u00b5 k t k where the rank is the number of elements in D less than t.", "mention_start": 6, "mention_end": 28, "dataset_mention": "a non-negative dataset"}, {"mentioned_in_paper": "542", "context_id": "296", "dataset_context": "We make use of six real-valued datasets in our experiments, whose characteristics are summarized in Table 1.", "mention_start": 15, "mention_end": 39, "dataset_mention": "six real-valued datasets"}, {"mentioned_in_paper": "542", "context_id": "297", "dataset_context": "The milan dataset consists of Internet usage measurements from Nov. 2013 in the Telecom Italia Call Data Records [40].", "mention_start": 0, "mention_end": 17, "dataset_mention": "The milan dataset"}, {"mentioned_in_paper": "542", "context_id": "298", "dataset_context": "The hepmass dataset consists of the first feature in the UCI [49] HEPMASS dataset.", "mention_start": 0, "mention_end": 19, "dataset_mention": "The hepmass dataset"}, {"mentioned_in_paper": "542", "context_id": "298", "dataset_context": "The hepmass dataset consists of the first feature in the UCI [49] HEPMASS dataset.", "mention_start": 53, "mention_end": 81, "dataset_mention": "the UCI [49] HEPMASS dataset"}, {"mentioned_in_paper": "542", "context_id": "299", "dataset_context": "The occupancy dataset consists of CO2 measurements from the UCI Occupancy Detection dataset.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The occupancy dataset"}, {"mentioned_in_paper": "542", "context_id": "299", "dataset_context": "The occupancy dataset consists of CO2 measurements from the UCI Occupancy Detection dataset.", "mention_start": 56, "mention_end": 91, "dataset_mention": "the UCI Occupancy Detection dataset"}, {"mentioned_in_paper": "542", "context_id": "300", "dataset_context": "The retail dataset consists of integer purchase quantities from the UCI Online Retail dataset.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The retail dataset"}, {"mentioned_in_paper": "542", "context_id": "300", "dataset_context": "The retail dataset consists of integer purchase quantities from the UCI Online Retail dataset.", "mention_start": 64, "mention_end": 93, "dataset_mention": "the UCI Online Retail dataset"}, {"mentioned_in_paper": "542", "context_id": "301", "dataset_context": "The power dataset consists of Global Active Power measurements from the UCI Individual Household Electric Power Consumption dataset.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The power dataset"}, {"mentioned_in_paper": "542", "context_id": "301", "dataset_context": "The power dataset consists of Global Active Power measurements from the UCI Individual Household Electric Power Consumption dataset.", "mention_start": 68, "mention_end": 131, "dataset_mention": "the UCI Individual Household Electric Power Consumption dataset"}, {"mentioned_in_paper": "542", "context_id": "302", "dataset_context": "The exponential dataset consists of synthetic values from an exponential distribution with \u03bb = 1.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The exponential dataset"}, {"mentioned_in_paper": "542", "context_id": "306", "dataset_context": "We evaluate quantile aggregation query times by pre-aggregating our datasets into cells of 200 values and maintaining quantile summaries for each cell.", "mention_start": 48, "mention_end": 76, "dataset_mention": "pre-aggregating our datasets"}, {"mentioned_in_paper": "542", "context_id": "318", "dataset_context": "As a baseline, sorting the milan dataset takes 7.0 seconds, selecting an exact quantile online takes 880ms, and streaming the data pointwise into a RandomW sketch with \u03f5 = 1/40 takes 220ms.", "mention_start": 14, "mention_end": 40, "dataset_mention": " sorting the milan dataset"}, {"mentioned_in_paper": "542", "context_id": "331", "dataset_context": "This is the cause for the spike at k = 4 in the milan dataset and users can can mitigate this by lowering the condition number threshold \u03ba max .", "mention_start": 44, "mention_end": 61, "dataset_mention": "the milan dataset"}, {"mentioned_in_paper": "542", "context_id": "342", "dataset_context": "The moments sketch achieves \u03f5 \u2264 10 \u22124 accuracy  on the synthetic exponential dataset, and \u03f5 \u2264 10 \u22123 accuracy on the high entropy hepmass dataset.", "mention_start": 50, "mention_end": 84, "dataset_mention": "the synthetic exponential dataset"}, {"mentioned_in_paper": "542", "context_id": "342", "dataset_context": "The moments sketch achieves \u03f5 \u2264 10 \u22124 accuracy  on the synthetic exponential dataset, and \u03f5 \u2264 10 \u22123 accuracy on the high entropy hepmass dataset.", "mention_start": 111, "mention_end": 144, "dataset_mention": "the high entropy hepmass dataset"}, {"mentioned_in_paper": "542", "context_id": "344", "dataset_context": "On the integer retail dataset we round estimates to the nearest integer.", "mention_start": 3, "mention_end": 29, "dataset_mention": "the integer retail dataset"}, {"mentioned_in_paper": "542", "context_id": "345", "dataset_context": "The EW-Hist summary, while efficient to merge, provides less accurate estimates than the moments sketch, especially in the long-tailed milan and retail datasets.", "mention_start": 118, "mention_end": 160, "dataset_mention": "the long-tailed milan and retail datasets"}, {"mentioned_in_paper": "542", "context_id": "350", "dataset_context": "Given the same total space budget, log moments improve accuracy on the long-tailed milan and retail datasets, and do not affect accuracy significantly on other datasets such as occupancy a dataset consisting of uniformly spaced points in the range [\u22121, 1], eventually failing to converge on datasets with fewer than five distinct values.", "mention_start": 66, "mention_end": 108, "dataset_mention": "the long-tailed milan and retail datasets"}, {"mentioned_in_paper": "542", "context_id": "350", "dataset_context": "Given the same total space budget, log moments improve accuracy on the long-tailed milan and retail datasets, and do not affect accuracy significantly on other datasets such as occupancy a dataset consisting of uniformly spaced points in the range [\u22121, 1], eventually failing to converge on datasets with fewer than five distinct values.", "mention_start": 176, "mention_end": 196, "dataset_mention": "occupancy a dataset"}, {"mentioned_in_paper": "542", "context_id": "351", "dataset_context": "If users are expecting to run queries on primarily low-cardinality datasets, fixed-universe sketches or heavy-hitters sketches may be more appropriate.", "mention_start": 41, "mention_end": 75, "dataset_mention": "primarily low-cardinality datasets"}, {"mentioned_in_paper": "542", "context_id": "352", "dataset_context": "To evaluate each component of our quantile estimator design, we compare the accuracy and estimation time of a variety of alternative techniques on the milan and hepmass datasets.", "mention_start": 146, "mention_end": 177, "dataset_mention": "the milan and hepmass datasets"}, {"mentioned_in_paper": "542", "context_id": "356", "dataset_context": "Figure 9 illustrates how on some long-tailed datasets, notably milan and retail, log moments reduce the error from \u03f5 > .15 to \u03f5 < .015.", "mention_start": 28, "mention_end": 53, "dataset_mention": "some long-tailed datasets"}, {"mentioned_in_paper": "542", "context_id": "369", "dataset_context": "For uniform comparisons with other estimators, on the milan dataset we only use the log moments, and on the hepmass dataset we only use the standard moments.", "mention_start": 49, "mention_end": 67, "dataset_mention": "the milan dataset"}, {"mentioned_in_paper": "542", "context_id": "369", "dataset_context": "For uniform comparisons with other estimators, on the milan dataset we only use the log moments, and on the hepmass dataset we only use the standard moments.", "mention_start": 103, "mention_end": 123, "dataset_mention": "the hepmass dataset"}, {"mentioned_in_paper": "542", "context_id": "381", "dataset_context": "Then, we ingest 26 million entries from the milan dataset at a one hour granularity and construct a cube over the grid ID and country dimensions, resulting in 10 million cells.", "mention_start": 39, "mention_end": 57, "dataset_mention": "the milan dataset"}, {"mentioned_in_paper": "542", "context_id": "414", "dataset_context": "For this benchmark, we aggregated the 80 million rows of the milan dataset at a 10-minute granularity, which produced 4320 panes that spanned the month of November.", "mention_start": 56, "mention_end": 74, "dataset_mention": "the milan dataset"}, {"mentioned_in_paper": "542", "context_id": "480", "dataset_context": "Precision loss is more severe on the occupancy dataset which is centered away from zero (c \u2248 1.5) compared with the hepmass dataset (c \u2248 0.4).", "mention_start": 33, "mention_end": 54, "dataset_mention": "the occupancy dataset"}, {"mentioned_in_paper": "542", "context_id": "480", "dataset_context": "Precision loss is more severe on the occupancy dataset which is centered away from zero (c \u2248 1.5) compared with the hepmass dataset (c \u2248 0.4).", "mention_start": 112, "mention_end": 131, "dataset_mention": "the hepmass dataset"}, {"mentioned_in_paper": "542", "context_id": "488", "dataset_context": "On the milan dataset, a moments sketch with k = 10 can be stored with 20 bits per value without noticeably affecting our quantile estimates, representing a 3\u00d7 space reduction compared to standard double precision floating point.", "mention_start": 3, "mention_end": 20, "dataset_mention": "the milan dataset"}, {"mentioned_in_paper": "542", "context_id": "490", "dataset_context": "However, the bounds are conservative since they only consider the midpoint c of a dataset and are otherwise both dataset-independent and agnostic to the maximum entropy principle.", "mention_start": 97, "mention_end": 120, "dataset_mention": "otherwise both dataset"}, {"mentioned_in_paper": "542", "context_id": "507", "dataset_context": "In our main evaluations, we group our datasets into cells of 200 elements and construct sketches for each cell to maintain a preaggregated collection of data summaries.", "mention_start": 24, "mention_end": 46, "dataset_mention": " we group our datasets"}, {"mentioned_in_paper": "542", "context_id": "511", "dataset_context": "In Figure 20 we measure the time taken per merge for different summaries constructed on cells of 2000 elements for the milan, hepmass, and exponential dataset, and cells of 10000 elements on a synthetic Gaussian dataset with 1 billion points.", "mention_start": 134, "mention_end": 158, "dataset_mention": " and exponential dataset"}, {"mentioned_in_paper": "542", "context_id": "511", "dataset_context": "In Figure 20 we measure the time taken per merge for different summaries constructed on cells of 2000 elements for the milan, hepmass, and exponential dataset, and cells of 10000 elements on a synthetic Gaussian dataset with 1 billion points.", "mention_start": 190, "mention_end": 219, "dataset_mention": "a synthetic Gaussian dataset"}, {"mentioned_in_paper": "542", "context_id": "521", "dataset_context": "error when we round estimates to the nearest integer on this integral dataset.", "mention_start": 56, "mention_end": 77, "dataset_mention": "this integral dataset"}, {"mentioned_in_paper": "542", "context_id": "533", "dataset_context": "In our experiments we needed to duplicate the hepmass dataset to yield 400 thousand summaries, and initialized summaries using the parameters in Table 2.", "mention_start": 32, "mention_end": 61, "dataset_mention": "duplicate the hepmass dataset"}, {"mentioned_in_paper": "543", "context_id": "273", "dataset_context": "For this survey paper are used Euclidean problems of the Padberg-Rinaldi data set of city problems that can be obtained from the GTSP Instances Library [19].", "mention_start": 53, "mention_end": 81, "dataset_mention": "the Padberg-Rinaldi data set"}, {"mentioned_in_paper": "544", "context_id": "0", "dataset_context": "Facial expression recognition is a challenging task due to two major problems: the presence of inter-subject variations in facial expression recognition dataset and impure expressions posed by human subjects.", "mention_start": 122, "mention_end": 160, "dataset_mention": "facial expression recognition dataset"}, {"mentioned_in_paper": "544", "context_id": "6", "dataset_context": "Our initial experimental results on the state-of-the-art datasets show that facial expression recognition carried out on the generated animated images using our HA-GAN framework outperforms the baseline deep neural network and produces comparable or even better results than the state-of-the-art methods for facial expression recognition.", "mention_start": 36, "mention_end": 65, "dataset_mention": "the state-of-the-art datasets"}, {"mentioned_in_paper": "544", "context_id": "20", "dataset_context": "\u2022 To the best of our knowledge, this is the first work which is aimed to address the problem of impure and non-uniform expressions exhibited by human subjects in FER datasets.", "mention_start": 161, "mention_end": 174, "dataset_mention": "FER datasets"}, {"mentioned_in_paper": "544", "context_id": "105", "dataset_context": "The MMI dataset [29] is one of the most challenging facial expression database due to two major reasons: 1. it is a small dataset containing only 236 video image sequences corresponding to six facial expressions of 31 subjects.", "mention_start": 0, "mention_end": 15, "dataset_mention": "The MMI dataset"}, {"mentioned_in_paper": "544", "context_id": "115", "dataset_context": "The Oulu-CASIA dataset [39] is divided into three Method Setting Accuracy LBP-TOP [40] Dynamic 59.51 HOG 3D [18] Dynamic 60.89 STM-Explet [22] parts based on the images obtained in three different lighting environments with two different cameras.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The Oulu-CASIA dataset"}, {"mentioned_in_paper": "544", "context_id": "124", "dataset_context": "The first problem that we address in this work is to eliminate the inter-subject variations present in facial expression datasets.", "mention_start": 103, "mention_end": 129, "dataset_mention": "facial expression datasets"}, {"mentioned_in_paper": "546", "context_id": "5", "dataset_context": "We illustrate our methods on the facial expression recognition problem and validate results on the FER+, AffectNet, Extended Cohn-Kanade (CK+), BU-3DFE, and JAFFE datasets.", "mention_start": 152, "mention_end": 171, "dataset_mention": " and JAFFE datasets"}, {"mentioned_in_paper": "546", "context_id": "10", "dataset_context": "The figure depicts some faces in the FER+ dataset classified by our method as having a surprise expression.", "mention_start": 33, "mention_end": 49, "dataset_mention": "the FER+ dataset"}, {"mentioned_in_paper": "546", "context_id": "19", "dataset_context": "It is often the case that datasets are comprised of vast training data but with relatively little labeled training data.", "mention_start": 6, "mention_end": 34, "dataset_mention": "often the case that datasets"}, {"mentioned_in_paper": "546", "context_id": "93", "dataset_context": "The groups used for the evaluation of the measures are obtained using K-means, whereas K equals the number of classes (8 in the case of the FER+ [2], AffectNet [24], CK+ [19] datasets, and 7 for JAFFE [20] and BU-3DFE [34] datasets).", "mention_start": 165, "mention_end": 183, "dataset_mention": " CK+ [19] datasets"}, {"mentioned_in_paper": "546", "context_id": "93", "dataset_context": "The groups used for the evaluation of the measures are obtained using K-means, whereas K equals the number of classes (8 in the case of the FER+ [2], AffectNet [24], CK+ [19] datasets, and 7 for JAFFE [20] and BU-3DFE [34] datasets).", "mention_start": 194, "mention_end": 231, "dataset_mention": "JAFFE [20] and BU-3DFE [34] datasets"}, {"mentioned_in_paper": "546", "context_id": "108", "dataset_context": "Figure 4 shows the top-5 retrieved images for some of the queries on CelebA dataset [18].", "mention_start": 69, "mention_end": 83, "dataset_mention": "CelebA dataset"}, {"mentioned_in_paper": "546", "context_id": "117", "dataset_context": "The Facial Expression dataset constitute a great challenge due to the subjectivity of the emotions [22].", "mention_start": 0, "mention_end": 29, "dataset_mention": "The Facial Expression dataset"}, {"mentioned_in_paper": "546", "context_id": "119", "dataset_context": "FER+ and AffectNet datasets contains many problems in the labels.", "mention_start": 0, "mention_end": 27, "dataset_mention": "FER+ and AffectNet datasets"}, {"mentioned_in_paper": "546", "context_id": "120", "dataset_context": "In [2] an effort was made to improve the quality of the labels of the FER+ (dataset used in our experiments) by re-tagging the dataset using crowd sourcing.", "mention_start": 66, "mention_end": 83, "dataset_mention": "the FER+ (dataset"}, {"mentioned_in_paper": "546", "context_id": "120", "dataset_context": "In [2] an effort was made to improve the quality of the labels of the FER+ (dataset used in our experiments) by re-tagging the dataset using crowd sourcing.", "mention_start": 112, "mention_end": 134, "dataset_mention": "re-tagging the dataset"}, {"mentioned_in_paper": "546", "context_id": "129", "dataset_context": "The ResNet18 architecture is selected to train the FER+ dataset.", "mention_start": 47, "mention_end": 63, "dataset_mention": "the FER+ dataset"}, {"mentioned_in_paper": "546", "context_id": "131", "dataset_context": "The results shown in Figure 7 present 64-dimensional embedded space using the Barnes-Hut t-SNE visualization scheme [29] using the Deep Gaussian Mixture Sub-space model for the FER+ dataset.", "mention_start": 173, "mention_end": 189, "dataset_mention": "the FER+ dataset"}, {"mentioned_in_paper": "546", "context_id": "138", "dataset_context": "Figure 9 shows 5 of the 40 groups obtained on AffectNet dataset.", "mention_start": 46, "mention_end": 63, "dataset_mention": "AffectNet dataset"}, {"mentioned_in_paper": "547", "context_id": "50", "dataset_context": "For example, in the some-thing2something v1 dataset, the class \"Pulling something from left to right\" and the class \"Pulling something from right to left\" are completely inseparable in basic convolution neural network.", "mention_start": 15, "mention_end": 51, "dataset_mention": "the some-thing2something v1 dataset"}, {"mentioned_in_paper": "547", "context_id": "64", "dataset_context": "In temporal datasets, videos often focus on objects or humans performing the interactions.", "mention_start": 3, "mention_end": 20, "dataset_mention": "temporal datasets"}, {"mentioned_in_paper": "547", "context_id": "166", "dataset_context": "Datasets we conduct experiments on six video recognition datasets, including Something-Something (V1 & V2) (Goyal et al. 2017), Kinetics-600 (Carreira and Zisserman 2017) (Carreira et al. 2018), UCF101 (Soomro, Zamir, and Shah 2012), HMDB51 (Carreira and Zisserman 2017), Multi-Moments in Time (Monfort et al. 2019) and Jester datasets.", "mention_start": 288, "mention_end": 335, "dataset_mention": "Time (Monfort et al. 2019) and Jester datasets"}, {"mentioned_in_paper": "547", "context_id": "167", "dataset_context": "Among them, K600 is a large action dataset that has 30k validation videos in 600 classes and 392k training videos.", "mention_start": 19, "mention_end": 42, "dataset_mention": "a large action dataset"}, {"mentioned_in_paper": "547", "context_id": "168", "dataset_context": "Multi-Moments in Time is a large-scale and multi-label video dataset which includes over two million action labels for over one million three second videos.", "mention_start": 25, "mention_end": 68, "dataset_mention": "a large-scale and multi-label video dataset"}, {"mentioned_in_paper": "547", "context_id": "169", "dataset_context": "The Something and Jester datasets focus more on temporal modeling and the relationship inside video sequences.", "mention_start": 0, "mention_end": 33, "dataset_mention": "The Something and Jester datasets"}, {"mentioned_in_paper": "547", "context_id": "183", "dataset_context": "The Something-Something (V1) dataset shows the abstract pre-defined action which human perform with common objects.", "mention_start": 0, "mention_end": 36, "dataset_mention": "The Something-Something (V1) dataset"}, {"mentioned_in_paper": "547", "context_id": "192", "dataset_context": "2, we report the results on three temporal-sensitive datasets: Something-Something V1, Something-Something V2 and Jester.", "mention_start": 27, "mention_end": 61, "dataset_mention": "three temporal-sensitive datasets"}, {"mentioned_in_paper": "547", "context_id": "195", "dataset_context": "We also show the results on four temporal-insensitive datasets: Kinetics-600, HMDB51, UCF101, Multi-Moments in Time.", "mention_start": 28, "mention_end": 62, "dataset_mention": "four temporal-insensitive datasets"}, {"mentioned_in_paper": "547", "context_id": "220", "dataset_context": "However, Something dataset mainly focuses on temporal modeling and is not sensitive to spatial information.", "mention_start": 8, "mention_end": 26, "dataset_mention": " Something dataset"}, {"mentioned_in_paper": "548", "context_id": "55", "dataset_context": "The untrained network is initially sampled to obtain a starting dataset D 0 , which is expected to exhibit large autocorrelation times.", "mention_start": 46, "mention_end": 71, "dataset_mention": "obtain a starting dataset"}, {"mentioned_in_paper": "548", "context_id": "60", "dataset_context": "Iterating this procedure, we obtain samples of increasing quality in each subsequent dataset D i .", "mention_start": 68, "mention_end": 92, "dataset_mention": "each subsequent dataset"}, {"mentioned_in_paper": "548", "context_id": "61", "dataset_context": "In principle, we reach the fixed point of this training procedure when the samples generated by f \u03b8 become as decorrelated as the shuffled samples from the bootstrapped dataset D i .", "mention_start": 151, "mention_end": 176, "dataset_mention": "the bootstrapped dataset"}, {"mentioned_in_paper": "550", "context_id": "112", "dataset_context": "The \"lifecycle stages of environmental datasets\" is an example of this kind of purpose. 12", "mention_start": 25, "mention_end": 47, "dataset_mention": "environmental datasets"}, {"mentioned_in_paper": "551", "context_id": "133", "dataset_context": "After validating ConvFocus, we used the algorithm to study the impact of OOF (both real and synthetic) on the performance of a cancer detection algorithm on the publicly available Camelyon 2016 challenge test dataset [15], consisting of 80 non-tumor and 48 tumor slides with pixel-level annotations of tumor locations.", "mention_start": 156, "mention_end": 216, "dataset_mention": "the publicly available Camelyon 2016 challenge test dataset"}, {"mentioned_in_paper": "552", "context_id": "22", "dataset_context": "A deep learning model, Convolutional Neural Network (ConvNet), is used to train the classifier with our custom dataset.", "mention_start": 99, "mention_end": 118, "dataset_mention": "our custom dataset"}, {"mentioned_in_paper": "552", "context_id": "34", "dataset_context": "Second, we provide our custom dataset.", "mention_start": 18, "mention_end": 37, "dataset_mention": "our custom dataset"}, {"mentioned_in_paper": "552", "context_id": "83", "dataset_context": "The models parameters are learned through fine-tuning from the pre-trained CaffeNet model [18] with our custom dataset.", "mention_start": 100, "mention_end": 118, "dataset_mention": "our custom dataset"}, {"mentioned_in_paper": "552", "context_id": "86", "dataset_context": "Then we will demonstrate our custom dataset used for training ConvNet and details about training in the following section.", "mention_start": 0, "mention_end": 43, "dataset_mention": "Then we will demonstrate our custom dataset"}, {"mentioned_in_paper": "552", "context_id": "94", "dataset_context": "There exist many publicly available indoor images datasets ( [20], [21]).", "mention_start": 12, "mention_end": 58, "dataset_mention": "many publicly available indoor images datasets"}, {"mentioned_in_paper": "552", "context_id": "169", "dataset_context": "In future work, we want to expand our dataset and experiments to more diverse indoor environments, such as stairways.", "mention_start": 26, "mention_end": 45, "dataset_mention": "expand our dataset"}, {"mentioned_in_paper": "553", "context_id": "230", "dataset_context": "VeRi-776 dataset There are 20 cameras were used to take images of this dataset, and images of each vehicle are taken by 2-18 cameras.", "mention_start": 0, "mention_end": 16, "dataset_mention": "VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "234", "dataset_context": "VehicleID dataset This dataset has no camera information, and all images are taken from the front or rear.", "mention_start": 0, "mention_end": 17, "dataset_mention": "VehicleID dataset"}, {"mentioned_in_paper": "553", "context_id": "234", "dataset_context": "VehicleID dataset This dataset has no camera information, and all images are taken from the front or rear.", "mention_start": 0, "mention_end": 30, "dataset_mention": "VehicleID dataset This dataset"}, {"mentioned_in_paper": "553", "context_id": "238", "dataset_context": "VERI Wild dataset There are 174 cameras were used to take images of this dataset.", "mention_start": 0, "mention_end": 17, "dataset_mention": "VERI Wild dataset"}, {"mentioned_in_paper": "553", "context_id": "249", "dataset_context": "For the VeRi-776 dataset, the batch is composed of 8 identities, each identity contains 8 images, and the initial learning rate is set to 3.5\u00d7 10-4.", "mention_start": 4, "mention_end": 24, "dataset_mention": "the VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "250", "dataset_context": "For the VehicleID dataset, the batch is composed of 16 identities, each identity contains 4 images, and the initial learning rate is set to 3.5\u00d7 10-5.", "mention_start": 4, "mention_end": 25, "dataset_mention": "the VehicleID dataset"}, {"mentioned_in_paper": "553", "context_id": "251", "dataset_context": "For VERI Wild dataset, the batch is composed of 32 identities, each identity contains 2 images, and the initial learning rate is set to 10-4.", "mention_start": 4, "mention_end": 21, "dataset_mention": "VERI Wild dataset"}, {"mentioned_in_paper": "553", "context_id": "255", "dataset_context": "The VeRi-776 dataset contains 576 training IDs, the VehicleID dataset contains 13164 training IDs, and the VERI Wild dataset contains 30671 training IDs.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "255", "dataset_context": "The VeRi-776 dataset contains 576 training IDs, the VehicleID dataset contains 13164 training IDs, and the VERI Wild dataset contains 30671 training IDs.", "mention_start": 47, "mention_end": 69, "dataset_mention": " the VehicleID dataset"}, {"mentioned_in_paper": "553", "context_id": "255", "dataset_context": "The VeRi-776 dataset contains 576 training IDs, the VehicleID dataset contains 13164 training IDs, and the VERI Wild dataset contains 30671 training IDs.", "mention_start": 98, "mention_end": 124, "dataset_mention": " and the VERI Wild dataset"}, {"mentioned_in_paper": "553", "context_id": "256", "dataset_context": "Further Table 1: In the VeRi-776 dataset, VehicleID dataset and VERI Wild dataset, we use L LSupCon (L L ) and L GSupCon (L G ) and the combination of these two losses, and respectively use resnet50 ibn a,resnext101 ibn a and resnet152 as the backbones.", "mention_start": 19, "mention_end": 40, "dataset_mention": "the VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "256", "dataset_context": "Further Table 1: In the VeRi-776 dataset, VehicleID dataset and VERI Wild dataset, we use L LSupCon (L L ) and L GSupCon (L G ) and the combination of these two losses, and respectively use resnet50 ibn a,resnext101 ibn a and resnet152 as the backbones.", "mention_start": 41, "mention_end": 59, "dataset_mention": " VehicleID dataset"}, {"mentioned_in_paper": "553", "context_id": "256", "dataset_context": "Further Table 1: In the VeRi-776 dataset, VehicleID dataset and VERI Wild dataset, we use L LSupCon (L L ) and L GSupCon (L G ) and the combination of these two losses, and respectively use resnet50 ibn a,resnext101 ibn a and resnet152 as the backbones.", "mention_start": 41, "mention_end": 81, "dataset_mention": " VehicleID dataset and VERI Wild dataset"}, {"mentioned_in_paper": "553", "context_id": "260", "dataset_context": "VeRi-776 dataset In TABLE 1, we find that in the VeRi-776 dataset, the experimental results of L GSupCon are not significantly better than that of L LSupCon , but the results of L LSupCon + L GSupCon are significantly better than that of any one of the two losses used alone, indicating that the number of training IDs (or images) for this dataset is not sufficient for the proposed L GSupCon to perform well.", "mention_start": 0, "mention_end": 16, "dataset_mention": "VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "260", "dataset_context": "VeRi-776 dataset In TABLE 1, we find that in the VeRi-776 dataset, the experimental results of L GSupCon are not significantly better than that of L LSupCon , but the results of L LSupCon + L GSupCon are significantly better than that of any one of the two losses used alone, indicating that the number of training IDs (or images) for this dataset is not sufficient for the proposed L GSupCon to perform well.", "mention_start": 44, "mention_end": 65, "dataset_mention": "the VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "261", "dataset_context": "VehicleID dataset The number of training IDs in the VehicleID dataset is significantly larger than that in the VeRi-776 dataset.", "mention_start": 0, "mention_end": 17, "dataset_mention": "VehicleID dataset"}, {"mentioned_in_paper": "553", "context_id": "261", "dataset_context": "VehicleID dataset The number of training IDs in the VehicleID dataset is significantly larger than that in the VeRi-776 dataset.", "mention_start": 48, "mention_end": 69, "dataset_mention": "the VehicleID dataset"}, {"mentioned_in_paper": "553", "context_id": "261", "dataset_context": "VehicleID dataset The number of training IDs in the VehicleID dataset is significantly larger than that in the VeRi-776 dataset.", "mention_start": 107, "mention_end": 127, "dataset_mention": "the VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "267", "dataset_context": "VERI Wild dataset For this dataset, as the number of training IDs in this dataset reaches 30671, which is the largest in the three dataset, it can be seen that with the increase of the number of positive images and negative IDs, the mAPs of using L GSupCon in the three backbones than those of using L LSupCon (i.e.", "mention_start": 0, "mention_end": 17, "dataset_mention": "VERI Wild dataset"}, {"mentioned_in_paper": "553", "context_id": "272", "dataset_context": "For the VeRi-776 dataset and VehicleID dataset, we use L LSupCon + L GSupCon as metric loss, while for the VERI Wild dataset, we only use L GSupCon as the metric loss.", "mention_start": 4, "mention_end": 24, "dataset_mention": "the VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "272", "dataset_context": "For the VeRi-776 dataset and VehicleID dataset, we use L LSupCon + L GSupCon as metric loss, while for the VERI Wild dataset, we only use L GSupCon as the metric loss.", "mention_start": 4, "mention_end": 46, "dataset_mention": "the VeRi-776 dataset and VehicleID dataset"}, {"mentioned_in_paper": "553", "context_id": "272", "dataset_context": "For the VeRi-776 dataset and VehicleID dataset, we use L LSupCon + L GSupCon as metric loss, while for the VERI Wild dataset, we only use L GSupCon as the metric loss.", "mention_start": 102, "mention_end": 124, "dataset_mention": "the VERI Wild dataset"}, {"mentioned_in_paper": "553", "context_id": "283", "dataset_context": "We list all the \u2206 V \u00d7V s matrices calculated by our trained re-id model on the VehicleID dataset, and VERI Wild dataset in TABLE 5, TABLE 6 and TABLE 7 in Appendix B, respectively.", "mention_start": 75, "mention_end": 96, "dataset_mention": "the VehicleID dataset"}, {"mentioned_in_paper": "553", "context_id": "283", "dataset_context": "We list all the \u2206 V \u00d7V s matrices calculated by our trained re-id model on the VehicleID dataset, and VERI Wild dataset in TABLE 5, TABLE 6 and TABLE 7 in Appendix B, respectively.", "mention_start": 97, "mention_end": 119, "dataset_mention": " and VERI Wild dataset"}, {"mentioned_in_paper": "553", "context_id": "288", "dataset_context": "In the VeRi-776 dataset, the mAP of with VABPP is 1.48% higher than that of without VABPP.", "mention_start": 3, "mention_end": 23, "dataset_mention": "the VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "289", "dataset_context": "In the three sub test sets of the VehicleID dataset, CMC@1s of with VABPP are 3.23%, 5.03% and 4.15% higher than those of VABPP, respectively.", "mention_start": 30, "mention_end": 51, "dataset_mention": "the VehicleID dataset"}, {"mentioned_in_paper": "553", "context_id": "290", "dataset_context": "In the three sub test sets of the VERI Wild dataset, mAPs of with VABPP are 3.48%, 3.83% and 4.10% higher than that of without VABPP, respectively.", "mention_start": 30, "mention_end": 51, "dataset_mention": "the VERI Wild dataset"}, {"mentioned_in_paper": "553", "context_id": "296", "dataset_context": "By analyzing the results in TABLE 2 and TABLE 3, we can also see that the performances of VABPP in VehicleID dataset and VERI Wild dataset are significantly better than that in VeRi-776 dataset.", "mention_start": 98, "mention_end": 116, "dataset_mention": "VehicleID dataset"}, {"mentioned_in_paper": "553", "context_id": "296", "dataset_context": "By analyzing the results in TABLE 2 and TABLE 3, we can also see that the performances of VABPP in VehicleID dataset and VERI Wild dataset are significantly better than that in VeRi-776 dataset.", "mention_start": 98, "mention_end": 138, "dataset_mention": "VehicleID dataset and VERI Wild dataset"}, {"mentioned_in_paper": "553", "context_id": "296", "dataset_context": "By analyzing the results in TABLE 2 and TABLE 3, we can also see that the performances of VABPP in VehicleID dataset and VERI Wild dataset are significantly better than that in VeRi-776 dataset.", "mention_start": 176, "mention_end": 193, "dataset_mention": "VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "298", "dataset_context": "1)The total number of training images in the VeRi-776 dataset is 37778, and there are 64 values need to be counted (TABLE 5).", "mention_start": 41, "mention_end": 61, "dataset_mention": "the VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "299", "dataset_context": "While the total number of training images in the VehicleID dataset and VERI Wild dataset are 113346 and 277797 respectively, there are only 4 (TABLE 6) and 36 (TABLE 7) values need to be counted respectively, which makes the statistical values in the later two datasets are more general.", "mention_start": 45, "mention_end": 66, "dataset_mention": "the VehicleID dataset"}, {"mentioned_in_paper": "553", "context_id": "299", "dataset_context": "While the total number of training images in the VehicleID dataset and VERI Wild dataset are 113346 and 277797 respectively, there are only 4 (TABLE 6) and 36 (TABLE 7) values need to be counted respectively, which makes the statistical values in the later two datasets are more general.", "mention_start": 45, "mention_end": 88, "dataset_mention": "the VehicleID dataset and VERI Wild dataset"}, {"mentioned_in_paper": "553", "context_id": "299", "dataset_context": "While the total number of training images in the VehicleID dataset and VERI Wild dataset are 113346 and 277797 respectively, there are only 4 (TABLE 6) and 36 (TABLE 7) values need to be counted respectively, which makes the statistical values in the later two datasets are more general.", "mention_start": 246, "mention_end": 269, "dataset_mention": "the later two datasets"}, {"mentioned_in_paper": "553", "context_id": "300", "dataset_context": "2)Although we divid the VeRi-776 dataset into 8 views, in fact, this dataset is far more than 8 views, because this dataset can be divided into 8 views in the horizontal position and can also be continuously divided in the vertical position.", "mention_start": 0, "mention_end": 40, "dataset_mention": "2)Although we divid the VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "303", "dataset_context": "If we continue to make detailed division, due to the small size of VeRi-776 dataset, the \u2206 V \u00d7V will be even less statistically significant.", "mention_start": 66, "mention_end": 83, "dataset_mention": "VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "304", "dataset_context": "While the VehicleID dataset and VERI Wild dataset have only two and six orientations respectively, and all the cameras used to capture these two datasets are almost at the same horizontal position.", "mention_start": 6, "mention_end": 27, "dataset_mention": "the VehicleID dataset"}, {"mentioned_in_paper": "553", "context_id": "304", "dataset_context": "While the VehicleID dataset and VERI Wild dataset have only two and six orientations respectively, and all the cameras used to capture these two datasets are almost at the same horizontal position.", "mention_start": 6, "mention_end": 49, "dataset_mention": "the VehicleID dataset and VERI Wild dataset"}, {"mentioned_in_paper": "553", "context_id": "304", "dataset_context": "While the VehicleID dataset and VERI Wild dataset have only two and six orientations respectively, and all the cameras used to capture these two datasets are almost at the same horizontal position.", "mention_start": 126, "mention_end": 153, "dataset_mention": "capture these two datasets"}, {"mentioned_in_paper": "553", "context_id": "305", "dataset_context": "We compare the proposed methods with some state-of-the-art methods in the VeRi-776 dataset, VehicleID dataset and VERI Wild dataset.", "mention_start": 70, "mention_end": 90, "dataset_mention": "the VeRi-776 dataset"}, {"mentioned_in_paper": "553", "context_id": "305", "dataset_context": "We compare the proposed methods with some state-of-the-art methods in the VeRi-776 dataset, VehicleID dataset and VERI Wild dataset.", "mention_start": 91, "mention_end": 109, "dataset_mention": " VehicleID dataset"}, {"mentioned_in_paper": "553", "context_id": "305", "dataset_context": "We compare the proposed methods with some state-of-the-art methods in the VeRi-776 dataset, VehicleID dataset and VERI Wild dataset.", "mention_start": 91, "mention_end": 131, "dataset_mention": " VehicleID dataset and VERI Wild dataset"}, {"mentioned_in_paper": "553", "context_id": "325", "dataset_context": "The reason is that there are too few images with the right view in the VeRi-776 dataset, and the statistics are not general.", "mention_start": 67, "mention_end": 87, "dataset_mention": "the VeRi-776 dataset"}, {"mentioned_in_paper": "557", "context_id": "179", "dataset_context": "We evaluate the performance of our methods on the following benchmarks: (1) categorizing academic papers in the citation network datasets-Cora, Citeseer and Pubmed [11] community different posts belong to in Reddit [3].", "mention_start": 107, "mention_end": 137, "dataset_mention": "the citation network datasets"}, {"mentioned_in_paper": "557", "context_id": "189", "dataset_context": "The hidden dimensions for the citation network datasets (i.e., Cora, Citeseer and Pubmed) are set to be 16.", "mention_start": 26, "mention_end": 55, "dataset_mention": "the citation network datasets"}, {"mentioned_in_paper": "557", "context_id": "190", "dataset_context": "For the Reddit dataset, the hidden dimensions are selected to be 256 as suggested by [3].", "mention_start": 4, "mention_end": 22, "dataset_mention": "the Reddit dataset"}, {"mentioned_in_paper": "557", "context_id": "258", "dataset_context": "For the Reddit dataset, we follow the suggestion by [21] to fix the weight of the bottom layer and pre-compute the product \u00c2H (0) given the input features for efficiency.", "mention_start": 4, "mention_end": 22, "dataset_mention": "the Reddit dataset"}, {"mentioned_in_paper": "558", "context_id": "3", "dataset_context": "In this context, the target domain could represents a poorlylabeled dataset, a different sensor, or an altogether new set of classes to identify.", "mention_start": 51, "mention_end": 75, "dataset_mention": "a poorlylabeled dataset"}, {"mentioned_in_paper": "558", "context_id": "6", "dataset_context": "Our goal is to address this shortcoming by comparing transfer learning within a DL framework to other ML approaches across transfer tasks and datasets.", "mention_start": 123, "mention_end": 150, "dataset_mention": "transfer tasks and datasets"}, {"mentioned_in_paper": "558", "context_id": "42", "dataset_context": "Our contributions are as follows: We lay out some of the important current transfer problems, then establish empirical benchmark comparisons on measured, challenging, and practical datasets for some of the main transfer problems and relevant ML approaches.", "mention_start": 166, "mention_end": 189, "dataset_mention": " and practical datasets"}, {"mentioned_in_paper": "558", "context_id": "46", "dataset_context": "The xView dataset 10 is a collection of satellite imagery of about a million images over 60 classes of objects at 0.3m ground resolution.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The xView dataset"}, {"mentioned_in_paper": "558", "context_id": "51", "dataset_context": "This subset of images illustrates some of the challenges associated with the xView dataset.", "mention_start": 73, "mention_end": 90, "dataset_mention": "the xView dataset"}, {"mentioned_in_paper": "558", "context_id": "55", "dataset_context": "For the xView dataset, the two most frequent classes make up 90% of the samples.", "mention_start": 4, "mention_end": 21, "dataset_mention": "the xView dataset"}, {"mentioned_in_paper": "558", "context_id": "62", "dataset_context": "Images in the xView dataset are collected at a resolution of 0.3m on the ground which means that a small car, for example, will have a smaller bounding box than a vehicle lot or a trailer.", "mention_start": 10, "mention_end": 27, "dataset_mention": "the xView dataset"}, {"mentioned_in_paper": "558", "context_id": "68", "dataset_context": "The gene sequence dataset used here was taken from the lung and breast mRNA microarray gene expression datasets from the Biomedical Knowledge Repository (BKR) developed at the National Library of Medicine (NLM). 14", "mention_start": 0, "mention_end": 25, "dataset_mention": "The gene sequence dataset"}, {"mentioned_in_paper": "558", "context_id": "68", "dataset_context": "The gene sequence dataset used here was taken from the lung and breast mRNA microarray gene expression datasets from the Biomedical Knowledge Repository (BKR) developed at the National Library of Medicine (NLM). 14", "mention_start": 51, "mention_end": 111, "dataset_mention": "the lung and breast mRNA microarray gene expression datasets"}, {"mentioned_in_paper": "558", "context_id": "116", "dataset_context": "We setup this transfer task with the xView dataset, where we transferred from high-resolution to low-resolution versions of the same classes.", "mention_start": 33, "mention_end": 50, "dataset_mention": "the xView dataset"}, {"mentioned_in_paper": "558", "context_id": "135", "dataset_context": "We again used the xView dataset, where the source classes were bus and cargo truck, and the target classes were passenger vehicle and utility truck.", "mention_start": 14, "mention_end": 31, "dataset_mention": "the xView dataset"}, {"mentioned_in_paper": "559", "context_id": "4", "dataset_context": "To derive insight into the proposed models' performance, several experiments have been conducted based on the well-known clinical dataset Medical Information Mart for Intensive Care III (MIMIC-III, v1.4) 2 .", "mention_start": 105, "mention_end": 137, "dataset_mention": "the well-known clinical dataset"}, {"mentioned_in_paper": "559", "context_id": "12", "dataset_context": "This paper aims to develop a deep learning model that can identify patients hospitalized in the ICUs at high risk for death during the ICU stay based on the EMR dataset accumulated by the first 48 hours of the first ICU admission.", "mention_start": 153, "mention_end": 168, "dataset_mention": "the EMR dataset"}, {"mentioned_in_paper": "559", "context_id": "15", "dataset_context": "In Section 4, we provide a basic statistics of MIMIC-III (v1.4) dataset.", "mention_start": 46, "mention_end": 71, "dataset_mention": "MIMIC-III (v1.4) dataset"}, {"mentioned_in_paper": "559", "context_id": "22", "dataset_context": "Among a given set of candidate algorithms, the SL technique builds an aggregate algorithm as the candidate algorithms' With freely-available datasets such as MIMIC-III, the development of novel models for mortality prediction is gaining increased attention.", "mention_start": 123, "mention_end": 149, "dataset_mention": "freely-available datasets"}, {"mentioned_in_paper": "559", "context_id": "24", "dataset_context": "Johnson et al. 8 compared multiple published mortality prediction works against gradient boosting and logistic regression model using a simple set of features extracted from MIMIC-III dataset.", "mention_start": 174, "mention_end": 191, "dataset_mention": "MIMIC-III dataset"}, {"mentioned_in_paper": "559", "context_id": "28", "dataset_context": "Some RNN models with LSTM units are also proposed and compared with baseline models to show better ICU mortality prediction accuracy [11] [12] [13] [14]. 1 shows the statistics of MIMIC-III (v1.4) dataset.", "mention_start": 180, "mention_end": 204, "dataset_mention": "MIMIC-III (v1.4) dataset"}, {"mentioned_in_paper": "559", "context_id": "74", "dataset_context": "First, the MIMIC-III dataset is collected from a single intuition, so our findings may not be generalizable to other clinical or geographic settings.", "mention_start": 6, "mention_end": 28, "dataset_mention": " the MIMIC-III dataset"}, {"mentioned_in_paper": "559", "context_id": "76", "dataset_context": "To conclude, in this work, we propose to apply deep learning models into mortality prediction of ICU patients on the MIMIC-III (v1.4) dataset.", "mention_start": 112, "mention_end": 141, "dataset_mention": "the MIMIC-III (v1.4) dataset"}, {"mentioned_in_paper": "559", "context_id": "83", "dataset_context": "For example, 1) more sophisticated data preprocessing steps and deep learning models will be conducted to capture the characteristics of the massive MIMIC-III datasets, and 2) more extensive ICU datasets will be employed to evaluate and improve our models.", "mention_start": 136, "mention_end": 167, "dataset_mention": "the massive MIMIC-III datasets"}, {"mentioned_in_paper": "559", "context_id": "83", "dataset_context": "For example, 1) more sophisticated data preprocessing steps and deep learning models will be conducted to capture the characteristics of the massive MIMIC-III datasets, and 2) more extensive ICU datasets will be employed to evaluate and improve our models.", "mention_start": 168, "mention_end": 203, "dataset_mention": " and 2) more extensive ICU datasets"}, {"mentioned_in_paper": "561", "context_id": "94", "dataset_context": "The whole training procedure is shown in Fig. 2. First, we train LSTD in the source domain, where we apply a large-scale source data set to train LSTD in Fig. 1.", "mention_start": 97, "mention_end": 136, "dataset_mention": "we apply a large-scale source data set"}, {"mentioned_in_paper": "562", "context_id": "161", "dataset_context": "However, to our knowledge, few released video face datasets, such as the popular BVS and BBT used in previous works [27], [58], could satisfy the large scale needs of all terms mentioned above.", "mention_start": 26, "mention_end": 59, "dataset_mention": " few released video face datasets"}, {"mentioned_in_paper": "562", "context_id": "163", "dataset_context": "The first one is the YouTube Celebrities (YTC) dataset.", "mention_start": 17, "mention_end": 54, "dataset_mention": "the YouTube Celebrities (YTC) dataset"}, {"mentioned_in_paper": "562", "context_id": "168", "dataset_context": "The third one UMDFaces is a newly released large scale face dataset, which contains still part and video part.", "mention_start": 26, "mention_end": 67, "dataset_mention": "a newly released large scale face dataset"}, {"mentioned_in_paper": "562", "context_id": "187", "dataset_context": "For our method, we find that the average difference between two singular values is large enough (more than 1) when training with initialization from pretrained models which can effectively alleviate the numerical problem of P ; while the average difference might be quite small (less than 1e For fair comparison, all compared deep hashing methods use such 10-layer backbone architecture for CNN feature learning, and the network weights are pre-trained for face classification task using the widely studied CASIA WebFace dataset [73] to accelerate convergence.", "mention_start": 487, "mention_end": 528, "dataset_mention": "the widely studied CASIA WebFace dataset"}, {"mentioned_in_paper": "562", "context_id": "205", "dataset_context": "On the other hand, PB is a TV-series dataset where appearance of characters is similar across scenes and episodes, and considerable number of more easily recognized close-up shots exist, resulting in relatively higher quality images with smaller intra class and intra video clip variations.", "mention_start": 24, "mention_end": 44, "dataset_mention": "a TV-series dataset"}, {"mentioned_in_paper": "562", "context_id": "226", "dataset_context": "Since all SMH methods treat one video as a set of frames and average the distances between the query image and each frame of the gallery video, and thus take the same time cost, we then choose DNNH as one representative method and record the total retrieval time cost of all queries on YTC dataset with an Intel i7-4770 PC.", "mention_start": 285, "mention_end": 297, "dataset_mention": "YTC dataset"}, {"mentioned_in_paper": "562", "context_id": "251", "dataset_context": "PLMH tries to capture the complex dataset structure with a number of sensitive parameters to be tuned.", "mention_start": 14, "mention_end": 41, "dataset_mention": "capture the complex dataset"}, {"mentioned_in_paper": "562", "context_id": "260", "dataset_context": "Fig. 7 shows some challenging cases for DHH, HER, MM-NN and MLBE on the YTC dataset with 48-bit code length.", "mention_start": 67, "mention_end": 83, "dataset_mention": "the YTC dataset"}, {"mentioned_in_paper": "562", "context_id": "332", "dataset_context": "From these retrieval tasks, we can find that DHH still achieves promising performance especially on the more challenging YTC and UMDFaces datasets, which demonstrates the flexibility of our framework.", "mention_start": 120, "mention_end": 146, "dataset_mention": "YTC and UMDFaces datasets"}, {"mentioned_in_paper": "563", "context_id": "12", "dataset_context": "Finally, we will open source the dataset that contains 6000 fingerprint images.", "mention_start": 8, "mention_end": 40, "dataset_mention": " we will open source the dataset"}, {"mentioned_in_paper": "563", "context_id": "32", "dataset_context": "3. We will finally open source the dataset since the open-source datasets in fingerprint gender classification are limited.", "mention_start": 19, "mention_end": 42, "dataset_mention": "open source the dataset"}, {"mentioned_in_paper": "563", "context_id": "32", "dataset_context": "3. We will finally open source the dataset since the open-source datasets in fingerprint gender classification are limited.", "mention_start": 49, "mention_end": 73, "dataset_mention": "the open-source datasets"}, {"mentioned_in_paper": "563", "context_id": "78", "dataset_context": "The fingerprint dataset used in experiments is obtained from ZK fingerprint acquisition equipment with 500 dpi, containing 200 persons (102 females and 98 males) and 6000 images.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The fingerprint dataset"}, {"mentioned_in_paper": "564", "context_id": "65", "dataset_context": "We observe that across the full population of 97 co-educational college online social networks from the Facebook100 dataset (FB100), the distribution of gender preferences are overdispersed, with a variance larger than the above model predicts (for details on the FB100 dataset, see Methods).", "mention_start": 100, "mention_end": 123, "dataset_mention": "the Facebook100 dataset"}, {"mentioned_in_paper": "564", "context_id": "65", "dataset_context": "We observe that across the full population of 97 co-educational college online social networks from the Facebook100 dataset (FB100), the distribution of gender preferences are overdispersed, with a variance larger than the above model predicts (for details on the FB100 dataset, see Methods).", "mention_start": 259, "mention_end": 277, "dataset_mention": "the FB100 dataset"}, {"mentioned_in_paper": "564", "context_id": "70", "dataset_context": "The friendship networks in the Add Health dataset show equivalent evidence of overdispersion in a directed setting (for details on the Add Health dataset, see Methods).", "mention_start": 27, "mention_end": 49, "dataset_mention": "the Add Health dataset"}, {"mentioned_in_paper": "564", "context_id": "70", "dataset_context": "The friendship networks in the Add Health dataset show equivalent evidence of overdispersion in a directed setting (for details on the Add Health dataset, see Methods).", "mention_start": 131, "mention_end": 153, "dataset_mention": "the Add Health dataset"}, {"mentioned_in_paper": "564", "context_id": "78", "dataset_context": "In Figure 2, we evaluate both bias (homophily) and overdispersion (monophily) in gender preferences, using the conventional homophily index \u0125r to measure bias and the estimates \u03c6r to measure overdispersion across the populations of college networks in the FB100 dataset.", "mention_start": 251, "mention_end": 269, "dataset_mention": "the FB100 dataset"}, {"mentioned_in_paper": "564", "context_id": "108", "dataset_context": "We observe only slight gender homophily across the population of college networks in the FB100 dataset, and accordingly in Figure 3A we observe limited performance using 1-hop methods (1-hop MV and ZGL) to predict gender in a single representative network.", "mention_start": 85, "mention_end": 102, "dataset_mention": "the FB100 dataset"}, {"mentioned_in_paper": "564", "context_id": "152", "dataset_context": "We analyze populations of networks from two sources, the Facebook100 (FB100) network dataset [52] (Supplementary Note 5) and the Add Health in-school friendship nomination dataset [41] (Supplementary Note 9).", "mention_start": 52, "mention_end": 92, "dataset_mention": " the Facebook100 (FB100) network dataset"}, {"mentioned_in_paper": "564", "context_id": "152", "dataset_context": "We analyze populations of networks from two sources, the Facebook100 (FB100) network dataset [52] (Supplementary Note 5) and the Add Health in-school friendship nomination dataset [41] (Supplementary Note 9).", "mention_start": 117, "mention_end": 179, "dataset_mention": "5) and the Add Health in-school friendship nomination dataset"}, {"mentioned_in_paper": "564", "context_id": "155", "dataset_context": "The Facebook100 dataset (FB100), analyzed in the main paper, consists of online friendship networks from Facebook that was collected in September 2005 from 100 U.S. colleges, primarily consisting of college-aged individuals [51].", "mention_start": 0, "mention_end": 23, "dataset_mention": "The Facebook100 dataset"}, {"mentioned_in_paper": "564", "context_id": "160", "dataset_context": "We train our models on the x% labeled individuals (training dataset), and measure classification performance on the remaining unlabeled nodes (testing dataset), using the same train/test splits across the different inference methods.", "mention_start": 23, "mention_end": 67, "dataset_mention": "the x% labeled individuals (training dataset"}, {"mentioned_in_paper": "564", "context_id": "160", "dataset_context": "We train our models on the x% labeled individuals (training dataset), and measure classification performance on the remaining unlabeled nodes (testing dataset), using the same train/test splits across the different inference methods.", "mention_start": 111, "mention_end": 158, "dataset_mention": "the remaining unlabeled nodes (testing dataset"}, {"mentioned_in_paper": "566", "context_id": "41", "dataset_context": "The smile data were collected from a recent database called the UvA-NEMO dataset (Dibeklioglu and Salah 2015).", "mention_start": 35, "mention_end": 80, "dataset_mention": "a recent database called the UvA-NEMO dataset"}, {"mentioned_in_paper": "567", "context_id": "103", "dataset_context": "The classifier was validated on the DISFA dataset [40] yielding an F-1 score of 70.2%.", "mention_start": 32, "mention_end": 49, "dataset_mention": "the DISFA dataset"}, {"mentioned_in_paper": "568", "context_id": "7", "dataset_context": "Our synthesised data is used to augment training of facial recognition networks with performance evaluated on the challenging CFP and CPLFW datasets.", "mention_start": 126, "mention_end": 148, "dataset_mention": "CFP and CPLFW datasets"}, {"mentioned_in_paper": "568", "context_id": "32", "dataset_context": "The difficulty of obtaining high-quality texture datasets motivates the development of methods such as our own, which aims to learn a nonlinear texture model from natural (non-scanned) images.", "mention_start": 18, "mention_end": 57, "dataset_mention": "obtaining high-quality texture datasets"}, {"mentioned_in_paper": "568", "context_id": "116", "dataset_context": "During development of the 3D GAN, tests were conducted by training on the controlled, Multi-PIE dataset [15].", "mention_start": 85, "mention_end": 103, "dataset_mention": " Multi-PIE dataset"}, {"mentioned_in_paper": "568", "context_id": "139", "dataset_context": "The relative pose distributions estimated for the non-synthetic datasets described in Section 4.2.1 and Table 1.", "mention_start": 46, "mention_end": 72, "dataset_mention": "the non-synthetic datasets"}, {"mentioned_in_paper": "568", "context_id": "144", "dataset_context": "It is unlikely, however, that the synthetic 3D GAN data would be more informative than the original, high-quality dataset.", "mention_start": 100, "mention_end": 121, "dataset_mention": " high-quality dataset"}, {"mentioned_in_paper": "568", "context_id": "146", "dataset_context": "In this section we demonstrate improvement to FR by making better use of noisy, in-the-wild datasets.", "mention_start": 79, "mention_end": 100, "dataset_mention": " in-the-wild datasets"}, {"mentioned_in_paper": "568", "context_id": "148", "dataset_context": "Evaluation was performed for two challenging, large-pose datasets: Celebrities in Frontal-Profile in the Wild (CFP) [33] and Cross-Pose LFW (CPLFW) [44], as well as their frontal-frontal counterparts.", "mention_start": 45, "mention_end": 65, "dataset_mention": " large-pose datasets"}, {"mentioned_in_paper": "568", "context_id": "149", "dataset_context": "Benefit from use of 3D GAN data arises from a combination of the balanced distribution of poses and expressions, the use of a 3D lighting model, the presence of additional synthetic identities, and the GAN's ability to \"clean\" noisy datasets.", "mention_start": 218, "mention_end": 241, "dataset_mention": "\"clean\" noisy datasets"}, {"mentioned_in_paper": "568", "context_id": "150", "dataset_context": "Our baseline FR experiments are trained on either CA-SIA Webface [42], MS1M-V3 [10] or our in-house dataset of 3.5 million images scraped from the internet, labelled as \"NetScrape\" in Figure 6 and Tables 1 and 2. These datasets were then augmented using the 3D GAN trained on either CelebA [27] or Flickr-Faces-HQ (FFHQ) [22].", "mention_start": 86, "mention_end": 107, "dataset_mention": "our in-house dataset"}, {"mentioned_in_paper": "568", "context_id": "150", "dataset_context": "Our baseline FR experiments are trained on either CA-SIA Webface [42], MS1M-V3 [10] or our in-house dataset of 3.5 million images scraped from the internet, labelled as \"NetScrape\" in Figure 6 and Tables 1 and 2. These datasets were then augmented using the 3D GAN trained on either CelebA [27] or Flickr-Faces-HQ (FFHQ) [22].", "mention_start": 183, "mention_end": 227, "dataset_mention": "Figure 6 and Tables 1 and 2. These datasets"}, {"mentioned_in_paper": "568", "context_id": "173", "dataset_context": "Despite these issues, our experiments show that the synthetic data is of adequate quality to successfully augment FR datasets.", "mention_start": 113, "mention_end": 125, "dataset_mention": "FR datasets"}, {"mentioned_in_paper": "568", "context_id": "178", "dataset_context": "The only changes made were to the number of layers, as noted in Tables 2 and 3. Table 2 presents results for a series of experiments in which we augmented the NetScrape dataset with 3D GAN data generated from CelebA.", "mention_start": 141, "mention_end": 176, "dataset_mention": "we augmented the NetScrape dataset"}, {"mentioned_in_paper": "568", "context_id": "179", "dataset_context": "Experiment 1 gives our baseline, trained only on the \"NetScrape\" dataset.", "mention_start": 48, "mention_end": 72, "dataset_mention": "the \"NetScrape\" dataset"}, {"mentioned_in_paper": "568", "context_id": "183", "dataset_context": "Evaluation is performed for the frontal-frontal (FF) and frontal-profile (FP) protocols of the CFP dataset as well as for LFW (view 2) and CPLFW.", "mention_start": 91, "mention_end": 106, "dataset_mention": "the CFP dataset"}, {"mentioned_in_paper": "568", "context_id": "194", "dataset_context": "Note that, in this experiment, the 3D GAN extracts useful information from the noisy FFHQ dataset, whereas the TB-GAN of [12] is trained using a dataset of high-quality texture scans.", "mention_start": 74, "mention_end": 97, "dataset_mention": "the noisy FFHQ dataset"}, {"mentioned_in_paper": "568", "context_id": "197", "dataset_context": "Finally, experiments were performed for a ResNet-100 architecture trained on the MS1M-V3 dataset.", "mention_start": 76, "mention_end": 96, "dataset_mention": "the MS1M-V3 dataset"}, {"mentioned_in_paper": "569", "context_id": "7", "dataset_context": "We experimentally verify that the new dataset created in such an omni-supervised manner can significantly improve the generalization ability of the learned FER model.", "mention_start": 0, "mention_end": 45, "dataset_mention": "We experimentally verify that the new dataset"}, {"mentioned_in_paper": "569", "context_id": "9", "dataset_context": "To tackle this, we propose to apply a dataset distillation strategy to compress the created dataset into several informative class-wise images, significantly improving the training efficiency.", "mention_start": 29, "mention_end": 45, "dataset_mention": "apply a dataset"}, {"mentioned_in_paper": "569", "context_id": "11", "dataset_context": "More importantly, the distilled dataset has shown its capabilities of boosting the performance of FER with negligible additional computational costs.", "mention_start": 17, "mention_end": 39, "dataset_mention": " the distilled dataset"}, {"mentioned_in_paper": "569", "context_id": "29", "dataset_context": "First, the size of the constructed dataset can be large (e.g.", "mention_start": 18, "mention_end": 42, "dataset_mention": "the constructed dataset"}, {"mentioned_in_paper": "569", "context_id": "31", "dataset_context": "Second, since the constructed dataset is annotated automatically, there might exist some incorrectly labeled samples.", "mention_start": 13, "mention_end": 37, "dataset_mention": "the constructed dataset"}, {"mentioned_in_paper": "569", "context_id": "33", "dataset_context": "To tackle the above-mentioned issues, we propose to utilize the dataset distillation strategy [19] to distill the key knowledge from the constructed large scale dataset.", "mention_start": 132, "mention_end": 168, "dataset_mention": "the constructed large scale dataset"}, {"mentioned_in_paper": "569", "context_id": "35", "dataset_context": "Comparing to the original constructed dataset, the computational cost of the distilled images is decreased significantly.", "mention_start": 13, "mention_end": 45, "dataset_mention": "the original constructed dataset"}, {"mentioned_in_paper": "569", "context_id": "36", "dataset_context": "Although with a surprisingly small size, the distilled dataset is capable of boosting the recognition accuracy to a higher stage in both inner-dataset evaluation and cross-dataset evaluation comparing to the models trained using only the labeled datasets.", "mention_start": 40, "mention_end": 62, "dataset_mention": " the distilled dataset"}, {"mentioned_in_paper": "569", "context_id": "36", "dataset_context": "Although with a surprisingly small size, the distilled dataset is capable of boosting the recognition accuracy to a higher stage in both inner-dataset evaluation and cross-dataset evaluation comparing to the models trained using only the labeled datasets.", "mention_start": 131, "mention_end": 150, "dataset_mention": "both inner-dataset"}, {"mentioned_in_paper": "569", "context_id": "36", "dataset_context": "Although with a surprisingly small size, the distilled dataset is capable of boosting the recognition accuracy to a higher stage in both inner-dataset evaluation and cross-dataset evaluation comparing to the models trained using only the labeled datasets.", "mention_start": 131, "mention_end": 179, "dataset_mention": "both inner-dataset evaluation and cross-dataset"}, {"mentioned_in_paper": "569", "context_id": "40", "dataset_context": "Unlike previous works utilizing an unlabeled large-scale dataset to pre-train the network and provide initialization for downstream fine-tuning, our method constructs a largescale dataset from the huge amount of unlabeled data, and utilize the constructed dataset as well as the existing small scale labeled dataset to improve the FER performance.", "mention_start": 144, "mention_end": 187, "dataset_mention": " our method constructs a largescale dataset"}, {"mentioned_in_paper": "569", "context_id": "40", "dataset_context": "Unlike previous works utilizing an unlabeled large-scale dataset to pre-train the network and provide initialization for downstream fine-tuning, our method constructs a largescale dataset from the huge amount of unlabeled data, and utilize the constructed dataset as well as the existing small scale labeled dataset to improve the FER performance.", "mention_start": 239, "mention_end": 263, "dataset_mention": "the constructed dataset"}, {"mentioned_in_paper": "569", "context_id": "41", "dataset_context": "-To improve the generality, decrease the training cost, and minimize training bias due to class imbalance, we propose to utilize the dataset distillation strategy to summarize the knowledge from the constructed dataset.", "mention_start": 194, "mention_end": 218, "dataset_mention": "the constructed dataset"}, {"mentioned_in_paper": "569", "context_id": "42", "dataset_context": "Specifically, by the dataset distillation strategy, the constructed dataset, which has around 140K images, is compressed into a significantly small image set, in which one image for each class.", "mention_start": 51, "mention_end": 75, "dataset_mention": " the constructed dataset"}, {"mentioned_in_paper": "569", "context_id": "64", "dataset_context": "On the contrary, our method is based on omni-supervised learning, combining a huge-scale dataset, which is constructed automatically from a huge-scale unlabeled dataset, with a small size labeled dataset.", "mention_start": 65, "mention_end": 96, "dataset_mention": " combining a huge-scale dataset"}, {"mentioned_in_paper": "569", "context_id": "64", "dataset_context": "On the contrary, our method is based on omni-supervised learning, combining a huge-scale dataset, which is constructed automatically from a huge-scale unlabeled dataset, with a small size labeled dataset.", "mention_start": 137, "mention_end": 168, "dataset_mention": "a huge-scale unlabeled dataset"}, {"mentioned_in_paper": "569", "context_id": "64", "dataset_context": "On the contrary, our method is based on omni-supervised learning, combining a huge-scale dataset, which is constructed automatically from a huge-scale unlabeled dataset, with a small size labeled dataset.", "mention_start": 174, "mention_end": 203, "dataset_mention": "a small size labeled dataset"}, {"mentioned_in_paper": "569", "context_id": "65", "dataset_context": "By this way, the constructed dataset participates in the FER learner training process, rather than acting as an initializer.", "mention_start": 12, "mention_end": 36, "dataset_mention": " the constructed dataset"}, {"mentioned_in_paper": "569", "context_id": "74", "dataset_context": "Those huge scale selected samples are constructed into a new dataset, which will collaborate with the original small size labeled dataset to strengthen the FER learner directly.", "mention_start": 97, "mention_end": 137, "dataset_mention": "the original small size labeled dataset"}, {"mentioned_in_paper": "569", "context_id": "75", "dataset_context": "Further, to decrease the computation cost, save the training time, and increase the generality of the network, we propose to utilize the dataset distillation strategy [19] to compress the huge-scale constructed dataset into a very small set, in which one image for one class (Sec.", "mention_start": 174, "mention_end": 218, "dataset_mention": "compress the huge-scale constructed dataset"}, {"mentioned_in_paper": "569", "context_id": "87", "dataset_context": "This subsection illustrates how to select reliable samples from a huge scale unlabeled dataset, which is denoted as {x ul i , y ul i }.", "mention_start": 64, "mention_end": 94, "dataset_mention": "a huge scale unlabeled dataset"}, {"mentioned_in_paper": "569", "context_id": "112", "dataset_context": "The efficacy of the auxiliary samples selected from the huge-scale unlabeled dataset are demonstrated in our experimental results, which will be discussed in Sec. 4", "mention_start": 52, "mention_end": 84, "dataset_mention": "the huge-scale unlabeled dataset"}, {"mentioned_in_paper": "569", "context_id": "120", "dataset_context": "Since the unlabelled data set probably consists of huge amounts of data, we utilize dataset distillation strategy [19] to compress the knowledge into a highly refined set, i.e., one image for each class in our case.", "mention_start": 6, "mention_end": 29, "dataset_mention": "the unlabelled data set"}, {"mentioned_in_paper": "569", "context_id": "138", "dataset_context": "In summary, utilize the distilled auxiliary samples rather than the vanilla auxiliary samples has a few advantages: 1) the distilled images are with a small size, introducing few computation cost; 2) comparing to the vanilla auxiliary collected samples, the distilled images are with high generality capability, which has been experimentally proved to benefit to the cross-dataset evaluation settings.", "mention_start": 362, "mention_end": 380, "dataset_mention": "the cross-dataset"}, {"mentioned_in_paper": "569", "context_id": "143", "dataset_context": "For example, if we conduct an inner-evaluation on RAF-DB 2.0, we use RAF-DB 2.0 training set and constructed dataset to train an FER learner, and then use the trained learner for evaluating the RAF-DB 2.0 test set.", "mention_start": 88, "mention_end": 116, "dataset_mention": "set and constructed dataset"}, {"mentioned_in_paper": "569", "context_id": "144", "dataset_context": "In Crossdataset evaluation, the training set and test set are from different databases.", "mention_start": 3, "mention_end": 15, "dataset_mention": "Crossdataset"}, {"mentioned_in_paper": "569", "context_id": "152", "dataset_context": "To save the space, we use RAF-DB to denote RAF-DB 2.0 dataset in our discussion.", "mention_start": 42, "mention_end": 61, "dataset_mention": "RAF-DB 2.0 dataset"}, {"mentioned_in_paper": "569", "context_id": "186", "dataset_context": "We conduct inner-database evaluation at first and cross-dataset evaluation at second.", "mention_start": 40, "mention_end": 63, "dataset_mention": "first and cross-dataset"}, {"mentioned_in_paper": "569", "context_id": "188", "dataset_context": "We conduct the inner-dataset evaluations on two in-the-wild databases, i.e., RAF-DB, FER-2013 and one lab-controlled database, i.e., CK+.", "mention_start": 11, "mention_end": 28, "dataset_mention": "the inner-dataset"}, {"mentioned_in_paper": "569", "context_id": "191", "dataset_context": "In the experiment, RAF-DB is utilized as the anchor dataset for training the primitive learner.", "mention_start": 40, "mention_end": 59, "dataset_mention": "the anchor dataset"}, {"mentioned_in_paper": "569", "context_id": "206", "dataset_context": "On this challenging dataset, we find that the constructed dataset by our omni-supervised learning strategy and the distilled data both outperformed previous works under a similar setting.", "mention_start": 28, "mention_end": 65, "dataset_mention": " we find that the constructed dataset"}, {"mentioned_in_paper": "569", "context_id": "207", "dataset_context": "For example, comparing to the baseline with VGG-Face backbone, introducing the constructed dataset in an omnisupervised manner achieves a higher prediction accuracy; more than that, the distilled data boosts the performance to a higher stage by distilled key knowledge and keeping a sample balance from the constructed data.", "mention_start": 74, "mention_end": 98, "dataset_mention": "the constructed dataset"}, {"mentioned_in_paper": "569", "context_id": "217", "dataset_context": "The cross-dataset evaluations and comparisons with previous state-of-the-arts are shown in Table.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The cross-dataset"}, {"mentioned_in_paper": "569", "context_id": "220", "dataset_context": "We make a comparison with [16], which has the latest results on cross-dataset evaluation settings.", "mention_start": 63, "mention_end": 77, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "569", "context_id": "228", "dataset_context": "Unlike [16] using explicit domain adaptation strategy to minimize the domain gap between source dataset and target dataset, the better performance of our method is from the introduced distilled auxiliary samples with ignorable training cost.", "mention_start": 89, "mention_end": 103, "dataset_mention": "source dataset"}, {"mentioned_in_paper": "569", "context_id": "229", "dataset_context": "The performance on JAFFE dataset is reported in Table. 5.", "mention_start": 19, "mention_end": 32, "dataset_mention": "JAFFE dataset"}, {"mentioned_in_paper": "569", "context_id": "230", "dataset_context": "As noted by [16], there is a high bias in this extremely small dataset, which has only 213 images.", "mention_start": 41, "mention_end": 70, "dataset_mention": "this extremely small dataset"}, {"mentioned_in_paper": "569", "context_id": "231", "dataset_context": "Due to this bias, the cross-dataset evaluation on JAFFE is comparatively lower than that on CK+.", "mention_start": 17, "mention_end": 35, "dataset_mention": " the cross-dataset"}, {"mentioned_in_paper": "569", "context_id": "234", "dataset_context": "6, for MMI dataset, our method achieves comparable accuracy with [16] with few computation cost, outperforms the baseline by a large margin, which is from 60.82% to 63.34%.", "mention_start": 6, "mention_end": 18, "dataset_mention": "MMI dataset"}, {"mentioned_in_paper": "569", "context_id": "235", "dataset_context": "We also conduct a cross-dataset evaluation on FER-2013.", "mention_start": 16, "mention_end": 31, "dataset_mention": "a cross-dataset"}, {"mentioned_in_paper": "569", "context_id": "236", "dataset_context": "FER-2013 is constructed for challenges and has huge variations introduced by expressive ways, head poses, scales, and etc. Due to the heavy variations existed in FER-2013, the performance under the cross-dataset setting is inferior to that on CK+.", "mention_start": 171, "mention_end": 211, "dataset_mention": " the performance under the cross-dataset"}, {"mentioned_in_paper": "569", "context_id": "245", "dataset_context": "Although there are large variations in MS1M, and a large domain gap between MS1M and the anchor dataset (RAF-DB), we can observe that the auxiliary samples selected are still highly related to activated facial expression.", "mention_start": 75, "mention_end": 103, "dataset_mention": "MS1M and the anchor dataset"}, {"mentioned_in_paper": "569", "context_id": "248", "dataset_context": "The high quality of those selected samples can explain why our method achieves a better performance after utilizing those auxiliary Inner-dataset comparison of our method with previous works on FER-2013.", "mention_start": 116, "mention_end": 145, "dataset_mention": "those auxiliary Inner-dataset"}, {"mentioned_in_paper": "570", "context_id": "6", "dataset_context": "Experiments on existing large-scale person/vehicle ReID datasets demonstrate that ADIN learns more robust and generalizable representations, as evidenced by its outstanding direct transfer performance across datasets, which is a criterion that can better measure the generalizability of large scale Re-ID methods.", "mention_start": 15, "mention_end": 64, "dataset_mention": "existing large-scale person/vehicle ReID datasets"}, {"mentioned_in_paper": "570", "context_id": "21", "dataset_context": "Existing ReID datasets are limited in data volume.", "mention_start": 0, "mention_end": 22, "dataset_mention": "Existing ReID datasets"}, {"mentioned_in_paper": "570", "context_id": "23", "dataset_context": "A recent study [2] showed that in 2014, there were 125 video surveillance cameras per thousand people in the U.S.; whereas most ReID datasets were collected only from 10 or fewer cameras (see Table 1).", "mention_start": 114, "mention_end": 141, "dataset_mention": " whereas most ReID datasets"}, {"mentioned_in_paper": "570", "context_id": "27", "dataset_context": "First, a ReID dataset consists of images from different subjects, where every subject has an indefinite number of images (Fig. 1 left), making the subject class distribution non i.i.d.", "mention_start": 6, "mention_end": 21, "dataset_mention": " a ReID dataset"}, {"mentioned_in_paper": "570", "context_id": "36", "dataset_context": "Since video timestamp or camera index are freely available in video surveillance as metadata and are provided by almost all existing person/vehicle ReID datasets, they can be potentially utilized as auxiliary supervision, although few image-based ReID methods have taken advantage of them.", "mention_start": 113, "mention_end": 161, "dataset_mention": "almost all existing person/vehicle ReID datasets"}, {"mentioned_in_paper": "570", "context_id": "43", "dataset_context": "In addition to setting up a new goal of generalizability, 2 HHL uses images from both source and target domain for domain adaptation, and thus has no single-dataset performance.", "mention_start": 146, "mention_end": 164, "dataset_mention": "no single-dataset"}, {"mentioned_in_paper": "570", "context_id": "45", "dataset_context": "Figure 2 : Top1 accuracy on a single-dataset (MSMT17 [1]) and direct transfer from MSMT17 to Market1501 [6] (left) and to DukeMTMC-ReID [7, 8] (right).", "mention_start": 27, "mention_end": 44, "dataset_mention": "a single-dataset"}, {"mentioned_in_paper": "570", "context_id": "46", "dataset_context": "In contrast to our ADIN (red dot in top-right) which achieved competitive performance on both a single dataset and direct transfer, we find other (single-dataset) top-performers suffer from very poor generalizability to unseen domains, indicating the misaligned goal between overfitting small-scale single dataset and generalizing to large-scale unseen scenarios in real life.", "mention_start": 131, "mention_end": 161, "dataset_mention": " we find other (single-dataset"}, {"mentioned_in_paper": "570", "context_id": "46", "dataset_context": "In contrast to our ADIN (red dot in top-right) which achieved competitive performance on both a single dataset and direct transfer, we find other (single-dataset) top-performers suffer from very poor generalizability to unseen domains, indicating the misaligned goal between overfitting small-scale single dataset and generalizing to large-scale unseen scenarios in real life.", "mention_start": 274, "mention_end": 313, "dataset_mention": "overfitting small-scale single dataset"}, {"mentioned_in_paper": "570", "context_id": "56", "dataset_context": "The disconnection between research-level datasets and community/city-level video warehouse remains to hinder the real-life applications of ReID.", "mention_start": 26, "mention_end": 49, "dataset_mention": "research-level datasets"}, {"mentioned_in_paper": "570", "context_id": "57", "dataset_context": "Table 1 summarizes mainstream person/vehicle ReID datasets.", "mention_start": 19, "mention_end": 58, "dataset_mention": "mainstream person/vehicle ReID datasets"}, {"mentioned_in_paper": "570", "context_id": "58", "dataset_context": "For person ReID, considering that even in a grocery store there are usually dozens even more than 100 cameras [17] and over 550 visitors per day [18], current datasets are more or less overly simplistic.", "mention_start": 150, "mention_end": 167, "dataset_mention": " current datasets"}, {"mentioned_in_paper": "570", "context_id": "60", "dataset_context": "The latest MSMT17 dataset [1] led positive progress towards real large-scale usage, by including geo-spatially diverse cameras (both indoor and outdoor) and varying time periods (morning, noon and afternoon) and illuminations.", "mention_start": 0, "mention_end": 25, "dataset_mention": "The latest MSMT17 dataset"}, {"mentioned_in_paper": "570", "context_id": "63", "dataset_context": "A recent VeRi-776 dataset [16] presented a relatively realistic benchmark with cameras spanning a large spatial coverage and other variations, which is one-step close to being representative for large-scale vehicle ReID.", "mention_start": 0, "mention_end": 25, "dataset_mention": "A recent VeRi-776 dataset"}, {"mentioned_in_paper": "570", "context_id": "64", "dataset_context": "The standard ReID pipeline picks a dataset, learning the model from its training set and evaluating the model's retrieval accuracy or mean average prevision (mAP) on the held out testing set (with non-overlapping subjects).", "mention_start": 0, "mention_end": 42, "dataset_mention": "The standard ReID pipeline picks a dataset"}, {"mentioned_in_paper": "570", "context_id": "65", "dataset_context": "However, this single-dataset evaluation is often insufficient in reflecting true generalizability (Fig. 2) since they overlook a fact, i.e., due to the low coverage of most datasets, the training and testing sets of the same ReID dataset tend to be highly similar in terms of spatiotemporal nuisances (even overlapping or sharing camera IDs).", "mention_start": 8, "mention_end": 28, "dataset_mention": " this single-dataset"}, {"mentioned_in_paper": "570", "context_id": "65", "dataset_context": "However, this single-dataset evaluation is often insufficient in reflecting true generalizability (Fig. 2) since they overlook a fact, i.e., due to the low coverage of most datasets, the training and testing sets of the same ReID dataset tend to be highly similar in terms of spatiotemporal nuisances (even overlapping or sharing camera IDs).", "mention_start": 215, "mention_end": 237, "dataset_mention": "the same ReID dataset"}, {"mentioned_in_paper": "570", "context_id": "67", "dataset_context": "In view of the above, increasing attention has been paid to domain adaption in ReID recently, i.e., training on one source dataset, tuning the trained model on some different target domain data, and finally evaluating model accuracy/mAP on the target dataset.", "mention_start": 111, "mention_end": 130, "dataset_mention": "one source dataset"}, {"mentioned_in_paper": "570", "context_id": "136", "dataset_context": "In view of current ReID dataset limitations, we propose to choose a new evaluation metric to reflect our ultimate goal of generalizability.", "mention_start": 11, "mention_end": 31, "dataset_mention": "current ReID dataset"}, {"mentioned_in_paper": "570", "context_id": "138", "dataset_context": "Since different existing ReID datasets were collected in very diverse settings, we propose to use the direct transfer performance, by directly applying a ReID model trained on one dataset's training set onto another's testing set to measure accuracy/mAP, without any re-training or adaptation.", "mention_start": 6, "mention_end": 38, "dataset_mention": "different existing ReID datasets"}, {"mentioned_in_paper": "570", "context_id": "162", "dataset_context": "As comparison baselines, we train the same dual-branch backbones (without any adversarial learning) on the source datasets, and test their direct transfer performance too.", "mention_start": 102, "mention_end": 122, "dataset_mention": "the source datasets"}, {"mentioned_in_paper": "570", "context_id": "173", "dataset_context": "In contrast to our ADIN, we find other (single-dataset) top-performers [9, 10, 11] generalize very poorly to unseen domains, indicating the misaligned goals between overfitting small-scale single dataset, and generalizing to large-scale unseen scenarios in real life.", "mention_start": 24, "mention_end": 54, "dataset_mention": " we find other (single-dataset"}, {"mentioned_in_paper": "570", "context_id": "173", "dataset_context": "In contrast to our ADIN, we find other (single-dataset) top-performers [9, 10, 11] generalize very poorly to unseen domains, indicating the misaligned goals between overfitting small-scale single dataset, and generalizing to large-scale unseen scenarios in real life.", "mention_start": 164, "mention_end": 203, "dataset_mention": "overfitting small-scale single dataset"}, {"mentioned_in_paper": "570", "context_id": "174", "dataset_context": "As in Fig. 2, our ADIN framework lies in the top-right corner, while others stay in the left region with high single-dataset accuracy but poor direct transfer performance.", "mention_start": 104, "mention_end": 124, "dataset_mention": "high single-dataset"}, {"mentioned_in_paper": "571", "context_id": "213", "dataset_context": "We evaluated our method on the experimental setup from L2X [2] on the BeerAdvocate [30] dataset.", "mention_start": 66, "mention_end": 95, "dataset_mention": "the BeerAdvocate [30] dataset"}, {"mentioned_in_paper": "571", "context_id": "235", "dataset_context": "Following details about data and models outlined by [34], we use a simplified version of the ListOps [32] dataset.", "mention_start": 88, "mention_end": 113, "dataset_mention": "the ListOps [32] dataset"}, {"mentioned_in_paper": "571", "context_id": "249", "dataset_context": "We construct a simple dataset by sampling a fixed number of balanced parentheses of various types (from 10 to 50 types) and model them with non-monotonic architecture, defined in [44].", "mention_start": 13, "mention_end": 29, "dataset_mention": "a simple dataset"}, {"mentioned_in_paper": "571", "context_id": "447", "dataset_context": "As a base, we used the BeerAdvocate [30] dataset, which consists of beer reviews and ratings for different aspects: Aroma, Taste, Palate and Appearance.", "mention_start": 18, "mention_end": 48, "dataset_mention": "the BeerAdvocate [30] dataset"}, {"mentioned_in_paper": "571", "context_id": "470", "dataset_context": "We dropped starting positions and represented each dataset entry as the obtained sequence of T = 10 or T = 20 observations.", "mention_start": 0, "mention_end": 58, "dataset_mention": "We dropped starting positions and represented each dataset"}, {"mentioned_in_paper": "571", "context_id": "500", "dataset_context": "Despite the fact that the dataset for this experiment is highly synthetic we can note that model initialization plays big role in the final performance of the gradient estimator.", "mention_start": 0, "mention_end": 33, "dataset_mention": "Despite the fact that the dataset"}, {"mentioned_in_paper": "571", "context_id": "502", "dataset_context": "We took the ListOps [32] dataset, containing arithmetical prefix expressions, e.g.", "mention_start": 8, "mention_end": 32, "dataset_mention": "the ListOps [32] dataset"}, {"mentioned_in_paper": "574", "context_id": "146", "dataset_context": "We consider the ShapeNet dataset [2] which contains a rich collection of 3D CAD models, and is widely used in recent research works related to 2D/3D data.", "mention_start": 12, "mention_end": 32, "dataset_mention": "the ShapeNet dataset"}, {"mentioned_in_paper": "575", "context_id": "35", "dataset_context": "Experiments on a newly collected dataset (LifeScans), the publicly available PeopleSnapshot dataset [6], and on the dataset used in [9] demonstrate that our model infers shapes with a reconstruction accuracy of 4mm in less than 10 seconds.", "mention_start": 53, "mention_end": 99, "dataset_mention": " the publicly available PeopleSnapshot dataset"}, {"mentioned_in_paper": "575", "context_id": "172", "dataset_context": "We call the resulting dataset LifeScans, which consists of rendered images paired with 3D animated scans in various shapes and poses.", "mention_start": 0, "mention_end": 29, "dataset_mention": "We call the resulting dataset"}, {"mentioned_in_paper": "575", "context_id": "210", "dataset_context": "We quantitatively evaluate our method on a separated test set of the LifeScans dataset containing 55 subjects.", "mention_start": 65, "mention_end": 86, "dataset_mention": "the LifeScans dataset"}, {"mentioned_in_paper": "575", "context_id": "247", "dataset_context": "To this end, we split the LifeScans dataset.", "mention_start": 21, "mention_end": 43, "dataset_mention": "the LifeScans dataset"}, {"mentioned_in_paper": "575", "context_id": "249", "dataset_context": "All forms of supervision can be synthetically generated from the LifeScans dataset.", "mention_start": 61, "mention_end": 82, "dataset_mention": "the LifeScans dataset"}, {"mentioned_in_paper": "575", "context_id": "256", "dataset_context": "We qualitatively compare our method against the most relevant work [6] on their PeopleSnapshot dataset.", "mention_start": 74, "mention_end": 102, "dataset_mention": "their PeopleSnapshot dataset"}, {"mentioned_in_paper": "575", "context_id": "273", "dataset_context": "To this end, we split the LifeScans dataset.", "mention_start": 21, "mention_end": 43, "dataset_mention": "the LifeScans dataset"}, {"mentioned_in_paper": "575", "context_id": "275", "dataset_context": "All forms of supervision can be synthetically generated from the LifeScans dataset.", "mention_start": 61, "mention_end": 82, "dataset_mention": "the LifeScans dataset"}, {"mentioned_in_paper": "575", "context_id": "282", "dataset_context": "We qualitatively compare our method against the most relevant work [6] on their PeopleSnapshot dataset.", "mention_start": 74, "mention_end": 102, "dataset_mention": "their PeopleSnapshot dataset"}, {"mentioned_in_paper": "575", "context_id": "294", "dataset_context": "We segment their dataset using the semi-automatically approach [11] and re-train our predictor to be able to process binary segmentation masks.", "mention_start": 0, "mention_end": 24, "dataset_mention": "We segment their dataset"}, {"mentioned_in_paper": "575", "context_id": "295", "dataset_context": "Additionally, we augment the LifeScans dataset with T-poses.", "mention_start": 24, "mention_end": 46, "dataset_mention": "the LifeScans dataset"}, {"mentioned_in_paper": "575", "context_id": "304", "dataset_context": "Extensive experiments on the LifeScans dataset demonstrate the performance and influence of key parameters of the predictor.", "mention_start": 25, "mention_end": 46, "dataset_mention": "the LifeScans dataset"}, {"mentioned_in_paper": "576", "context_id": "7", "dataset_context": "Furthermore, we show that our method obtains state-of-the-art results for pixel-wise and image-wise classification on the EBAY dataset and is able to learn color names for various domains.", "mention_start": 117, "mention_end": 134, "dataset_mention": "the EBAY dataset"}, {"mentioned_in_paper": "576", "context_id": "77", "dataset_context": "For the purpose of this paper we collect two datasets: one of domain-specific color names, and one class-specific basic color term dataset.", "mention_start": 90, "mention_end": 138, "dataset_mention": " and one class-specific basic color term dataset"}, {"mentioned_in_paper": "576", "context_id": "79", "dataset_context": "Domain-specific color name dataset: we collect several domain-specific datasets from Google search engine by using the query of 'color name + object': 5 colors of eyes, 7 colors of lips, 9 colors of horses and tomatoes in 6 growing stages.", "mention_start": 46, "mention_end": 79, "dataset_mention": "several domain-specific datasets"}, {"mentioned_in_paper": "576", "context_id": "85", "dataset_context": "Since existing methods report on the eleven based color terms we also collect a class-specific dataset for these color names.", "mention_start": 78, "mention_end": 102, "dataset_mention": "a class-specific dataset"}, {"mentioned_in_paper": "576", "context_id": "101", "dataset_context": "We compare results on the EBAY dataset which contains a total of 440 images, consisting of ten images for the eleven color names for four different categories (cars, shoes, dresses, and pottery).", "mention_start": 22, "mention_end": 38, "dataset_mention": "the EBAY dataset"}, {"mentioned_in_paper": "576", "context_id": "105", "dataset_context": "The results are on our class-specific dataset and summarized in Table III.", "mention_start": 19, "mention_end": 45, "dataset_mention": "our class-specific dataset"}, {"mentioned_in_paper": "576", "context_id": "107", "dataset_context": "In Table II we compare our results testing on the EBAY dataset with previous related work: PLSA [6], SS net [7] with different training data.", "mention_start": 46, "mention_end": 62, "dataset_mention": "the EBAY dataset"}, {"mentioned_in_paper": "576", "context_id": "111", "dataset_context": "When comparing the methods based on the class-agnostic data set, we see that our method struggles to learn a good attention model.", "mention_start": 36, "mention_end": 63, "dataset_mention": "the class-agnostic data set"}, {"mentioned_in_paper": "576", "context_id": "113", "dataset_context": "However, when we use the class-specific dataset with similar objects as in the EBAY dataset (cars, shoes, dresses, and pottery) results improve significantly.", "mention_start": 20, "mention_end": 47, "dataset_mention": "the class-specific dataset"}, {"mentioned_in_paper": "576", "context_id": "113", "dataset_context": "However, when we use the class-specific dataset with similar objects as in the EBAY dataset (cars, shoes, dresses, and pottery) results improve significantly.", "mention_start": 74, "mention_end": 91, "dataset_mention": "the EBAY dataset"}, {"mentioned_in_paper": "576", "context_id": "121", "dataset_context": "Humans are asked to choose the main color label for the object in each image; eight candidates without color blindness are asked to give one color label for each of 110 randomly chosen images from the EBAY dataset.", "mention_start": 196, "mention_end": 213, "dataset_mention": "the EBAY dataset"}, {"mentioned_in_paper": "576", "context_id": "122", "dataset_context": "We compute testing accuracy comparing to the ground truth of EBAY dataset and report the average accuracy (88.98%) as the human evaluation baseline.", "mention_start": 61, "mention_end": 73, "dataset_mention": "EBAY dataset"}, {"mentioned_in_paper": "576", "context_id": "125", "dataset_context": "Here we evaluate our method on the four groups from our domain-specific dataset.", "mention_start": 52, "mention_end": 79, "dataset_mention": "our domain-specific dataset"}, {"mentioned_in_paper": "576", "context_id": "137", "dataset_context": "In addition, we show that the pixel-wise and image-wise predictions of the network obtain state-of-the-art results on the EBAY dataset.", "mention_start": 117, "mention_end": 134, "dataset_mention": "the EBAY dataset"}, {"mentioned_in_paper": "577", "context_id": "175", "dataset_context": "SD-saliency-900 dataset [21] consists of 900 steel surface defect images with high-quality annotation.", "mention_start": 0, "mention_end": 23, "dataset_mention": "SD-saliency-900 dataset"}, {"mentioned_in_paper": "577", "context_id": "178", "dataset_context": "Fabric defect dataset [60] provides the largest dataset for fabric defect detection with pixel-wise labels so far.", "mention_start": 0, "mention_end": 21, "dataset_mention": "Fabric defect dataset"}, {"mentioned_in_paper": "577", "context_id": "178", "dataset_context": "Fabric defect dataset [60] provides the largest dataset for fabric defect detection with pixel-wise labels so far.", "mention_start": 0, "mention_end": 55, "dataset_mention": "Fabric defect dataset [60] provides the largest dataset"}, {"mentioned_in_paper": "577", "context_id": "181", "dataset_context": "NRSD-MN dataset [49] contains 3936 man-made images and 165 natural images collected from the no-service rail surface, most of which are challenging with diverse defect shapes and rust disturbance.", "mention_start": 0, "mention_end": 15, "dataset_mention": "NRSD-MN dataset"}, {"mentioned_in_paper": "577", "context_id": "183", "dataset_context": "2) Datasets Description: Along with the visual comparison, we follow the widely-used metrics to quantitatively evaluate detection performance, including False Positive Rate (FPR), False Negative Rate (FNR), Accuracy (ACC), Mean Absolute Error (MAE) and F1-measure (F1).", "mention_start": 0, "mention_end": 11, "dataset_mention": "2) Datasets"}, {"mentioned_in_paper": "577", "context_id": "198", "dataset_context": "1) Comparison on SD-saliency-900 dataset: The visual comparison of our method and other methods for partial SDsaliency-900 dataset is shown in Fig. 5, and corresponding quantitative results are listed in Table II.", "mention_start": 17, "mention_end": 40, "dataset_mention": "SD-saliency-900 dataset"}, {"mentioned_in_paper": "577", "context_id": "198", "dataset_context": "1) Comparison on SD-saliency-900 dataset: The visual comparison of our method and other methods for partial SDsaliency-900 dataset is shown in Fig. 5, and corresponding quantitative results are listed in Table II.", "mention_start": 99, "mention_end": 130, "dataset_mention": "partial SDsaliency-900 dataset"}, {"mentioned_in_paper": "577", "context_id": "250", "dataset_context": "2) Comparison on Fabric defect dataset: Fig. 6 and Table III illustrates the visual and quantitative comparison of different methods on fabric defect dataset respectively.", "mention_start": 17, "mention_end": 38, "dataset_mention": "Fabric defect dataset"}, {"mentioned_in_paper": "577", "context_id": "250", "dataset_context": "2) Comparison on Fabric defect dataset: Fig. 6 and Table III illustrates the visual and quantitative comparison of different methods on fabric defect dataset respectively.", "mention_start": 135, "mention_end": 157, "dataset_mention": "fabric defect dataset"}, {"mentioned_in_paper": "577", "context_id": "302", "dataset_context": "3) Comparison on NRSD-MN dataset: Sample surface images on NRSD-MN dataset and corresponding prediction results are visualized in Fig. 7, and the quantitative results of various models are tabulated in Table IV.", "mention_start": 17, "mention_end": 32, "dataset_mention": "NRSD-MN dataset"}, {"mentioned_in_paper": "577", "context_id": "302", "dataset_context": "3) Comparison on NRSD-MN dataset: Sample surface images on NRSD-MN dataset and corresponding prediction results are visualized in Fig. 7, and the quantitative results of various models are tabulated in Table IV.", "mention_start": 58, "mention_end": 74, "dataset_mention": "NRSD-MN dataset"}, {"mentioned_in_paper": "577", "context_id": "348", "dataset_context": "To validate the contribution of each component in our DefT network, a series of ablation experiences are conducted on the SD-saliency-900 dataset.", "mention_start": 117, "mention_end": 145, "dataset_mention": "the SD-saliency-900 dataset"}, {"mentioned_in_paper": "577", "context_id": "358", "dataset_context": "Meanwhile, the curves of PR and F-measure reported in Fig. 8 In this section, we compare different types of position encoding methods on the fabric defect dataset: 1) absolute position encoding used in ViT [26] (APE); 2) relative position encoding used in Swin transformer [31] (RPE); 3) LPB and CFFN specially designed in our DefT (Ours).", "mention_start": 136, "mention_end": 162, "dataset_mention": "the fabric defect dataset"}, {"mentioned_in_paper": "577", "context_id": "365", "dataset_context": "To better understand the impact of inductive bias, we set two training settings to compare our hybrid transformer DefT with pure transformer Swin-Unet: i) training with the full NRSD-MN dataset for the 60, 300, 510, 700 epoch; and ii) training with the 30%, 60%, 100% NRSD-MN dataset for the full epoch.", "mention_start": 168, "mention_end": 193, "dataset_mention": "the full NRSD-MN dataset"}, {"mentioned_in_paper": "577", "context_id": "365", "dataset_context": "To better understand the impact of inductive bias, we set two training settings to compare our hybrid transformer DefT with pure transformer Swin-Unet: i) training with the full NRSD-MN dataset for the 60, 300, 510, 700 epoch; and ii) training with the 30%, 60%, 100% NRSD-MN dataset for the full epoch.", "mention_start": 262, "mention_end": 283, "dataset_mention": " 100% NRSD-MN dataset"}, {"mentioned_in_paper": "577", "context_id": "378", "dataset_context": "Experimental results over the three defect datasets show that the proposed method is comparable to or outperforms existing transformerand CNN-based methods in terms of detection performance, data efficiency, training efficiency, and generalization.", "mention_start": 26, "mention_end": 51, "dataset_mention": "the three defect datasets"}, {"mentioned_in_paper": "580", "context_id": "5", "dataset_context": "Our approach obtains 3-5% improvement over the state-of-the-art on outfit compatibility prediction and fill-in-the-blank tasks using our dataset, as well as an established smaller dataset, while supporting a variety of useful queries 1 .", "mention_start": 156, "mention_end": 187, "dataset_mention": "an established smaller dataset"}, {"mentioned_in_paper": "580", "context_id": "37", "dataset_context": "Since many of the current fashion datasets either do not contain outfit compatibility annotations [20], or are limited in size and the type of annotations they provide [12], we collect our own dataset which we describe in Section 3. In Section 4 we discuss our type-aware embedding model, which enables us to perform complex queries on our data.", "mention_start": 14, "mention_end": 42, "dataset_mention": "the current fashion datasets"}, {"mentioned_in_paper": "580", "context_id": "65", "dataset_context": "Polyvore Dataset: The Polyvore fashion website enables users to create outfits as compositions of clothing items, each containing rich multi-modal information such as product images, text descriptions, associated tags, popularity score, and type information.", "mention_start": 0, "mention_end": 16, "dataset_mention": "Polyvore Dataset"}, {"mentioned_in_paper": "580", "context_id": "67", "dataset_context": "supplied a dataset of Polyvore outfits (referred to as the Maryland Polyvore dataset [12]).", "mention_start": 0, "mention_end": 18, "dataset_mention": "supplied a dataset"}, {"mentioned_in_paper": "580", "context_id": "67", "dataset_context": "supplied a dataset of Polyvore outfits (referred to as the Maryland Polyvore dataset [12]).", "mention_start": 55, "mention_end": 84, "dataset_mention": "the Maryland Polyvore dataset"}, {"mentioned_in_paper": "580", "context_id": "121", "dataset_context": "For experiments on the Maryland Polyvore dataset [12] in Section 5.1 we use the provided splits which separate the outfits into 17,316 for training, 3,076 for testing, and 1,407 for validation.", "mention_start": 19, "mention_end": 48, "dataset_mention": "the Maryland Polyvore dataset"}, {"mentioned_in_paper": "580", "context_id": "127", "dataset_context": "We set \u03bb 3 = 5e \u22121 from Eq. ( 6) for the Maryland Polyvore dataset (\u03bb 3 = 5e \u22125 for experiments on our dataset), and all other \u03bb parameters from Eq. ( 4) and Eq. ( 6) to 5e \u22124 .", "mention_start": 37, "mention_end": 66, "dataset_mention": "the Maryland Polyvore dataset"}, {"mentioned_in_paper": "580", "context_id": "144", "dataset_context": "Same as the previous approach, but where each learned pairwise type-dependent transformation is responsible for four pairwise comparisons Table 2 : Comparison of different methods on the Maryland Polyvore dataset [12] using their unrestricted randomly sampled negatives on the fill-in-the-blank and outfit compatibility tasks.", "mention_start": 182, "mention_end": 212, "dataset_mention": "the Maryland Polyvore dataset"}, {"mentioned_in_paper": "580", "context_id": "162", "dataset_context": "The Table 3 : Comparison of different methods on the Maryland Polyvore Dataset [12] on the fill-in-the-blank and outfit compatibility tasks using our category-aware negative sampling method.", "mention_start": 48, "mention_end": 78, "dataset_mention": "the Maryland Polyvore Dataset"}, {"mentioned_in_paper": "580", "context_id": "195", "dataset_context": "Analogous to the Maryland dataset, the next three lines of Table 7 show a consistent performance improvement as we add in the remaining pieces of our model.", "mention_start": 13, "mention_end": 33, "dataset_mention": "the Maryland dataset"}, {"mentioned_in_paper": "580", "context_id": "217", "dataset_context": "First, on an established dataset, respecting type produces better performance at established tasks.", "mention_start": 9, "mention_end": 32, "dataset_mention": "an established dataset"}, {"mentioned_in_paper": "581", "context_id": "1", "dataset_context": "The performance on small dataset is almost saturated and people are moving towards larger datasets.", "mention_start": 68, "mention_end": 98, "dataset_mention": "moving towards larger datasets"}, {"mentioned_in_paper": "581", "context_id": "8", "dataset_context": "The tool will be made open-source to help study model and dataset design.", "mention_start": 48, "mention_end": 65, "dataset_mention": "model and dataset"}, {"mentioned_in_paper": "581", "context_id": "44", "dataset_context": "3. We collect a synthetic activity video dataset and develop a diagnostic toolkit to perform IPT.", "mention_start": 14, "mention_end": 48, "dataset_mention": "a synthetic activity video dataset"}, {"mentioned_in_paper": "581", "context_id": "108", "dataset_context": "We collected a 3D animation dataset and created a toolbox for generating synthetic human activity videos.", "mention_start": 13, "mention_end": 35, "dataset_mention": "a 3D animation dataset"}, {"mentioned_in_paper": "581", "context_id": "121", "dataset_context": "In order to create diverse combination of human appearance and activity, we collected a large synthetic human dataset.", "mention_start": 85, "mention_end": 117, "dataset_mention": "a large synthetic human dataset"}, {"mentioned_in_paper": "581", "context_id": "123", "dataset_context": "The motion capture data comes from CMU Mocap dataset [4].", "mention_start": 35, "mention_end": 52, "dataset_mention": "CMU Mocap dataset"}, {"mentioned_in_paper": "582", "context_id": "356", "dataset_context": "Excluding the participants who did not complete both tasks made sampling easier as it assured that the data set was balanced but could also introduce a bias in the analysis.", "mention_start": 83, "mention_end": 111, "dataset_mention": "it assured that the data set"}, {"mentioned_in_paper": "582", "context_id": "572", "dataset_context": "From the resulting data set of 51 distinct solutions, we extracted several measures of TD, some through carefully designed manual procedures and others using the static code analyzing tool SonarQube.", "mention_start": 5, "mention_end": 27, "dataset_mention": "the resulting data set"}, {"mentioned_in_paper": "584", "context_id": "32", "dataset_context": "Javadi and Mirroshandel [32] provide a deep learning approach, applying a convolutional deep neural network to the Sperm Morphology Analysis dataset (MHSMA).", "mention_start": 110, "mention_end": 148, "dataset_mention": "the Sperm Morphology Analysis dataset"}, {"mentioned_in_paper": "586", "context_id": "5", "dataset_context": "Our results are stateof-the-art in the Kinetics-400 dataset and are competitive on UCF-101 under the ZSAR evaluation.", "mention_start": 35, "mention_end": 59, "dataset_mention": "the Kinetics-400 dataset"}, {"mentioned_in_paper": "586", "context_id": "9", "dataset_context": "Currently, Kinetics-700 [3] is the largest HAR dataset, with 700 action classes and at least 700 videos per class, totaling 647,907.", "mention_start": 30, "mention_end": 54, "dataset_mention": "the largest HAR dataset"}, {"mentioned_in_paper": "586", "context_id": "45", "dataset_context": "In practice, these captioning models provide information on humans, objects, scenes, and their relationships, avoiding the need for manual definitions for affinity/prior functions on interactions while improving the performance; and, lastly, (iii) the predictions using objects and sentences are easily combined to reach state-of-the-art (SOTA) performance on the Kinetics-400 dataset and competitive results on UCF-101.", "mention_start": 359, "mention_end": 384, "dataset_mention": "the Kinetics-400 dataset"}, {"mentioned_in_paper": "586", "context_id": "51", "dataset_context": "where p sz \u2200z \u2208 Z u is the classification score of a textual description over the set of unseen classes Z u given by a supervised model, as described in Section II-A; p vy \u2200y \u2208 Y are the classification scores of objects given by an off-theshelf classifier pre-trained in the ImageNet dataset; finally, g yz \u2200y \u2208 Y and \u2200z \u2208 Z u is an affinity score, that is, a term computed to estimate which objects are most related to which actions, inspired by Jain et al. [9], but with significant improvements as described in detail in Section II-B.", "mention_start": 270, "mention_end": 291, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "586", "context_id": "70", "dataset_context": "Our experiments were conducted on the UCF-101 [23] and Kinetics-400 [1] datasets.", "mention_start": 34, "mention_end": 80, "dataset_mention": "the UCF-101 [23] and Kinetics-400 [1] datasets"}, {"mentioned_in_paper": "586", "context_id": "78", "dataset_context": "For sentence encoding, we retrained the Transformer-based observers [17] from Estevam et al. [16] on the ActivityNet Captions dataset [24], without any class label from Ac-tivityNet, replacing their i3D features with our ResNet152 features.", "mention_start": 100, "mention_end": 133, "dataset_mention": "the ActivityNet Captions dataset"}, {"mentioned_in_paper": "586", "context_id": "80", "dataset_context": "We evaluate our model using accuracy and following two protocols for the UCF-101 dataset: conventional and TruZe [25].", "mention_start": 69, "mention_end": 88, "dataset_mention": "the UCF-101 dataset"}, {"mentioned_in_paper": "586", "context_id": "88", "dataset_context": "Finally, we evaluate the performance on the Kinetics-400 dataset.", "mention_start": 39, "mention_end": 64, "dataset_mention": "the Kinetics-400 dataset"}, {"mentioned_in_paper": "586", "context_id": "90", "dataset_context": "When a random subset of classes is used, we perform the evaluations with 50 runs in all the protocols and datasets and report the average results.", "mention_start": 83, "mention_end": 114, "dataset_mention": "all the protocols and datasets"}, {"mentioned_in_paper": "586", "context_id": "91", "dataset_context": "As shown in Table I, our complete method presented a higher performance in the UCF-101 dataset than other approaches in the literature under three split configurations.", "mention_start": 74, "mention_end": 94, "dataset_mention": "the UCF-101 dataset"}, {"mentioned_in_paper": "586", "context_id": "95", "dataset_context": "Table II shows the results obtained in the UCF-101 datasets under the TruZe protocol.", "mention_start": 39, "mention_end": 59, "dataset_mention": "the UCF-101 datasets"}, {"mentioned_in_paper": "586", "context_id": "102", "dataset_context": "The Kinetics-400 dataset is very challenging for ZSAR.", "mention_start": 0, "mention_end": 24, "dataset_mention": "The Kinetics-400 dataset"}, {"mentioned_in_paper": "586", "context_id": "109", "dataset_context": "Fig. 2 illustrates a similar effect in the UCF101 dataset.", "mention_start": 39, "mention_end": 57, "dataset_mention": "the UCF101 dataset"}, {"mentioned_in_paper": "588", "context_id": "31", "dataset_context": "Despite the remarkable performance, a common issue of these approaches is that FR datasets contain a large number of identities, and only a few identities are presented in each mini-batch, which complicates finding an optimal decision boundary [12].", "mention_start": 73, "mention_end": 90, "dataset_mention": "that FR datasets"}, {"mentioned_in_paper": "588", "context_id": "135", "dataset_context": "Due to the large number of classes in FR datasets and having a small number of identities within a mini-batch, the conventional FR methods could not cover a large number of identities at each step of loss calculation [12].", "mention_start": 38, "mention_end": 49, "dataset_mention": "FR datasets"}, {"mentioned_in_paper": "588", "context_id": "160", "dataset_context": "We study the performance of the coupled-encoder on four FR datasets.", "mention_start": 51, "mention_end": 67, "dataset_mention": "four FR datasets"}, {"mentioned_in_paper": "588", "context_id": "165", "dataset_context": "Most of the FR datasets are imbalanced in two aspects: 1) the number of per identity samples, and 2) the number of profile and frontal samples for each identity.", "mention_start": 8, "mention_end": 23, "dataset_mention": "the FR datasets"}, {"mentioned_in_paper": "588", "context_id": "167", "dataset_context": "We initialize the encoders sub-networks with pre-trained weights on the VggFace2 dataset [5] with the Softmax loss to mitigate this problem.", "mention_start": 68, "mention_end": 88, "dataset_mention": "the VggFace2 dataset"}, {"mentioned_in_paper": "588", "context_id": "180", "dataset_context": "CMU Multi-PIE dataset [10] includes 750,000 images of 337 identities with variations in pose, illumination, and expression in the controlled environment.", "mention_start": 0, "mention_end": 21, "dataset_mention": "CMU Multi-PIE dataset"}, {"mentioned_in_paper": "588", "context_id": "195", "dataset_context": "Table 2 demonstrates the performance of our model in comparison with SOTA models on the Multi-PIE dataset and we investigate the effect of face angle on the FR performance.", "mention_start": 84, "mention_end": 105, "dataset_mention": "the Multi-PIE dataset"}, {"mentioned_in_paper": "588", "context_id": "204", "dataset_context": "The Celebrities in Frontal-Profile in the Wild (CFP) dataset [36] includes facial images of 500 different identities with 10 frontal and 4 profile samples per identity.", "mention_start": 38, "mention_end": 60, "dataset_mention": "the Wild (CFP) dataset"}, {"mentioned_in_paper": "588", "context_id": "206", "dataset_context": "Considering results on the CFP-FP dataset in Table 1, our framework performs better than the other SOTA methods with at least a margin of 0.23% in terms of accuracy and 1.72% improvements in terms of EER.", "mention_start": 23, "mention_end": 41, "dataset_mention": "the CFP-FP dataset"}, {"mentioned_in_paper": "588", "context_id": "207", "dataset_context": "Table 3. Verification accuracy (%) for IJB-B and IJB-C dataset.", "mention_start": 39, "mention_end": 62, "dataset_mention": "IJB-B and IJB-C dataset"}, {"mentioned_in_paper": "588", "context_id": "208", "dataset_context": "Results of the [35, 32, 4] are copied from [38] Metohd IJB-C(TAR@FAR) IJB-B(TAR@FAR) The IJB-B [49] is challenging in the wild dataset, which was further extended to IJB-C [24].", "mention_start": 117, "mention_end": 134, "dataset_mention": "the wild dataset"}, {"mentioned_in_paper": "588", "context_id": "211", "dataset_context": "Baseline for comparison on IJB-B and IJB-C datasets are PR-REM [4], FNM [32], FaceNet [35], GOTs [49, 24], VGG-CNN [49, 24], PF-cpGAN [38], CFR-GAN [15], SSA [19], and Lin et al. [20].", "mention_start": 27, "mention_end": 51, "dataset_mention": "IJB-B and IJB-C datasets"}, {"mentioned_in_paper": "588", "context_id": "217", "dataset_context": "We report on the Multi-PIE dataset to better illustrate the influence of shift in distribution caused by variation in view angle, see Fig. 4. The encoders are pretrained ResNet50 with Softmax on the VggFace2 dataset.", "mention_start": 13, "mention_end": 34, "dataset_mention": "the Multi-PIE dataset"}, {"mentioned_in_paper": "588", "context_id": "217", "dataset_context": "We report on the Multi-PIE dataset to better illustrate the influence of shift in distribution caused by variation in view angle, see Fig. 4. The encoders are pretrained ResNet50 with Softmax on the VggFace2 dataset.", "mention_start": 194, "mention_end": 215, "dataset_mention": "the VggFace2 dataset"}, {"mentioned_in_paper": "588", "context_id": "230", "dataset_context": "In Table 5, we further study the effect of varying the number of negative samples on the Setting1 of Multi-PIE dataset.", "mention_start": 100, "mention_end": 118, "dataset_mention": "Multi-PIE dataset"}, {"mentioned_in_paper": "589", "context_id": "11", "dataset_context": "Building finely annotated largescale datasets is also laborious, expensive, and sometimes impractical.", "mention_start": 9, "mention_end": 45, "dataset_mention": "finely annotated largescale datasets"}, {"mentioned_in_paper": "589", "context_id": "30", "dataset_context": "Transferring to the VOC [31] dataset, ReCo improves MoCo-v2 [32] by at least 6.3% mAP for lowshot classification with k=1,2,4,8,16 and 0.9% AP 50 for object detection.", "mention_start": 16, "mention_end": 36, "dataset_mention": "the VOC [31] dataset"}, {"mentioned_in_paper": "589", "context_id": "210", "dataset_context": "Specifically, ReCo surpasses MoCo-v2 by 13.4% top-1 accuracy with 1% labeled data, which demonstrates that the semantic structure learned by exploring instance relations can be more advantageous under insufficient data settings.", "mention_start": 176, "mention_end": 222, "dataset_mention": "more advantageous under insufficient data set"}, {"mentioned_in_paper": "589", "context_id": "234", "dataset_context": "We also evaluate the learned model on largescale COCO dataset [91].", "mention_start": 38, "mention_end": 61, "dataset_mention": "largescale COCO dataset"}, {"mentioned_in_paper": "590", "context_id": "5", "dataset_context": "The extensive experiments on 3 widely-used benchmark datasets demonstrate that, our propositions essentially facilitate the CNN baseline model to achieve the state-of-the-art performance without any other highlevel domain knowledge or low-level technical trick.", "mention_start": 29, "mention_end": 61, "dataset_mention": "3 widely-used benchmark datasets"}, {"mentioned_in_paper": "590", "context_id": "33", "dataset_context": " [2] takes advantage of the deep convolutional models pre-trained on ImageNet [5] and fine-tunes it on person re-identification datasets using softmax loss.", "mention_start": 102, "mention_end": 136, "dataset_mention": "person re-identification datasets"}, {"mentioned_in_paper": "590", "context_id": "48", "dataset_context": "Since person re-identification dataset are relatively small (e.g Market1501 containing only 12,936 images for training), effective means for preventing overfitting is necessary for building high-accuracy person re-identification model.", "mention_start": 6, "mention_end": 38, "dataset_mention": "person re-identification dataset"}, {"mentioned_in_paper": "590", "context_id": "59", "dataset_context": "ImageNet dataset contains over 14 million images, which has provided plenty of visual concepts.", "mention_start": 0, "mention_end": 16, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "590", "context_id": "63", "dataset_context": "To well fine-tune CNN model pre-trained on ImageNet towards the relative small-scale datasets (e.g., person reidentification dataset), one critical issue is to alleviate overfitting problem during training.", "mention_start": 43, "mention_end": 93, "dataset_mention": "ImageNet towards the relative small-scale datasets"}, {"mentioned_in_paper": "590", "context_id": "63", "dataset_context": "To well fine-tune CNN model pre-trained on ImageNet towards the relative small-scale datasets (e.g., person reidentification dataset), one critical issue is to alleviate overfitting problem during training.", "mention_start": 100, "mention_end": 132, "dataset_mention": " person reidentification dataset"}, {"mentioned_in_paper": "590", "context_id": "96", "dataset_context": "Datasets: To evaluate the effectiveness of our propositions, we conduct experiments on three widely used person reidentification datasets, including Market-1501 [7], DukeMTMC-reID [8] and CUHK03 [9].", "mention_start": 104, "mention_end": 137, "dataset_mention": "person reidentification datasets"}, {"mentioned_in_paper": "590", "context_id": "97", "dataset_context": "The Market-1501 dataset consists of image samples from 6 cameras with different resolutions.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The Market-1501 dataset"}, {"mentioned_in_paper": "590", "context_id": "101", "dataset_context": "DukeMTMC-reID dataset contain 1,812 identities with 8 cameras.", "mention_start": 0, "mention_end": 21, "dataset_mention": "DukeMTMC-reID dataset"}, {"mentioned_in_paper": "590", "context_id": "104", "dataset_context": "CUHK03 dataset consists of 14,097 image samples captured by 6 cameras from 1,467 persons.", "mention_start": 0, "mention_end": 14, "dataset_mention": "CUHK03 dataset"}, {"mentioned_in_paper": "590", "context_id": "106", "dataset_context": "On CUHK03 dataset, we use the test protocol in [9].", "mention_start": 3, "mention_end": 17, "dataset_mention": "CUHK03 dataset"}, {"mentioned_in_paper": "590", "context_id": "123", "dataset_context": "Performance comparison between our method and the the other state-of-the-art approaches on Market-1501 dataset is shown in Table 1.", "mention_start": 91, "mention_end": 110, "dataset_mention": "Market-1501 dataset"}, {"mentioned_in_paper": "590", "context_id": "134", "dataset_context": "Performance comparison between our method and the the other state-of-the-art approaches on DukeMTMC-reID dataset is shown in Table 2.", "mention_start": 91, "mention_end": 112, "dataset_mention": "DukeMTMC-reID dataset"}, {"mentioned_in_paper": "590", "context_id": "138", "dataset_context": "Table 3 lists the performance comparison between our approach and the state-of-the-art ones on CUHK03 dataset.", "mention_start": 95, "mention_end": 109, "dataset_mention": "CUHK03 dataset"}, {"mentioned_in_paper": "590", "context_id": "153", "dataset_context": "mAP will drop from 78.8% to 73.4% on Market-1501 dataset, and drop from 68.8% to 61.8% on DukeMTMC-reID dataset, if \"bottleneck\" layer is used.", "mention_start": 37, "mention_end": 56, "dataset_mention": "Market-1501 dataset"}, {"mentioned_in_paper": "590", "context_id": "153", "dataset_context": "mAP will drop from 78.8% to 73.4% on Market-1501 dataset, and drop from 68.8% to 61.8% on DukeMTMC-reID dataset, if \"bottleneck\" layer is used.", "mention_start": 89, "mention_end": 111, "dataset_mention": "DukeMTMC-reID dataset"}, {"mentioned_in_paper": "590", "context_id": "157", "dataset_context": "The experiment is conducted on DukeMTMC-reID dataset using Resnet50.", "mention_start": 31, "mention_end": 52, "dataset_mention": "DukeMTMC-reID dataset"}, {"mentioned_in_paper": "591", "context_id": "4", "dataset_context": "Similar setting arises in numerous datasets where multiple actors interact.", "mention_start": 26, "mention_end": 43, "dataset_mention": "numerous datasets"}, {"mentioned_in_paper": "591", "context_id": "41", "dataset_context": "In addition, we apply the approach on two real customer interaction datasets.", "mention_start": 37, "mention_end": 76, "dataset_mention": "two real customer interaction datasets"}, {"mentioned_in_paper": "591", "context_id": "42", "dataset_context": "The first dataset is an email interaction dataset where we observe email send, open, click.", "mention_start": 21, "mention_end": 49, "dataset_mention": "an email interaction dataset"}, {"mentioned_in_paper": "591", "context_id": "44", "dataset_context": "The second customer interaction dataset is based on paid and organic search clicks.", "mention_start": 0, "mention_end": 39, "dataset_mention": "The second customer interaction dataset"}, {"mentioned_in_paper": "591", "context_id": "57", "dataset_context": "Fourth, the approach is applied on an email marketing dataset and a search ad dataset.", "mention_start": 34, "mention_end": 61, "dataset_mention": "an email marketing dataset"}, {"mentioned_in_paper": "591", "context_id": "57", "dataset_context": "Fourth, the approach is applied on an email marketing dataset and a search ad dataset.", "mention_start": 34, "mention_end": 85, "dataset_mention": "an email marketing dataset and a search ad dataset"}, {"mentioned_in_paper": "591", "context_id": "178", "dataset_context": "The approach presented in Section 3 is tested on a real customer behavior dataset.", "mention_start": 49, "mention_end": 81, "dataset_mention": "a real customer behavior dataset"}, {"mentioned_in_paper": "591", "context_id": "190", "dataset_context": "The visits from the search channel are recorded in the web-analytics dataset as well.", "mention_start": 51, "mention_end": 76, "dataset_mention": "the web-analytics dataset"}, {"mentioned_in_paper": "591", "context_id": "197", "dataset_context": "The web-analytics dataset is used to create features to predict email and search related events of interest.", "mention_start": 0, "mention_end": 25, "dataset_mention": "The web-analytics dataset"}, {"mentioned_in_paper": "591", "context_id": "198", "dataset_context": "The email interaction dataset consists of information related to the emails sent by the organization to its customers.", "mention_start": 0, "mention_end": 29, "dataset_mention": "The email interaction dataset"}, {"mentioned_in_paper": "591", "context_id": "204", "dataset_context": "Each dataset described above uses the same identifier for the customer, this identifier is used for merging the datasets.", "mention_start": 99, "mention_end": 120, "dataset_mention": "merging the datasets"}, {"mentioned_in_paper": "591", "context_id": "214", "dataset_context": "The customer behavior dataset is a good source to partially validate the usefulness of the approach on a real-world data.", "mention_start": 0, "mention_end": 29, "dataset_mention": "The customer behavior dataset"}, {"mentioned_in_paper": "592", "context_id": "22", "dataset_context": "We also introduce a pre-training procedure that extends the LayoutLM to both English and non-English datasets by using semantic entities.", "mention_start": 72, "mention_end": 109, "dataset_mention": "both English and non-English datasets"}, {"mentioned_in_paper": "592", "context_id": "53", "dataset_context": "After that, the model was trained with the MVLM objective using a much smaller dataset (about 17,000 samples) with 100 epochs.", "mention_start": 63, "mention_end": 86, "dataset_mention": "a much smaller dataset"}, {"mentioned_in_paper": "592", "context_id": "58", "dataset_context": "Our experiments on internal datasets show that this transferred pre-training setting yields significantly better results than without using the word embedding weights from BERT.", "mention_start": 19, "mention_end": 36, "dataset_mention": "internal datasets"}, {"mentioned_in_paper": "592", "context_id": "60", "dataset_context": "Thus, we plan to verify our approach on the recently introduced XFUN dataset [14].", "mention_start": 63, "mention_end": 76, "dataset_mention": "XFUN dataset"}, {"mentioned_in_paper": "592", "context_id": "85", "dataset_context": "We can pre-train the Lay-outLM model seamlessly on multiple IE datasets with different key-value fields.", "mention_start": 51, "mention_end": 71, "dataset_mention": "multiple IE datasets"}, {"mentioned_in_paper": "592", "context_id": "86", "dataset_context": "To this end, we named this pre-training objective as Span extraction semantic pre-training, which allows us to pretrain LayoutLM with several small-sized IE-annotated datasets in low-resource languages, instead of using large scale document data such as [12].", "mention_start": 133, "mention_end": 175, "dataset_mention": "several small-sized IE-annotated datasets"}, {"mentioned_in_paper": "592", "context_id": "88", "dataset_context": "We use two internal self-collected datasets: one for pre-training the language model and one for evaluating IE performance as a downstream task.", "mention_start": 7, "mention_end": 43, "dataset_mention": "two internal self-collected datasets"}, {"mentioned_in_paper": "592", "context_id": "91", "dataset_context": "Japanese pre-training BizDocs is a self-collected dataset.", "mention_start": 33, "mention_end": 57, "dataset_mention": "a self-collected dataset"}, {"mentioned_in_paper": "592", "context_id": "97", "dataset_context": "Japanese invoice is another Invoice documents dataset that disjoints with the aforementioned pre-training data.", "mention_start": 20, "mention_end": 53, "dataset_mention": "another Invoice documents dataset"}, {"mentioned_in_paper": "592", "context_id": "111", "dataset_context": "This model is intended to be used with Japanese datasets.", "mention_start": 39, "mention_end": 56, "dataset_mention": "Japanese datasets"}, {"mentioned_in_paper": "592", "context_id": "112", "dataset_context": "To conduct our experiments, we firstly pre-train the JP-LayoutLM-base model on the Japanese Pre-training BizDocs dataset with the proposed span-extraction objectives.", "mention_start": 78, "mention_end": 120, "dataset_mention": "the Japanese Pre-training BizDocs dataset"}, {"mentioned_in_paper": "592", "context_id": "113", "dataset_context": "Then, we perform fine-tuning on two IE datasets: Japanese invoice and CORD with the pre-trained weight JP-LayoutLM-base and EN-LayoutLMbase respectively.", "mention_start": 31, "mention_end": 47, "dataset_mention": "two IE datasets"}, {"mentioned_in_paper": "592", "context_id": "141", "dataset_context": "This means that we can accumulate future IE datasets when fine-tuning as a continual learning scheme to further improve the base pre-train LayoutLM model.", "mention_start": 11, "mention_end": 52, "dataset_mention": "that we can accumulate future IE datasets"}, {"mentioned_in_paper": "592", "context_id": "147", "dataset_context": "The proposed method shows promising results on two IE datasets of business documents.", "mention_start": 47, "mention_end": 62, "dataset_mention": "two IE datasets"}, {"mentioned_in_paper": "593", "context_id": "34", "dataset_context": "\u2022 Politically polarized malicious entities: We identified numerous politically polarized entities which dominated our dataset of malicious pages, and published URLs from untrustworthy web domains.", "mention_start": 103, "mention_end": 125, "dataset_mention": "dominated our dataset"}, {"mentioned_in_paper": "593", "context_id": "74", "dataset_context": "We used the same dataset of Facebook posts to obtain a true positive dataset of pages posting malicious URLs.", "mention_start": 46, "mention_end": 76, "dataset_mention": "obtain a true positive dataset"}, {"mentioned_in_paper": "593", "context_id": "81", "dataset_context": "Researchers have adopted similar methodology in the past, where they identified malicious URLs in the 40 most recent Tweets of users to obtain a ground truth dataset of spammers on Twitter [32] The WOT API returns a reputation score for a given domain.", "mention_start": 135, "mention_end": 165, "dataset_mention": "obtain a ground truth dataset"}, {"mentioned_in_paper": "593", "context_id": "106", "dataset_context": "Table 3 provides a detailed description of the number of posts and pages along with their WOT categories in our malicious pages dataset of 627 pages.", "mention_start": 108, "mention_end": 135, "dataset_mention": "our malicious pages dataset"}, {"mentioned_in_paper": "593", "context_id": "254", "dataset_context": "To keep the network size comparable, we averaged out the results for 10 random samples of 627 benign pages each (same size as malicious pages dataset) drawn from the entire dataset of 1,278 benign pages.", "mention_start": 125, "mention_end": 149, "dataset_mention": "malicious pages dataset"}, {"mentioned_in_paper": "593", "context_id": "297", "dataset_context": "They studied a large anonymized dataset of 187 million asynchronous \"wall\" messages between Facebook users, and used a set of automated techniques to detect and characterize coordinated spam campaigns.", "mention_start": 0, "mention_end": 39, "dataset_mention": "They studied a large anonymized dataset"}, {"mentioned_in_paper": "594", "context_id": "2", "dataset_context": "Using a simple bag-ofwords model based image classification algorithm, we leveraged the performance of the deformable model objector from 35.9% to 39.5% in average precision over 20 categories on standard PASCAL VOC 2007 detection dataset.", "mention_start": 195, "mention_end": 238, "dataset_mention": "standard PASCAL VOC 2007 detection dataset"}, {"mentioned_in_paper": "594", "context_id": "13", "dataset_context": "This labor difference is more salient for large-scale image dataset: In the standard large scale ImageNet dataset [9], there are 14, 197, 122 images of 21, 841 synsets (object categories) labeled for the image classification task.", "mention_start": 71, "mention_end": 113, "dataset_mention": "the standard large scale ImageNet dataset"}, {"mentioned_in_paper": "594", "context_id": "24", "dataset_context": "Using a simple bag-of-words model based image classification algorithm, we leveraged the performance of the deformable model objector from 35.9% to 39.5% in average precision, which led to the state-of-the-art performance on the standard PASCAL VOC 2007 detection dataset.", "mention_start": 224, "mention_end": 271, "dataset_mention": "the standard PASCAL VOC 2007 detection dataset"}, {"mentioned_in_paper": "594", "context_id": "60", "dataset_context": "To demonstrate the advantage of our approach, we test the CLOD on the very challenging PASCAL Visual Object Challenge 2007 (VOC2007) datasets [1].", "mention_start": 86, "mention_end": 141, "dataset_mention": "PASCAL Visual Object Challenge 2007 (VOC2007) datasets"}, {"mentioned_in_paper": "594", "context_id": "61", "dataset_context": "First, we give a detailed description of VOC2007 dataset and the cropped dataset for our CLOD framework.", "mention_start": 40, "mention_end": 56, "dataset_mention": "VOC2007 dataset"}, {"mentioned_in_paper": "594", "context_id": "61", "dataset_context": "First, we give a detailed description of VOC2007 dataset and the cropped dataset for our CLOD framework.", "mention_start": 40, "mention_end": 80, "dataset_mention": "VOC2007 dataset and the cropped dataset"}, {"mentioned_in_paper": "594", "context_id": "62", "dataset_context": "Then, we evaluate our classification algorithm on PASCAL VOC2007 classification dataset.", "mention_start": 49, "mention_end": 87, "dataset_mention": "PASCAL VOC2007 classification dataset"}, {"mentioned_in_paper": "594", "context_id": "63", "dataset_context": "After that we compare the CLOD performance with the state the art detection performance on PASCAL VOC2007 detection dataset.", "mention_start": 91, "mention_end": 123, "dataset_mention": "PASCAL VOC2007 detection dataset"}, {"mentioned_in_paper": "594", "context_id": "64", "dataset_context": "A. Datasets and Metrics 1) Datasets: PASCAL VOC2007 datasets [1] has 20 categories, containing 9,963 images and 24,640 objects.", "mention_start": 0, "mention_end": 11, "dataset_mention": "A. Datasets"}, {"mentioned_in_paper": "594", "context_id": "64", "dataset_context": "A. Datasets and Metrics 1) Datasets: PASCAL VOC2007 datasets [1] has 20 categories, containing 9,963 images and 24,640 objects.", "mention_start": 0, "mention_end": 35, "dataset_mention": "A. Datasets and Metrics 1) Datasets"}, {"mentioned_in_paper": "594", "context_id": "64", "dataset_context": "A. Datasets and Metrics 1) Datasets: PASCAL VOC2007 datasets [1] has 20 categories, containing 9,963 images and 24,640 objects.", "mention_start": 36, "mention_end": 60, "dataset_mention": " PASCAL VOC2007 datasets"}, {"mentioned_in_paper": "594", "context_id": "70", "dataset_context": "We prepare a regionlevel dataset by cropping the detection ground truth boxes according the detection annotations.", "mention_start": 0, "mention_end": 32, "dataset_mention": "We prepare a regionlevel dataset"}, {"mentioned_in_paper": "594", "context_id": "71", "dataset_context": "This cropped dataset contains \"train\" and \"val\".", "mention_start": 0, "mention_end": 20, "dataset_mention": "This cropped dataset"}, {"mentioned_in_paper": "594", "context_id": "74", "dataset_context": "We refer this region-level dataset as pure ground-truth-box set.", "mention_start": 0, "mention_end": 34, "dataset_mention": "We refer this region-level dataset"}, {"mentioned_in_paper": "594", "context_id": "78", "dataset_context": "For this task, the positive examples are the sum of ground truth in \"train\" and \"val\" , and negative examples are a random selection from the false alarms from the detection boxes in the \"trainval\" dataset.", "mention_start": 182, "mention_end": 205, "dataset_mention": "the \"trainval\" dataset"}, {"mentioned_in_paper": "594", "context_id": "79", "dataset_context": "We refer this region-level dataset as groundtruth-false-alarms set.", "mention_start": 0, "mention_end": 34, "dataset_mention": "We refer this region-level dataset"}, {"mentioned_in_paper": "594", "context_id": "90", "dataset_context": "In this section, we first tune the parameters of our classification algorithm using the image-level dataset and compare the performance with other state-of-the-art classification algorithms.", "mention_start": 83, "mention_end": 107, "dataset_mention": "the image-level dataset"}, {"mentioned_in_paper": "594", "context_id": "99", "dataset_context": "2) Region-level Classification: From Section III-A1, there are two kinds of region-level dataset.With the exact same experiment setup, we train our region-level classifier on the \"train\" + \"val\" subsets of the cropped ground truth dataset and support-region dataset.", "mention_start": 75, "mention_end": 96, "dataset_mention": "region-level dataset"}, {"mentioned_in_paper": "594", "context_id": "99", "dataset_context": "2) Region-level Classification: From Section III-A1, there are two kinds of region-level dataset.With the exact same experiment setup, we train our region-level classifier on the \"train\" + \"val\" subsets of the cropped ground truth dataset and support-region dataset.", "mention_start": 205, "mention_end": 238, "dataset_mention": "the cropped ground truth dataset"}, {"mentioned_in_paper": "594", "context_id": "99", "dataset_context": "2) Region-level Classification: From Section III-A1, there are two kinds of region-level dataset.With the exact same experiment setup, we train our region-level classifier on the \"train\" + \"val\" subsets of the cropped ground truth dataset and support-region dataset.", "mention_start": 205, "mention_end": 265, "dataset_mention": "the cropped ground truth dataset and support-region dataset"}, {"mentioned_in_paper": "594", "context_id": "108", "dataset_context": "But it is quite interesting that the classifier from the pure ground-truth-box dataset is better than the classifier from ground-truth-false-alarms set.", "mention_start": 53, "mention_end": 86, "dataset_mention": "the pure ground-truth-box dataset"}, {"mentioned_in_paper": "594", "context_id": "110", "dataset_context": "Therefore, by taking all of the above into consideration, we choose the classification classifier from the pure-ground-truth dataset as our classification classifier in our CLOD framework.", "mention_start": 102, "mention_end": 132, "dataset_mention": "the pure-ground-truth dataset"}, {"mentioned_in_paper": "594", "context_id": "117", "dataset_context": "Table III compares the CLOD with other state-of-the-art performance on PASCAL VOC 2007 detection dataset.", "mention_start": 71, "mention_end": 104, "dataset_mention": "PASCAL VOC 2007 detection dataset"}, {"mentioned_in_paper": "594", "context_id": "120", "dataset_context": "The Figure 3 shows detailed precision-recall curve of CLOD and original detection algorithm over 20 categories in PASCAL VOC2007 detection dataset.", "mention_start": 114, "mention_end": 146, "dataset_mention": "PASCAL VOC2007 detection dataset"}, {"mentioned_in_paper": "595", "context_id": "4", "dataset_context": "By experiments with the COMPAS dataset on recidivism prediction, our algorithm CorelsEnum successfully enumerated all of several tens of thousands of good rule lists of length at most = 3 in around 1,000 seconds, while a state-of-the-art top-K rule list learner based on Lawler's method combined with CORELS, proposed by Hara and Ishihata in 2018, found only 40 models until the timeout of 6,000 seconds.", "mention_start": 20, "mention_end": 38, "dataset_mention": "the COMPAS dataset"}, {"mentioned_in_paper": "595", "context_id": "11", "dataset_context": "Table 1 : An example of a pair of competing rule lists of length = 3 with similar accuracies, 62.5% and 60.9%, for predicting two-year recidivism on the COMPAS dataset.", "mention_start": 148, "mention_end": 167, "dataset_mention": "the COMPAS dataset"}, {"mentioned_in_paper": "595", "context_id": "20", "dataset_context": "For example, we show in Table 1 a part of results of experiments in Sec. 5 on the COMPAS dataset [3] for the task of predicting two-year recidivism.", "mention_start": 77, "mention_end": 96, "dataset_mention": "the COMPAS dataset"}, {"mentioned_in_paper": "595", "context_id": "42", "dataset_context": "\u2022 By experiments on the COMPAS dataset [3],with a large value of \u03b5 = 15%, our CorelsEnum successfully computed the Rashomon set R \u03b5 of around 23,354 all good rule lists of length at most = 3 in 1,000 seconds, while the previous one for top-K rule lists, Corel-sLawler [11], listed only top-40 rule lists before the timeout of 6,000 seconds.", "mention_start": 20, "mention_end": 38, "dataset_mention": "the COMPAS dataset"}, {"mentioned_in_paper": "595", "context_id": "170", "dataset_context": "In this section, we analyze the class of rule lists on the COMPAS dataset [3] through the lens of the Rashomon effect using our proposed algorithm.", "mention_start": 54, "mention_end": 73, "dataset_mention": "the COMPAS dataset"}, {"mentioned_in_paper": "595", "context_id": "190", "dataset_context": "Next, we examine the predictive multiplicity of the Rashomon set on the COMPAS dataset.", "mention_start": 67, "mention_end": 86, "dataset_mention": "the COMPAS dataset"}, {"mentioned_in_paper": "595", "context_id": "198", "dataset_context": "Fig. 4 shows the histogram of these values with \u03b5 = 1% on the training datasetg S and test dataset S .", "mention_start": 71, "mention_end": 98, "dataset_mention": "datasetg S and test dataset"}, {"mentioned_in_paper": "595", "context_id": "208", "dataset_context": "To evaluate the usefulness of CorelsEnum, we conducted experiments on the COMPAS dataset, and analyzed the computed Rashomon set of the rule lists from the perspectives of predictive multiplicity and fairness.", "mention_start": 69, "mention_end": 88, "dataset_mention": "the COMPAS dataset"}, {"mentioned_in_paper": "596", "context_id": "5", "dataset_context": "We conduct experiments on three publicly available Crowd Counting datasets, and achieve significant improvement over the previous approaches.", "mention_start": 26, "mention_end": 74, "dataset_mention": "three publicly available Crowd Counting datasets"}, {"mentioned_in_paper": "596", "context_id": "29", "dataset_context": "2. Our proposed approach achieves state-of-art performance on three challenging crowd counting datasets and reduces the mean absolute error by 20 % and 15 % on UCF QNRF [13] and UCF CC [12] datasets.", "mention_start": 80, "mention_end": 103, "dataset_mention": "crowd counting datasets"}, {"mentioned_in_paper": "596", "context_id": "29", "dataset_context": "2. Our proposed approach achieves state-of-art performance on three challenging crowd counting datasets and reduces the mean absolute error by 20 % and 15 % on UCF QNRF [13] and UCF CC [12] datasets.", "mention_start": 160, "mention_end": 198, "dataset_mention": "UCF QNRF [13] and UCF CC [12] datasets"}, {"mentioned_in_paper": "596", "context_id": "43", "dataset_context": "However, these approaches also use hand crafted features, and do not perform well on the more recent crowd counting datasets [30].", "mention_start": 84, "mention_end": 124, "dataset_mention": "the more recent crowd counting datasets"}, {"mentioned_in_paper": "596", "context_id": "87", "dataset_context": "We use the VGG-16 network [24] trained on the Imagenet dataset to initialize the convolution layers in local feature block.", "mention_start": 42, "mention_end": 62, "dataset_mention": "the Imagenet dataset"}, {"mentioned_in_paper": "596", "context_id": "128", "dataset_context": "Performance of various methods on the UCF-QNRF dataset.", "mention_start": 34, "mention_end": 54, "dataset_mention": "the UCF-QNRF dataset"}, {"mentioned_in_paper": "596", "context_id": "137", "dataset_context": "UCF-QNRF [13] is the largest annotated crowd counting dataset with 1535 crowd images and 1.2 million annotations.", "mention_start": 17, "mention_end": 61, "dataset_mention": "the largest annotated crowd counting dataset"}, {"mentioned_in_paper": "596", "context_id": "147", "dataset_context": "We normalize the training and test images using the mean and variance values computed on the ImageNet dataset.", "mention_start": 89, "mention_end": 109, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "596", "context_id": "148", "dataset_context": "We initialize the five convolutional layers in the local feature block by using the first 5 layers from a Vgg Net [24] trained on the ImageNet dataset.", "mention_start": 130, "mention_end": 150, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "596", "context_id": "155", "dataset_context": "In Table 2, we show the results for ablation study conducted on the UCF-QNRF dataset [13].", "mention_start": 63, "mention_end": 84, "dataset_mention": "the UCF-QNRF dataset"}, {"mentioned_in_paper": "596", "context_id": "163", "dataset_context": "To analyze the effects of varying the context information, we conduct experiments on the UCF-QNRF dataset [13].", "mention_start": 84, "mention_end": 105, "dataset_mention": "the UCF-QNRF dataset"}, {"mentioned_in_paper": "596", "context_id": "172", "dataset_context": "The UCF Crowd Counting dataset [12] contains 50 crowd images with widely varying crowd count.", "mention_start": 0, "mention_end": 30, "dataset_mention": "The UCF Crowd Counting dataset"}, {"mentioned_in_paper": "596", "context_id": "176", "dataset_context": "Since the dataset is small in comparison to the UCF-QNRF dataset, we use the CTN network trained on UCF-QNRF to initialize the model.", "mention_start": 44, "mention_end": 64, "dataset_mention": "the UCF-QNRF dataset"}, {"mentioned_in_paper": "596", "context_id": "181", "dataset_context": "The Shanghaitech crowd counting dataset contains of two parts.", "mention_start": 0, "mention_end": 39, "dataset_mention": "The Shanghaitech crowd counting dataset"}, {"mentioned_in_paper": "596", "context_id": "186", "dataset_context": "Since the dataset is small in comparison to the UCF-QNRF dataset, we use the CTN network trained on UCF-QNRF for initializing the model.", "mention_start": 44, "mention_end": 64, "dataset_mention": "the UCF-QNRF dataset"}, {"mentioned_in_paper": "596", "context_id": "189", "dataset_context": "CTN outperforms all the approaches approaches except SANet [6] on both Part-A and Part-B datasets.", "mention_start": 66, "mention_end": 97, "dataset_mention": "both Part-A and Part-B datasets"}, {"mentioned_in_paper": "596", "context_id": "198", "dataset_context": "We show some qualitative results on UCF-QNRF [13] dataset in Figure 4.", "mention_start": 36, "mention_end": 57, "dataset_mention": "UCF-QNRF [13] dataset"}, {"mentioned_in_paper": "597", "context_id": "106", "dataset_context": "We initialize the visual Backbone V and the language Backbone L using the ResNet-101 [13] model pre-trained from ImageNet [5] and the RoBERTa model pre-trained from language corpus datasets, respectively.", "mention_start": 165, "mention_end": 189, "dataset_mention": "language corpus datasets"}, {"mentioned_in_paper": "597", "context_id": "130", "dataset_context": "In RefCOCO+ dataset, positional words are not allowed in the referring expression, which is a pure dataset with appearance-based referring expression, whereas RefCOCO imposes no restriction on the phrase.", "mention_start": 3, "mention_end": 19, "dataset_mention": "RefCOCO+ dataset"}, {"mentioned_in_paper": "597", "context_id": "130", "dataset_context": "In RefCOCO+ dataset, positional words are not allowed in the referring expression, which is a pure dataset with appearance-based referring expression, whereas RefCOCO imposes no restriction on the phrase.", "mention_start": 91, "mention_end": 106, "dataset_mention": "a pure dataset"}, {"mentioned_in_paper": "597", "context_id": "200", "dataset_context": "We found that SiRi could further improve the performance of TransVG by an average of 2% at top-1 accuracy on all four REC datasets, and the performance has also been effectively improved on Phrase Grounding dataset Flickr30k dataset.", "mention_start": 109, "mention_end": 130, "dataset_mention": "all four REC datasets"}, {"mentioned_in_paper": "597", "context_id": "200", "dataset_context": "We found that SiRi could further improve the performance of TransVG by an average of 2% at top-1 accuracy on all four REC datasets, and the performance has also been effectively improved on Phrase Grounding dataset Flickr30k dataset.", "mention_start": 189, "mention_end": 214, "dataset_mention": "Phrase Grounding dataset"}, {"mentioned_in_paper": "597", "context_id": "200", "dataset_context": "We found that SiRi could further improve the performance of TransVG by an average of 2% at top-1 accuracy on all four REC datasets, and the performance has also been effectively improved on Phrase Grounding dataset Flickr30k dataset.", "mention_start": 189, "mention_end": 232, "dataset_mention": "Phrase Grounding dataset Flickr30k dataset"}, {"mentioned_in_paper": "597", "context_id": "201", "dataset_context": "For LAVT [47], We report the results of SiRi in RES dataset RefCOCO+ three splits val, testA, testB in Table 5.", "mention_start": 47, "mention_end": 59, "dataset_mention": "RES dataset"}, {"mentioned_in_paper": "597", "context_id": "210", "dataset_context": "We test the SiRi model on three RES datasets, i.e., RefCOCO, RefCOCO+, RefCOCOg.", "mention_start": 26, "mention_end": 44, "dataset_mention": "three RES datasets"}, {"mentioned_in_paper": "597", "context_id": "215", "dataset_context": "We evaluate the SiRi mechanism on the Flickr30k entities dataset.", "mention_start": 34, "mention_end": 64, "dataset_mention": "the Flickr30k entities dataset"}, {"mentioned_in_paper": "597", "context_id": "231", "dataset_context": "Extensive experiments prove our method helps the Transformer encoder better perceive the relationship between the visual and the corresponding expression, outperforming state-of-the-art methods on the three visual grounding datasets.", "mention_start": 196, "mention_end": 232, "dataset_mention": "the three visual grounding datasets"}, {"mentioned_in_paper": "598", "context_id": "2", "dataset_context": "First, we used the Los Alamos National Laboratory (LANL) Memento Aggregator to collect mementos of an initial set of URIs obtained from four sources: (a) the Moz Top 500, (b) the dataset used in our previous study, (c) the HTTP Archive, and (d) the Web Archives for Historical Research group.", "mention_start": 170, "mention_end": 186, "dataset_mention": " (b) the dataset"}, {"mentioned_in_paper": "598", "context_id": "37", "dataset_context": "The main purpose of this paper is to document how the dataset of mementos was created so it can be reused by other studies.", "mention_start": 37, "mention_end": 61, "dataset_mention": "document how the dataset"}, {"mentioned_in_paper": "599", "context_id": "6", "dataset_context": "The collected speech dataset will be made publicly available once ready.", "mention_start": 14, "mention_end": 28, "dataset_mention": "speech dataset"}, {"mentioned_in_paper": "599", "context_id": "10", "dataset_context": "The main goal of this research project is the automatic detection of trust from speech, and in this paper, we focus on the collection of a speech trust dataset.", "mention_start": 136, "mention_end": 159, "dataset_mention": "a speech trust dataset"}, {"mentioned_in_paper": "599", "context_id": "132", "dataset_context": "Therefore, the collected speech corpus, the Trust-UBA Database, shall be a valuable contribution to the research community, as a useful first dataset for studying trust in speech.", "mention_start": 126, "mention_end": 149, "dataset_mention": "a useful first dataset"}, {"mentioned_in_paper": "601", "context_id": "46", "dataset_context": "For the experiment, we constructed a dataset of images controlled using natural language instruction by using MNIST [7] dataset and manually created modifications.", "mention_start": 19, "mention_end": 44, "dataset_mention": " we constructed a dataset"}, {"mentioned_in_paper": "601", "context_id": "46", "dataset_context": "For the experiment, we constructed a dataset of images controlled using natural language instruction by using MNIST [7] dataset and manually created modifications.", "mention_start": 109, "mention_end": 127, "dataset_mention": "MNIST [7] dataset"}, {"mentioned_in_paper": "602", "context_id": "158", "dataset_context": "We evaluate CKConv on ShapeNetPart [47] dataset for object part segmentation.", "mention_start": 22, "mention_end": 47, "dataset_mention": "ShapeNetPart [47] dataset"}, {"mentioned_in_paper": "603", "context_id": "119", "dataset_context": "We follow this approach, but extend it so that attributes for each object (e.g., color, shape and material, as in the CLEVR dataset) can be specified.", "mention_start": 113, "mention_end": 131, "dataset_mention": "the CLEVR dataset"}, {"mentioned_in_paper": "603", "context_id": "122", "dataset_context": "Finally, our experiments on non CLEVR datasets simply we use a pre-trained LostGAN [50] model.", "mention_start": 27, "mention_end": 46, "dataset_mention": "non CLEVR datasets"}, {"mentioned_in_paper": "603", "context_id": "190", "dataset_context": "To quantify this, we evaluate on the Packed COCO and VG datasets.", "mention_start": 32, "mention_end": 64, "dataset_mention": "the Packed COCO and VG datasets"}, {"mentioned_in_paper": "603", "context_id": "207", "dataset_context": "Semantically Equivalent Noisy SGs mIOU R@0.3 R@0.5 mIOU R@0.3 R@0.5 Sg2Im [17] COCO dataset.", "mention_start": 0, "mention_end": 91, "dataset_mention": "Semantically Equivalent Noisy SGs mIOU R@0.3 R@0.5 mIOU R@0.3 R@0.5 Sg2Im [17] COCO dataset"}, {"mentioned_in_paper": "603", "context_id": "216", "dataset_context": "We further note that our model achieves good results on the VG dataset, which was manually annotated, suggesting it is robust to annotation noise.", "mention_start": 56, "mention_end": 70, "dataset_mention": "the VG dataset"}, {"mentioned_in_paper": "603", "context_id": "222", "dataset_context": "Inception Human COCO VG CLEVR Sg2Im [17] 5.4 \u00b1 0.3 7.6 \u00b1 1.0 3.2% WSGC (ours) 5.6 \u00b1 0.1 8.0 \u00b1 1.1 96.8% GT Layout 5.5 \u00b1 0.4 8.2 \u00b1 1.0 -Table 3 : Results for SG-to-image on Packed datasets (16+ objects).", "mention_start": 171, "mention_end": 187, "dataset_mention": "Packed datasets"}, {"mentioned_in_paper": "603", "context_id": "284", "dataset_context": "Table 4 shows a comparison of WSGC-E and WSGC-S on the standard COCO and VG datasets, where WSGC-E runs in a reasonable time so that comparison is possible.", "mention_start": 51, "mention_end": 84, "dataset_mention": "the standard COCO and VG datasets"}, {"mentioned_in_paper": "603", "context_id": "285", "dataset_context": "The size of the graphs on the standard datasets is less than an average of 1000 triplets per image, while on the packed datasets it is 24, 000 triplets per image.", "mention_start": 108, "mention_end": 128, "dataset_mention": "the packed datasets"}, {"mentioned_in_paper": "603", "context_id": "286", "dataset_context": "Thus it is impossible to run the WSGC-E on packed datasets.", "mention_start": 43, "mention_end": 58, "dataset_mention": "packed datasets"}, {"mentioned_in_paper": "603", "context_id": "292", "dataset_context": "Inspecting the converse weights p conv that were learned on the standard COCO dataset reveals that all weights have converged to values close to 0 and 1, and align well with the expected true converse relation.", "mention_start": 60, "mention_end": 85, "dataset_mention": "the standard COCO dataset"}, {"mentioned_in_paper": "603", "context_id": "318", "dataset_context": "For the CLEVR dataset [18], we use a novel generator, which we refer to as AttSPADE.", "mention_start": 4, "mention_end": 21, "dataset_mention": "the CLEVR dataset"}, {"mentioned_in_paper": "603", "context_id": "335", "dataset_context": "Evaluation is done on the COCO-Stuff and VG datasets.", "mention_start": 22, "mention_end": 52, "dataset_mention": "the COCO-Stuff and VG datasets"}, {"mentioned_in_paper": "603", "context_id": "354", "dataset_context": "In addition, our end-to-end model, which includes the WSGC and AttSPADE model, outperforms most of the baselines on the COCO and Visual Genome datasets.", "mention_start": 115, "mention_end": 151, "dataset_mention": "the COCO and Visual Genome datasets"}, {"mentioned_in_paper": "603", "context_id": "373", "dataset_context": "Here we describe the specific characteristics of the packed datasets presented in the paper.", "mention_start": 49, "mention_end": 68, "dataset_mention": "the packed datasets"}, {"mentioned_in_paper": "603", "context_id": "374", "dataset_context": "For every packed dataset, only samples with at least 16 objects per image were included.", "mention_start": 4, "mention_end": 24, "dataset_mention": "every packed dataset"}, {"mentioned_in_paper": "603", "context_id": "376", "dataset_context": "For VG, since Standard VG contains a limited number of relations we supplement the dataset with relations as follows.", "mention_start": 54, "mention_end": 90, "dataset_mention": "relations we supplement the dataset"}, {"mentioned_in_paper": "604", "context_id": "7", "dataset_context": "We report the results of our experiments on six major image classification datasets and show that the proposed approach significantly outperforms the baseline algorithms.", "mention_start": 44, "mention_end": 83, "dataset_mention": "six major image classification datasets"}, {"mentioned_in_paper": "604", "context_id": "23", "dataset_context": "Class imbalance is avoided in nearly all competitive datasets during the evaluation and training procedures (see Fig. 1).", "mention_start": 30, "mention_end": 61, "dataset_mention": "nearly all competitive datasets"}, {"mentioned_in_paper": "604", "context_id": "27", "dataset_context": "For example, for a fine-grained coral categorization dataset, endangered coral species have a significantly lower representation compared to the more abundant ones [4].", "mention_start": 16, "mention_end": 60, "dataset_mention": "a fine-grained coral categorization dataset"}, {"mentioned_in_paper": "604", "context_id": "47", "dataset_context": "4-The proposed approach has been extensively tested on six major classification datasets and has shown to outperform baseline procedures and stateof-the-art approaches (Sec.", "mention_start": 55, "mention_end": 88, "dataset_mention": "six major classification datasets"}, {"mentioned_in_paper": "604", "context_id": "77", "dataset_context": "A scaling kernel along-with the standard SVM was used in [22] to improve the generalization ability of learned classifiers for skewed datasets.", "mention_start": 127, "mention_end": 142, "dataset_mention": "skewed datasets"}, {"mentioned_in_paper": "604", "context_id": "268", "dataset_context": "The class imbalance problem is present in nearly all realworld object and image datasets.", "mention_start": 42, "mention_end": 88, "dataset_mention": "nearly all realworld object and image datasets"}, {"mentioned_in_paper": "604", "context_id": "271", "dataset_context": "Consequently, from the perspective of class imbalance, the currently available image classification datasets can be divided into three categories:", "mention_start": 54, "mention_end": 108, "dataset_mention": " the currently available image classification datasets"}, {"mentioned_in_paper": "604", "context_id": "272", "dataset_context": "1) Datasets with a significant class imbalance both in the training and the testing split (e.g., DIL, MLC), 2) Datasets with unbalanced class distributions but with experimental protocols that are designed in a way that an equal number of images from all classes are used during the training process (e.g., MIT-67, Caltech-101).", "mention_start": 0, "mention_end": 11, "dataset_mention": "1) Datasets"}, {"mentioned_in_paper": "604", "context_id": "272", "dataset_context": "1) Datasets with a significant class imbalance both in the training and the testing split (e.g., DIL, MLC), 2) Datasets with unbalanced class distributions but with experimental protocols that are designed in a way that an equal number of images from all classes are used during the training process (e.g., MIT-67, Caltech-101).", "mention_start": 107, "mention_end": 119, "dataset_mention": " 2) Datasets"}, {"mentioned_in_paper": "604", "context_id": "288", "dataset_context": "1) Imbalanced Datasets: Melanoma Detection : Edinburgh Dermofit Image Library (DIL) consists of 1300 high quality skin lesion images based on diagnosis from dermatologists and dermatopathologists.", "mention_start": 0, "mention_end": 22, "dataset_mention": "1) Imbalanced Datasets"}, {"mentioned_in_paper": "604", "context_id": "299", "dataset_context": "2) Imbalanced Datasets-Balanced Protocols: Object Classification: Caltech-101 contains a total of 9,144 images, divided into 102 categories (101 objects + background).", "mention_start": 0, "mention_end": 22, "dataset_mention": "2) Imbalanced Datasets"}, {"mentioned_in_paper": "604", "context_id": "307", "dataset_context": "We will, however, evaluate our approach both on the standard split (80 images for training, 20 for testing) and the complete dataset with imbalanced train/test splits of 60%/40% and 30%/70%.", "mention_start": 98, "mention_end": 132, "dataset_mention": "testing) and the complete dataset"}, {"mentioned_in_paper": "604", "context_id": "308", "dataset_context": "3) Balanced Datasets-Balanced Protocols: Handwritten Digit Classification: MNIST consists of 70,000 images of digits (0-9).", "mention_start": 0, "mention_end": 20, "dataset_mention": "3) Balanced Datasets"}, {"mentioned_in_paper": "604", "context_id": "332", "dataset_context": " [40] 99.7% Maxout NIN [41] 99.8%  dataset, we perform 3-fold cross validation on the 5 classes (namely Actinic Keratosis, Basal Cell Carcinoma, Melanocytic Nevus, Squamous Cell Carcinoma and Seborrhoeic Keratosis) comprising of a total of 960 images.", "mention_start": 0, "mention_end": 42, "dataset_mention": " [40] 99.7% Maxout NIN [41] 99.8%  dataset"}, {"mentioned_in_paper": "604", "context_id": "333", "dataset_context": "In the second experiment, we perform 3-fold cross validation on all of the 10 classes in the DIL dataset.", "mention_start": 88, "mention_end": 104, "dataset_mention": "the DIL dataset"}, {"mentioned_in_paper": "604", "context_id": "335", "dataset_context": "For the MLC dataset, in the first experiment we train on two-thirds of the data from 2008 and test on the remaining one third.", "mention_start": 4, "mention_end": 19, "dataset_mention": "the MLC dataset"}, {"mentioned_in_paper": "604", "context_id": "339", "dataset_context": "For similar reasons, we used the RGB color space instead of LAB, which was shown to perform better on the MLC dataset [4].", "mention_start": 101, "mention_end": 117, "dataset_mention": "the MLC dataset"}, {"mentioned_in_paper": "604", "context_id": "345", "dataset_context": "For example, the confusion matrices for DIL and MLC datasets in Fig. 6 (corresponding to Exp. 1 and Exp. 2 respectively), show an improvement of 9.5% and 11.8% in the average class accuracy.", "mention_start": 39, "mention_end": 60, "dataset_mention": "DIL and MLC datasets"}, {"mentioned_in_paper": "604", "context_id": "353", "dataset_context": "Note that for the MNIST digit dataset, nearly all the top performing approaches use distortions (affine and/or elastic) and data augmentation to achieve a significant boost in performance.", "mention_start": 14, "mention_end": 37, "dataset_mention": "the MNIST digit dataset"}, {"mentioned_in_paper": "604", "context_id": "357", "dataset_context": "We report our results on the standard splits (Tables V, VI), to compare with the state-of-the-art approaches, and show that our results are superior to the state-ofthe-art on MIT-67 and competitive on the Caltech-101 dataset.", "mention_start": 200, "mention_end": 224, "dataset_mention": "the Caltech-101 dataset"}, {"mentioned_in_paper": "605", "context_id": "101", "dataset_context": "The Market1501 dataset is composed of 32668 pedestrian images taken by 6 cameras, a total of 1501 categories.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The Market1501 dataset"}, {"mentioned_in_paper": "605", "context_id": "103", "dataset_context": "The DukeMTMC-reID dataset was captured by 8 cameras, including 16522 training images of 702 identities, 2228 query images of 702 identities and 17661 gallery images of 1110 identities.", "mention_start": 0, "mention_end": 25, "dataset_mention": "The DukeMTMC-reID dataset"}, {"mentioned_in_paper": "605", "context_id": "104", "dataset_context": "The CUHK03 dataset is divided into CUHK03 labeled and CUHK03 detected according to different annotation methods,     Comparison between q h6 and C(q h6 , q h4 , q h2 ) of which there are 14096 images and 14097 images, respectively, which were captured by two cameras.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The CUHK03 dataset"}, {"mentioned_in_paper": "605", "context_id": "115", "dataset_context": "Training our model takes about sixteen, eighteen and eight hours with a singal NVIDIA Tesla P100 GPU for the Market1501, DukeMTMC-reID, and CUHK03 datasets, respectively.", "mention_start": 135, "mention_end": 155, "dataset_mention": " and CUHK03 datasets"}, {"mentioned_in_paper": "605", "context_id": "120", "dataset_context": "We conducted a large number of comparative experiments on Market-1501, DukeMTMC-reID and CUHK03 datasets to study the effectiveness of each branch, module and hyperparameters.", "mention_start": 70, "mention_end": 104, "dataset_mention": " DukeMTMC-reID and CUHK03 datasets"}, {"mentioned_in_paper": "605", "context_id": "121", "dataset_context": " 2, competitive results are obtained on the three data sets, especially on the CUHK03 dataset.", "mention_start": 74, "mention_end": 93, "dataset_mention": "the CUHK03 dataset"}, {"mentioned_in_paper": "606", "context_id": "197", "dataset_context": "Hence, we follow a similar approach as in [69] and simulate MOT data from the Crowd-Human [45] person detection dataset.", "mention_start": 73, "mention_end": 119, "dataset_mention": "the Crowd-Human [45] person detection dataset"}, {"mentioned_in_paper": "606", "context_id": "204", "dataset_context": "To avoid overfitting to the small MOT17 dataset, we then fine-tune for additional 40 epochs on the combined CrowdHuman and MOT17 datasets.", "mention_start": 24, "mention_end": 47, "dataset_mention": "the small MOT17 dataset"}, {"mentioned_in_paper": "606", "context_id": "204", "dataset_context": "To avoid overfitting to the small MOT17 dataset, we then fine-tune for additional 40 epochs on the combined CrowdHuman and MOT17 datasets.", "mention_start": 107, "mention_end": 137, "dataset_mention": "CrowdHuman and MOT17 datasets"}, {"mentioned_in_paper": "606", "context_id": "217", "dataset_context": "We indicated additional training Data: CH=CrowdHuman [45], PD=Parallel Domain [50], 6M=6 tracking datasets as in [66], JTA [14], M=Market1501 [67] and C=CUHK03 [27].", "mention_start": 83, "mention_end": 106, "dataset_mention": " 6M=6 tracking datasets"}, {"mentioned_in_paper": "606", "context_id": "226", "dataset_context": "Only [50, 53, 54] which follow [66] and pretrain on 6 additional tracking datasets (6M) surpass our performance.", "mention_start": 51, "mention_end": 82, "dataset_mention": "6 additional tracking datasets"}, {"mentioned_in_paper": "606", "context_id": "231", "dataset_context": "However, we surpass or perform on-par with several modern methods [55, 66, 68] that were trained on significantly more data, i.e., 6 additional tracking datasets (6M).", "mention_start": 130, "mention_end": 161, "dataset_mention": " 6 additional tracking datasets"}, {"mentioned_in_paper": "606", "context_id": "246", "dataset_context": "Our full pipeline including pretraining on the CrowdHuman dataset provides a MOTA and IDF1 of 71.3 and 73.4,", "mention_start": 43, "mention_end": 65, "dataset_mention": "the CrowdHuman dataset"}, {"mentioned_in_paper": "608", "context_id": "79", "dataset_context": "Second, there are no pretrained models which include a decoder stage trained on the classification task of ILSVRC CLS-LOC dataset [25] because classification gives a single whole image label instead of a local label as in detection.", "mention_start": 106, "mention_end": 129, "dataset_mention": "ILSVRC CLS-LOC dataset"}, {"mentioned_in_paper": "608", "context_id": "81", "dataset_context": "The model pre-trained on the classification task of ILSVRC CLS-LOC dataset [25] makes the accuracy of our detector higher and converge faster compared to a randomly initialized model.", "mention_start": 52, "mention_end": 74, "dataset_mention": "ILSVRC CLS-LOC dataset"}, {"mentioned_in_paper": "608", "context_id": "105", "dataset_context": "Our experiments are all based on Residual-101 [14], which is pre-trained on the ILSVRC CLS-LOC dataset [25].", "mention_start": 75, "mention_end": 102, "dataset_mention": "the ILSVRC CLS-LOC dataset"}, {"mentioned_in_paper": "609", "context_id": "3", "dataset_context": "We will address this problem by presenting a heuristic approach to author name disambiguation in bibliometric datasets for largescale research assessments.", "mention_start": 97, "mention_end": 118, "dataset_mention": "bibliometric datasets"}, {"mentioned_in_paper": "609", "context_id": "42", "dataset_context": "In Section 3 we will illustrate the field of application and dataset used in the study, while Section 4 will present the general scheme and the algorithmic details of the heuristic approach proposed.", "mention_start": 45, "mention_end": 68, "dataset_mention": "application and dataset"}, {"mentioned_in_paper": "609", "context_id": "132", "dataset_context": "Our approach provides appealing expandability properties, since it requires only minimal manual information that is essentially stable over time to process an evolving bibliometric dataset.", "mention_start": 147, "mention_end": 188, "dataset_mention": "process an evolving bibliometric dataset"}, {"mentioned_in_paper": "609", "context_id": "137", "dataset_context": "Finally, our method provides a viable way to efficiently integrate external data into existing bibliometric databases and provide a cleaner and richer dataset for bibliometric applications.", "mention_start": 129, "mention_end": 158, "dataset_mention": "a cleaner and richer dataset"}, {"mentioned_in_paper": "609", "context_id": "192", "dataset_context": "Since the final objective is actually to have a robust dataset for reliable bibliometric analysis of the very university system, this operation contributes to eliminating some of the noise that would be present in the analysis.", "mention_start": 46, "mention_end": 62, "dataset_mention": "a robust dataset"}, {"mentioned_in_paper": "609", "context_id": "227", "dataset_context": "In the chosen dataset, the number of cases in question is certainly negligible, with perfect homonyms equal to 0.043% of the total researches in the Italian university system.", "mention_start": 3, "mention_end": 21, "dataset_mention": "the chosen dataset"}, {"mentioned_in_paper": "609", "context_id": "292", "dataset_context": "Reviewing the application of the algorithm to the bibliometric dataset under examination, the most aggressive filter is definitely the first, dealing with the author addresses (Table 3).", "mention_start": 46, "mention_end": 70, "dataset_mention": "the bibliometric dataset"}, {"mentioned_in_paper": "609", "context_id": "336", "dataset_context": "These values are fully comparable with those obtained in the previous section, based on a sample observation of the entire national dataset.", "mention_start": 111, "mention_end": 139, "dataset_mention": "the entire national dataset"}, {"mentioned_in_paper": "610", "context_id": "117", "dataset_context": "In this paper, we analyze a total of 50 Stack Exchange instances consisting of 25 randomly chosen Area 51 datasets and another 25 randomly chosen non-Area 51 datasets (see Table 1).", "mention_start": 78, "mention_end": 114, "dataset_mention": "25 randomly chosen Area 51 datasets"}, {"mentioned_in_paper": "610", "context_id": "117", "dataset_context": "In this paper, we analyze a total of 50 Stack Exchange instances consisting of 25 randomly chosen Area 51 datasets and another 25 randomly chosen non-Area 51 datasets (see Table 1).", "mention_start": 78, "mention_end": 166, "dataset_mention": "25 randomly chosen Area 51 datasets and another 25 randomly chosen non-Area 51 datasets"}, {"mentioned_in_paper": "610", "context_id": "268", "dataset_context": "These results hold for both questions and answers datasets of each of the 50 Stack Exchange instances.", "mention_start": 23, "mention_end": 58, "dataset_mention": "both questions and answers datasets"}, {"mentioned_in_paper": "610", "context_id": "295", "dataset_context": "Recall our dataset includes, besides the 39 Stack Exchange instances with K * = 4, a total of 11 instances with K * > 4. We name this group of instances Emerging, but, for now, we focus on Sustainable and Transitioning instances.", "mention_start": 0, "mention_end": 18, "dataset_mention": "Recall our dataset"}, {"mentioned_in_paper": "610", "context_id": "320", "dataset_context": "We note that some of the Transitioning Stack Exchange instances are among the five smallest datasets in our analysis, as Table 2 indicates.", "mention_start": 74, "mention_end": 100, "dataset_mention": "the five smallest datasets"}, {"mentioned_in_paper": "610", "context_id": "363", "dataset_context": "We note that publicly available statistics from Area 51 datasets [46] and previously mentioned work [20, 36] stress the importance of this core community from the Frequent and Permanent Activity Archetypes.", "mention_start": 48, "mention_end": 64, "dataset_mention": "Area 51 datasets"}, {"mentioned_in_paper": "610", "context_id": "391", "dataset_context": "Besides the aforementioned limitation regarding feature selection and corresponding clustering quality and interpretation (as other binary features might yield equally good clustering quality but other interpretations), we reflect on the generalization and practical implications of our approach with respect to other Q&A datasets.", "mention_start": 311, "mention_end": 330, "dataset_mention": "other Q&A datasets"}, {"mentioned_in_paper": "610", "context_id": "395", "dataset_context": "However, once time series granularity and our proposed features have been adjusted for a potentially new dataset, we expect the clustering to yield comparable results, since our proposed features yield clear-cut separated clusters.", "mention_start": 86, "mention_end": 112, "dataset_mention": "a potentially new dataset"}, {"mentioned_in_paper": "614", "context_id": "1", "dataset_context": "To train the feature extraction model, we construct a large scale photo-realistic face image dataset with groundtruth correspondence between multi-view face images, which are synthesized from real photographs via an inverse rendering procedure.", "mention_start": 51, "mention_end": 100, "dataset_mention": "a large scale photo-realistic face image dataset"}, {"mentioned_in_paper": "614", "context_id": "30", "dataset_context": "To solve this problem, we synthesize a largescale face image dataset with different poses and expressions together with ground-truth 3D shapes.", "mention_start": 22, "mention_end": 68, "dataset_mention": " we synthesize a largescale face image dataset"}, {"mentioned_in_paper": "614", "context_id": "87", "dataset_context": "The mean and basis identities are constructed from the Basel Face Model [35], while the mean and basis expressions are constructed using the FaceWarehouse dataset [36].", "mention_start": 136, "mention_end": 162, "dataset_mention": "the FaceWarehouse dataset"}, {"mentioned_in_paper": "614", "context_id": "91", "dataset_context": "We first select 4308 face images from the 300W dataset [37] and the Multi-PIE dataset [38], and then follow the approach of [39] to fit the parametric face model to each image and produce the shape, albedo, and camera parameters.", "mention_start": 38, "mention_end": 54, "dataset_mention": "the 300W dataset"}, {"mentioned_in_paper": "614", "context_id": "91", "dataset_context": "We first select 4308 face images from the 300W dataset [37] and the Multi-PIE dataset [38], and then follow the approach of [39] to fit the parametric face model to each image and produce the shape, albedo, and camera parameters.", "mention_start": 38, "mention_end": 85, "dataset_mention": "the 300W dataset [37] and the Multi-PIE dataset"}, {"mentioned_in_paper": "614", "context_id": "139", "dataset_context": "In this work, the alignment between the 3D face model and the 2D face image is done via 68 landmark vertices, chosen on the 3D face mesh to match the 68 landmark points used in the Multi-PIE data set [38] (see Fig. 7 left).", "mention_start": 176, "mention_end": 199, "dataset_mention": "the Multi-PIE data set"}, {"mentioned_in_paper": "614", "context_id": "142", "dataset_context": "Specifically, we Figure 7 : We perform face alignment using 68 landmarks (shown on the left), which are chosen on the 3D face mesh according to the 68 feature points of the Multi-PIE dataset [38].", "mention_start": 168, "mention_end": 190, "dataset_mention": "the Multi-PIE dataset"}, {"mentioned_in_paper": "614", "context_id": "201", "dataset_context": "We also apply DFF for dense matching between two face images with different poses, using images from the AFLW2000-3D dataset [14].", "mention_start": 100, "mention_end": 124, "dataset_mention": "the AFLW2000-3D dataset"}, {"mentioned_in_paper": "614", "context_id": "211", "dataset_context": "To evaluate the performance of our approach for large pose face alignment, we test it using face images in the wild from the AFLW dataset 1 and the AFLW2000-3D dataset 2.", "mention_start": 120, "mention_end": 137, "dataset_mention": "the AFLW dataset"}, {"mentioned_in_paper": "614", "context_id": "211", "dataset_context": "To evaluate the performance of our approach for large pose face alignment, we test it using face images in the wild from the AFLW dataset 1 and the AFLW2000-3D dataset 2.", "mention_start": 120, "mention_end": 167, "dataset_mention": "the AFLW dataset 1 and the AFLW2000-3D dataset"}, {"mentioned_in_paper": "614", "context_id": "212", "dataset_context": "The AFLW dataset contains face images with 21 visible ground truth landmarks, while the AFLW2000-3D dataset consists of fitted 3D faces for the first 2000 AFLW samples and can be used for 3D face alignment evaluation.", "mention_start": 0, "mention_end": 16, "dataset_mention": "The AFLW dataset"}, {"mentioned_in_paper": "614", "context_id": "212", "dataset_context": "The AFLW dataset contains face images with 21 visible ground truth landmarks, while the AFLW2000-3D dataset consists of fitted 3D faces for the first 2000 AFLW samples and can be used for 3D face alignment evaluation.", "mention_start": 83, "mention_end": 107, "dataset_mention": "the AFLW2000-3D dataset"}, {"mentioned_in_paper": "614", "context_id": "216", "dataset_context": "Since the results reported in [14] are obtained using a model trained with the 300W-LP dataset, we also learn the generic descent directions using the 300W-LP dataset for consistency.", "mention_start": 75, "mention_end": 94, "dataset_mention": "the 300W-LP dataset"}, {"mentioned_in_paper": "614", "context_id": "216", "dataset_context": "Since the results reported in [14] are obtained using a model trained with the 300W-LP dataset, we also learn the generic descent directions using the 300W-LP dataset for consistency.", "mention_start": 146, "mention_end": 166, "dataset_mention": "the 300W-LP dataset"}, {"mentioned_in_paper": "614", "context_id": "217", "dataset_context": "All 24384 face images from the AFLW dataset, with yaw angles ranging from \u221290 \u2022 to 90 \u2022 , are used for testing.", "mention_start": 27, "mention_end": 43, "dataset_mention": "the AFLW dataset"}, {"mentioned_in_paper": "614", "context_id": "220", "dataset_context": "For the AFLW2000-3D dataset, we follow the same experimental setting as [14].", "mention_start": 4, "mention_end": 27, "dataset_mention": "the AFLW2000-3D dataset"}, {"mentioned_in_paper": "614", "context_id": "232", "dataset_context": "More alignment results by our method on AFLW dataset are given in Fig. 12.", "mention_start": 40, "mention_end": 52, "dataset_mention": "AFLW dataset"}, {"mentioned_in_paper": "614", "context_id": "234", "dataset_context": "The experiments are conducted on the 300W dataset, and we use the training data of the LFPW and HELEN datasets, and the whole AFW dataset, to train the alignment model.", "mention_start": 33, "mention_end": 49, "dataset_mention": "the 300W dataset"}, {"mentioned_in_paper": "614", "context_id": "234", "dataset_context": "The experiments are conducted on the 300W dataset, and we use the training data of the LFPW and HELEN datasets, and the whole AFW dataset, to train the alignment model.", "mention_start": 82, "mention_end": 110, "dataset_mention": "the LFPW and HELEN datasets"}, {"mentioned_in_paper": "614", "context_id": "234", "dataset_context": "The experiments are conducted on the 300W dataset, and we use the training data of the LFPW and HELEN datasets, and the whole AFW dataset, to train the alignment model.", "mention_start": 111, "mention_end": 137, "dataset_mention": " and the whole AFW dataset"}, {"mentioned_in_paper": "614", "context_id": "236", "dataset_context": "Since the 300W dataset does not provide groundtruth camera parameters, the large-pose face alignment method described in Sec.", "mention_start": 6, "mention_end": 22, "dataset_mention": "the 300W dataset"}, {"mentioned_in_paper": "615", "context_id": "148", "dataset_context": "For PASCAL \u2192 Watercolor, we utilize Pascal VOC dataset as the source domain.", "mention_start": 35, "mention_end": 54, "dataset_mention": "Pascal VOC dataset"}, {"mentioned_in_paper": "615", "context_id": "156", "dataset_context": "Besides, the BDD-100k dataset includes ten categories.", "mention_start": 8, "mention_end": 29, "dataset_mention": " the BDD-100k dataset"}, {"mentioned_in_paper": "615", "context_id": "170", "dataset_context": "The first row of Fig. 6 shows one detection example from 78.8 59.9 47.9 41.0 34.8 66.9 54.9 SCL [32] 82. the FoggyCityscapes dataset.", "mention_start": 57, "mention_end": 132, "dataset_mention": "78.8 59.9 47.9 41.0 34.8 66.9 54.9 SCL [32] 82. the FoggyCityscapes dataset"}, {"mentioned_in_paper": "615", "context_id": "229", "dataset_context": "Based on Fog-gyCityscapes and Watercolor dataset, the adaptation performance of the traditional disentanglement is 34.1% and 54.6%, which is weaker than our method.", "mention_start": 9, "mention_end": 48, "dataset_mention": "Fog-gyCityscapes and Watercolor dataset"}, {"mentioned_in_paper": "617", "context_id": "197", "dataset_context": "1) VeRi-776: The VeRi-776 dataset contains 49,357 images of 776 different vehicles captured by 20 cameras involving various viewpoints, illumination changes, and background clutters.", "mention_start": 12, "mention_end": 33, "dataset_mention": " The VeRi-776 dataset"}, {"mentioned_in_paper": "617", "context_id": "201", "dataset_context": "2) VehicleID: VehicleID dataset is released after the VeRi-776, which includes 221,763 images with 26,267 identities and 250 models.", "mention_start": 13, "mention_end": 31, "dataset_mention": " VehicleID dataset"}, {"mentioned_in_paper": "617", "context_id": "247", "dataset_context": "Fig. 6 visualizes the vehicle re-ID results on VeRi-776 dataset qualitatively.", "mention_start": 47, "mention_end": 63, "dataset_mention": "VeRi-776 dataset"}, {"mentioned_in_paper": "617", "context_id": "264", "dataset_context": "Table V illustrates the mAP, Top-1, and Top-5 metrics of the proposed appearance module and state-of-theart methods on the large-scale VehicleID dataset.", "mention_start": 118, "mention_end": 152, "dataset_mention": "the large-scale VehicleID dataset"}, {"mentioned_in_paper": "617", "context_id": "266", "dataset_context": "Note that VehicleID dataset does not provide any other information besides images, hence we can not utilize the spatio-temporal module in the performance comparison.", "mention_start": 5, "mention_end": 27, "dataset_mention": "that VehicleID dataset"}, {"mentioned_in_paper": "617", "context_id": "272", "dataset_context": "Although our DFR-ST without the spatiotemporal module is a bit inferior to HPGN [59], the proposed DFR-ST can still obtain rather competitive performance on the challenging VehicleID dataset, which confirms the competitiveness of our proposed appearance module.", "mention_start": 172, "mention_end": 190, "dataset_mention": "VehicleID dataset"}, {"mentioned_in_paper": "618", "context_id": "95", "dataset_context": "To train an ensemble we use CIFAR-100 dataset that we transfer to CIFAR-10 dataset [13].", "mention_start": 28, "mention_end": 45, "dataset_mention": "CIFAR-100 dataset"}, {"mentioned_in_paper": "618", "context_id": "95", "dataset_context": "To train an ensemble we use CIFAR-100 dataset that we transfer to CIFAR-10 dataset [13].", "mention_start": 66, "mention_end": 82, "dataset_mention": "CIFAR-10 dataset"}, {"mentioned_in_paper": "621", "context_id": "9", "dataset_context": "The evaluations on four banchmarking datasets justify its effectiveness.", "mention_start": 19, "mention_end": 45, "dataset_mention": "four banchmarking datasets"}, {"mentioned_in_paper": "621", "context_id": "151", "dataset_context": "For the task of set-based object categorization, we use the ETH-80 dataset [55].", "mention_start": 55, "mention_end": 74, "dataset_mention": "the ETH-80 dataset"}, {"mentioned_in_paper": "621", "context_id": "152", "dataset_context": "The widely used YouTube Celebrities (YTC) dataset [32] is applied to the face recognition task.", "mention_start": 16, "mention_end": 49, "dataset_mention": "YouTube Celebrities (YTC) dataset"}, {"mentioned_in_paper": "621", "context_id": "153", "dataset_context": "We utilize the MDSD dataset [56] for the task of video-based dynamic scene classification and Virus [57] for virus cell classification task.", "mention_start": 11, "mention_end": 27, "dataset_mention": "the MDSD dataset"}, {"mentioned_in_paper": "621", "context_id": "154", "dataset_context": "The ETH-80 dataset consists of 8 categories, such as apples, cows, cups, dogs, horses, pears, tomatoes and cars, with each category has 10 image sets, and each image set contains 41 images of different perspectives.We randomly select five from each category for traning and the remaining five for testing.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The ETH-80 dataset"}, {"mentioned_in_paper": "621", "context_id": "156", "dataset_context": "The Virus dataset is composed of 15 categories, each of which consists of 5 image sets.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The Virus dataset"}, {"mentioned_in_paper": "621", "context_id": "159", "dataset_context": "Some examples of Virus dataset are presented in the second line of Fig. 2. The MDSD dataset contains 13 different categories of dynamic scenes, with each class consists of 10 video sequences collected in an unconstrained setting.", "mention_start": 17, "mention_end": 30, "dataset_mention": "Virus dataset"}, {"mentioned_in_paper": "621", "context_id": "159", "dataset_context": "Some examples of Virus dataset are presented in the second line of Fig. 2. The MDSD dataset contains 13 different categories of dynamic scenes, with each class consists of 10 video sequences collected in an unconstrained setting.", "mention_start": 67, "mention_end": 91, "dataset_mention": "Fig. 2. The MDSD dataset"}, {"mentioned_in_paper": "621", "context_id": "162", "dataset_context": "For the YouTube Celebrities dataset, there are 1910 video clips of 47 subjects involved in it.", "mention_start": 4, "mention_end": 35, "dataset_mention": "the YouTube Celebrities dataset"}, {"mentioned_in_paper": "621", "context_id": "169", "dataset_context": "In order to study the effectiveness of the proposed method, we compare the proposed method with some representative image set classification methods including: Covariance Discriminant Learning(CDL) [32], Grassmann Discriminant Analysis (GDA) [35], Projection Metric Learning (PML) [36], Log-Euclidean Metric Learning (LEML) [16], Multiple Manifolds Metric Learning(MMML) [23], SPD Manifold Leaning (SPDML-AIRM, SPDML-Stein) [33], Generalized Dictionary Learning and Sparse Coding using Frobenius Norm(Frob SRC) [46], Logarithm Mapping for Sparse Representation(Log SRC) [45], and Log-Euclidean Kernels for Sparse Representation(LogEK SRC) [47] We should emphasize that we use the source codes of all the comparative methods provided by the original authors TABLE I: Average recognition rates and standard deviations of different menthods on ETH-80 [55], Virus [57], MDSD [56] and YTC [32] datasets.", "mention_start": 865, "mention_end": 897, "dataset_mention": " MDSD [56] and YTC [32] datasets"}, {"mentioned_in_paper": "621", "context_id": "185", "dataset_context": "According to these two figures, the best value of \u03bb 1 on the ETH-80, Virus, MDSD and YTC datasets are set to 0.01, 0.5, 0.05, and 0.1, respectively.", "mention_start": 75, "mention_end": 97, "dataset_mention": " MDSD and YTC datasets"}, {"mentioned_in_paper": "621", "context_id": "186", "dataset_context": "For \u03bb 2 , 0.5, 1, 0.1, and 0.05 are the selected proper values for the ETH-80, Virus, MDSD, and YTC datasets, respectively.", "mention_start": 91, "mention_end": 108, "dataset_mention": " and YTC datasets"}, {"mentioned_in_paper": "621", "context_id": "189", "dataset_context": "To this end, we make a comparison between LogEK CRC, Log CRC, CRC, and SPD CRC on the ETH-80, Virus, MDSD, and YTC datasets to observe their difference in classification performance.", "mention_start": 106, "mention_end": 123, "dataset_mention": " and YTC datasets"}, {"mentioned_in_paper": "621", "context_id": "199", "dataset_context": "Table III presents the testing time of the proposed methods and several sparse representation methods on the ETH-80 dataset.", "mention_start": 105, "mention_end": 123, "dataset_mention": "the ETH-80 dataset"}, {"mentioned_in_paper": "623", "context_id": "6", "dataset_context": "Evaluations are conducted on the SUNCG dataset, achieving state-of-the-art performance and fast speed.", "mention_start": 29, "mention_end": 46, "dataset_mention": "the SUNCG dataset"}, {"mentioned_in_paper": "623", "context_id": "46", "dataset_context": "We evaluate our network on the SUNCG dataset [40] and achieve state-of-theart results.", "mention_start": 27, "mention_end": 44, "dataset_mention": "the SUNCG dataset"}, {"mentioned_in_paper": "623", "context_id": "51", "dataset_context": "-We achieve state-of-the-art results on the SUNCG dataset, reaching an IoU of 84.5% for scene completion and 70.5% for semantic scene completion.", "mention_start": 40, "mention_end": 57, "dataset_mention": "the SUNCG dataset"}, {"mentioned_in_paper": "623", "context_id": "139", "dataset_context": "We train and evaluate our network on the SUNCG dataset, which is a manually created large-scale synthetic scene dataset [40].", "mention_start": 37, "mention_end": 54, "dataset_mention": "the SUNCG dataset"}, {"mentioned_in_paper": "623", "context_id": "139", "dataset_context": "We train and evaluate our network on the SUNCG dataset, which is a manually created large-scale synthetic scene dataset [40].", "mention_start": 83, "mention_end": 119, "dataset_mention": "large-scale synthetic scene dataset"}, {"mentioned_in_paper": "623", "context_id": "163", "dataset_context": "In this section, we evaluate our network on the standard SUNCG test dataset.", "mention_start": 43, "mention_end": 75, "dataset_mention": "the standard SUNCG test dataset"}, {"mentioned_in_paper": "623", "context_id": "169", "dataset_context": "We also give results on real-word noisy NYU dataset [39].", "mention_start": 24, "mention_end": 51, "dataset_mention": "real-word noisy NYU dataset"}, {"mentioned_in_paper": "623", "context_id": "206", "dataset_context": "We trained the network described above from scrath on NYU dataset.", "mention_start": 54, "mention_end": 65, "dataset_mention": "NYU dataset"}, {"mentioned_in_paper": "623", "context_id": "210", "dataset_context": "Table 5 gives detail results on NYU dataset.", "mention_start": 32, "mention_end": 43, "dataset_mention": "NYU dataset"}, {"mentioned_in_paper": "623", "context_id": "236", "dataset_context": "Besides, we propose a novel end-to-end sparse convolutional network architecture for 3D semantic scene completion and set a new accurancy record on the SUNCG dataset.", "mention_start": 147, "mention_end": 165, "dataset_mention": "the SUNCG dataset"}, {"mentioned_in_paper": "627", "context_id": "6", "dataset_context": "The performance of the approach is evaluated against datasets from two different industry-related case studies: while one involves the detection of instances of a number of different object classes in the context of a quality control application, the other stems from the visual inspection domain and deals with the localization of images areas whose pixels correspond to scene surface points affected by a specific sort of defect.", "mention_start": 35, "mention_end": 61, "dataset_mention": "evaluated against datasets"}, {"mentioned_in_paper": "627", "context_id": "208", "dataset_context": "As can be observed in Table 2, segmentation and clustering mIOU for experiments E-SCR*-NRGB is lower than the mIOU for experiments E-SCR*-N, with a large gap in performance in a number of cases, what suggests that the RGB features actually do not contribute -rather the opposite-on improving segmentation performance when scribble annotations alone are used as supervision information for the visual inspection dataset. 1 If Rp = Gp = Bp = 0, then nRGBp = (0, 0, 0).", "mention_start": 388, "mention_end": 418, "dataset_mention": "the visual inspection dataset"}, {"mentioned_in_paper": "627", "context_id": "272", "dataset_context": "Two industry-related application cases and the corresponding datasets have been used as benchmark for our approach.", "mention_start": 0, "mention_end": 69, "dataset_mention": "Two industry-related application cases and the corresponding datasets"}, {"mentioned_in_paper": "628", "context_id": "2", "dataset_context": "Therefore, we propose a novel physicallybased rendered LIGHT Specularity (LIGHTS) Dataset for the evaluation of the specular highlight detection task.", "mention_start": 21, "mention_end": 89, "dataset_mention": "a novel physicallybased rendered LIGHT Specularity (LIGHTS) Dataset"}, {"mentioned_in_paper": "628", "context_id": "15", "dataset_context": "Therefore, we propose the LIGHTS Dataset 1 , constructed from high-quality architectural 3D models with variation in lighting design and rendering parameters to create near photo-realistic scenes (see Fig. 1).", "mention_start": 21, "mention_end": 40, "dataset_mention": "the LIGHTS Dataset"}, {"mentioned_in_paper": "628", "context_id": "22", "dataset_context": "(1) A photo-realistic dataset for specular highlight analysis;", "mention_start": 0, "mention_end": 29, "dataset_mention": "(1) A photo-realistic dataset"}, {"mentioned_in_paper": "628", "context_id": "38", "dataset_context": "In the lab constrained dataset of [20], they used 100 objects (generally single color) captured only in three illumination conditions.", "mention_start": 3, "mention_end": 30, "dataset_mention": "the lab constrained dataset"}, {"mentioned_in_paper": "628", "context_id": "41", "dataset_context": "The LIGHTS dataset provides indoor photo-realistic images based on Physically Based Rendering scenes.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The LIGHTS dataset"}, {"mentioned_in_paper": "628", "context_id": "52", "dataset_context": "In contrast to prior datasets our dataset has rich material properties in contrast to SunCG [5], a high amount of camera variation in contrast to MatterPort3D [2], and three orders of magnitiude more images than prior directly related datasets [11].", "mention_start": 15, "mention_end": 29, "dataset_mention": "prior datasets"}, {"mentioned_in_paper": "628", "context_id": "52", "dataset_context": "In contrast to prior datasets our dataset has rich material properties in contrast to SunCG [5], a high amount of camera variation in contrast to MatterPort3D [2], and three orders of magnitiude more images than prior directly related datasets [11].", "mention_start": 15, "mention_end": 41, "dataset_mention": "prior datasets our dataset"}, {"mentioned_in_paper": "628", "context_id": "52", "dataset_context": "In contrast to prior datasets our dataset has rich material properties in contrast to SunCG [5], a high amount of camera variation in contrast to MatterPort3D [2], and three orders of magnitiude more images than prior directly related datasets [11].", "mention_start": 211, "mention_end": 243, "dataset_mention": "prior directly related datasets"}, {"mentioned_in_paper": "628", "context_id": "53", "dataset_context": "The LIGHTS dataset is organized using similarly principles to Matterport3D for easy inclusion within existing pipelines.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The LIGHTS dataset"}, {"mentioned_in_paper": "628", "context_id": "66", "dataset_context": "In contrast to prior datasets that ignore indirect specular effects, e.g.", "mention_start": 15, "mention_end": 29, "dataset_mention": "prior datasets"}, {"mentioned_in_paper": "628", "context_id": "88", "dataset_context": "We evaluate a single-image method [12], our multi-view method and [16] on the proposed LIGHTS dataset.", "mention_start": 73, "mention_end": 101, "dataset_mention": "the proposed LIGHTS dataset"}, {"mentioned_in_paper": "628", "context_id": "100", "dataset_context": "5.1) for [12, 16] and our method over the full (3rd column) and subset (2nd column) of the LIGHTS dataset.", "mention_start": 86, "mention_end": 105, "dataset_mention": "the LIGHTS dataset"}, {"mentioned_in_paper": "628", "context_id": "112", "dataset_context": "In this paper we presented the LIGHTS dataset, which simulates real light transportation, different material and object properties as well as a wide variation of light setups.", "mention_start": 3, "mention_end": 45, "dataset_mention": "this paper we presented the LIGHTS dataset"}, {"mentioned_in_paper": "628", "context_id": "113", "dataset_context": "We have shown that our dataset provides a chellange to existing methods and the proposed pipeline is able to distinguish between specular and non-specular regions that can be confusing for other methods especially for the single-image methods.", "mention_start": 8, "mention_end": 30, "dataset_mention": "shown that our dataset"}, {"mentioned_in_paper": "628", "context_id": "122", "dataset_context": "Figure 6 demonstrates the simplicity of the current labconstrained state-of-the-art datasets for specular highlight detection (i.e.", "mention_start": 40, "mention_end": 92, "dataset_mention": "the current labconstrained state-of-the-art datasets"}, {"mentioned_in_paper": "628", "context_id": "123", "dataset_context": "first three rows) and a comparison between different single image specular highlight detector and sample scenes from the LIGHTS dataset where they fail to address.", "mention_start": 117, "mention_end": 135, "dataset_mention": "the LIGHTS dataset"}, {"mentioned_in_paper": "628", "context_id": "124", "dataset_context": "Figures 3 and 4, similarly to figure 2 in the main paper, showcase the qualitative performance of our multi-view specular detection pipeline for more scene samples of the LIGHTS dataset side-by-side with the estimation from Lin et al. [16] and of Souza et al. [12].", "mention_start": 166, "mention_end": 185, "dataset_mention": "the LIGHTS dataset"}, {"mentioned_in_paper": "628", "context_id": "127", "dataset_context": "Finally, table 2 reports the speed comparison of our facebased multi-view detector against pixel-based technique of Lin et al. [16] on the presented views of LIGHTS dataset for each of the 18 scenes.", "mention_start": 157, "mention_end": 172, "dataset_mention": "LIGHTS dataset"}, {"mentioned_in_paper": "629", "context_id": "82", "dataset_context": "They collect the large scale co-purchase dataset from Amazon.com that we base our experiments on.", "mention_start": 13, "mention_end": 48, "dataset_mention": "the large scale co-purchase dataset"}, {"mentioned_in_paper": "629", "context_id": "107", "dataset_context": "Specifically in the Amazon dataset, two items that are not labeled as compatible are not necessarily incompatible.", "mention_start": 16, "mention_end": 34, "dataset_mention": "the Amazon dataset"}, {"mentioned_in_paper": "629", "context_id": "170", "dataset_context": "One challenge with large scale datasets like the Amazon dataset is label noise, e.g., there exist shirts that are falsely labeled as shoes.", "mention_start": 19, "mention_end": 63, "dataset_mention": "large scale datasets like the Amazon dataset"}, {"mentioned_in_paper": "630", "context_id": "40", "dataset_context": "Since training neural networks is NP-Hard for worst-case datasets (Blum & Rivest, 1992), recent works have analyzed neural networks under certain data assumptions to better understand their performance in practice.", "mention_start": 46, "mention_end": 65, "dataset_mention": "worst-case datasets"}, {"mentioned_in_paper": "630", "context_id": "213", "dataset_context": "Indeed, in Section 6.3 we show empirically that for certain linearly separable datasets, gradient flow converges to a solution in the PAR which is in agreement with our results.", "mention_start": 51, "mention_end": 87, "dataset_mention": "certain linearly separable datasets"}, {"mentioned_in_paper": "632", "context_id": "159", "dataset_context": "It is important to highlight that we achieve quite similar best and last performance for all levels of label noise in CIFAR datasets, indicating that the proposed method is robust to varying noise levels.", "mention_start": 118, "mention_end": 132, "dataset_mention": "CIFAR datasets"}, {"mentioned_in_paper": "632", "context_id": "211", "dataset_context": "Performance will likely drop in carefully annotated datasets with near 0% noise because the loss distribution is not a two-component mixture.", "mention_start": 32, "mention_end": 60, "dataset_mention": "carefully annotated datasets"}, {"mentioned_in_paper": "633", "context_id": "7", "dataset_context": "Extensive experiments on synthetic and realistic datasets prove the superiority of our proposed framework.", "mention_start": 25, "mention_end": 57, "dataset_mention": "synthetic and realistic datasets"}, {"mentioned_in_paper": "633", "context_id": "20", "dataset_context": "Each circle represents the performance of a model in terms of FPS and PSNR on the GoPro [1] dataset with 1280 \u00d7 720 images using an RTX 2080Ti GPU.", "mention_start": 78, "mention_end": 99, "dataset_mention": "the GoPro [1] dataset"}, {"mentioned_in_paper": "633", "context_id": "34", "dataset_context": "We repeatedly input the deblurred image to the network by multiple times and report the deblurring results on the GoPro dataset.", "mention_start": 110, "mention_end": 127, "dataset_mention": "the GoPro dataset"}, {"mentioned_in_paper": "633", "context_id": "46", "dataset_context": "Our proposed simple yet efficient idempotent network achieves state-of-the-art deblurring performance on the Go-Pro dataset with 1280 \u00d7 720 images and runs in real-time.", "mention_start": 105, "mention_end": 123, "dataset_mention": "the Go-Pro dataset"}, {"mentioned_in_paper": "633", "context_id": "172", "dataset_context": "IV. EXPERIMENTS A. Experimental Details Datasets.", "mention_start": 0, "mention_end": 48, "dataset_mention": "IV. EXPERIMENTS A. Experimental Details Datasets"}, {"mentioned_in_paper": "633", "context_id": "173", "dataset_context": "Following the general setting of single image deblurring task [1] - [4], [7], [9], [11], [30], we use the GoPro dataset [1] to train our proposed model.", "mention_start": 101, "mention_end": 119, "dataset_mention": "the GoPro dataset"}, {"mentioned_in_paper": "633", "context_id": "174", "dataset_context": "The blurry images of the GoPro dataset are synthesized by averaging different numbers (7-13) of successive latent frames from 240 FPS video sequences captured by a GoPro Hero 4 camera.", "mention_start": 21, "mention_end": 38, "dataset_mention": "the GoPro dataset"}, {"mentioned_in_paper": "633", "context_id": "177", "dataset_context": "We also evaluate the generalization ability of our method on a real-world blurry scenes dataset, i.e., RealBlur [18], a commonly used dataset in recent years.", "mention_start": 61, "mention_end": 95, "dataset_mention": "a real-world blurry scenes dataset"}, {"mentioned_in_paper": "633", "context_id": "179", "dataset_context": "Following the original settings of the RealBlur dataset, we also conduct photometric alignment between the outputted deblurred images and ground-truth sharp images before computing PSNR and SSIM.", "mention_start": 35, "mention_end": 55, "dataset_mention": "the RealBlur dataset"}, {"mentioned_in_paper": "633", "context_id": "194", "dataset_context": "The experimental results in terms of PSNR, SSIM, Parameters and Time for different deblurring methods on GoPro test dataset are shown in Table I.", "mention_start": 104, "mention_end": 123, "dataset_mention": "GoPro test dataset"}, {"mentioned_in_paper": "633", "context_id": "197", "dataset_context": "On the GoPro test dataset, our method achieves comparable high performance to state-of-the-art methods with smaller parameters and faster inference time.", "mention_start": 3, "mention_end": 25, "dataset_mention": "the GoPro test dataset"}, {"mentioned_in_paper": "633", "context_id": "203", "dataset_context": "We also perform a quantitative comparison of generalization results on the RealBlur [18] dataset for models only pre-trained on the GoPro dataset.", "mention_start": 71, "mention_end": 96, "dataset_mention": "the RealBlur [18] dataset"}, {"mentioned_in_paper": "633", "context_id": "203", "dataset_context": "We also perform a quantitative comparison of generalization results on the RealBlur [18] dataset for models only pre-trained on the GoPro dataset.", "mention_start": 128, "mention_end": 145, "dataset_mention": "the GoPro dataset"}, {"mentioned_in_paper": "633", "context_id": "207", "dataset_context": "Due to the different image sizes of the RealBlur dataset, we test the average inference time of each model, and our model maintains a very fast inference speed.", "mention_start": 36, "mention_end": 56, "dataset_mention": "the RealBlur dataset"}, {"mentioned_in_paper": "633", "context_id": "208", "dataset_context": "To further demonstrate the deblurring performance and generalization ability for different blur levels, we resynthesized a multi-blurring level dataset following the synthesis pipeline of the GoPro dataset [1].", "mention_start": 103, "mention_end": 151, "dataset_mention": " we resynthesized a multi-blurring level dataset"}, {"mentioned_in_paper": "633", "context_id": "208", "dataset_context": "To further demonstrate the deblurring performance and generalization ability for different blur levels, we resynthesized a multi-blurring level dataset following the synthesis pipeline of the GoPro dataset [1].", "mention_start": 187, "mention_end": 205, "dataset_mention": "the GoPro dataset"}, {"mentioned_in_paper": "633", "context_id": "209", "dataset_context": "Quantitative results in Table III show that our model achieves better deblurring performance on the multi-blurring level dataset, indicating the superiority of our idempotent framework for dynamic scene non-uniform deblurring.", "mention_start": 96, "mention_end": 128, "dataset_mention": "the multi-blurring level dataset"}, {"mentioned_in_paper": "633", "context_id": "215", "dataset_context": "Fig. 5 shows several blurry images from the GoPro test dataset and their corresponding deblurring results produced by the above methods.", "mention_start": 40, "mention_end": 62, "dataset_mention": "the GoPro test dataset"}, {"mentioned_in_paper": "633", "context_id": "219", "dataset_context": "We compare the qualitative results on the RealBlur test datasets, as shown in Fig. 6.", "mention_start": 38, "mention_end": 64, "dataset_mention": "the RealBlur test datasets"}, {"mentioned_in_paper": "633", "context_id": "227", "dataset_context": "In this section, we perform ablation studies on the GoPro test dataset to analyze the effectiveness of each component of our proposed method.", "mention_start": 47, "mention_end": 70, "dataset_mention": "the GoPro test dataset"}, {"mentioned_in_paper": "633", "context_id": "258", "dataset_context": "The evaluation results on the GoPro test dataset are reported in Table V.", "mention_start": 26, "mention_end": 48, "dataset_mention": "the GoPro test dataset"}, {"mentioned_in_paper": "633", "context_id": "270", "dataset_context": "We report the quantitative results of iterative residual deblurring from every deblurring unit on the GoPro test dataset [1] in Table VII.", "mention_start": 98, "mention_end": 120, "dataset_mention": "the GoPro test dataset"}, {"mentioned_in_paper": "633", "context_id": "283", "dataset_context": "To verify the robustness of our model to noise, we analyze the performance of our pre-trained model by adding different levels of Gaussian noise to the blurry input of the GoPro dataset [1].", "mention_start": 167, "mention_end": 185, "dataset_mention": "the GoPro dataset"}, {"mentioned_in_paper": "633", "context_id": "291", "dataset_context": "Following the same training pipeline of Grid-DehazeNet [49], our model is trained on Indoor Training Set (ITS) and tested on the Synthetic Objective Testing Set (SOTS) Indoor Subset in RESIDE dataset [51].", "mention_start": 184, "mention_end": 199, "dataset_mention": "RESIDE dataset"}, {"mentioned_in_paper": "633", "context_id": "292", "dataset_context": "Table IX shows that with the idempotent constraint, our model achieves state-ofthe-art performance on the SOTS indoor dataset.", "mention_start": 101, "mention_end": 125, "dataset_mention": "the SOTS indoor dataset"}, {"mentioned_in_paper": "633", "context_id": "294", "dataset_context": "Fig. 11 shows the visual comparison of dehazing results on SOTS indoor dataset.", "mention_start": 59, "mention_end": 78, "dataset_mention": "SOTS indoor dataset"}, {"mentioned_in_paper": "633", "context_id": "301", "dataset_context": "Fig. 10 shows the visual comparison of deraining results on R100H dataset.", "mention_start": 60, "mention_end": 73, "dataset_mention": "R100H dataset"}, {"mentioned_in_paper": "634", "context_id": "1", "dataset_context": "We collect two novel datasets of product images and their MSRP prices for this purpose: a bicycle dataset and a car dataset.", "mention_start": 87, "mention_end": 105, "dataset_mention": " a bicycle dataset"}, {"mentioned_in_paper": "634", "context_id": "1", "dataset_context": "We collect two novel datasets of product images and their MSRP prices for this purpose: a bicycle dataset and a car dataset.", "mention_start": 87, "mention_end": 123, "dataset_mention": " a bicycle dataset and a car dataset"}, {"mentioned_in_paper": "634", "context_id": "29", "dataset_context": "In this work, we choose to use bicycles and cars as target product datasets, due to the wide visual variances in bike and car models, close visual correlations to prices, and relevance of online shopping for cars and bikes.", "mention_start": 58, "mention_end": 75, "dataset_mention": "product datasets"}, {"mentioned_in_paper": "634", "context_id": "83", "dataset_context": "We train PriceNet-Reg and PriceNet-Class from scratch on the bikes dataset.", "mention_start": 57, "mention_end": 74, "dataset_mention": "the bikes dataset"}, {"mentioned_in_paper": "634", "context_id": "84", "dataset_context": "Due to of the small size of our car dataset (1,400 car images compared to more than 20,000 bike images), we did not have enough data to train a deep neural network from random weight initialization for cars.", "mention_start": 28, "mention_end": 43, "dataset_mention": "our car dataset"}, {"mentioned_in_paper": "634", "context_id": "97", "dataset_context": "On the bikes  dataset, our PriceNet architecture achieves the strongest results in each metric, with an MAE of $165.87 on prices ranging from $70 to $1,700.", "mention_start": 2, "mention_end": 21, "dataset_mention": "the bikes  dataset"}, {"mentioned_in_paper": "634", "context_id": "98", "dataset_context": "On the cars dataset, the SqueezeNet transfer CNN and PriceNet achieves similar performance.", "mention_start": 3, "mention_end": 19, "dataset_mention": "the cars dataset"}, {"mentioned_in_paper": "634", "context_id": "100", "dataset_context": "We assign labels of 25, 50, 75, 100 for the bikes dataset (4 classes), and 20, 40, 60, 80, 100 for the cars dataset (5 classes).", "mention_start": 39, "mention_end": 57, "dataset_mention": "the bikes dataset"}, {"mentioned_in_paper": "634", "context_id": "100", "dataset_context": "We assign labels of 25, 50, 75, 100 for the bikes dataset (4 classes), and 20, 40, 60, 80, 100 for the cars dataset (5 classes).", "mention_start": 98, "mention_end": 115, "dataset_mention": "the cars dataset"}, {"mentioned_in_paper": "635", "context_id": "247", "dataset_context": "We chose for our first example a feed-forward neural network classifier on the MNIST dataset [8].", "mention_start": 75, "mention_end": 92, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "636", "context_id": "124", "dataset_context": "For example, compared with the recent state-of-the-art method CIBHash, our method brings an averaged increase of 2.8%, 0.8%, 2.4% (averaged over different code lengths) on CIFAR-10, NUS-WIDE, and MSCOCO datasets, respectively.", "mention_start": 191, "mention_end": 211, "dataset_mention": " and MSCOCO datasets"}, {"mentioned_in_paper": "637", "context_id": "7", "dataset_context": "Experiments on several largescale person ReID datasets validate the effectiveness of our proposed unsupervised method ICE, which is competitive with even supervised methods.", "mention_start": 15, "mention_end": 54, "dataset_mention": "several largescale person ReID datasets"}, {"mentioned_in_paper": "637", "context_id": "16", "dataset_context": "With the help of a large-scale source dataset, state-ofthe-art UDA methods [11, 42] significantly enhance the performance of unsupervised ReID.", "mention_start": 17, "mention_end": 45, "dataset_mention": "a large-scale source dataset"}, {"mentioned_in_paper": "637", "context_id": "17", "dataset_context": "However, the performance of UDA methods is strongly influenced by source dataset's scale and quality.", "mention_start": 65, "mention_end": 80, "dataset_mention": "source dataset"}, {"mentioned_in_paper": "637", "context_id": "34", "dataset_context": "Without any identity annotation, ICE significantly outperforms state-of-the-art UDA and fully unsupervised methods on main-stream person ReID datasets.", "mention_start": 117, "mention_end": 150, "dataset_mention": "main-stream person ReID datasets"}, {"mentioned_in_paper": "637", "context_id": "45", "dataset_context": "All these UDA-based methods require a labeled source dataset.", "mention_start": 36, "mention_end": 60, "dataset_mention": "a labeled source dataset"}, {"mentioned_in_paper": "637", "context_id": "69", "dataset_context": "Given a person ReID dataset X = {x 1 , x 2 , ..., x N }, our objective is to train a robust model on X without annotation.", "mention_start": 6, "mention_end": 27, "dataset_mention": "a person ReID dataset"}, {"mentioned_in_paper": "637", "context_id": "152", "dataset_context": "We analyze the sensitivity of each hyper-parameter on the Market-1501 dataset.", "mention_start": 54, "mention_end": 77, "dataset_mention": "the Market-1501 dataset"}, {"mentioned_in_paper": "637", "context_id": "183", "dataset_context": "On medium datasets (e.g., Market1501 and DukeMTMC-reID) without strong camera variance, our proposed camera-agnostic intra-class variance constraint L h ins is enough to make L s ins beneficial to ReID.", "mention_start": 3, "mention_end": 18, "dataset_mention": "medium datasets"}, {"mentioned_in_paper": "637", "context_id": "194", "dataset_context": "With a camera-agnostic memory, the performance of ICE(agnostic) remarkably surpasses the camera-agnostic baseline SpCL, especially on Market1501 and MSMT17 datasets.", "mention_start": 133, "mention_end": 164, "dataset_mention": "Market1501 and MSMT17 datasets"}, {"mentioned_in_paper": "638", "context_id": "13", "dataset_context": "However, the egocentric camera does not even see the actor or Fig. 1 : The top view nature of egocentric camera reduces the visible object size (second column) compared to that in ImageNet dataset [4] (first column).", "mention_start": 179, "mention_end": 196, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "638", "context_id": "24", "dataset_context": "We observe significant performance improvement in the standard models on increasing the object size and making them comparable to the size of objects typically found in the ImageNet dataset.", "mention_start": 169, "mention_end": 189, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "638", "context_id": "37", "dataset_context": "They have trained and tested their model on a huge dataset of 1 million sports video from 1000 action categories downloaded from YouTube.", "mention_start": 44, "mention_end": 58, "dataset_mention": "a huge dataset"}, {"mentioned_in_paper": "638", "context_id": "39", "dataset_context": "We resize the region of interest to match the size of objects in the egocentric dataset and third person benchmark datasets as explained in the last section.", "mention_start": 65, "mention_end": 87, "dataset_mention": "the egocentric dataset"}, {"mentioned_in_paper": "638", "context_id": "39", "dataset_context": "We resize the region of interest to match the size of objects in the egocentric dataset and third person benchmark datasets as explained in the last section.", "mention_start": 65, "mention_end": 123, "dataset_mention": "the egocentric dataset and third person benchmark datasets"}, {"mentioned_in_paper": "638", "context_id": "57", "dataset_context": "Table 1 : Accuracy comparison of our method with state-of-the-art and statistics of egocentric video datasets fine-tune the models on the egocentric data and extract 2048 dimensional feature vector from the second fully connected layer to be given as an input to the LSTM module.", "mention_start": 83, "mention_end": 109, "dataset_mention": "egocentric video datasets"}, {"mentioned_in_paper": "638", "context_id": "67", "dataset_context": "For ADL [14] and UTE [26] datasets, we use the annotations provided by Singh et al. [23], who have annotated a subset of the original dataset.", "mention_start": 4, "mention_end": 34, "dataset_mention": "ADL [14] and UTE [26] datasets"}, {"mentioned_in_paper": "638", "context_id": "69", "dataset_context": "For testing on actions involving no hand-object interaction, we use HUJI dataset [20].", "mention_start": 67, "mention_end": 80, "dataset_mention": "HUJI dataset"}, {"mentioned_in_paper": "638", "context_id": "89", "dataset_context": "Since most of the datasets contain only one kind, to validate the hypothesis, we trained our model after combining GTEA and HUJI datasets and got an accuracy of over 86.85%.", "mention_start": 88, "mention_end": 137, "dataset_mention": "our model after combining GTEA and HUJI datasets"}, {"mentioned_in_paper": "638", "context_id": "94", "dataset_context": "The top view nature of egocentric camera reduces the size of objects compared to the size of objects in the ImageNet dataset.", "mention_start": 104, "mention_end": 124, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "638", "context_id": "99", "dataset_context": "Both the streams, individually as well as jointly, improve state-of-the-art by a significant margin across all categories and datasets.", "mention_start": 106, "mention_end": 134, "dataset_mention": "all categories and datasets"}, {"mentioned_in_paper": "638", "context_id": "103", "dataset_context": "We also tested on long-term actions datasets.", "mention_start": 18, "mention_end": 44, "dataset_mention": "long-term actions datasets"}, {"mentioned_in_paper": "638", "context_id": "109", "dataset_context": "To validate the applicability of our model for such a scenario, we have mixed the samples from GTEA and HUJI datasets.", "mention_start": 94, "mention_end": 117, "dataset_mention": "GTEA and HUJI datasets"}, {"mentioned_in_paper": "639", "context_id": "25", "dataset_context": "Our contributions are as follows: \u2022 Evaluating on in-distribution-dataset, we find VTs are more accurate but slower than CNNs counterparts.", "mention_start": 49, "mention_end": 73, "dataset_mention": "in-distribution-dataset"}, {"mentioned_in_paper": "639", "context_id": "26", "dataset_context": "In addition, the results on out-ofdistribution (OOD) dataset reveals that VTs are also more generalizable to distribution shift.", "mention_start": 27, "mention_end": 60, "dataset_mention": "out-ofdistribution (OOD) dataset"}, {"mentioned_in_paper": "639", "context_id": "68", "dataset_context": "The detection models are trained and evaluated on the COCO dataset [Lin et al., 2014] which consists of 81 classes.", "mention_start": 50, "mention_end": 66, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "639", "context_id": "69", "dataset_context": "The segmentation models are trained and evaluated on COCO-Stuff [Caesar et al., 2018] dataset which contains 172 classes -80 \"things\" classes, 91 \"stuff\" classes, and 1 unlabelled class.", "mention_start": 79, "mention_end": 93, "dataset_mention": " 2018] dataset"}, {"mentioned_in_paper": "639", "context_id": "71", "dataset_context": "We choose COCO dataset for our experiments because it is a challenging benchmark dataset with common and naturally occurring real-world scenes, making it suitable for comparative experiments on dense prediction models.", "mention_start": 10, "mention_end": 22, "dataset_mention": "COCO dataset"}, {"mentioned_in_paper": "639", "context_id": "91", "dataset_context": "The performance of the model on such unseen data indicates its generalizability to OOD datasets.", "mention_start": 83, "mention_end": 95, "dataset_mention": "OOD datasets"}, {"mentioned_in_paper": "639", "context_id": "92", "dataset_context": "The detection and segmentation models trained on COCO and COCO-Stuff are evaluated on BDD100K [Yu et al., 2020] and BDD10K datasets, respectively.", "mention_start": 105, "mention_end": 131, "dataset_mention": " 2020] and BDD10K datasets"}, {"mentioned_in_paper": "639", "context_id": "93", "dataset_context": "BDD dataset has a different distribution from that of COCO since it is composed of road scenes with traffic elements like pedestrians, vehicles, road, and traffic signs.", "mention_start": 0, "mention_end": 11, "dataset_mention": "BDD dataset"}, {"mentioned_in_paper": "639", "context_id": "144", "dataset_context": "To simulate the natural transformations in the realworld, we apply the 15 common corruptions proposed by [Hendrycks and Dietterich, 2019] to the validation set of COCO and COCO-Stuff with severity 3. We compare the performance of the VTs and CNNs on the corrupted datasets in Figure 5 and observe that VTs are more robust than CNNs for both detection and segmentation.", "mention_start": 249, "mention_end": 272, "dataset_mention": "the corrupted datasets"}, {"mentioned_in_paper": "639", "context_id": "160", "dataset_context": "We create a texture-conflict dataset of COCO and COCO-Stuff by applying rich texture from objects (such as bear and zebra) as a style to other validation images containing multiple objects of a single class.", "mention_start": 10, "mention_end": 36, "dataset_mention": "a texture-conflict dataset"}, {"mentioned_in_paper": "639", "context_id": "161", "dataset_context": "A model is said to predict texture in this texture-  conflict dataset if it predicts the class of the applied texture.", "mention_start": 37, "mention_end": 69, "dataset_mention": "this texture-  conflict dataset"}, {"mentioned_in_paper": "639", "context_id": "172", "dataset_context": "\u2022 VTs outperform CNNs in in-distribution dataset while having lower inference speed, but less computational complexity.", "mention_start": 25, "mention_end": 48, "dataset_mention": "in-distribution dataset"}, {"mentioned_in_paper": "639", "context_id": "174", "dataset_context": "\u2022 VTs generalize better to OOD datasets.", "mention_start": 27, "mention_end": 39, "dataset_mention": "OOD datasets"}, {"mentioned_in_paper": "640", "context_id": "56", "dataset_context": "3-A large dataset (45211 records) was dataset from a Portuguese bank.", "mention_start": 0, "mention_end": 17, "dataset_mention": "3-A large dataset"}, {"mentioned_in_paper": "640", "context_id": "56", "dataset_context": "3-A large dataset (45211 records) was dataset from a Portuguese bank.", "mention_start": 0, "mention_end": 45, "dataset_mention": "3-A large dataset (45211 records) was dataset"}, {"mentioned_in_paper": "640", "context_id": "65", "dataset_context": "The UCI Machine Learning Repository provided the bank marketing data set for this study.", "mention_start": 45, "mention_end": 72, "dataset_mention": "the bank marketing data set"}, {"mentioned_in_paper": "640", "context_id": "113", "dataset_context": "Different data mining methods (Neural Networks, Logistic Regression, Discriminant Analyses, Naive Bayes, Support Vector Machines and Decision Trees) are applied to the bank marketing data set, and their classification performances are compared in part 4.7.", "mention_start": 163, "mention_end": 191, "dataset_mention": "the bank marketing data set"}, {"mentioned_in_paper": "640", "context_id": "114", "dataset_context": "According to the results obtained from the analyses of the bank market dataset, the decision tree method is found to be more appropriate than the others for the development of an intelligent system.", "mention_start": 55, "mention_end": 78, "dataset_mention": "the bank market dataset"}, {"mentioned_in_paper": "640", "context_id": "160", "dataset_context": "The bank marketing data sets are raw, unprocessed data sets.", "mention_start": 0, "mention_end": 28, "dataset_mention": "The bank marketing data sets"}, {"mentioned_in_paper": "640", "context_id": "160", "dataset_context": "The bank marketing data sets are raw, unprocessed data sets.", "mention_start": 37, "mention_end": 59, "dataset_mention": " unprocessed data sets"}, {"mentioned_in_paper": "641", "context_id": "325", "dataset_context": "There are four unknowns - 1 ,  1 ,  2 and  2 (for an N-D dataset there would be 2N unknowns).", "mention_start": 34, "mention_end": 64, "dataset_mention": "  2 and  2 (for an N-D dataset"}, {"mentioned_in_paper": "641", "context_id": "350", "dataset_context": "Illustrative use case Fig. 9 shows results that can be obtained with our ACO-based subspace and view optimization framework using the sales campaign dataset described in Section 7.1.", "mention_start": 130, "mention_end": 156, "dataset_mention": "the sales campaign dataset"}, {"mentioned_in_paper": "641", "context_id": "553", "dataset_context": "We used the Iris dataset [53] as a walk-through example in this video.", "mention_start": 8, "mention_end": 24, "dataset_mention": "the Iris dataset"}, {"mentioned_in_paper": "641", "context_id": "562", "dataset_context": "For the first task, we used a somewhat contrived 3D data set: a hollow tube with a stick in the middle that was not aligned with any of the data axes.", "mention_start": 27, "mention_end": 60, "dataset_mention": "a somewhat contrived 3D data set"}, {"mentioned_in_paper": "641", "context_id": "568", "dataset_context": "For the data understanding task, we also used the sales force dataset but now we first showed the participants a 1-minute video that introduced the attributes of the data.", "mention_start": 45, "mention_end": 69, "dataset_mention": "the sales force dataset"}, {"mentioned_in_paper": "644", "context_id": "117", "dataset_context": "We first describe a large-scale synthetic inpainting dataset we created, based on corrupting patches of the original image to use as the guidance image.", "mention_start": 18, "mention_end": 60, "dataset_mention": "a large-scale synthetic inpainting dataset"}, {"mentioned_in_paper": "645", "context_id": "235", "dataset_context": "In Hous-ton2013 dataset under DeepFool attack, it even yields an OA of about 58%, while 1-D CNN, DRNN and WFCG can only get 12.05%, 13.02% and 16.22%.", "mention_start": 3, "mention_end": 23, "dataset_mention": "Hous-ton2013 dataset"}, {"mentioned_in_paper": "645", "context_id": "238", "dataset_context": "As for MSSA, it achieves OAs greater than 70% for all the  To demonstrate the classification results from the visual aspect, Fig. 5 illustrates the reference maps about aforementioned methods on PaviaU dataset with FGSM algorithm.", "mention_start": 194, "mention_end": 209, "dataset_mention": "PaviaU dataset"}, {"mentioned_in_paper": "645", "context_id": "251", "dataset_context": "However, the performances of TE on Honston2013 dataset are worse than other two datasets, where the best OA value is merely 40.3%.", "mention_start": 34, "mention_end": 54, "dataset_mention": "Honston2013 dataset"}, {"mentioned_in_paper": "645", "context_id": "252", "dataset_context": "Class-imbalance might be the reason for this abnormal situation, because most of pixels in Honston2013 datasets are background pixels.", "mention_start": 90, "mention_end": 111, "dataset_mention": "Honston2013 datasets"}, {"mentioned_in_paper": "645", "context_id": "257", "dataset_context": "Take the results of MSSA in Houston2013 dataset for example, MSSA gets an OA of 79.51%, while the suboptimum performance is only 43.23%.", "mention_start": 28, "mention_end": 47, "dataset_mention": "Houston2013 dataset"}, {"mentioned_in_paper": "645", "context_id": "271", "dataset_context": "In Fig. 7, 200 pixels of each category in Houston2013 dataset are sampled to execute correlation analysis based on t-SNE algorithm [49].", "mention_start": 41, "mention_end": 61, "dataset_mention": "Houston2013 dataset"}, {"mentioned_in_paper": "646", "context_id": "4", "dataset_context": "First, we assume that the inlier dataset is related to some narrow application field (e.g.", "mention_start": 16, "mention_end": 40, "dataset_mention": "that the inlier dataset"}, {"mentioned_in_paper": "646", "context_id": "6", "dataset_context": "Second, we assume that there exists a general-purpose dataset which is much more diverse than the inlier dataset (e.g.", "mention_start": 35, "mention_end": 61, "dataset_mention": "a general-purpose dataset"}, {"mentioned_in_paper": "646", "context_id": "6", "dataset_context": "Second, we assume that there exists a general-purpose dataset which is much more diverse than the inlier dataset (e.g.", "mention_start": 93, "mention_end": 112, "dataset_mention": "the inlier dataset"}, {"mentioned_in_paper": "646", "context_id": "8", "dataset_context": "We consider pixels from the general-purpose dataset as noisy negative samples since most (but not all) of them are outliers.", "mention_start": 24, "mention_end": 51, "dataset_mention": "the general-purpose dataset"}, {"mentioned_in_paper": "646", "context_id": "10", "dataset_context": "Our experiments target two dense open-set recognition benchmarks (WildDash 1 and Fishyscapes) and one dense open-set recognition dataset (StreetHazard).", "mention_start": 23, "mention_end": 136, "dataset_mention": "two dense open-set recognition benchmarks (WildDash 1 and Fishyscapes) and one dense open-set recognition dataset"}, {"mentioned_in_paper": "646", "context_id": "12", "dataset_context": "Deep convolutional approaches have recently achieved proficiency on realistic semantic segmentation datasets such as Vistas [1] or Ade20k [2].", "mention_start": 68, "mention_end": 108, "dataset_mention": "realistic semantic segmentation datasets"}, {"mentioned_in_paper": "646", "context_id": "15", "dataset_context": "Early semantic segmentation approaches involved small datasets and few classes.", "mention_start": 0, "mention_end": 62, "dataset_mention": "Early semantic segmentation approaches involved small datasets"}, {"mentioned_in_paper": "646", "context_id": "19", "dataset_context": "For instance, none of the 20000 images from the Vistas dataset [1] include persons in non-standard poses, crashed vehicles or rubble.", "mention_start": 43, "mention_end": 62, "dataset_mention": "the Vistas dataset"}, {"mentioned_in_paper": "646", "context_id": "39", "dataset_context": "We propose a novel approach for dense outlier detection based on discriminative training with noisy negative images from a very large and diverse test-agnostic dataset.", "mention_start": 121, "mention_end": 167, "dataset_mention": "a very large and diverse test-agnostic dataset"}, {"mentioned_in_paper": "646", "context_id": "43", "dataset_context": "Evaluation on two rigorous benchmarks and several other datasets indicates that our approach outperforms the state of the art [5, 10, 12, 4], especially on large outliers.", "mention_start": 14, "mention_end": 64, "dataset_mention": "two rigorous benchmarks and several other datasets"}, {"mentioned_in_paper": "646", "context_id": "47", "dataset_context": "Our consolidated experiments evaluate performance on established dense open-set benchmarks (WildDash 1 [9], Fishyscapes Static and Fishyscapes Lost and Found [12]), the StreetHazard dataset [10], and the proposed WD-Pascal dataset [14, 15].", "mention_start": 164, "mention_end": 189, "dataset_mention": " the StreetHazard dataset"}, {"mentioned_in_paper": "646", "context_id": "47", "dataset_context": "Our consolidated experiments evaluate performance on established dense open-set benchmarks (WildDash 1 [9], Fishyscapes Static and Fishyscapes Lost and Found [12]), the StreetHazard dataset [10], and the proposed WD-Pascal dataset [14, 15].", "mention_start": 195, "mention_end": 230, "dataset_mention": " and the proposed WD-Pascal dataset"}, {"mentioned_in_paper": "646", "context_id": "48", "dataset_context": "Our experiments show that the proposed approach is broadly applicable without any dataset-specific tweaking.", "mention_start": 51, "mention_end": 89, "dataset_mention": "broadly applicable without any dataset"}, {"mentioned_in_paper": "646", "context_id": "49", "dataset_context": "All our experiments use the same negative dataset and involve the same hyper-parameters.", "mention_start": 24, "mention_end": 49, "dataset_mention": "the same negative dataset"}, {"mentioned_in_paper": "646", "context_id": "74", "dataset_context": "A negative dataset can also be exploited to train a separate prediction head which directly predicts the outlier probability [14].", "mention_start": 0, "mention_end": 18, "dataset_mention": "A negative dataset"}, {"mentioned_in_paper": "646", "context_id": "75", "dataset_context": "However, these approaches are sensitive to the choice of the negative dataset.", "mention_start": 56, "mention_end": 77, "dataset_mention": "the negative dataset"}, {"mentioned_in_paper": "646", "context_id": "77", "dataset_context": "However, experiments suggest that diverse negative datasets lead to better outlier detection than synthetic negative samples [26, 27].", "mention_start": 8, "mention_end": 59, "dataset_mention": " experiments suggest that diverse negative datasets"}, {"mentioned_in_paper": "646", "context_id": "86", "dataset_context": "Some of the described novelty detection methods are capable of dense inference [20], however they address simple datasets and do not report pixel-level metrics.", "mention_start": 105, "mention_end": 121, "dataset_mention": "simple datasets"}, {"mentioned_in_paper": "646", "context_id": "100", "dataset_context": "The proposed composite dataset (MSeg) collects almost 200 000 densely annotated training images by merging public datasets such as Ade20k, IDD, COCO etc. Currently, this is the only approach that outperforms our submission to the WildDash 1 benchmark.", "mention_start": 0, "mention_end": 30, "dataset_mention": "The proposed composite dataset"}, {"mentioned_in_paper": "646", "context_id": "100", "dataset_context": "The proposed composite dataset (MSeg) collects almost 200 000 densely annotated training images by merging public datasets such as Ade20k, IDD, COCO etc. Currently, this is the only approach that outperforms our submission to the WildDash 1 benchmark.", "mention_start": 99, "mention_end": 122, "dataset_mention": "merging public datasets"}, {"mentioned_in_paper": "646", "context_id": "127", "dataset_context": "We propose to train our model by sampling negative data from an extremely diverse testagnostic dataset such as ImageNet-1k.", "mention_start": 61, "mention_end": 102, "dataset_mention": "an extremely diverse testagnostic dataset"}, {"mentioned_in_paper": "646", "context_id": "136", "dataset_context": "We perform many inlier epochs during one negative epoch, since our negative training dataset is much larger than our inlier datasets.", "mention_start": 112, "mention_end": 132, "dataset_mention": "our inlier datasets"}, {"mentioned_in_paper": "646", "context_id": "138", "dataset_context": "Unlike [12], we refrain from training on pixels labeled with the ignore class since we wish to use the same negative dataset in all experiments.", "mention_start": 98, "mention_end": 124, "dataset_mention": "the same negative dataset"}, {"mentioned_in_paper": "646", "context_id": "155", "dataset_context": "Thus, the BDD-Anomaly dataset [10] collects all BDD images without trains and motorcycles into the training split, and places all other BDD images into the test split.", "mention_start": 5, "mention_end": 29, "dataset_mention": " the BDD-Anomaly dataset"}, {"mentioned_in_paper": "646", "context_id": "156", "dataset_context": "Cityscapes-IDD [36] proposes training on Cityscapes, and evaluating on cars (inliers) and rickshaws (outliers) from the IDD dataset.", "mention_start": 115, "mention_end": 131, "dataset_mention": "the IDD dataset"}, {"mentioned_in_paper": "646", "context_id": "169", "dataset_context": "It also includes a subset of the Lost and Found dataset [38] where the outliers correspond to small obstacles on the road.", "mention_start": 29, "mention_end": 55, "dataset_mention": "the Lost and Found dataset"}, {"mentioned_in_paper": "646", "context_id": "170", "dataset_context": "The StreetHazards dataset [10] contains fully synthetic road-driving images while out-of-domain objects correspond to anomalies.", "mention_start": 0, "mention_end": 25, "dataset_mention": "The StreetHazards dataset"}, {"mentioned_in_paper": "646", "context_id": "186", "dataset_context": "We evaluate our models on the WildDash 1 benchmark, the Fishyscapes benchmark, and on the test subset of the StreetHazard dataset.", "mention_start": 104, "mention_end": 129, "dataset_mention": "the StreetHazard dataset"}, {"mentioned_in_paper": "646", "context_id": "188", "dataset_context": "All models have been trained on positive images from the Vistas dataset.", "mention_start": 53, "mention_end": 71, "dataset_mention": "the Vistas dataset"}, {"mentioned_in_paper": "646", "context_id": "277", "dataset_context": "FS Lost and Found comprises 300 images taken from the Lost and Found dataset.", "mention_start": 50, "mention_end": 76, "dataset_mention": "the Lost and Found dataset"}, {"mentioned_in_paper": "646", "context_id": "298", "dataset_context": "The main idea is to discriminate an application-specific inlier dataset (e.g.", "mention_start": 20, "mention_end": 71, "dataset_mention": "discriminate an application-specific inlier dataset"}, {"mentioned_in_paper": "646", "context_id": "299", "dataset_context": "Vistas, Cityscapes), from a diverse general-purpose dataset (e.g.", "mention_start": 25, "mention_end": 59, "dataset_mention": "a diverse general-purpose dataset"}, {"mentioned_in_paper": "646", "context_id": "301", "dataset_context": "Pixels from the latter dataset represent noisy test-agnostic negative samples.", "mention_start": 12, "mention_end": 30, "dataset_mention": "the latter dataset"}, {"mentioned_in_paper": "646", "context_id": "310", "dataset_context": "We achieve state-of-the-art AP accuracy on the Street-Hazard dataset despite a strong domain shift between our negative dataset (ImageNet-1k-bb) and the test dataset.", "mention_start": 43, "mention_end": 68, "dataset_mention": "the Street-Hazard dataset"}, {"mentioned_in_paper": "646", "context_id": "310", "dataset_context": "We achieve state-of-the-art AP accuracy on the Street-Hazard dataset despite a strong domain shift between our negative dataset (ImageNet-1k-bb) and the test dataset.", "mention_start": 107, "mention_end": 127, "dataset_mention": "our negative dataset"}, {"mentioned_in_paper": "646", "context_id": "310", "dataset_context": "We achieve state-of-the-art AP accuracy on the Street-Hazard dataset despite a strong domain shift between our negative dataset (ImageNet-1k-bb) and the test dataset.", "mention_start": 107, "mention_end": 165, "dataset_mention": "our negative dataset (ImageNet-1k-bb) and the test dataset"}, {"mentioned_in_paper": "646", "context_id": "315", "dataset_context": "Most reported experiments feature the same model, hyper parameters, training procedure, and the negative dataset: only the inliers are different.", "mention_start": 87, "mention_end": 112, "dataset_mention": " and the negative dataset"}, {"mentioned_in_paper": "647", "context_id": "124", "dataset_context": "Dataset We train and validate our method on the MS-COCO 2017 dataset.", "mention_start": 44, "mention_end": 68, "dataset_mention": "the MS-COCO 2017 dataset"}, {"mentioned_in_paper": "650", "context_id": "56", "dataset_context": "We hypothesize that the synthetic voices of current conversational agents, built on a read speech dataset, cause humans to behave as if they are mere machines rather than social actors.", "mention_start": 83, "mention_end": 105, "dataset_mention": "a read speech dataset"}, {"mentioned_in_paper": "650", "context_id": "57", "dataset_context": "We also hypothesize that humans will exhibit more social responses when interacting with a conversational agent that has a synthetic voice built on a spontaneous speech dataset.", "mention_start": 148, "mention_end": 176, "dataset_mention": "a spontaneous speech dataset"}, {"mentioned_in_paper": "651", "context_id": "45", "dataset_context": "In the supervised scheme, batch and online data settings are considered.", "mention_start": 25, "mention_end": 51, "dataset_mention": " batch and online data set"}, {"mentioned_in_paper": "651", "context_id": "94", "dataset_context": "This example employed the MED data set with the settings identical to those in [12] except that only two constituent rankers, LDI and pLSI, were used to comprise the ensemble ranker for plotting purpose.", "mention_start": 22, "mention_end": 38, "dataset_mention": "the MED data set"}, {"mentioned_in_paper": "651", "context_id": "297", "dataset_context": "We emphasize that the Pr@1 of gEnM is 48% higher than that of EnM for the CISI data set and is close to 100% for the MED.", "mention_start": 70, "mention_end": 87, "dataset_mention": "the CISI data set"}, {"mentioned_in_paper": "652", "context_id": "138", "dataset_context": "We thus leveraged the World of Code dataset for gathering additional information about projects, authors, and code blobs.", "mention_start": 31, "mention_end": 43, "dataset_mention": "Code dataset"}, {"mentioned_in_paper": "652", "context_id": "586", "dataset_context": "The Devpost dataset does not include the start date of the hackathon events but it is essential information needed to answer our research questions.", "mention_start": 0, "mention_end": 19, "dataset_mention": "The Devpost dataset"}, {"mentioned_in_paper": "652", "context_id": "620", "dataset_context": "considering code clones/ snippets while looking for code reuse (e.g. by looking at the associated CTAG tokens -a dataset available in World of Code), identifying other factors that affect code reuse, including code quality [10, 12, 13], project popularity [15], the developers' mastery on the project topics [19], the supply chain of a particular software [2, 11] etc., and if copying code might have any effect on a developer's pull request being accepted [14].", "mention_start": 98, "mention_end": 120, "dataset_mention": "CTAG tokens -a dataset"}, {"mentioned_in_paper": "653", "context_id": "23", "dataset_context": "A popular benchmark of this kind is the SHREC'17 Track: 3D Hand Gesture Recognition Using a Depth and Skeletal Dataset [2], featuring dynamic gestures involving global motions and fingers' articulation that can be used to build interactive applications.", "mention_start": 89, "mention_end": 118, "dataset_mention": "a Depth and Skeletal Dataset"}, {"mentioned_in_paper": "653", "context_id": "24", "dataset_context": "Many methods for offline classification of segmented gestures have been evaluated on this benchmark or the similar Dynamic Hand Gesture dataset (DHG) 14/28 [3].", "mention_start": 103, "mention_end": 143, "dataset_mention": "the similar Dynamic Hand Gesture dataset"}, {"mentioned_in_paper": "653", "context_id": "26", "dataset_context": "Garcia-Hernando et al. [4] proposed a benchmark of hand actions captured with both RGB, depth, and magnetic sensors and inverse kinematics and the dataset.", "mention_start": 94, "mention_end": 154, "dataset_mention": " and magnetic sensors and inverse kinematics and the dataset"}, {"mentioned_in_paper": "654", "context_id": "5", "dataset_context": "Experimental results on light field datasets with wide baselines and multi-view datasets show that the proposed method significantly outperforms state-of-the-art methods both quantitatively and visually.", "mention_start": 24, "mention_end": 44, "dataset_mention": "light field datasets"}, {"mentioned_in_paper": "654", "context_id": "5", "dataset_context": "Experimental results on light field datasets with wide baselines and multi-view datasets show that the proposed method significantly outperforms state-of-the-art methods both quantitatively and visually.", "mention_start": 50, "mention_end": 88, "dataset_mention": "wide baselines and multi-view datasets"}, {"mentioned_in_paper": "654", "context_id": "30", "dataset_context": "Extensive experiments on both light field (LF) and multiview benchmark datasets demonstrate the significant superiority of our method over state-of-the-art methods.", "mention_start": 25, "mention_end": 79, "dataset_mention": "both light field (LF) and multiview benchmark datasets"}, {"mentioned_in_paper": "654", "context_id": "36", "dataset_context": "In Section 5, we conduct extensive experiments and analysis to evaluate our framework on both LF and multiview datasets and discuss the limitation of our method.", "mention_start": 88, "mention_end": 119, "dataset_mention": "both LF and multiview datasets"}, {"mentioned_in_paper": "654", "context_id": "183", "dataset_context": "We trained and tested our network on both LF and multiview datasets.", "mention_start": 37, "mention_end": 67, "dataset_mention": "both LF and multiview datasets"}, {"mentioned_in_paper": "654", "context_id": "184", "dataset_context": "For LF datasets, we reconstructed the 3-D LF containing 5 SAIs, i.e., inputting SAIs at two ends as source views to reconstruct middle three ones.", "mention_start": 4, "mention_end": 15, "dataset_mention": "LF datasets"}, {"mentioned_in_paper": "654", "context_id": "185", "dataset_context": "Specifically, we trained our network with 29 LF images from the Inria Sparse LF dataset [52].", "mention_start": 59, "mention_end": 87, "dataset_mention": "the Inria Sparse LF dataset"}, {"mentioned_in_paper": "654", "context_id": "189", "dataset_context": "The test dataset consists of 7 LF images from the Inria Sparse LF dataset [52].", "mention_start": 46, "mention_end": 73, "dataset_mention": "the Inria Sparse LF dataset"}, {"mentioned_in_paper": "654", "context_id": "192", "dataset_context": "Note that MPI [58] is a high angular-resolution LF dataset where each LF image contains 101 SAIs distributed on a scanline.", "mention_start": 22, "mention_end": 58, "dataset_mention": "a high angular-resolution LF dataset"}, {"mentioned_in_paper": "654", "context_id": "194", "dataset_context": "For multi-view datasets, we trained and tested our network on both DTU [59] and RealEstate10K [11] datasets.", "mention_start": 61, "mention_end": 107, "dataset_mention": "both DTU [59] and RealEstate10K [11] datasets"}, {"mentioned_in_paper": "654", "context_id": "195", "dataset_context": "For DTU [59] dataset, we used the dataset preprocessed by [4], and trained our network on 79 scenes, and tested on 18 scenes.", "mention_start": 4, "mention_end": 20, "dataset_mention": "DTU [59] dataset"}, {"mentioned_in_paper": "654", "context_id": "196", "dataset_context": "For RealEstate10K [11] dataset, we trained our network on 85 scenes, and tested on 17 scenes.", "mention_start": 4, "mention_end": 30, "dataset_mention": "RealEstate10K [11] dataset"}, {"mentioned_in_paper": "654", "context_id": "203", "dataset_context": "To let the MLP directly perceive correspondence relation between the source view and the target view, we used the source-totarget disparity instead of the source-to-source disparity to construct the geometry code and learn the content information in our method on the LF datasets.", "mention_start": 263, "mention_end": 279, "dataset_mention": "the LF datasets"}, {"mentioned_in_paper": "654", "context_id": "215", "dataset_context": "Table 1 lists the quantitative comparison of different methods on the Inria Sparse dataset, where it can be observed that:", "mention_start": 66, "mention_end": 90, "dataset_mention": "the Inria Sparse dataset"}, {"mentioned_in_paper": "654", "context_id": "229", "dataset_context": "We also evaluated different methods under different disparity ranges on the MPI dataset [58].", "mention_start": 72, "mention_end": 87, "dataset_mention": "the MPI dataset"}, {"mentioned_in_paper": "654", "context_id": "254", "dataset_context": "The transformation between views in these datasets has more degrees of freedom than that of LF datasets, leading to more complicated parallax structure, and thus more challenging for view synthesis methods.", "mention_start": 92, "mention_end": 103, "dataset_mention": "LF datasets"}, {"mentioned_in_paper": "654", "context_id": "255", "dataset_context": "On the DTU dataset [59], we compared the proposed method with three IBR methods named FVS [16], SVNVS [17], and Guo et al. [18], as well as two NeRFbased methods named pixelNeRF [42] and IBRNet [13].", "mention_start": 3, "mention_end": 18, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "654", "context_id": "256", "dataset_context": "On the RealEstate10K dataset [11], we compared the proposed method with SVNVS [17], Guo et al. [18], and IBRNet [13].", "mention_start": 3, "mention_end": 28, "dataset_mention": "the RealEstate10K dataset"}, {"mentioned_in_paper": "654", "context_id": "257", "dataset_context": "We did not compare the proposed method with FVS [16] and pixelNeRF [42] on the RealEstate10K dataset [11] since they require either off-the-shelf depth maps or scale matrix which are not provided by the RealEstate10K dataset [11].", "mention_start": 75, "mention_end": 100, "dataset_mention": "the RealEstate10K dataset"}, {"mentioned_in_paper": "654", "context_id": "257", "dataset_context": "We did not compare the proposed method with FVS [16] and pixelNeRF [42] on the RealEstate10K dataset [11] since they require either off-the-shelf depth maps or scale matrix which are not provided by the RealEstate10K dataset [11].", "mention_start": 199, "mention_end": 224, "dataset_mention": "the RealEstate10K dataset"}, {"mentioned_in_paper": "654", "context_id": "260", "dataset_context": "\u2022 our method achieves higher average PSNR and SSIM values than our preliminary conference version Guo et al. [18] on both DTU [59] and RealEstate10K [11], and especially the improvement on the DTU dataset (a) [10.8, 58.4 with more complicated parallax structures is more than 2 dB, which is credited to the newly proposed global content information, the adaptive PSV fusion, the feature-space warping, and the weight smoothness loss term.", "mention_start": 188, "mention_end": 204, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "654", "context_id": "267", "dataset_context": "We compared the visual results of different methods on both the DTU dataset [59] and the RealEstate10K dataset [11] in Figs. 9 and 10, which further demonstrate the advantage of our proposed method.", "mention_start": 55, "mention_end": 75, "dataset_mention": "both the DTU dataset"}, {"mentioned_in_paper": "654", "context_id": "267", "dataset_context": "We compared the visual results of different methods on both the DTU dataset [59] and the RealEstate10K dataset [11] in Figs. 9 and 10, which further demonstrate the advantage of our proposed method.", "mention_start": 55, "mention_end": 110, "dataset_mention": "both the DTU dataset [59] and the RealEstate10K dataset"}, {"mentioned_in_paper": "654", "context_id": "269", "dataset_context": "TABLE 4 Quantitative comparisons (PSNR/SSIM) of different methods on the DTU dataset [59].", "mention_start": 69, "mention_end": 84, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "654", "context_id": "278", "dataset_context": "We evaluated the proposed method with 2, 3 and 4 source views fed on the DTU dataset [59].", "mention_start": 68, "mention_end": 84, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "654", "context_id": "284", "dataset_context": "To validate the effectiveness of the key components of our method, i.e., content embedding, global embedding, adaptive PSV fusion, feature-space warping, and weightsmoothness loss, we carried out comprehensive ablation studies on the DTU dataset [59] under the setting of 2 input views.", "mention_start": 229, "mention_end": 245, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "654", "context_id": "302", "dataset_context": "According to Table 4, our method is inferior to IBRNet [13] and SVNVS [17] on some scenes of the DTU dataset, e.g., scan66, scan67 and scan106 which contain many texture-less or repeated texture regions.", "mention_start": 92, "mention_end": 108, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "654", "context_id": "306", "dataset_context": "To illustrate this limitation, we retrained and tested our network on the Tanks and Temples dataset [61] under the setting of 2 input views.", "mention_start": 69, "mention_end": 99, "dataset_mention": "the Tanks and Temples dataset"}, {"mentioned_in_paper": "654", "context_id": "307", "dataset_context": "Note that unlike the DTU [59] and   RealEstate10K [11] datasets, where the distributions of cameras poses are same on all scenes, or the camera is always moving forward, the Tanks and Temples dataset [61] has more complex and irregular camera trajectories, resulting in more various relative camera pose patterns between the source view and the target view, making it more challenging to synthesize high-quality views.", "mention_start": 15, "mention_end": 63, "dataset_mention": "the DTU [59] and   RealEstate10K [11] datasets"}, {"mentioned_in_paper": "654", "context_id": "307", "dataset_context": "Note that unlike the DTU [59] and   RealEstate10K [11] datasets, where the distributions of cameras poses are same on all scenes, or the camera is always moving forward, the Tanks and Temples dataset [61] has more complex and irregular camera trajectories, resulting in more various relative camera pose patterns between the source view and the target view, making it more challenging to synthesize high-quality views.", "mention_start": 169, "mention_end": 199, "dataset_mention": " the Tanks and Temples dataset"}, {"mentioned_in_paper": "654", "context_id": "308", "dataset_context": "Specifically, following the method FVS [16], we split the Tanks and Temples dataset [61] into training and testing datasets, i.e., 17 out of 21 scenes for training and the remaining 4 scenes for testing.", "mention_start": 53, "mention_end": 83, "dataset_mention": "the Tanks and Temples dataset"}, {"mentioned_in_paper": "654", "context_id": "314", "dataset_context": "Owing to the contentaware warping, as well as the blending and refinement modules that are elaborately designed to handle occlusions and recover spatial correlations, the proposed view synthesis framework reconstructs novel views with much higher quality on both LF datasets and multi-view datasets, compared with state-of-the-art methods.", "mention_start": 257, "mention_end": 274, "dataset_mention": "both LF datasets"}, {"mentioned_in_paper": "654", "context_id": "314", "dataset_context": "Owing to the contentaware warping, as well as the blending and refinement modules that are elaborately designed to handle occlusions and recover spatial correlations, the proposed view synthesis framework reconstructs novel views with much higher quality on both LF datasets and multi-view datasets, compared with state-of-the-art methods.", "mention_start": 257, "mention_end": 298, "dataset_mention": "both LF datasets and multi-view datasets"}, {"mentioned_in_paper": "655", "context_id": "116", "dataset_context": "If the ground-truth masks are given in the HAR datasets, our training can be easily performed.", "mention_start": 39, "mention_end": 55, "dataset_mention": "the HAR datasets"}, {"mentioned_in_paper": "655", "context_id": "117", "dataset_context": "Unfortunately, the public HAR datasets rarely provide pixel-level annotations.", "mention_start": 14, "mention_end": 38, "dataset_mention": " the public HAR datasets"}, {"mentioned_in_paper": "655", "context_id": "120", "dataset_context": "Specifically, we train the segmentation network on the MS-COCO dataset, where FPN (Lin et al. 2017) For training the masks, we only exploit the annotations of the target person and discard all the unnecessary labels.", "mention_start": 50, "mention_end": 70, "dataset_mention": "the MS-COCO dataset"}, {"mentioned_in_paper": "655", "context_id": "122", "dataset_context": "We evaluate our approach on two major public datasets, namely WIDER-Attribute and RAP.", "mention_start": 28, "mention_end": 53, "dataset_mention": "two major public datasets"}, {"mentioned_in_paper": "655", "context_id": "123", "dataset_context": "We utilize a ResNet-101 model (He et al. 2016) pre-trained on the ImageNet dataset (Deng et al. 2009), as the backbone of our Da-HAR.", "mention_start": 62, "mention_end": 82, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "655", "context_id": "129", "dataset_context": "Note that we keep the ratio of height/width at 2 for the RAP dataset.", "mention_start": 53, "mention_end": 68, "dataset_mention": "the RAP dataset"}, {"mentioned_in_paper": "655", "context_id": "141", "dataset_context": "We can see that our approach outperforms all the existing methods on the WIDER-Attribute dataset.", "mention_start": 69, "mention_end": 96, "dataset_mention": "the WIDER-Attribute dataset"}, {"mentioned_in_paper": "655", "context_id": "154", "dataset_context": "The samples in the RAP dataset are collected from real world surveillance scenarios, and compared to the ones in WIDER-Attribute, there are less distractions.", "mention_start": 15, "mention_end": 30, "dataset_mention": "the RAP dataset"}, {"mentioned_in_paper": "655", "context_id": "161", "dataset_context": "To validate our contributions, we further perform ablation studies on the WIDER-Attribute dataset.", "mention_start": 69, "mention_end": 97, "dataset_mention": "the WIDER-Attribute dataset"}, {"mentioned_in_paper": "655", "context_id": "201", "dataset_context": "Extensive experiments are carried out on the WIDER-Attribute and RAP datasets and state of the art results are reached, which demonstrate the effectiveness of the proposed Da-HAR.", "mention_start": 41, "mention_end": 77, "dataset_mention": "the WIDER-Attribute and RAP datasets"}, {"mentioned_in_paper": "656", "context_id": "53", "dataset_context": "In the vision task, Mnih et al. [42] first propose a recurrent visual attention model to control the amount of computation on the augmented MNIST dataset [35].", "mention_start": 125, "mention_end": 153, "dataset_mention": "the augmented MNIST dataset"}, {"mentioned_in_paper": "656", "context_id": "116", "dataset_context": "We demonstrate some experiments on toy datasets in Appendix B to better understand how the RDA mechanism works.", "mention_start": 35, "mention_end": 47, "dataset_mention": "toy datasets"}, {"mentioned_in_paper": "656", "context_id": "172", "dataset_context": "Although we intuitively propose to train MGNet by gradient re-scaling, it harms the convergence speed, and such that we cannot afford to explore MGNet on ImageNet dataset.", "mention_start": 153, "mention_end": 170, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "656", "context_id": "179", "dataset_context": "We visualize the predicted glimpse-region results taken from ImageNet100 validation dataset in Figure 7 which includes three common cases: 1) Figure 7 (a) shows the examples that the model has correct prediction from the first glimpse while still seeking a precise glimpseregion.", "mention_start": 61, "mention_end": 91, "dataset_mention": "ImageNet100 validation dataset"}, {"mentioned_in_paper": "658", "context_id": "0", "dataset_context": "When analyzing a dataset, it can be useful to assess how smooth the decision boundaries need to be for a model to better fit the data.", "mention_start": 5, "mention_end": 24, "dataset_mention": "analyzing a dataset"}, {"mentioned_in_paper": "658", "context_id": "20", "dataset_context": "These methods, however, require a database of use-cases (datasets and their associated preferred algorithms) whose clustering would provide general guidance on model selection, and might need to be retrained when adding new use-cases.", "mention_start": 45, "mention_end": 65, "dataset_mention": "use-cases (datasets"}, {"mentioned_in_paper": "658", "context_id": "117", "dataset_context": "First, the synthetic sim 1000 3 dataset contains two non-linearly separable classes, each of them generated by a Gaussian distribution.", "mention_start": 6, "mention_end": 39, "dataset_mention": " the synthetic sim 1000 3 dataset"}, {"mentioned_in_paper": "658", "context_id": "118", "dataset_context": "Regarding real data, the Gastrointestinal Lesions in Regular Colonoscopy dataset (lesions) was chosen as it is high-dimensional and in such cases DTs can be better in selecting only the most informative features [4].", "mention_start": 52, "mention_end": 80, "dataset_mention": "Regular Colonoscopy dataset"}, {"mentioned_in_paper": "658", "context_id": "119", "dataset_context": "The mushroom dataset is interesting as it can be accurately modeled using simple rules.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The mushroom dataset"}, {"mentioned_in_paper": "658", "context_id": "120", "dataset_context": "The rest of the datasets used are cases where the underlying structure is not as clear as above: the Spambase (spam), the Congressional Voting Records (votes), Student Performance (student-math), and the Wine (wine) datasets, as well as a part of the Vehicle Silhouettes dataset (xab).", "mention_start": 195, "mention_end": 224, "dataset_mention": " and the Wine (wine) datasets"}, {"mentioned_in_paper": "658", "context_id": "120", "dataset_context": "The rest of the datasets used are cases where the underlying structure is not as clear as above: the Spambase (spam), the Congressional Voting Records (votes), Student Performance (student-math), and the Wine (wine) datasets, as well as a part of the Vehicle Silhouettes dataset (xab).", "mention_start": 246, "mention_end": 278, "dataset_mention": "the Vehicle Silhouettes dataset"}, {"mentioned_in_paper": "659", "context_id": "43", "dataset_context": "\u2022 The proposed SSDR-AL achieves state-of-the-art performance on S3DIS [2] and Semantic3D [9] datasets, which significantly reduces the annotation cost by up to 63.0% and 24.0% compared to the baseline method in achieving 90% performance of fully supervised learning, respectively.", "mention_start": 64, "mention_end": 101, "dataset_mention": "S3DIS [2] and Semantic3D [9] datasets"}, {"mentioned_in_paper": "659", "context_id": "151", "dataset_context": "To verify the strong generalization of SSDR-AL, we also conduct experiments on the Semantic3D dataset, which contains 15 point clouds and 8 classes for training.", "mention_start": 78, "mention_end": 101, "dataset_mention": "the Semantic3D dataset"}, {"mentioned_in_paper": "659", "context_id": "156", "dataset_context": "We produce 456, 764 superpoints and randomly select 0.5% superpoints with labels as seed samples to initialize the labeled set on S3DIS [2] dataset (produce 456, 764 superpoints and randomly select 0.8% superpoints with labels as seed samples on Semantic3D [9] dataset).", "mention_start": 129, "mention_end": 147, "dataset_mention": "S3DIS [2] dataset"}, {"mentioned_in_paper": "659", "context_id": "156", "dataset_context": "We produce 456, 764 superpoints and randomly select 0.5% superpoints with labels as seed samples to initialize the labeled set on S3DIS [2] dataset (produce 456, 764 superpoints and randomly select 0.8% superpoints with labels as seed samples on Semantic3D [9] dataset).", "mention_start": 245, "mention_end": 268, "dataset_mention": "Semantic3D [9] dataset"}, {"mentioned_in_paper": "659", "context_id": "158", "dataset_context": "Randlanet has trained 30 epochs with the initial learning rate 0.01 that decreases by 16% after each epoch on S3DIS dataset (has trained 50 epochs with the initial learning rate 0.01 that decreases by 8% after each epoch on Semantic3D dataset).", "mention_start": 110, "mention_end": 123, "dataset_mention": "S3DIS dataset"}, {"mentioned_in_paper": "659", "context_id": "158", "dataset_context": "Randlanet has trained 30 epochs with the initial learning rate 0.01 that decreases by 16% after each epoch on S3DIS dataset (has trained 50 epochs with the initial learning rate 0.01 that decreases by 8% after each epoch on Semantic3D dataset).", "mention_start": 224, "mention_end": 242, "dataset_mention": "Semantic3D dataset"}, {"mentioned_in_paper": "659", "context_id": "160", "dataset_context": "On S3DIS dataset, we select 10k (3k on Seman-tic3D dataset) the most informative and representative candidate superpoints from the unlabeled set and then utilize a noise-aware iterative labeling strategy with the threshold \u03b8 = 0.9 to annotate them until the 10k (3k on Semantic3D dataset) click budget is exhausted.", "mention_start": 3, "mention_end": 16, "dataset_mention": "S3DIS dataset"}, {"mentioned_in_paper": "659", "context_id": "160", "dataset_context": "On S3DIS dataset, we select 10k (3k on Seman-tic3D dataset) the most informative and representative candidate superpoints from the unlabeled set and then utilize a noise-aware iterative labeling strategy with the threshold \u03b8 = 0.9 to annotate them until the 10k (3k on Semantic3D dataset) click budget is exhausted.", "mention_start": 38, "mention_end": 58, "dataset_mention": "Seman-tic3D dataset"}, {"mentioned_in_paper": "659", "context_id": "160", "dataset_context": "On S3DIS dataset, we select 10k (3k on Seman-tic3D dataset) the most informative and representative candidate superpoints from the unlabeled set and then utilize a noise-aware iterative labeling strategy with the threshold \u03b8 = 0.9 to annotate them until the 10k (3k on Semantic3D dataset) click budget is exhausted.", "mention_start": 268, "mention_end": 287, "dataset_mention": "Semantic3D dataset"}, {"mentioned_in_paper": "659", "context_id": "165", "dataset_context": "We compare SSDR-AL with other state-of-the-art methods on S3DIS [2] and Semantic3D [9] datasets shown in Figure 3.", "mention_start": 58, "mention_end": 95, "dataset_mention": "S3DIS [2] and Semantic3D [9] datasets"}, {"mentioned_in_paper": "659", "context_id": "168", "dataset_context": "From these observations in Figure 3 (a), we conclude the findings that our SSDR-AL significantly outperforms other methods at a fixed amount of annotation budget measured in clicks, especially it has been obviously in Table 2. Comparing the percentage of labeled points required to achieve 90% accuracy on S3DIS dataset for different active learning methods. is the results in the original paper [34].", "mention_start": 305, "mention_end": 319, "dataset_mention": "S3DIS dataset"}, {"mentioned_in_paper": "659", "context_id": "177", "dataset_context": "It is worth noting that the red line stops early be- 1 The mIoU performance of Randlanet [10] based on fully supervised learning is 64.72% on S3DIS dataset.", "mention_start": 142, "mention_end": 155, "dataset_mention": "S3DIS dataset"}, {"mentioned_in_paper": "659", "context_id": "186", "dataset_context": "To better understand the effectiveness of the weightbased superpoint uncertainty estimation, spatial-structural diversity reasoning, and noise-aware iterative labeling, we conduct several ablation studies on S3DIS [2] dataset.", "mention_start": 207, "mention_end": 225, "dataset_mention": "S3DIS [2] dataset"}, {"mentioned_in_paper": "659", "context_id": "214", "dataset_context": "We show the effectiveness of SSDR-AL on both S3DIS and Seman-tic3D datasets through systematic and comprehensive experiments.", "mention_start": 40, "mention_end": 75, "dataset_mention": "both S3DIS and Seman-tic3D datasets"}, {"mentioned_in_paper": "660", "context_id": "5", "dataset_context": "The proposed approach is implemented in AnyLogic \u00ae ABS software with a real travel data set of Los Angeles, California.", "mention_start": 69, "mention_end": 91, "dataset_mention": "a real travel data set"}, {"mentioned_in_paper": "662", "context_id": "10", "dataset_context": "Furthermore, we survey publicly available underwater fish datasets, and compare various DL techniques in the underwater fish monitoring domains.", "mention_start": 12, "mention_end": 66, "dataset_mention": " we survey publicly available underwater fish datasets"}, {"mentioned_in_paper": "662", "context_id": "41", "dataset_context": "We also survey publicly available underwater fish image datasets.", "mention_start": 8, "mention_end": 64, "dataset_mention": "survey publicly available underwater fish image datasets"}, {"mentioned_in_paper": "662", "context_id": "46", "dataset_context": "We also provide a detailed analysis of fish datasets and comprehensively review the literature on four key tasks in underwater fish video and image processing.", "mention_start": 39, "mention_end": 52, "dataset_mention": "fish datasets"}, {"mentioned_in_paper": "662", "context_id": "53", "dataset_context": "An interested reader should study (Yang et al., 2021) before reading our paper, due to the background technical details provided on image acquisition, which are key to developing effective DL datasets and models, as we discussed in our paper.", "mention_start": 178, "mention_end": 200, "dataset_mention": "effective DL datasets"}, {"mentioned_in_paper": "662", "context_id": "183", "dataset_context": "Perhaps, the most important factor when considering a supervised learning dataset is its size.", "mention_start": 51, "mention_end": 81, "dataset_mention": "a supervised learning dataset"}, {"mentioned_in_paper": "662", "context_id": "193", "dataset_context": "The second factor to consider when preparing a dataset for DL training is having a balance.", "mention_start": 35, "mention_end": 54, "dataset_mention": "preparing a dataset"}, {"mentioned_in_paper": "662", "context_id": "209", "dataset_context": "Table 1 lists publicly available underwater fish datasets, their sources, and where to get them, in addition to a summary of their features, their labels, and their sizes.", "mention_start": 0, "mention_end": 57, "dataset_mention": "Table 1 lists publicly available underwater fish datasets"}, {"mentioned_in_paper": "662", "context_id": "211", "dataset_context": "Although the number of these fish datasets is still small (17), the diversity of aquatic species they cover is already quite wide.", "mention_start": 23, "mention_end": 42, "dataset_mention": "these fish datasets"}, {"mentioned_in_paper": "662", "context_id": "259", "dataset_context": "In later stages, they can customise their model to adequately capture their dataset.", "mention_start": 50, "mention_end": 83, "dataset_mention": "adequately capture their dataset"}, {"mentioned_in_paper": "662", "context_id": "305", "dataset_context": "Deepfish dataset described in the first row of Table 1).", "mention_start": 0, "mention_end": 16, "dataset_mention": "Deepfish dataset"}, {"mentioned_in_paper": "662", "context_id": "308", "dataset_context": "FishPak dataset in Table 1) present in them.", "mention_start": 0, "mention_end": 15, "dataset_mention": "FishPak dataset"}, {"mentioned_in_paper": "662", "context_id": "332", "dataset_context": "One possible reason for the lack of comprehensive research for fish counting is the scarcity of large publicly available underwater fish datasets.", "mention_start": 96, "mention_end": 145, "dataset_mention": "large publicly available underwater fish datasets"}, {"mentioned_in_paper": "662", "context_id": "333", "dataset_context": "In addition, properly annotating fish datasets to train robust DL models is time-prohibitive and expensive.", "mention_start": 32, "mention_end": 46, "dataset_mention": "fish datasets"}, {"mentioned_in_paper": "662", "context_id": "351", "dataset_context": "They achieved fish detection F-scores et al., 2014) and their own dataset, respectively.", "mention_start": 45, "mention_end": 73, "dataset_mention": " 2014) and their own dataset"}, {"mentioned_in_paper": "662", "context_id": "359", "dataset_context": "The YOLO technique achieved Mean Average Precision (mAP) of 86.96% on the Fish4Knowledge dataset (Giordano et al., 2016).", "mention_start": 70, "mention_end": 96, "dataset_mention": "the Fish4Knowledge dataset"}, {"mentioned_in_paper": "662", "context_id": "456", "dataset_context": "Due to these requirements, making a large dataset is most of the time, very challenging, which makes the datasets limited and small.", "mention_start": 94, "mention_end": 113, "dataset_mention": "makes the datasets"}, {"mentioned_in_paper": "662", "context_id": "464", "dataset_context": "In contrast to the techniques listed above for improving model generalisation, data Augmentation addresses overfitting from the source of the problem (i.e. the original dataset).", "mention_start": 137, "mention_end": 176, "dataset_mention": "the problem (i.e. the original dataset"}, {"mentioned_in_paper": "662", "context_id": "481", "dataset_context": "This is extremely useful since many image datasets have lowlevel spatial features and properties that are better learnt in massive datasets.", "mention_start": 123, "mention_end": 139, "dataset_mention": "massive datasets"}, {"mentioned_in_paper": "662", "context_id": "500", "dataset_context": "Weak supervision in particular can be very useful in underwater fish monitoring, where the limited dataset size and the time-and cost-prohibitive nature of labelling limits achieving a useful dataset for developing effective, smart, and automated habitat monitoring tools and techniques.", "mention_start": 172, "mention_end": 199, "dataset_mention": "achieving a useful dataset"}, {"mentioned_in_paper": "663", "context_id": "4", "dataset_context": "On the COCO dataset, HoughNet's best model achieves 46.4 AP (and 65.1 AP 50 ), performing on par with the state-of-the-art in bottom-up object detection and outperforming most major one-stage and two-stage methods.", "mention_start": 3, "mention_end": 19, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "663", "context_id": "36", "dataset_context": "On the COCO dataset, HoughNet achieves comparable results with the state-of-the-art bottom-up detector CenterNet [16], while being the fastest among bottom-up detectors.", "mention_start": 3, "mention_end": 19, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "663", "context_id": "206", "dataset_context": "It is a subset of the COCO train2017 dataset, containing 25K images (about 20% of train2017) and around 184K objects across 80 object categories.", "mention_start": 18, "mention_end": 44, "dataset_mention": "the COCO train2017 dataset"}, {"mentioned_in_paper": "663", "context_id": "210", "dataset_context": "Further details on minitrain and the dataset itself can be found at https://github.com/giddyyupp/", "mention_start": 19, "mention_end": 44, "dataset_mention": "minitrain and the dataset"}, {"mentioned_in_paper": "663", "context_id": "291", "dataset_context": "Candle is not among the 80 classes of COCO dataset.", "mention_start": 38, "mention_end": 50, "dataset_mention": "COCO dataset"}, {"mentioned_in_paper": "663", "context_id": "294", "dataset_context": "is the number of classes) matrix to visualize voting relations among classes on the COCO dataset using the R-101-DCN backbone of HoughNet.", "mention_start": 80, "mention_end": 96, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "663", "context_id": "307", "dataset_context": "This linear dependence on the number of object classes might be problematic when the number of classes increases dramatically, which is the case for newer datasets such as the 1000-class LVIS dataset [63].", "mention_start": 171, "mention_end": 199, "dataset_mention": "the 1000-class LVIS dataset"}, {"mentioned_in_paper": "663", "context_id": "323", "dataset_context": "We conducted our experiments on the ImageNet VID dataset [64].", "mention_start": 32, "mention_end": 56, "dataset_mention": "the ImageNet VID dataset"}, {"mentioned_in_paper": "663", "context_id": "324", "dataset_context": "ImageNet VID dataset has 30 object categories.", "mention_start": 0, "mention_end": 20, "dataset_mention": "ImageNet VID dataset"}, {"mentioned_in_paper": "663", "context_id": "350", "dataset_context": "To extend HoughNet for the instance segmentation task, we add the new branches as  Detection Voters Fig. 10 : Sample car detection of HoughNet from KITTI dataset and its vote map.", "mention_start": 147, "mention_end": 161, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "663", "context_id": "355", "dataset_context": "We present the instance segmentation results on the COCO dataset in Table 9.", "mention_start": 48, "mention_end": 64, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "663", "context_id": "378", "dataset_context": "We experimented with car classes of KITTI dataset [69] using the training and validation splits from SubCNN [70].", "mention_start": 36, "mention_end": 49, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "663", "context_id": "399", "dataset_context": "We conducted our experiments on COCO dataset which has 17 keypoints for person object instances.", "mention_start": 32, "mention_end": 44, "dataset_mention": "COCO dataset"}, {"mentioned_in_paper": "663", "context_id": "432", "dataset_context": "For quantitative comparison, we use the Cityscapes [77] dataset.", "mention_start": 35, "mention_end": 63, "dataset_mention": "the Cityscapes [77] dataset"}, {"mentioned_in_paper": "664", "context_id": "84", "dataset_context": "We use the body expression dataset 1 for the evaluation of the proposed method.", "mention_start": 7, "mention_end": 34, "dataset_mention": "the body expression dataset"}, {"mentioned_in_paper": "664", "context_id": "92", "dataset_context": "First, we double the dataset by swapping left and right side of body poses since this swap will not change the expressed emotional intensity.", "mention_start": 6, "mention_end": 28, "dataset_mention": " we double the dataset"}, {"mentioned_in_paper": "664", "context_id": "93", "dataset_context": "Second, we sample from raw pose sequences (120Hz) at a fixed rate(30Hz) and get four samples from every pose sequence, then further quadrupling the dataset.", "mention_start": 118, "mention_end": 155, "dataset_mention": " then further quadrupling the dataset"}, {"mentioned_in_paper": "665", "context_id": "122", "dataset_context": "Visual Space and Dataset Split The visual features with 2048 dimensions we use in all experiments are extracted by powerful deep Convolutional Neural Networks (CNN), ResNet [12], which is pre-trained with ImageNet [32].", "mention_start": 0, "mention_end": 24, "dataset_mention": "Visual Space and Dataset"}, {"mentioned_in_paper": "665", "context_id": "124", "dataset_context": "Training Details For aligned feature generator, we set the aligned space dimension of coarse-grained dataset (AWA2, APY) to 64 and fine-grained datasets (CUB, SUN, FLO) to 256, because fine-grained datasets often need more information to train an effective classifier.", "mention_start": 85, "mention_end": 108, "dataset_mention": "coarse-grained dataset"}, {"mentioned_in_paper": "665", "context_id": "125", "dataset_context": "For coarse-grained datasets, all the network setting comes from [33].", "mention_start": 4, "mention_end": 27, "dataset_mention": "coarse-grained datasets"}, {"mentioned_in_paper": "668", "context_id": "2", "dataset_context": "Our method is verified on the validation set of the Body Language Dataset (BoLD) and achieves 0.26235 Emotion Recognition Score on the test set, surpassing the previous best result of 0.2530.", "mention_start": 48, "mention_end": 73, "dataset_mention": "the Body Language Dataset"}, {"mentioned_in_paper": "668", "context_id": "28", "dataset_context": "The CAER video dataset for context-based emotion recognition was presented in [17], along with a two-stream architecture which employed adaptive-fusion to merge the two steams.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The CAER video dataset"}, {"mentioned_in_paper": "668", "context_id": "29", "dataset_context": "In [21], Mittal et al. designed a deep architecture with several branches, focusing on different interpretations of the surrounding context (e.g., environment and interaction context) to significantly increase resulting predictions in the EMOTIC dataset.", "mention_start": 234, "mention_end": 253, "dataset_mention": "the EMOTIC dataset"}, {"mentioned_in_paper": "668", "context_id": "33", "dataset_context": "The dataset used in the challenge is the BoLD (Body Language Dataset) corpus [19] consisting of 9,876 video clips of humans expressing emotion, primarily through body movements.", "mention_start": 37, "mention_end": 68, "dataset_mention": "the BoLD (Body Language Dataset"}, {"mentioned_in_paper": "668", "context_id": "44", "dataset_context": "TSNs have already been shown to achieve good results for the BoLD dataset in its introductory paper [19].", "mention_start": 57, "mention_end": 73, "dataset_mention": "the BoLD dataset"}, {"mentioned_in_paper": "669", "context_id": "10", "dataset_context": "A large IntelTAU dataset was published last year [4].", "mention_start": 0, "mention_end": 24, "dataset_mention": "A large IntelTAU dataset"}, {"mentioned_in_paper": "669", "context_id": "11", "dataset_context": "It solves several problems with previously presented datasets.", "mention_start": 32, "mention_end": 61, "dataset_mention": "previously presented datasets"}, {"mentioned_in_paper": "669", "context_id": "13", "dataset_context": "Another quite large and diverse dataset, also introduced last year, is CubePlus [8].", "mention_start": 0, "mention_end": 39, "dataset_mention": "Another quite large and diverse dataset"}, {"mentioned_in_paper": "669", "context_id": "17", "dataset_context": "It lists the technical and scientific difficulties encountered in preparing a new dataset for the 2-nd Illumination Estimation Challenge starting 1-st July 2020, which is organized by the Institute for Information Transmission Problems (IITP RAS) in cooperation with University of Zagreb, Croatia.", "mention_start": 66, "mention_end": 89, "dataset_mention": "preparing a new dataset"}, {"mentioned_in_paper": "669", "context_id": "41", "dataset_context": "This drawback can only be eliminated with the help of the three-dimensional color targets, as was done, for example, in the [7] dataset.", "mention_start": 119, "mention_end": 135, "dataset_mention": "the [7] dataset"}, {"mentioned_in_paper": "669", "context_id": "49", "dataset_context": "Even a record-setting IntelTAU dataset contains no more than three thousand photos per sensor, which often contains repeated images of the same scene; moreover, this applies to older datasets as well.", "mention_start": 5, "mention_end": 38, "dataset_mention": "a record-setting IntelTAU dataset"}, {"mentioned_in_paper": "669", "context_id": "49", "dataset_context": "Even a record-setting IntelTAU dataset contains no more than three thousand photos per sensor, which often contains repeated images of the same scene; moreover, this applies to older datasets as well.", "mention_start": 176, "mention_end": 191, "dataset_mention": "older datasets"}, {"mentioned_in_paper": "669", "context_id": "50", "dataset_context": "For example, a classic dataset for light source estimation, ColorChecker [5], has less than a thousand images.", "mention_start": 12, "mention_end": 30, "dataset_mention": " a classic dataset"}, {"mentioned_in_paper": "669", "context_id": "53", "dataset_context": "Each subsequent dataset is often collected using a camera different from the ones used to collect previous datasets, and color targets may also differ.", "mention_start": 0, "mention_end": 23, "dataset_mention": "Each subsequent dataset"}, {"mentioned_in_paper": "669", "context_id": "54", "dataset_context": "Thus, it is problematic to combine the accumulated datasets into one.", "mention_start": 34, "mention_end": 59, "dataset_mention": "the accumulated datasets"}, {"mentioned_in_paper": "669", "context_id": "56", "dataset_context": "Rare examples of consistent dataset additions are CubePlus, which expands the Cube dataset, and IntelTAU, which expands IntelTUT.", "mention_start": 17, "mention_end": 35, "dataset_mention": "consistent dataset"}, {"mentioned_in_paper": "669", "context_id": "56", "dataset_context": "Rare examples of consistent dataset additions are CubePlus, which expands the Cube dataset, and IntelTAU, which expands IntelTUT.", "mention_start": 65, "mention_end": 90, "dataset_mention": "expands the Cube dataset"}, {"mentioned_in_paper": "669", "context_id": "57", "dataset_context": "A possible solution to the above problems is to substantially expand and annotate one of the large modern datasets containing a three-dimensional color target.", "mention_start": 89, "mention_end": 114, "dataset_mention": "the large modern datasets"}, {"mentioned_in_paper": "669", "context_id": "58", "dataset_context": "The authors adhere to this approach using the CubePlus dataset as a basis.", "mention_start": 42, "mention_end": 62, "dataset_mention": "the CubePlus dataset"}, {"mentioned_in_paper": "669", "context_id": "59", "dataset_context": "Most modern and well-known datasets use flat color targets of different manufacturers, such datasets include: NUS [9], REC [5], INTEL-TUT [10], INTEL-TAU [4], MLS [6].", "mention_start": 0, "mention_end": 35, "dataset_mention": "Most modern and well-known datasets"}, {"mentioned_in_paper": "669", "context_id": "63", "dataset_context": "Among three-dimensional color targets that were used in datasets, only two can be listed: a gray ball (as in the GrayBall [7] dataset) and SpyderCube (CubePlus [8]).", "mention_start": 108, "mention_end": 133, "dataset_mention": "the GrayBall [7] dataset"}, {"mentioned_in_paper": "669", "context_id": "72", "dataset_context": "Hence, it is advisable to use Datacolor's SpyderCube (Fig. 3) to create the most representative and largest dataset among existing ones -about 2000 images have already been collected with it, and this target makes it possible to determine the plane on the color histogram that contains the sources' colors.", "mention_start": 71, "mention_end": 115, "dataset_mention": "the most representative and largest dataset"}, {"mentioned_in_paper": "670", "context_id": "90", "dataset_context": "For example, let a microarray data set have 10 gene numbers 10 feature numbers which can be represented by", "mention_start": 12, "mention_end": 38, "dataset_mention": " let a microarray data set"}, {"mentioned_in_paper": "670", "context_id": "97", "dataset_context": "We used six multi-category cancer-related human gene expression data sets [4], which were downloaded from http://www.gems-system.org", "mention_start": 8, "mention_end": 73, "dataset_mention": "six multi-category cancer-related human gene expression data sets"}, {"mentioned_in_paper": "670", "context_id": "121", "dataset_context": "In this study, we tested and compared a hybrid filter and wrapper feature selection method's performance on the classification of six multi-category cancer microarray expression data sets.", "mention_start": 129, "mention_end": 187, "dataset_mention": "six multi-category cancer microarray expression data sets"}, {"mentioned_in_paper": "671", "context_id": "58", "dataset_context": "Deit [10] introduces several training strategies that allow ViT to be also effective when using the smaller ImageNet-1K dataset.", "mention_start": 96, "mention_end": 127, "dataset_mention": "the smaller ImageNet-1K dataset"}, {"mentioned_in_paper": "671", "context_id": "166", "dataset_context": "There are two accepted datasets to evaluate the performance of WSOL methods, CUB-200 [11] and ImageNet-1K [7].", "mention_start": 10, "mention_end": 31, "dataset_mention": "two accepted datasets"}, {"mentioned_in_paper": "671", "context_id": "168", "dataset_context": "ImageNet-1K is an outstanding classification dataset with 1000 classes, containing 1,281,197 training images and 50000 validation images.", "mention_start": 15, "mention_end": 52, "dataset_mention": "an outstanding classification dataset"}, {"mentioned_in_paper": "671", "context_id": "199", "dataset_context": "In this section, we will verify the components of CaFT by ablation study on CUB-200 dataset, and introduce the effect of some details.", "mention_start": 75, "mention_end": 91, "dataset_mention": "CUB-200 dataset"}, {"mentioned_in_paper": "671", "context_id": "224", "dataset_context": "Experiments on the CUB-200 and ImageNet-1K datasets show the effectiveness of CaFT.", "mention_start": 15, "mention_end": 51, "dataset_mention": "the CUB-200 and ImageNet-1K datasets"}, {"mentioned_in_paper": "672", "context_id": "2", "dataset_context": "While smaller-scale experiments have been previously conducted, herein we carry out a large-scale investigation, specifically, one involving 26 ML algorithms, 250 datasets (regression and both binary and multinomial classification), 6 score metrics, and 28,857,600 algorithm runs.", "mention_start": 158, "mention_end": 171, "dataset_mention": " 250 datasets"}, {"mentioned_in_paper": "672", "context_id": "22", "dataset_context": "They also conducted an empirical study involving 38 binary classification datasets from OpenML, and six ML algorithms: elastic net, decision tree, k-nearest neighbors, support vector machine, random forest, and xgboost.", "mention_start": 39, "mention_end": 82, "dataset_mention": "involving 38 binary classification datasets"}, {"mentioned_in_paper": "672", "context_id": "26", "dataset_context": "They performed an empirical study involving 59 datasets from OpenML and two ML algorithms: support vector machine and random forest.", "mention_start": 34, "mention_end": 55, "dataset_mention": "involving 59 datasets"}, {"mentioned_in_paper": "672", "context_id": "36", "dataset_context": "\u2022 Consider significantly more datasets.", "mention_start": 11, "mention_end": 38, "dataset_mention": "significantly more datasets"}, {"mentioned_in_paper": "672", "context_id": "38", "dataset_context": "Our setup involves numerous runs across a plethora of algorithms and datasets, comparing tuned and untuned performance over six distinct metrics.", "mention_start": 54, "mention_end": 77, "dataset_mention": "algorithms and datasets"}, {"mentioned_in_paper": "672", "context_id": "41", "dataset_context": "We used the recently introduced PMLB repository [9], which includes 166 classification datasets and 122 regression datasets.", "mention_start": 67, "mention_end": 95, "dataset_mention": "166 classification datasets"}, {"mentioned_in_paper": "672", "context_id": "41", "dataset_context": "We used the recently introduced PMLB repository [9], which includes 166 classification datasets and 122 regression datasets.", "mention_start": 67, "mention_end": 123, "dataset_mention": "166 classification datasets and 122 regression datasets"}, {"mentioned_in_paper": "672", "context_id": "42", "dataset_context": "As we were interested in performing numerous runs, we retained the 144 classification datasets with number of samples \u2264 10992 and number of features \u2264 100, and the 106 regression datasets with number of samples \u2264 8192 and number of features \u2264 100. Figure 1 presents a summary of dataset characteristics.", "mention_start": 50, "mention_end": 94, "dataset_mention": " we retained the 144 classification datasets"}, {"mentioned_in_paper": "672", "context_id": "42", "dataset_context": "As we were interested in performing numerous runs, we retained the 144 classification datasets with number of samples \u2264 10992 and number of features \u2264 100, and the 106 regression datasets with number of samples \u2264 8192 and number of features \u2264 100. Figure 1 presents a summary of dataset characteristics.", "mention_start": 155, "mention_end": 187, "dataset_mention": " and the 106 regression datasets"}, {"mentioned_in_paper": "672", "context_id": "88", "dataset_context": "We performed a large-scale experiment of hyperparameter-tuning effectiveness, across multiple ML algorithms and datasets.", "mention_start": 84, "mention_end": 120, "dataset_mention": "multiple ML algorithms and datasets"}, {"mentioned_in_paper": "673", "context_id": "195", "dataset_context": "We We evaluated on the MNIST and CIFAR-10 image datasets, following Neal et al. (2018) to split each dataset into 6 inlier classes and 4 outlier classes.", "mention_start": 19, "mention_end": 56, "dataset_mention": "the MNIST and CIFAR-10 image datasets"}, {"mentioned_in_paper": "673", "context_id": "221", "dataset_context": "Much of the focus regarding recent work on Bayesian neural networks concerns their performance on open-category and out-of-distribution tasks with high dimensional image datasets.", "mention_start": 147, "mention_end": 178, "dataset_mention": "high dimensional image datasets"}, {"mentioned_in_paper": "674", "context_id": "4", "dataset_context": "We train our 3D-PFNet using a three-step training strategy to leverage a diverse source of training data, including image and video based human pose datasets and 3D motion capture (MoCap) data.", "mention_start": 115, "mention_end": 157, "dataset_mention": "image and video based human pose datasets"}, {"mentioned_in_paper": "674", "context_id": "38", "dataset_context": "Our 3D Pose Forecasting Network (3D-PFNet) is trained by leveraging a diverse source of training data, including image and video based human pose datasets and MoCap data.", "mention_start": 112, "mention_end": 154, "dataset_mention": "image and video based human pose datasets"}, {"mentioned_in_paper": "674", "context_id": "71", "dataset_context": "The recently introduced hourglass networks [19] have demonstrated state-of-the-art performance on large-scale human pose datasets [2].", "mention_start": 98, "mention_end": 129, "dataset_mention": "large-scale human pose datasets"}, {"mentioned_in_paper": "674", "context_id": "122", "dataset_context": "However, 3D pose data is hard to collect and thus are often unavailable in in-the-wild human pose datasets.", "mention_start": 74, "mention_end": 106, "dataset_mention": "in-the-wild human pose datasets"}, {"mentioned_in_paper": "674", "context_id": "129", "dataset_context": "We therefore pre-train the hourglass network by lever-aging large human pose datasets that provide 2D body joint annotations.", "mention_start": 48, "mention_end": 85, "dataset_mention": "lever-aging large human pose datasets"}, {"mentioned_in_paper": "674", "context_id": "148", "dataset_context": "Dataset We evaluate pose forecasting in 2D using the Penn Action dataset [39].", "mention_start": 49, "mention_end": 72, "dataset_mention": "the Penn Action dataset"}, {"mentioned_in_paper": "674", "context_id": "152", "dataset_context": "During training, we also leverage two other datasets: MPII Human Pose (MPII) [2] and Human3.6M", "mention_start": 24, "mention_end": 52, "dataset_mention": "leverage two other datasets"}, {"mentioned_in_paper": "674", "context_id": "246", "dataset_context": "We train the 3D-PFNet using a three-step training strategy to leverage a diverse source of training data, including image and video based human pose datasets and 3D Mo-Cap data.", "mention_start": 115, "mention_end": 157, "dataset_mention": "image and video based human pose datasets"}, {"mentioned_in_paper": "675", "context_id": "25", "dataset_context": "State merging arXiv: 2201.12451v3 [cs. LG] 14 Apr 2022 works by first building a prefix tree from a finite dataset: a deterministic automaton that simply memorizes the training data, and will not recognize any held-out strings beyond the finite set used to build the prefix tree.", "mention_start": 97, "mention_end": 114, "dataset_mention": "a finite dataset"}, {"mentioned_in_paper": "675", "context_id": "130", "dataset_context": "To enforce that the dataset is roughly balanced across sequence lengths, we sample half the x uniformly over \u03a3 n 2 , and, for the other half, enforce that the full string x must be valid in L. Given some x, the y's are deterministic to compute.", "mention_start": 3, "mention_end": 27, "dataset_mention": "enforce that the dataset"}, {"mentioned_in_paper": "675", "context_id": "156", "dataset_context": "Our results are not directly comparable with Weiss et al. (2018b) on the same data, as their method learns from active membership and equivalence queries, while ours is designed for the more constrained setting of a static dataset.", "mention_start": 213, "mention_end": 230, "dataset_mention": "a static dataset"}, {"mentioned_in_paper": "675", "context_id": "201", "dataset_context": "Taking a train set of the same form as state merging, we collect all the hidden states from every prefix of every train string, each of which is associated with a label, i.e., whether the prefix is in L. This yields a dataset of the form {(h ij , y ij )}, where h ij is the RNN hidden state on word j of example i, and y ij records whether w i,:j \u2208 L.", "mention_start": 200, "mention_end": 225, "dataset_mention": "L. This yields a dataset"}, {"mentioned_in_paper": "676", "context_id": "39", "dataset_context": "Our IPDF method is extensively evaluated on the new SYM-SOL dataset as well as traditional pose estimation benchmarks.", "mention_start": 44, "mention_end": 67, "dataset_mention": "the new SYM-SOL dataset"}, {"mentioned_in_paper": "676", "context_id": "148", "dataset_context": "To highlight the strengths of our method, we put it to the test on a range of challenging pose estimation datasets.", "mention_start": 89, "mention_end": 114, "dataset_mention": "pose estimation datasets"}, {"mentioned_in_paper": "676", "context_id": "152", "dataset_context": "Access to the full set of equivalent rotations opens new avenues of evaluating model performance rarely possible with pose estimation datasets.", "mention_start": 118, "mention_end": 142, "dataset_mention": "pose estimation datasets"}, {"mentioned_in_paper": "676", "context_id": "165", "dataset_context": "The two SYMSOL datasets test expressiveness, but the solids are relatively simple and the dataset does not require generalization to unseen objects.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The two SYMSOL datasets"}, {"mentioned_in_paper": "676", "context_id": "165", "dataset_context": "The two SYMSOL datasets test expressiveness, but the solids are relatively simple and the dataset does not require generalization to unseen objects.", "mention_start": 63, "mention_end": 97, "dataset_mention": "relatively simple and the dataset"}, {"mentioned_in_paper": "676", "context_id": "168", "dataset_context": "The Pascal3D+ dataset (Xiang et al., 2014) is a popular benchmark for pose estimation on real images, consisting 3).", "mention_start": 0, "mention_end": 21, "dataset_mention": "The Pascal3D+ dataset"}, {"mentioned_in_paper": "676", "context_id": "206", "dataset_context": "In contrast to the full coverage of SO(3) and the presence of symmetries and ambiguities in the SYMSOL and ModelNet10-SO(3) datasets, Pascal3D+ serves as a check that pose estimation performance in the unambiguous case is not sacrificed.", "mention_start": 92, "mention_end": 132, "dataset_mention": "the SYMSOL and ModelNet10-SO(3) datasets"}, {"mentioned_in_paper": "676", "context_id": "211", "dataset_context": "The results of Table 4, and specifically the success of the regression method of Liao et al. (2019), show that approximate or exact symmetries are not an issue in the particular split of the T-LESS dataset used in Gilitschenski et al. (2019).", "mention_start": 186, "mention_end": 205, "dataset_mention": "the T-LESS dataset"}, {"mentioned_in_paper": "676", "context_id": "216", "dataset_context": "On the new and difficult SYMSOL dataset, the implicit method is far superior while being simple to implement as it does not require any onerous calculations of a normalization constant.", "mention_start": 3, "mention_end": 39, "dataset_mention": "the new and difficult SYMSOL dataset"}, {"mentioned_in_paper": "676", "context_id": "230", "dataset_context": "We train IPDF on a modified SYMSOL I dataset, where the objects are also translated in space.", "mention_start": 17, "mention_end": 44, "dataset_mention": "a modified SYMSOL I dataset"}, {"mentioned_in_paper": "676", "context_id": "235", "dataset_context": "We evaluate the spread metric on the SYMSOL I dataset, where the full set of ground truths is known at test time, for IPDF and the method of Deng et al. (2020).", "mention_start": 33, "mention_end": 53, "dataset_mention": "the SYMSOL I dataset"}, {"mentioned_in_paper": "676", "context_id": "306", "dataset_context": "The latter is the predominant scenario for pose estimation datasets, where annotations are not provided for near or exact symmetries.", "mention_start": 43, "mention_end": 67, "dataset_mention": "pose estimation datasets"}, {"mentioned_in_paper": "676", "context_id": "359", "dataset_context": "For the ModelNet10-SO(3) and SYMSOL datasets we trained a single model per shape category, and we found no benefit with increasing the number of components (we used 10 for ModelNet10 and 16 for SYMSOL).", "mention_start": 4, "mention_end": 44, "dataset_mention": "the ModelNet10-SO(3) and SYMSOL datasets"}, {"mentioned_in_paper": "676", "context_id": "370", "dataset_context": "Note that our implicit pose distribution is trained as a single model for the whole of SYMSOL I and ModelNet10-SO(3) datasets, so the comparisons against Deng et al. (2020), Gilitschenski et al. (2019), and Prokudin et al. (2018) favor the baselines.", "mention_start": 87, "mention_end": 125, "dataset_mention": "SYMSOL I and ModelNet10-SO(3) datasets"}, {"mentioned_in_paper": "678", "context_id": "55", "dataset_context": "TL is typically based on a two-stage training paradigm: first pretraining a base model on the source dataset and then finetuning a new model on the target dataset with part or all of the pretrained parameters as initialization.", "mention_start": 89, "mention_end": 108, "dataset_mention": "the source dataset"}, {"mentioned_in_paper": "678", "context_id": "186", "dataset_context": "As for the first work in continual UR learning over tasks, we find two public datasets to back up our key claim.", "mention_start": 58, "mention_end": 86, "dataset_mention": " we find two public datasets"}, {"mentioned_in_paper": "678", "context_id": "187", "dataset_context": "They are the Tencent TL dataset released by PeterRec [43], referred to as TTL 9, and the movielens 10 dataset, referred to as ML.", "mention_start": 9, "mention_end": 31, "dataset_mention": "the Tencent TL dataset"}, {"mentioned_in_paper": "678", "context_id": "187", "dataset_context": "They are the Tencent TL dataset released by PeterRec [43], referred to as TTL 9, and the movielens 10 dataset, referred to as ML.", "mention_start": 80, "mention_end": 109, "dataset_mention": " and the movielens 10 dataset"}, {"mentioned_in_paper": "678", "context_id": "240", "dataset_context": "Besides, MTL is unable to leverage all 11 Note it is not easy to find an ideal publicly available dataset, where all tasks share expected similarities.", "mention_start": 64, "mention_end": 105, "dataset_mention": "find an ideal publicly available dataset"}, {"mentioned_in_paper": "678", "context_id": "275", "dataset_context": "We also expect some high-quality real-world benchmark datasets could be released so as to facilitate research in this difficult area.", "mention_start": 8, "mention_end": 62, "dataset_mention": "expect some high-quality real-world benchmark datasets"}, {"mentioned_in_paper": "679", "context_id": "3", "dataset_context": "Speci cally, we present (i) the implementation of di erent layers, including convolution, ReLU, and pooling, in a CoNN using CeNN, (ii) modi ed CoNN structures with CeNN-friendly layers to reduce computational overheads typically associated with a CoNN, (iii) a mixed-signal CeNN architecture that performs CoNN computations in the analog and mixed signal domain, and (iv) design space exploration that identi es what CeNN-based algorithm and architectural features fare best compared to existing algorithms and architectures when evaluated over common datasets -MNIST and CIFAR-10.", "mention_start": 545, "mention_end": 561, "dataset_mention": "common datasets"}, {"mentioned_in_paper": "679", "context_id": "4", "dataset_context": "Notably, the proposed approach can lead to 8.7\u00d7 improvements in energy-delay product (EDP) per digit classi cation for the MNIST dataset at iso-accuracy when compared with the state-of-the-art DNN engine, while our approach could o er 4.3\u00d7 improvements in EDP when compared to other network implementations for the CIFAR-10 dataset.", "mention_start": 118, "mention_end": 136, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "679", "context_id": "4", "dataset_context": "Notably, the proposed approach can lead to 8.7\u00d7 improvements in energy-delay product (EDP) per digit classi cation for the MNIST dataset at iso-accuracy when compared with the state-of-the-art DNN engine, while our approach could o er 4.3\u00d7 improvements in EDP when compared to other network implementations for the CIFAR-10 dataset.", "mention_start": 310, "mention_end": 331, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "679", "context_id": "31", "dataset_context": "We have conducted detail studies of energy, delay, and accuracy per classi cation for the MNIST and CIFAR-10 datasets, and compared our networks and architecture with other algorithms and architectures [14, 18, 28, 41, 50, 59] that address the same problem.", "mention_start": 85, "mention_end": 117, "dataset_mention": "the MNIST and CIFAR-10 datasets"}, {"mentioned_in_paper": "679", "context_id": "32", "dataset_context": "For the MNIST dataset, at iso-accuracy, our results demonstrate an 8.7\u00d7 improvement in energy-delay product (EDP) when compared with a state-of-the-art accelerator.", "mention_start": 4, "mention_end": 21, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "679", "context_id": "34", "dataset_context": "For the CIFAR-10 dataset, a 4.3\u00d7 improvement in EDP is observed when comparing with a state-of-the-art quantized approach [18].", "mention_start": 4, "mention_end": 24, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "679", "context_id": "381", "dataset_context": "While our CeNN architecture can be applied to di erent datasets, we speci cally compare our approach to other e orts in the context of the MNIST and CIFAR-10 dataset given the wealth of comparison points available.", "mention_start": 46, "mention_end": 63, "dataset_mention": "di erent datasets"}, {"mentioned_in_paper": "679", "context_id": "381", "dataset_context": "While our CeNN architecture can be applied to di erent datasets, we speci cally compare our approach to other e orts in the context of the MNIST and CIFAR-10 dataset given the wealth of comparison points available.", "mention_start": 134, "mention_end": 165, "dataset_mention": "the MNIST and CIFAR-10 dataset"}, {"mentioned_in_paper": "679", "context_id": "471", "dataset_context": "We next consider a state-of-the-art digital DNN engine presented in [59] with 28 nm technology node for the MNIST dataset at iso-accuracy with our CeNN based design.", "mention_start": 104, "mention_end": 121, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "679", "context_id": "505", "dataset_context": "For the CIFAR-10 dataset, images with size 32\u00d732 are used.", "mention_start": 4, "mention_end": 24, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "679", "context_id": "564", "dataset_context": "us, AlexNet for Cifar-10 dataset should be su cient to illustrate how our approach can be applied to larger networks and how our approach compares other existing works.", "mention_start": 15, "mention_end": 32, "dataset_mention": "Cifar-10 dataset"}, {"mentioned_in_paper": "681", "context_id": "6", "dataset_context": "The experimental results of three different baselines on large public autonomous driving dataset demonstrate the superiority of the proposed framework.", "mention_start": 57, "mention_end": 96, "dataset_mention": "large public autonomous driving dataset"}, {"mentioned_in_paper": "681", "context_id": "34", "dataset_context": "The effectiveness of the proposed framework has been verified on the large-scale public autonomous driving dataset-nuScenes [13].", "mention_start": 65, "mention_end": 114, "dataset_mention": "the large-scale public autonomous driving dataset"}, {"mentioned_in_paper": "681", "context_id": "100", "dataset_context": "Then a standard 2D detection backbone can then be employed; 3) CenterPoint [18] is a strong anchorfree baseline, which ranks the top among all LiDAR-only method in public nuScenes [13] and Waymo [29] dataset.", "mention_start": 163, "mention_end": 207, "dataset_mention": "public nuScenes [13] and Waymo [29] dataset"}, {"mentioned_in_paper": "681", "context_id": "112", "dataset_context": "For generating the raster images of HDMaps, NuScenes dataset provides APIs to retrieve and query a certain record, such as \"drivable area\", \"walkway\", and \"carpark area\" specifically within the map layers.", "mention_start": 43, "mention_end": 60, "dataset_mention": " NuScenes dataset"}, {"mentioned_in_paper": "681", "context_id": "118", "dataset_context": "We evaluate MapFusion on nuScenes dataset with the three baseline detectors which are introduced before, including SECOND, PointPillars and CenterPoint.", "mention_start": 25, "mention_end": 41, "dataset_mention": "nuScenes dataset"}, {"mentioned_in_paper": "681", "context_id": "131", "dataset_context": "All the experiments are evaluated on nuScenes validation dataset with the metrics defined in Sec.", "mention_start": 37, "mention_end": 64, "dataset_mention": "nuScenes validation dataset"}, {"mentioned_in_paper": "681", "context_id": "153", "dataset_context": "Fig. 6 demonstrates the quantitative detection results of CenterPoint [18] with the proposed MapFusion on the nuScenes \"val\" dataset.", "mention_start": 106, "mention_end": 132, "dataset_mention": "the nuScenes \"val\" dataset"}, {"mentioned_in_paper": "683", "context_id": "20", "dataset_context": "\u2022 Dataset (DVSNOISE20): labeled real-world neuromorphic camera events for benchmarking denoising.", "mention_start": 0, "mention_end": 9, "dataset_mention": "\u2022 Dataset"}, {"mentioned_in_paper": "683", "context_id": "185", "dataset_context": "The DVSNOISE20 dataset, calibration, and code are available at: http://issl.udayton.edu.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The DVSNOISE20 dataset"}, {"mentioned_in_paper": "684", "context_id": "43", "dataset_context": "\u2022 We empirically study the properties of the proposed approach, and experimentally validate the proposed approach compared to state-of-arts approaches with serval data sets.", "mention_start": 155, "mention_end": 172, "dataset_mention": "serval data sets"}, {"mentioned_in_paper": "684", "context_id": "49", "dataset_context": "Moreover, Section VI has been experimentally validated compared to state-of-arts approaches with serval data sets.", "mention_start": 96, "mention_end": 113, "dataset_mention": "serval data sets"}, {"mentioned_in_paper": "684", "context_id": "144", "dataset_context": "-The other type of data set is the Irish Election Data Set obtained from Preflib.", "mention_start": 31, "mention_end": 58, "dataset_mention": "the Irish Election Data Set"}, {"mentioned_in_paper": "684", "context_id": "145", "dataset_context": "The produced synthetic data set has over 20000 numbers of agent rankings on the set of items with 20 numbers.", "mention_start": 0, "mention_end": 31, "dataset_mention": "The produced synthetic data set"}, {"mentioned_in_paper": "684", "context_id": "147", "dataset_context": "The Irish Election Data Set obtained from Preflib contains an uncomplete record of votes for two separate elections held in Dublin, Ireland in 2002, which contains the Dublin North, West, and Meath data sets.", "mention_start": 0, "mention_end": 27, "dataset_mention": "The Irish Election Data Set"}, {"mentioned_in_paper": "684", "context_id": "147", "dataset_context": "The Irish Election Data Set obtained from Preflib contains an uncomplete record of votes for two separate elections held in Dublin, Ireland in 2002, which contains the Dublin North, West, and Meath data sets.", "mention_start": 187, "mention_end": 207, "dataset_mention": " and Meath data sets"}, {"mentioned_in_paper": "684", "context_id": "148", "dataset_context": "In the experiments the subset of the West data set which contains 29,988 over 9 alternatives is adopted.", "mention_start": 33, "mention_end": 50, "dataset_mention": "the West data set"}, {"mentioned_in_paper": "684", "context_id": "156", "dataset_context": "In this subsection, according to the evaluation measures separately, first we present the results for the synthetic data set we create, and then the results for the Irish Election Data Set.", "mention_start": 160, "mention_end": 188, "dataset_mention": "the Irish Election Data Set"}, {"mentioned_in_paper": "684", "context_id": "175", "dataset_context": "Real data set: For the real data set -Irish Election Data Set, the trust for every agent's ranking is not known, so we only compare the performance under the evaluation metrics 2 and 3 between the traditional preference completion algorithm and the preference completion algorithm with certainty considered.", "mention_start": 18, "mention_end": 61, "dataset_mention": "the real data set -Irish Election Data Set"}, {"mentioned_in_paper": "684", "context_id": "190", "dataset_context": "The properties of the proposed approach about certainty and conflict have been studied empirically, and the proposed approach has been experimentally validated compared to state-of-arts approaches with serval data sets.", "mention_start": 201, "mention_end": 218, "dataset_mention": "serval data sets"}, {"mentioned_in_paper": "686", "context_id": "31", "dataset_context": "Experimental results on Bohai SST Dataset, which is chosen from NOAA OI SST V2 High Resolution Dataset are reported in Section III.", "mention_start": 24, "mention_end": 41, "dataset_mention": "Bohai SST Dataset"}, {"mentioned_in_paper": "686", "context_id": "31", "dataset_context": "Experimental results on Bohai SST Dataset, which is chosen from NOAA OI SST V2 High Resolution Dataset are reported in Section III.", "mention_start": 63, "mention_end": 102, "dataset_mention": "NOAA OI SST V2 High Resolution Dataset"}, {"mentioned_in_paper": "686", "context_id": "85", "dataset_context": "We take the corresponding subset to the Bohai sea from the dataset mentioned above to form a 16 by 15 grid and contains a total of 12868 daily values, named Bohai SST Dataset.", "mention_start": 150, "mention_end": 174, "dataset_mention": " named Bohai SST Dataset"}, {"mentioned_in_paper": "686", "context_id": "139", "dataset_context": "We use Bohai SST data set to do this experiment, and compare the proposed method to a classical regression methods SVR [15].", "mention_start": 7, "mention_end": 25, "dataset_mention": "Bohai SST data set"}, {"mentioned_in_paper": "687", "context_id": "74", "dataset_context": "The challenge of applying RL to optimal medication dosing is that all available data are offline sampled; that is, data are collected previously and models can only be fit to a retrospective dataset.", "mention_start": 174, "mention_end": 198, "dataset_mention": "a retrospective dataset"}, {"mentioned_in_paper": "688", "context_id": "166", "dataset_context": "In this section, we perform a thorough comparison of our proposed Curriculum DeepSDF to DeepSDF along with comprehensive ablation studies for the shape reconstruction task on the ShapeNet dataset [5].", "mention_start": 174, "mention_end": 195, "dataset_mention": "the ShapeNet dataset"}, {"mentioned_in_paper": "688", "context_id": "172", "dataset_context": "We conducted experiments on the ShapeNet dataset [5] for the shape reconstruction task.", "mention_start": 28, "mention_end": 48, "dataset_mention": "the ShapeNet dataset"}, {"mentioned_in_paper": "689", "context_id": "7", "dataset_context": "In this paper, we conducted experiments on two automotive event datasets, establishing new state-of-the-art classification results for spiking neural networks.", "mention_start": 42, "mention_end": 72, "dataset_mention": "two automotive event datasets"}, {"mentioned_in_paper": "689", "context_id": "8", "dataset_context": "Based on these results, we combined our SNNs with SSD to propose the first spiking neural networks capable of performing object detection on the complex GEN1 Automotive Detection event dataset.", "mention_start": 140, "mention_end": 192, "dataset_mention": "the complex GEN1 Automotive Detection event dataset"}, {"mentioned_in_paper": "689", "context_id": "20", "dataset_context": "2) We propose a new challenging dataset for classification on automotive event data: GEN1 Automotive Classification, generated using the Prophesee object detection dataset of the same name.", "mention_start": 132, "mention_end": 171, "dataset_mention": "the Prophesee object detection dataset"}, {"mentioned_in_paper": "689", "context_id": "21", "dataset_context": "3) We train four different spiking neural networks for classification tasks based on popular neural network architectures (SqueezeNet, VGG, MobileNet, DenseNet) and evaluate them on two automotive event datasets, setting new state-of-the-art results for spiking neural networks.", "mention_start": 181, "mention_end": 211, "dataset_mention": "two automotive event datasets"}, {"mentioned_in_paper": "689", "context_id": "22", "dataset_context": "4) We present spiking neural networks for object detection composed of a spiking backbone and SSD bounding box regression heads that achieve qualitative results on the real-world GEN1 Automotive Detection event dataset.", "mention_start": 164, "mention_end": 218, "dataset_mention": "the real-world GEN1 Automotive Detection event dataset"}, {"mentioned_in_paper": "689", "context_id": "23", "dataset_context": "To the best of our knowledge, it constitutes the first spiking neural networks capable of doing object detection on real-world event dataset.", "mention_start": 115, "mention_end": 140, "dataset_mention": "real-world event dataset"}, {"mentioned_in_paper": "689", "context_id": "48", "dataset_context": "Reference [12] presented SLAYER, an error backpropagation method for SNNs capable of learning both synaptic weights and axonal delays, allowing them to tackle bigger datasets with deeper networks.", "mention_start": 158, "mention_end": 174, "dataset_mention": "bigger datasets"}, {"mentioned_in_paper": "689", "context_id": "128", "dataset_context": "We evaluated our spiking neural networks on two automotive classification datasets: Prophesee NCARS and a new event dataset we called Prophesee GEN1 Automotive Classification, generated from the object detection dataset Prophesee GEN1.", "mention_start": 44, "mention_end": 82, "dataset_mention": "two automotive classification datasets"}, {"mentioned_in_paper": "689", "context_id": "128", "dataset_context": "We evaluated our spiking neural networks on two automotive classification datasets: Prophesee NCARS and a new event dataset we called Prophesee GEN1 Automotive Classification, generated from the object detection dataset Prophesee GEN1.", "mention_start": 83, "mention_end": 123, "dataset_mention": " Prophesee NCARS and a new event dataset"}, {"mentioned_in_paper": "689", "context_id": "129", "dataset_context": "We then evaluated our object detection spiking networks on this specific dataset, the Prophesee GEN1 Automotive Detection dataset.", "mention_start": 81, "mention_end": 129, "dataset_mention": " the Prophesee GEN1 Automotive Detection dataset"}, {"mentioned_in_paper": "689", "context_id": "130", "dataset_context": "1) Prophesee NCARS dataset: The Prophesee NCARS dataset [28] is a classification composed of 24k samples of length 100ms captured with a Prophesee GEN1 event camera mounted behind the windshield of a moving car.", "mention_start": 0, "mention_end": 26, "dataset_mention": "1) Prophesee NCARS dataset"}, {"mentioned_in_paper": "689", "context_id": "130", "dataset_context": "1) Prophesee NCARS dataset: The Prophesee NCARS dataset [28] is a classification composed of 24k samples of length 100ms captured with a Prophesee GEN1 event camera mounted behind the windshield of a moving car.", "mention_start": 27, "mention_end": 55, "dataset_mention": " The Prophesee NCARS dataset"}, {"mentioned_in_paper": "689", "context_id": "133", "dataset_context": "2) Prophesee GEN1 Detection dataset: Composed of 39 hours of recording, the Prophesee GEN1 Automotive Detection dataset [29] is the largest event-based dataset to date.", "mention_start": 0, "mention_end": 35, "dataset_mention": "2) Prophesee GEN1 Detection dataset"}, {"mentioned_in_paper": "689", "context_id": "133", "dataset_context": "2) Prophesee GEN1 Detection dataset: Composed of 39 hours of recording, the Prophesee GEN1 Automotive Detection dataset [29] is the largest event-based dataset to date.", "mention_start": 71, "mention_end": 119, "dataset_mention": " the Prophesee GEN1 Automotive Detection dataset"}, {"mentioned_in_paper": "689", "context_id": "133", "dataset_context": "2) Prophesee GEN1 Detection dataset: Composed of 39 hours of recording, the Prophesee GEN1 Automotive Detection dataset [29] is the largest event-based dataset to date.", "mention_start": 127, "mention_end": 159, "dataset_mention": "the largest event-based dataset"}, {"mentioned_in_paper": "689", "context_id": "135", "dataset_context": "3) Prophesee GEN1 Classification dataset: We generated a classification dataset from the Prophesee GEN1 Detection dataset by cropping each bounding box (car or pedestrian) form individual samples.", "mention_start": 0, "mention_end": 40, "dataset_mention": "3) Prophesee GEN1 Classification dataset"}, {"mentioned_in_paper": "689", "context_id": "135", "dataset_context": "3) Prophesee GEN1 Classification dataset: We generated a classification dataset from the Prophesee GEN1 Detection dataset by cropping each bounding box (car or pedestrian) form individual samples.", "mention_start": 84, "mention_end": 121, "dataset_mention": "the Prophesee GEN1 Detection dataset"}, {"mentioned_in_paper": "689", "context_id": "165", "dataset_context": "We present the best accuracies obtained by our SNNs on the Prophesee NCARS dataset compared with other state-ofthe-art models in Table I.", "mention_start": 55, "mention_end": 82, "dataset_mention": "the Prophesee NCARS dataset"}, {"mentioned_in_paper": "689", "context_id": "168", "dataset_context": "Table II provides extensive results of all our spiking neural networks on both automotive classification datasets.", "mention_start": 74, "mention_end": 113, "dataset_mention": "both automotive classification datasets"}, {"mentioned_in_paper": "689", "context_id": "169", "dataset_context": "Spiking SqueezeNet models, while having a very low number of parameters and number of ACCs per timestep, are not competitive with other architectures for the NCARS and the GEN1 classification datasets.", "mention_start": 153, "mention_end": 200, "dataset_mention": "the NCARS and the GEN1 classification datasets"}, {"mentioned_in_paper": "689", "context_id": "184", "dataset_context": "The spiking backbones were pretrained on the NCARS dataset.", "mention_start": 41, "mention_end": 58, "dataset_mention": "the NCARS dataset"}, {"mentioned_in_paper": "689", "context_id": "220", "dataset_context": "We designed trained four different spiking neural networks models based on SqueezeNet, VGG, MobileNet and DenseNet, setting new state-of-the-art results on two automotive classification event datasets for spiking neural networks.", "mention_start": 155, "mention_end": 200, "dataset_mention": "two automotive classification event datasets"}, {"mentioned_in_paper": "689", "context_id": "221", "dataset_context": "We then used these networks combined to SSD bounding box regression heads to design the first spiking neural networks capable of doing object detection on the real-world event dataset Prophesee GEN1, achieving 0.19mAP with less than 10M parameters.", "mention_start": 155, "mention_end": 183, "dataset_mention": "the real-world event dataset"}, {"mentioned_in_paper": "691", "context_id": "25", "dataset_context": "Stacking (sometimes called stacked generalization) extends the cross-validation technique that partitions the data set into a held-in data set and a held-out data set; training the models on the held-in data; and then choosing whichever of those trained models performs best on the held-out data.", "mention_start": 124, "mention_end": 142, "dataset_mention": "a held-in data set"}, {"mentioned_in_paper": "691", "context_id": "25", "dataset_context": "Stacking (sometimes called stacked generalization) extends the cross-validation technique that partitions the data set into a held-in data set and a held-out data set; training the models on the held-in data; and then choosing whichever of those trained models performs best on the held-out data.", "mention_start": 124, "mention_end": 166, "dataset_mention": "a held-in data set and a held-out data set"}, {"mentioned_in_paper": "691", "context_id": "95", "dataset_context": "It was one of 3 strategies used to obtain diversity when constructing an ensemble for the KDDCup 1999 dataset [35].", "mention_start": 86, "mention_end": 109, "dataset_mention": "the KDDCup 1999 dataset"}, {"mentioned_in_paper": "691", "context_id": "138", "dataset_context": "With the exception of the audit and vote datasets (last 2 datasets), we find that LOFB-DRF performed at least as good as RF.", "mention_start": 22, "mention_end": 49, "dataset_mention": "the audit and vote datasets"}, {"mentioned_in_paper": "691", "context_id": "138", "dataset_context": "With the exception of the audit and vote datasets (last 2 datasets), we find that LOFB-DRF performed at least as good as RF.", "mention_start": 22, "mention_end": 66, "dataset_mention": "the audit and vote datasets (last 2 datasets"}, {"mentioned_in_paper": "691", "context_id": "148", "dataset_context": "In the worst case scenario, only 16.67 times faster classification with 95% pruning level in the squash-unstored dataset.", "mention_start": 92, "mention_end": 120, "dataset_mention": "the squash-unstored dataset"}, {"mentioned_in_paper": "691", "context_id": "151", "dataset_context": "Note that the audit and vote datasets were not listed in the table as the RFs for these datasets (refer to the last 2 datasets in Table 5) outperformed all LOFB-DRFs, however, by a very small amount as shown in Table 2.", "mention_start": 5, "mention_end": 37, "dataset_mention": "that the audit and vote datasets"}, {"mentioned_in_paper": "692", "context_id": "38", "dataset_context": "We conduct experiments on UTKFace and CelebA datasets using the proposed method, and as a result, demonstrate the superiority of the method.", "mention_start": 26, "mention_end": 53, "dataset_mention": "UTKFace and CelebA datasets"}, {"mentioned_in_paper": "692", "context_id": "121", "dataset_context": "For training, we use the randomly chosen 23, 408 images for UTKFace dataset and 28, 970 images for CelebA dataset without any augmentation methods.", "mention_start": 59, "mention_end": 75, "dataset_mention": "UTKFace dataset"}, {"mentioned_in_paper": "692", "context_id": "121", "dataset_context": "For training, we use the randomly chosen 23, 408 images for UTKFace dataset and 28, 970 images for CelebA dataset without any augmentation methods.", "mention_start": 98, "mention_end": 113, "dataset_mention": "CelebA dataset"}, {"mentioned_in_paper": "693", "context_id": "22", "dataset_context": "We use the Affect Game AnnotatIoN (AGAIN) dataset [7], which includes telemetry and annotations of arousal for almost 1,000 play sessions of nine different games across three different genres (Racing, Shooter, and Platformer).", "mention_start": 18, "mention_end": 49, "dataset_mention": "Game AnnotatIoN (AGAIN) dataset"}, {"mentioned_in_paper": "693", "context_id": "59", "dataset_context": "A common issue with ordinal affect modelling is the lack of sufficiently labelled datasets.", "mention_start": 60, "mention_end": 90, "dataset_mention": "sufficiently labelled datasets"}, {"mentioned_in_paper": "693", "context_id": "64", "dataset_context": "This study employs the AGAIN dataset 1, which was designed to provide a diverse and robust database for general affect modelling in the domain of videogames [7].", "mention_start": 19, "mention_end": 36, "dataset_mention": "the AGAIN dataset"}, {"mentioned_in_paper": "693", "context_id": "67", "dataset_context": "The AGAIN dataset includes 9 games in total; 3 games for each of the racing, shooter, and platformer genres (see Fig. 2).", "mention_start": 0, "mention_end": 17, "dataset_mention": "The AGAIN dataset"}, {"mentioned_in_paper": "693", "context_id": "71", "dataset_context": "1) Racing: The AGAIN dataset includes three car-racing games, where players have to navigate in a closed-loop track until the timer runs out.", "mention_start": 10, "mention_end": 28, "dataset_mention": " The AGAIN dataset"}, {"mentioned_in_paper": "693", "context_id": "81", "dataset_context": "The clean AGAIN dataset consists of 122 players playing 995 sessions of 2-minute games [7].", "mention_start": 0, "mention_end": 23, "dataset_mention": "The clean AGAIN dataset"}, {"mentioned_in_paper": "693", "context_id": "96", "dataset_context": "Beyond these specific features, the AGAIN dataset also provides 13 general features [7].", "mention_start": 31, "mention_end": 49, "dataset_mention": " the AGAIN dataset"}, {"mentioned_in_paper": "693", "context_id": "104", "dataset_context": "Due to these properties, the collected annotation trace preserves the subjective and ordinal nature of the player experience [8] and makes the dataset optimal for modelling through preference learning (see Section IV-A).", "mention_start": 102, "mention_end": 150, "dataset_mention": "the player experience [8] and makes the dataset"}, {"mentioned_in_paper": "693", "context_id": "105", "dataset_context": "This paper uses the preprocessed, cleaned dataset from the AGAIN database, from which unresponsive participants and outliers have already been removed [7].", "mention_start": 33, "mention_end": 49, "dataset_mention": " cleaned dataset"}, {"mentioned_in_paper": "693", "context_id": "131", "dataset_context": "Moreover, by keeping two observations per pair, the baseline of the transformed dataset is always 50%.", "mention_start": 63, "mention_end": 87, "dataset_mention": "the transformed dataset"}, {"mentioned_in_paper": "693", "context_id": "210", "dataset_context": "In quick and casual games-such as those featured in the AGAIN dataset-the intensity of the gameplay increases over time to such a degree that a relatively simple algorithm can achieve up to 86% average accuracy when predicting the change in player arousal.", "mention_start": 52, "mention_end": 69, "dataset_mention": "the AGAIN dataset"}, {"mentioned_in_paper": "693", "context_id": "216", "dataset_context": "Normative datasets used for research into game-playing AI use arcade-type games [1], similar to the ones included in AGAIN and used in this study.", "mention_start": 0, "mention_end": 18, "dataset_mention": "Normative datasets"}, {"mentioned_in_paper": "694", "context_id": "82", "dataset_context": "We train our models from scratch using the Scene Flow dataset with a constant learning rate of 0.001 for 10 epochs.", "mention_start": 39, "mention_end": 61, "dataset_mention": "the Scene Flow dataset"}, {"mentioned_in_paper": "700", "context_id": "17", "dataset_context": "That is largely because point cloud datasets [31, 2, 6] are typically considerably smaller and less diverse than image datasets, such as ImageNet [7] and MSCOCO [20], which have millions of training data.", "mention_start": 24, "mention_end": 44, "dataset_mention": "point cloud datasets"}, {"mentioned_in_paper": "700", "context_id": "18", "dataset_context": "For example, ModelNet40 [31], one of the most widely used point cloud dataset, includes only 12,311 models with 40 categories.", "mention_start": 57, "mention_end": 77, "dataset_mention": "point cloud dataset"}, {"mentioned_in_paper": "700", "context_id": "135", "dataset_context": "We evaluate RSMix on ModelNet40 [31] and ModelNet10 [31], which are widely used point cloud classification benchmark datasets.", "mention_start": 79, "mention_end": 125, "dataset_mention": "point cloud classification benchmark datasets"}, {"mentioned_in_paper": "701", "context_id": "6", "dataset_context": "In our experiments we use the public available DeepFlow for optical flow estimation and FCN8s for the semantic information as inputs and show on the KITTI 2015 dataset that mono-stixels provide a compact and reliable depth reconstruction of both the static and moving parts of the scene.", "mention_start": 145, "mention_end": 167, "dataset_mention": "the KITTI 2015 dataset"}, {"mentioned_in_paper": "701", "context_id": "38", "dataset_context": "The Cityscape dataset [16] gives an overview of the performance of pixel-wise semantic segmentation methods in street scenes and shows that deep neural networks allow to perform semantic segmentation on a previously unprecendented level of performance.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The Cityscape dataset"}, {"mentioned_in_paper": "701", "context_id": "140", "dataset_context": "We evaluate our approach on the KITTI-Stereo'15 [25] dataset that contains 200 images captured from a forward facing camera in different street scenes.", "mention_start": 28, "mention_end": 60, "dataset_mention": "the KITTI-Stereo'15 [25] dataset"}, {"mentioned_in_paper": "701", "context_id": "148", "dataset_context": "These keyframes do not exist for all images in the dataset, thus, the evaluation is done on 171 images out of the 200 images of the KITTI-Stereo'15 dataset.", "mention_start": 127, "mention_end": 155, "dataset_mention": "the KITTI-Stereo'15 dataset"}, {"mentioned_in_paper": "701", "context_id": "151", "dataset_context": "First, we train our net on the Cityscape dataset [16].", "mention_start": 26, "mention_end": 48, "dataset_mention": "the Cityscape dataset"}, {"mentioned_in_paper": "701", "context_id": "152", "dataset_context": "Second, we fine-tune this net on 470 images of the KITTI dataset collected from the labeled subsets in [28], [29], [30], [31], [32], [33].", "mention_start": 46, "mention_end": 64, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "702", "context_id": "256", "dataset_context": "In the experiments, the performance of LowrankTLP for hyperlink prediction (Task 1) and multiple graph alignment (Task 2) was evaluated in simulation and three real datasets.", "mention_start": 138, "mention_end": 173, "dataset_mention": "simulation and three real datasets"}, {"mentioned_in_paper": "702", "context_id": "259", "dataset_context": "In Section 6.3, we test the practical application of Lowrank-TLP for hyperlink prediction (Task 1) on the DBLP dataset of scientific publication records.", "mention_start": 101, "mention_end": 118, "dataset_mention": "the DBLP dataset"}, {"mentioned_in_paper": "702", "context_id": "314", "dataset_context": "We downloaded the DBLP dataset of scientific publication records from AMiner (Extraction and Mining of Academic Social Networks) [40].", "mention_start": 0, "mention_end": 30, "dataset_mention": "We downloaded the DBLP dataset"}, {"mentioned_in_paper": "702", "context_id": "327", "dataset_context": "We obtained a dataset of 134 CT scan images of an anonymized female patient.", "mention_start": 0, "mention_end": 21, "dataset_mention": "We obtained a dataset"}, {"mentioned_in_paper": "702", "context_id": "365", "dataset_context": "We downloaded the IsoBase dataset [4], [35], [41], containing protein-protein interactions (PPI) networks for five species: H. sapiens (HS), D. melanogaster (DM), S. cerevisiae (SC), C. elegans (CE) and M. musculus (MM).", "mention_start": 0, "mention_end": 33, "dataset_mention": "We downloaded the IsoBase dataset"}, {"mentioned_in_paper": "703", "context_id": "159", "dataset_context": "Apart from the dimensionality of the data, there are other challenging factors that include complexity and heterogeneous datasets [95] which makes the area of big data challenging [96, 97].", "mention_start": 91, "mention_end": 129, "dataset_mention": "complexity and heterogeneous datasets"}, {"mentioned_in_paper": "704", "context_id": "13", "dataset_context": "Such representation bias in action datasets may provide shortcuts to solve the data-label fitting problem.", "mention_start": 28, "mention_end": 43, "dataset_mention": "action datasets"}, {"mentioned_in_paper": "704", "context_id": "17", "dataset_context": "Notice that both the test dataset and downstream task are different from those used for training, which implies better generalization ability of the learned representations by suppressing static cues.", "mention_start": 0, "mention_end": 33, "dataset_mention": "Notice that both the test dataset"}, {"mentioned_in_paper": "704", "context_id": "19", "dataset_context": "The first one is supervised learning by using manual annotations on the UCF101 dataset (Soomro et al. 2012), while the second one is our self-supervised method trained on the same dataset by mitigating the representation bias.", "mention_start": 68, "mention_end": 86, "dataset_mention": "the UCF101 dataset"}, {"mentioned_in_paper": "704", "context_id": "20", "dataset_context": "Then, the learned feature representations are evaluated on the HMDB51 dataset (Kuehne et al. 2011).", "mention_start": 58, "mention_end": 77, "dataset_mention": "the HMDB51 dataset"}, {"mentioned_in_paper": "704", "context_id": "162", "dataset_context": "Accuracy comparison with other self-supervised learning methods on the UCF101 and HMDB51 is reported in Tables 2 and 3. When using the S3D as backbone, combining the S 2 VC with MoCo brings a 4.1% improvement on Top1 accuracy and 7.5% improvement on Top5 accuracy on the UCF101 dataset.", "mention_start": 266, "mention_end": 285, "dataset_mention": "the UCF101 dataset"}, {"mentioned_in_paper": "704", "context_id": "234", "dataset_context": "This result indicates that the feature learned without dealing with the representation bias problem may still be wrongly guided by static visual cues even when pre-training on a larger-scale dataset.", "mention_start": 176, "mention_end": 198, "dataset_mention": "a larger-scale dataset"}, {"mentioned_in_paper": "705", "context_id": "6", "dataset_context": "Finally, our Panoptic-DeepLab also performs on par with several topdown approaches on the challenging COCO dataset.", "mention_start": 101, "mention_end": 114, "dataset_mention": "COCO dataset"}, {"mentioned_in_paper": "705", "context_id": "28", "dataset_context": "We conduct experiments on several popular panoptic segmentation datasets.", "mention_start": 26, "mention_end": 72, "dataset_mention": "several popular panoptic segmentation datasets"}, {"mentioned_in_paper": "705", "context_id": "109", "dataset_context": "Mapillary Vistas [54] : A large-scale traffic-related dataset, containing 18K, 2K, and 5K images for training, validation and testing, respectively.", "mention_start": 23, "mention_end": 61, "dataset_mention": " A large-scale traffic-related dataset"}, {"mentioned_in_paper": "705", "context_id": "161", "dataset_context": "We notice there are two types of annotations in the COCO dataset, panoptic annotations and instance annotations.", "mention_start": 48, "mention_end": 64, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "705", "context_id": "196", "dataset_context": "Fig. 8 shows an example to illustrate the difference between instance annotation and panoptic annotation on the COCO dataset.", "mention_start": 108, "mention_end": 124, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "707", "context_id": "20", "dataset_context": "Considering a medical dataset with n patients, (X i , y i ) for i \u2208 [n], we let X i be the continuous or binary available medical variables and y i \u2208 {0, 1} the label (e.g. a positive or negative diagnosis).", "mention_start": 12, "mention_end": 29, "dataset_mention": "a medical dataset"}, {"mentioned_in_paper": "707", "context_id": "35", "dataset_context": "We used the PhysioNet 2019 Early Sepsis Prediction time series dataset (Reyna et al., 2019), which was collected from the ICUs of three hospitals.", "mention_start": 8, "mention_end": 70, "dataset_mention": "the PhysioNet 2019 Early Sepsis Prediction time series dataset"}, {"mentioned_in_paper": "707", "context_id": "50", "dataset_context": "In Table 1, we report the performance of our approach and baselines on the PhysioNet dataset in terms of accuracy, precision, recall and conciseness of the checklist.", "mention_start": 70, "mention_end": 92, "dataset_mention": "the PhysioNet dataset"}, {"mentioned_in_paper": "707", "context_id": "67", "dataset_context": "From our experiments, it is clear that learning thresholds is beneficial, therefore demonstrating the usefulness of our approach in the continuous clinical data setting.", "mention_start": 131, "mention_end": 164, "dataset_mention": "the continuous clinical data set"}, {"mentioned_in_paper": "708", "context_id": "143", "dataset_context": "The MNIST dataset contains 60, 000 images of handwritten digits from 0 to 9 [16].", "mention_start": 0, "mention_end": 17, "dataset_mention": "The MNIST dataset"}, {"mentioned_in_paper": "708", "context_id": "156", "dataset_context": "The CIFAR 10 dataset [14] contains 60, 000 32\u00d732 color images across 10 object classes.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The CIFAR 10 dataset"}, {"mentioned_in_paper": "708", "context_id": "159", "dataset_context": "For the experiments involving CIFAR dataset, we adopt the architecture proposed by Goodfellow et al. [8].", "mention_start": 4, "mention_end": 43, "dataset_mention": "the experiments involving CIFAR dataset"}, {"mentioned_in_paper": "708", "context_id": "174", "dataset_context": "Overall, the CIFAR dataset experiments demonstrate that our model can scale to more complicated real life datasets and still outperform the traditional GANs in low data scenarios.", "mention_start": 8, "mention_end": 26, "dataset_mention": " the CIFAR dataset"}, {"mentioned_in_paper": "708", "context_id": "174", "dataset_context": "Overall, the CIFAR dataset experiments demonstrate that our model can scale to more complicated real life datasets and still outperform the traditional GANs in low data scenarios.", "mention_start": 78, "mention_end": 114, "dataset_mention": "more complicated real life datasets"}, {"mentioned_in_paper": "708", "context_id": "175", "dataset_context": "The TU-Berlin dataset [6], contains 20, 000 hand-drawn sketches evenly distributed among 250 object categories, which amounts to 80 images per category.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The TU-Berlin dataset"}, {"mentioned_in_paper": "710", "context_id": "12", "dataset_context": "This is verifiable through a quick pilot study on existing scene sketch dataset [25] : (i) scene sketches include an average of 49.7% of the objects present in photos.", "mention_start": 50, "mention_end": 79, "dataset_mention": "existing scene sketch dataset"}, {"mentioned_in_paper": "710", "context_id": "43", "dataset_context": "(v) Our method is not only robust to partial input scene sketches but also yields state-of-theart performance on existing scene sketch datasets.", "mention_start": 113, "mention_end": 143, "dataset_mention": "existing scene sketch datasets"}, {"mentioned_in_paper": "710", "context_id": "46", "dataset_context": "Starting with graph-matching of deformable-part models [41], several deep learning approaches have surfaced with the advent of FG-SBIR datasets [64, 65].", "mention_start": 126, "mention_end": 143, "dataset_mention": "FG-SBIR datasets"}, {"mentioned_in_paper": "710", "context_id": "57", "dataset_context": "While Liu et al. [45] introduced FG-SBIR for scene-sketches using graph convo-lutional networks, it largely avoided the challenging setup of partial sketches by filtering existing datasets [25] having too few foreground instances (i.e., partial sketches).", "mention_start": 160, "mention_end": 188, "dataset_mention": "filtering existing datasets"}, {"mentioned_in_paper": "710", "context_id": "160", "dataset_context": "Datasets: We use two benchmark scene sketch datasets that support scene-level FG-SBIR tasks: (a) SketchyScene [91] comprise of sketch templates with paired cartoon style photos.", "mention_start": 16, "mention_end": 52, "dataset_mention": "two benchmark scene sketch datasets"}, {"mentioned_in_paper": "710", "context_id": "163", "dataset_context": "(b) Unlike SketchyScene [91] that comprise of cartoon style photos, SketchyCOCO [25] includes natural photos from COCO Stuff dataset [14] with paired scene sketches.", "mention_start": 113, "mention_end": 132, "dataset_mention": "COCO Stuff dataset"}, {"mentioned_in_paper": "710", "context_id": "177", "dataset_context": "Since it requires vectored sketch data unavailable in existing scene sketch datasets [25, 91], we compare with On-the-fly on object sketches [79] using vectored data.", "mention_start": 54, "mention_end": 84, "dataset_mention": "existing scene sketch datasets"}, {"mentioned_in_paper": "710", "context_id": "192", "dataset_context": "We perform a comparative study on scene sketches from SketchyScene [91] and SketchyCOCO [25] datasets.", "mention_start": 54, "mention_end": 101, "dataset_mention": "SketchyScene [91] and SketchyCOCO [25] datasets"}, {"mentioned_in_paper": "710", "context_id": "209", "dataset_context": "Fig. 3 shows qualitative retrieval results on scene-sketch datasets.", "mention_start": 46, "mention_end": 67, "dataset_mention": "scene-sketch datasets"}, {"mentioned_in_paper": "710", "context_id": "211", "dataset_context": "Hence for a fair comparison and to investigate the generalisation of our method to partial object-level sketches, we compare FG-SBIR using QMUL-Shoe-V2 [79] dataset.", "mention_start": 138, "mention_end": 164, "dataset_mention": "QMUL-Shoe-V2 [79] dataset"}, {"mentioned_in_paper": "713", "context_id": "120", "dataset_context": "As for the experimental results on the Office-Home dataset (Table 2), the first two conclusions we draw above still hold.", "mention_start": 35, "mention_end": 58, "dataset_mention": "the Office-Home dataset"}, {"mentioned_in_paper": "713", "context_id": "125", "dataset_context": "For the Office31 dataset, when the ResNet50 model is used as the backbone, our best method achieves an average accuracy of 90.1% over six tasks and outperforms state-of-the- art methods.", "mention_start": 4, "mention_end": 24, "dataset_mention": "the Office31 dataset"}, {"mentioned_in_paper": "713", "context_id": "128", "dataset_context": "On the Office-Home dataset, our best method also achieves the best performance regardless of the employed backbones.", "mention_start": 3, "mention_end": 26, "dataset_mention": "the Office-Home dataset"}, {"mentioned_in_paper": "714", "context_id": "448", "dataset_context": "We are happy to share the source code and the dataset for some experiments if deemed necessary by the reviewers.", "mention_start": 16, "mention_end": 53, "dataset_mention": "share the source code and the dataset"}, {"mentioned_in_paper": "715", "context_id": "6", "dataset_context": "We conduct experiments on two datasets, namely the NYC Yellow Taxi and the PEMS road traffic datasets.", "mention_start": 39, "mention_end": 101, "dataset_mention": " namely the NYC Yellow Taxi and the PEMS road traffic datasets"}, {"mentioned_in_paper": "715", "context_id": "33", "dataset_context": "Hence, the model can be used in settings where the number of nodes is different throughout the dataset, i.e., a road network with a varying number of loop detectors over a few years.", "mention_start": 69, "mention_end": 102, "dataset_mention": "different throughout the dataset"}, {"mentioned_in_paper": "715", "context_id": "152", "dataset_context": "The experiments are done on two open-source datasets.", "mention_start": 28, "mention_end": 52, "dataset_mention": "two open-source datasets"}, {"mentioned_in_paper": "715", "context_id": "157", "dataset_context": "The NYC Yellow Taxi dataset is first cleaned for erroneous logs.", "mention_start": 0, "mention_end": 27, "dataset_mention": "The NYC Yellow Taxi dataset"}, {"mentioned_in_paper": "715", "context_id": "166", "dataset_context": "The PEMS dataset is taken from (Li et al. 2018) and hence follows the same train, validation and test splits and is split into vectors of 24 hours with 12 hours for burn-in and 12 hours for prediction.", "mention_start": 0, "mention_end": 16, "dataset_mention": "The PEMS dataset"}, {"mentioned_in_paper": "715", "context_id": "168", "dataset_context": "In this section, we do experiments on the NYC Yellow Taxicab dataset.", "mention_start": 37, "mention_end": 68, "dataset_mention": "the NYC Yellow Taxicab dataset"}, {"mentioned_in_paper": "715", "context_id": "172", "dataset_context": "We run experiments on the New York City Yellow Taxi Cab dataset, comparing the inferred adjacency matrix against commonly used adjacency matrices based on heuristics.", "mention_start": 22, "mention_end": 63, "dataset_mention": "the New York City Yellow Taxi Cab dataset"}, {"mentioned_in_paper": "715", "context_id": "189", "dataset_context": "For the NYC Yellow Taxicab dataset, we found a random initialization of the encoder to be too noisy and led to bad training of the model.", "mention_start": 4, "mention_end": 34, "dataset_mention": "the NYC Yellow Taxicab dataset"}, {"mentioned_in_paper": "715", "context_id": "223", "dataset_context": "In this section, we do experiments on the PEMS dataset.", "mention_start": 37, "mention_end": 54, "dataset_mention": "the PEMS dataset"}, {"mentioned_in_paper": "715", "context_id": "290", "dataset_context": "Through experiments on the NYC Yellow Taxicab dataset and the PEMS dataset, we have shown that using a learned, dynamical adjacency matrix can outperform models using fixed adjacency matrices based on common heuristics such as locality and time series correlations.", "mention_start": 23, "mention_end": 53, "dataset_mention": "the NYC Yellow Taxicab dataset"}, {"mentioned_in_paper": "715", "context_id": "290", "dataset_context": "Through experiments on the NYC Yellow Taxicab dataset and the PEMS dataset, we have shown that using a learned, dynamical adjacency matrix can outperform models using fixed adjacency matrices based on common heuristics such as locality and time series correlations.", "mention_start": 23, "mention_end": 74, "dataset_mention": "the NYC Yellow Taxicab dataset and the PEMS dataset"}, {"mentioned_in_paper": "715", "context_id": "293", "dataset_context": "For example, we see that in the case of the NYC Yellow Taxicab dataset, the learned adjacency matrix sends the most messages to the zones with the highest amount of activity.", "mention_start": 39, "mention_end": 70, "dataset_mention": "the NYC Yellow Taxicab dataset"}, {"mentioned_in_paper": "715", "context_id": "296", "dataset_context": "Another example of the interpretability is the case of the PEMS dataset nodes with congestion.", "mention_start": 55, "mention_end": 71, "dataset_mention": "the PEMS dataset"}, {"mentioned_in_paper": "716", "context_id": "161", "dataset_context": "As illustrated in Table 2, online approaches generally demonstrate a stronger ability in terms of robustness according to R of VOT2018 dataset, while offline approaches have better accuracy with higher A. Among these formulations, ONR shows the best performance which indicates that robust localization is of overriding significance for good tracking.", "mention_start": 126, "mention_end": 142, "dataset_mention": "VOT2018 dataset"}, {"mentioned_in_paper": "716", "context_id": "177", "dataset_context": "VOT2018 dataset consists of 60 challenging videos.", "mention_start": 0, "mention_end": 15, "dataset_mention": "VOT2018 dataset"}, {"mentioned_in_paper": "716", "context_id": "178", "dataset_context": "The tracker's overall performance is evaluated upon robustness and accuracy, defined using failure rate and UPDT [3] SiamRPN ++ [23] ATOM [5] DiMP [2] DROL [47] Ocean [46] FCOT [4] RPT [28] Ours A\u2191 0.536 0.600 0.590 0.597 0.616 0.592 0.600 0.629 0.608 R\u2193 0.184 0.234 0.204 0.153 0.122 0.117 0.108 0.103 0.080 EAO\u2191 0.378 0.414 0.401 0.440 0.481 0.489 0.508 0.510 0.564 Table 5. Results on VOT2018 challenge dataset [21] in terms of expected average overlap (EAO), robustness (R), and accuracy (A).", "mention_start": 387, "mention_end": 413, "dataset_mention": "VOT2018 challenge dataset"}, {"mentioned_in_paper": "716", "context_id": "180", "dataset_context": "SiamMask [41] SiamRPN ++ [23] ATOM [5] STN [38] Ocean [46] DiMP [2] DRNet [22] RPT [28] Table 7. Results on OTB2015 and NFS (30fps) datasets [42, 20] in terms of overall AUC score.", "mention_start": 108, "mention_end": 140, "dataset_mention": "OTB2015 and NFS (30fps) datasets"}, {"mentioned_in_paper": "716", "context_id": "185", "dataset_context": "Compared with VOT2018, VOT2019 dataset includes more challenging sequences for evaluating the tracker's robustness and accuracy.", "mention_start": 22, "mention_end": 38, "dataset_mention": " VOT2019 dataset"}, {"mentioned_in_paper": "716", "context_id": "188", "dataset_context": "OTB2015 dataset contains a total amount of 100 sequences with motion, scale change, and illumination change, etc.", "mention_start": 0, "mention_end": 15, "dataset_mention": "OTB2015 dataset"}, {"mentioned_in_paper": "716", "context_id": "197", "dataset_context": "UAV123 dataset contains 123 sequences collected from a UAV perspective.", "mention_start": 0, "mention_end": 14, "dataset_mention": "UAV123 dataset"}, {"mentioned_in_paper": "716", "context_id": "207", "dataset_context": "GOT-10k is a recently-proposed large-scale dataset for both training and testing, with no overlap in object classes between training and testing.", "mention_start": 11, "mention_end": 50, "dataset_mention": "a recently-proposed large-scale dataset"}, {"mentioned_in_paper": "716", "context_id": "274", "dataset_context": "We provided the detailed results in figures on OTB2015 [42], NFS [20], UAV123 [29], and LaSOT [8] datasets, as shown in Figure 8.", "mention_start": 83, "mention_end": 106, "dataset_mention": " and LaSOT [8] datasets"}, {"mentioned_in_paper": "716", "context_id": "275", "dataset_context": "Our methods performs among the top methods for all the above datasets.", "mention_start": 47, "mention_end": 69, "dataset_mention": "all the above datasets"}, {"mentioned_in_paper": "717", "context_id": "17", "dataset_context": "Our paper focuses on a more general approach, where we publish a synthetic trajectory dataset that shares similar properties with the original one while satisfying DP.", "mention_start": 51, "mention_end": 93, "dataset_mention": "we publish a synthetic trajectory dataset"}, {"mentioned_in_paper": "717", "context_id": "18", "dataset_context": "While publishing synthetic datasets would not work better than directly publishing aggregate statistics [42], this paradigm can easily enable any (even unseen) down-stream data analysis tasks without modifying existing algorithms and has been adopted in, e.g., the 2020 US Census data publication [10] and a dataset for population flow analysis [29].", "mention_start": 6, "mention_end": 35, "dataset_mention": "publishing synthetic datasets"}, {"mentioned_in_paper": "717", "context_id": "18", "dataset_context": "While publishing synthetic datasets would not work better than directly publishing aggregate statistics [42], this paradigm can easily enable any (even unseen) down-stream data analysis tasks without modifying existing algorithms and has been adopted in, e.g., the 2020 US Census data publication [10] and a dataset for population flow analysis [29].", "mention_start": 260, "mention_end": 315, "dataset_mention": " the 2020 US Census data publication [10] and a dataset"}, {"mentioned_in_paper": "717", "context_id": "50", "dataset_context": "We conduct empirical experiments on both synthetic and real-world trajectory datasets to compare PrivTrace with the state-of-the-art.", "mention_start": 36, "mention_end": 85, "dataset_mention": "both synthetic and real-world trajectory datasets"}, {"mentioned_in_paper": "717", "context_id": "59", "dataset_context": "\u2022 We conduct extensive experiments on both synthetic and real-world trajectory datasets to validate the effectiveness of PrivTrace.", "mention_start": 38, "mention_end": 87, "dataset_mention": "both synthetic and real-world trajectory datasets"}, {"mentioned_in_paper": "717", "context_id": "68", "dataset_context": "We are interested in the following question: Given a sensitive trajectory dataset D o , how to generate a synthetic trajectory dataset D s that shares similar properties with D o while satisfying DP.", "mention_start": 50, "mention_end": 81, "dataset_mention": "a sensitive trajectory dataset"}, {"mentioned_in_paper": "717", "context_id": "68", "dataset_context": "We are interested in the following question: Given a sensitive trajectory dataset D o , how to generate a synthetic trajectory dataset D s that shares similar properties with D o while satisfying DP.", "mention_start": 103, "mention_end": 134, "dataset_mention": "a synthetic trajectory dataset"}, {"mentioned_in_paper": "717", "context_id": "69", "dataset_context": "Generating the synthetic trajectory dataset facilitates down-stream data analysis tasks without modifications.", "mention_start": 11, "mention_end": 43, "dataset_mention": "the synthetic trajectory dataset"}, {"mentioned_in_paper": "717", "context_id": "73", "dataset_context": "The length distribution and diameter distribution capture the frequency of different trajectory lengths and trajectory diameters in a trajectory dataset, respectively.", "mention_start": 132, "mention_end": 152, "dataset_mention": "a trajectory dataset"}, {"mentioned_in_paper": "717", "context_id": "93", "dataset_context": "An algorithm A satisfies \u03b5-differential privacy (\u03b5-DP), where \u03b5 > 0, if and only if for any two neighboring datasets D and D , we have", "mention_start": 87, "mention_end": 116, "dataset_mention": "any two neighboring datasets"}, {"mentioned_in_paper": "717", "context_id": "117", "dataset_context": "Given a set of geographical states and a trajectory dataset, we need to learn some models that can capture the transition pattern of the trajectory dataset.", "mention_start": 15, "mention_end": 59, "dataset_mention": "geographical states and a trajectory dataset"}, {"mentioned_in_paper": "717", "context_id": "117", "dataset_context": "Given a set of geographical states and a trajectory dataset, we need to learn some models that can capture the transition pattern of the trajectory dataset.", "mention_start": 132, "mention_end": 155, "dataset_mention": "the trajectory dataset"}, {"mentioned_in_paper": "717", "context_id": "139", "dataset_context": "Both AdaTrace and DPT adopt Markov chain model to capture the transition pattern of the trajectory dataset.", "mention_start": 84, "mention_end": 106, "dataset_mention": "the trajectory dataset"}, {"mentioned_in_paper": "717", "context_id": "280", "dataset_context": "We run experiments on one synthetic and two realworld trajectory datasets.", "mention_start": 22, "mention_end": 73, "dataset_mention": "one synthetic and two realworld trajectory datasets"}, {"mentioned_in_paper": "717", "context_id": "290", "dataset_context": "We adopt four metrics to measure the similarity between the original dataset D o and synthetic dataset D s :", "mention_start": 56, "mention_end": 102, "dataset_mention": "the original dataset D o and synthetic dataset"}, {"mentioned_in_paper": "717", "context_id": "343", "dataset_context": "Interestingly, comparing AdaTrace and DPT on all datasets, we observe that AdaTrace performs better on the Brinkhoff and Taxi datasets, while DPT performs better on the Geolife dataset.", "mention_start": 102, "mention_end": 134, "dataset_mention": "the Brinkhoff and Taxi datasets"}, {"mentioned_in_paper": "717", "context_id": "343", "dataset_context": "Interestingly, comparing AdaTrace and DPT on all datasets, we observe that AdaTrace performs better on the Brinkhoff and Taxi datasets, while DPT performs better on the Geolife dataset.", "mention_start": 164, "mention_end": 184, "dataset_mention": "the Geolife dataset"}, {"mentioned_in_paper": "717", "context_id": "353", "dataset_context": "We experiment on the Taxi dataset and divide the map into an 80 \u00d7 80 uniform grid.", "mention_start": 17, "mention_end": 33, "dataset_mention": "the Taxi dataset"}, {"mentioned_in_paper": "717", "context_id": "398", "dataset_context": "However, our empirical experiments show that the time consumption of PrivTrace on real-world trajectory datasets is acceptable, e.g., less than 10 minutes (see Table 5 in Appendix A for more details).", "mention_start": 81, "mention_end": 112, "dataset_mention": "real-world trajectory datasets"}, {"mentioned_in_paper": "717", "context_id": "404", "dataset_context": "There are a number of previous studies focusing on estimating the density distribution of a location dataset while satisfying DP.", "mention_start": 90, "mention_end": 108, "dataset_mention": "a location dataset"}, {"mentioned_in_paper": "717", "context_id": "425", "dataset_context": "Our paper focuses on a more general paradigm that publishes a synthetic trajectory dataset while satisfying DP.", "mention_start": 21, "mention_end": 90, "dataset_mention": "a more general paradigm that publishes a synthetic trajectory dataset"}, {"mentioned_in_paper": "717", "context_id": "432", "dataset_context": "The core idea of game-based methods is to formulate the dataset synthesis problem as a zero-sum game [21, 27, 45].", "mention_start": 42, "mention_end": 63, "dataset_mention": "formulate the dataset"}, {"mentioned_in_paper": "717", "context_id": "574", "dataset_context": "The authors divide the Adult dataset R into subgroups according to the income attribute: The groups of records whose income exceeds 50K per year R high and the groups of records whose income is less than 50K per year R low .", "mention_start": 19, "mention_end": 36, "dataset_mention": "the Adult dataset"}, {"mentioned_in_paper": "717", "context_id": "579", "dataset_context": "A similar evaluation is conducted on the Texas and Purchases datasets.", "mention_start": 37, "mention_end": 69, "dataset_mention": "the Texas and Purchases datasets"}, {"mentioned_in_paper": "717", "context_id": "588", "dataset_context": "We conduct experiments on the Taxi dataset and four metrics.", "mention_start": 26, "mention_end": 42, "dataset_mention": "the Taxi dataset"}, {"mentioned_in_paper": "717", "context_id": "609", "dataset_context": "Here, by searching the Internet, we learn that Porto, the city where the Taxi dataset resides, has a population of 1,310,000.", "mention_start": 68, "mention_end": 85, "dataset_mention": "the Taxi dataset"}, {"mentioned_in_paper": "717", "context_id": "615", "dataset_context": "We conduct experiments on the Taxi dataset and two metrics.", "mention_start": 26, "mention_end": 42, "dataset_mention": "the Taxi dataset"}, {"mentioned_in_paper": "718", "context_id": "126", "dataset_context": "However, AutoML can also encompass functionality that supports this goal indirectly, e.g., a natural-language user interface (UI) or a meta-model designed to analyze dataset similarity.", "mention_start": 157, "mention_end": 173, "dataset_mention": "analyze dataset"}, {"mentioned_in_paper": "718", "context_id": "166", "dataset_context": "One way to boost search speed is to rely on low-fidelity approximations for pipeline evaluations, such as via dataset subsampling for training/testing or early-stopping for training algorithms.", "mention_start": 105, "mention_end": 117, "dataset_mention": "via dataset"}, {"mentioned_in_paper": "718", "context_id": "266", "dataset_context": "It enables this by constructing a semantic context around datasets and other objects, making it possible to interpret requests such as \"please model the median number of households\" or \"predict the proximity to the ocean\".", "mention_start": 32, "mention_end": 66, "dataset_mention": "a semantic context around datasets"}, {"mentioned_in_paper": "718", "context_id": "316", "dataset_context": "Data generation and selection both have promise too, although the latter refines rather than enriches a dataset and thus strains against its own performance ceiling, i.e., it is limited by what data is available.", "mention_start": 92, "mention_end": 111, "dataset_mention": "enriches a dataset"}, {"mentioned_in_paper": "718", "context_id": "320", "dataset_context": "Specifically, algorithms for the other three AutoDL trends typically scale reasonably well with respect to the size of a problem dataset, i.e., the yardstick of interest for these approaches, as they simply involve operations applied to instances of data.", "mention_start": 118, "mention_end": 136, "dataset_mention": "a problem dataset"}, {"mentioned_in_paper": "718", "context_id": "451", "dataset_context": "As shown in Table 1, many works scale down the model [171, 324], sub-sample the dataset [63], reduce the number of training epochs [324, 325], set a constrained time budget [77, 175], early-stop the training [13], or explore different generalizable metrics [260].", "mention_start": 64, "mention_end": 87, "dataset_mention": " sub-sample the dataset"}, {"mentioned_in_paper": "718", "context_id": "583", "dataset_context": "This process has since been refined into an algorithm named Hyperband by hedging its aggressiveness [159], and Hyperband has subsequently been fused with BayesOpt techniques into BOHB [77], which has proven itself a highly efficient and effective HPO strategy for certain \"well-behaved\" datasets [65, 144, 295, 311].", "mention_start": 263, "mention_end": 295, "dataset_mention": "certain \"well-behaved\" datasets"}, {"mentioned_in_paper": "718", "context_id": "637", "dataset_context": "Nonetheless, there have been recent HPO investigations on larger-scale datasets, such as the CIFAR-style AlexNet [174] and the vision dataset ImageNet [52, 60].", "mention_start": 57, "mention_end": 79, "dataset_mention": "larger-scale datasets"}, {"mentioned_in_paper": "718", "context_id": "637", "dataset_context": "Nonetheless, there have been recent HPO investigations on larger-scale datasets, such as the CIFAR-style AlexNet [174] and the vision dataset ImageNet [52, 60].", "mention_start": 88, "mention_end": 141, "dataset_mention": "the CIFAR-style AlexNet [174] and the vision dataset"}, {"mentioned_in_paper": "718", "context_id": "1057", "dataset_context": "For example, a recent work achieved 87.3% accuracy on the challenging ImageNet dataset by combining, with the aid of manually designed hyperparameters, automated data engineering with NAS [264].", "mention_start": 69, "mention_end": 86, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "718", "context_id": "1092", "dataset_context": "However, while these works encourage good practice in researchers, many do focus on small-scale models and datasets.", "mention_start": 83, "mention_end": 115, "dataset_mention": "small-scale models and datasets"}, {"mentioned_in_paper": "719", "context_id": "171", "dataset_context": "The proposed 3D reconstruction pipeline was tested on several sequences from RobotDataset [28] 1.", "mention_start": 77, "mention_end": 89, "dataset_mention": "RobotDataset"}, {"mentioned_in_paper": "720", "context_id": "62", "dataset_context": "While more advanced search and ranking methods are often complex and their performance is questionable, especially on temporal datasets such as Web archives, the results on Tempas solely correspond to their temporal popularity on the external data source.", "mention_start": 117, "mention_end": 135, "dataset_mention": "temporal datasets"}, {"mentioned_in_paper": "720", "context_id": "65", "dataset_context": "With Tempas we have built an indexing framework over websites or their URLs respectively, based on tags and timestamps from the Delicious dataset (cf.", "mention_start": 123, "mention_end": 145, "dataset_mention": "the Delicious dataset"}, {"mentioned_in_paper": "720", "context_id": "76", "dataset_context": "We operate on the tag dataset described above where websites, considered as our documents d \u2208 D, are tagged with labels l \u2286 L at a given time t \u2208 T .", "mention_start": 14, "mention_end": 29, "dataset_mention": "the tag dataset"}, {"mentioned_in_paper": "720", "context_id": "116", "dataset_context": "The different levels of co-occurring tags to be considered as sub-topics of the original query in Tempas provide an easy way to explore the available dataset.", "mention_start": 128, "mention_end": 157, "dataset_mention": "explore the available dataset"}, {"mentioned_in_paper": "720", "context_id": "130", "dataset_context": "Furthermore, we are planning on incorporating different meta data than tags that is available in a temporal fashion or can be extracted from an archive as derivative dataset.", "mention_start": 154, "mention_end": 173, "dataset_mention": "derivative dataset"}, {"mentioned_in_paper": "721", "context_id": "0", "dataset_context": "Advances in the realm of Generative Adversarial Networks (GANs) [5] have led to architectures capable of producing amazingly realistic images such as StyleGAN2 [7], which, when trained on the FFHQ dataset [6], generates images of human faces from random vectors in a lower-dimensional latent space.", "mention_start": 187, "mention_end": 204, "dataset_mention": "the FFHQ dataset"}, {"mentioned_in_paper": "722", "context_id": "33", "dataset_context": "We propose a novel benchmark for SDCA, CityUHK-X-BEV, which repurposes the CityUHK-X dataset [28] to the SDCA problem, by adding ground-plane annotations.", "mention_start": 59, "mention_end": 92, "dataset_mention": "repurposes the CityUHK-X dataset"}, {"mentioned_in_paper": "722", "context_id": "63", "dataset_context": "Crowd scene datasets tend to be collected in public spaces and focus on busy scenes [63, 25, 60, 28], where the estimation of head locations is difficult due to small people sizes and significant occlusion.", "mention_start": 0, "mention_end": 20, "dataset_mention": "Crowd scene datasets"}, {"mentioned_in_paper": "722", "context_id": "64", "dataset_context": "These are the scenes that we emphasize in this work, where we augment a popular crowd counting dataset with rich annotations required for SDCA.", "mention_start": 69, "mention_end": 102, "dataset_mention": "a popular crowd counting dataset"}, {"mentioned_in_paper": "722", "context_id": "89", "dataset_context": "DET datasets [11, 39, 65] are not suitable for SDCA since the number of people per image tends to be low.", "mention_start": 0, "mention_end": 12, "dataset_mention": "DET datasets"}, {"mentioned_in_paper": "722", "context_id": "90", "dataset_context": "CC datasets are more relevant, as they contain abundant examples of people gathering in clusters-often within 1 to 2 meters, the range of droplet transmission [61, 15, 31] -and the scene/camera configurations are most suitable for monitoring social distances in public spaces.", "mention_start": 0, "mention_end": 11, "dataset_mention": "CC datasets"}, {"mentioned_in_paper": "722", "context_id": "91", "dataset_context": "However, geometric meta-information is unavailable in most CC datasets.", "mention_start": 53, "mention_end": 70, "dataset_mention": "most CC datasets"}, {"mentioned_in_paper": "722", "context_id": "94", "dataset_context": "Like other CC datasets, it only provides image annotations for each head in the scene.", "mention_start": 0, "mention_end": 22, "dataset_mention": "Like other CC datasets"}, {"mentioned_in_paper": "722", "context_id": "189", "dataset_context": "In this section we present experimental evaluations of SDCA on the CityUHK-X-BEV dataset.", "mention_start": 63, "mention_end": 88, "dataset_mention": "the CityUHK-X-BEV dataset"}, {"mentioned_in_paper": "722", "context_id": "261", "dataset_context": "The original CityUHK-X dataset [4] contained the head annotations of all people in the scene, as well as extrinsic camera parameters in the form of height h and pitch angle \u03b8 relative to ground plane.", "mention_start": 0, "mention_end": 30, "dataset_mention": "The original CityUHK-X dataset"}, {"mentioned_in_paper": "722", "context_id": "283", "dataset_context": "In CityUHK-X-BEV dataset, the camera focal lengths (f u , f v ) are given and for generality, we suppose there is no optical skew nor image center displacement.", "mention_start": 3, "mention_end": 24, "dataset_mention": "CityUHK-X-BEV dataset"}, {"mentioned_in_paper": "722", "context_id": "300", "dataset_context": "As shown in figure 4, camera poses varies even within the same scenes of the CityUHK-X-BEV dataset.", "mention_start": 72, "mention_end": 98, "dataset_mention": "the CityUHK-X-BEV dataset"}, {"mentioned_in_paper": "723", "context_id": "4", "dataset_context": "We conduct experiments on two benchmark datasets, i.e., the Oxford 102 Category Flower Dataset and the Caltech-UCSD Birds-200-2011 Dataset and the experimental results clearly demonstrate the efficacy of our proposed approach.", "mention_start": 55, "mention_end": 94, "dataset_mention": " the Oxford 102 Category Flower Dataset"}, {"mentioned_in_paper": "723", "context_id": "4", "dataset_context": "We conduct experiments on two benchmark datasets, i.e., the Oxford 102 Category Flower Dataset and the Caltech-UCSD Birds-200-2011 Dataset and the experimental results clearly demonstrate the efficacy of our proposed approach.", "mention_start": 55, "mention_end": 138, "dataset_mention": " the Oxford 102 Category Flower Dataset and the Caltech-UCSD Birds-200-2011 Dataset"}, {"mentioned_in_paper": "723", "context_id": "6", "dataset_context": "To help users efficiently organize and manage such media data from a very huge collection, it is necessary and practical to collect labeled visual datasets at large scale to develop automatic tools with robust machine learning approaches [18, 7, 16, 19, 17, 8].", "mention_start": 131, "mention_end": 155, "dataset_mention": "labeled visual datasets"}, {"mentioned_in_paper": "723", "context_id": "103", "dataset_context": "Our experiments are conducted on two datasets, i.e., the Oxford 102 Category Flower Dataset [22], and the Caltech-UCSD Birds-200-2011 Dataset [32].", "mention_start": 52, "mention_end": 91, "dataset_mention": " the Oxford 102 Category Flower Dataset"}, {"mentioned_in_paper": "723", "context_id": "103", "dataset_context": "Our experiments are conducted on two datasets, i.e., the Oxford 102 Category Flower Dataset [22], and the Caltech-UCSD Birds-200-2011 Dataset [32].", "mention_start": 97, "mention_end": 141, "dataset_mention": " and the Caltech-UCSD Birds-200-2011 Dataset"}, {"mentioned_in_paper": "723", "context_id": "105", "dataset_context": "The Oxford 102 Category Flower Dataset [22] consists of 8,189 images from 102 categories of flowers which commonly occurs in the United Kingdom, and each category has 40 to 258 images.", "mention_start": 0, "mention_end": 38, "dataset_mention": "The Oxford 102 Category Flower Dataset"}, {"mentioned_in_paper": "723", "context_id": "144", "dataset_context": "We run experiments on the Oxford 102 Category Flower Dataset.", "mention_start": 22, "mention_end": 60, "dataset_mention": "the Oxford 102 Category Flower Dataset"}, {"mentioned_in_paper": "723", "context_id": "148", "dataset_context": "The Caltech-UCSD Birds-200-2011 Dataset consists of 11,169 bird images from 200 categories and each each category has 60 images averagely.", "mention_start": 0, "mention_end": 39, "dataset_mention": "The Caltech-UCSD Birds-200-2011 Dataset"}, {"mentioned_in_paper": "723", "context_id": "157", "dataset_context": "For the purpose of fair comparison, we train K = 5 StackGAN++ with different noise levels individually and use the K generated synthetic images to extract visual features for image labeling on the Oxford 102 Category Flower Dataset.", "mention_start": 192, "mention_end": 231, "dataset_mention": "the Oxford 102 Category Flower Dataset"}, {"mentioned_in_paper": "723", "context_id": "166", "dataset_context": "Obviously, in both the 102 Category Flower Dataset and Caltech-UCSD Birds-200-2011 Dataset, the observations indicate that the density is not sufficient to ensure the good quality of neighbor images for JCNN-NN.", "mention_start": 13, "mention_end": 50, "dataset_mention": "both the 102 Category Flower Dataset"}, {"mentioned_in_paper": "723", "context_id": "166", "dataset_context": "Obviously, in both the 102 Category Flower Dataset and Caltech-UCSD Birds-200-2011 Dataset, the observations indicate that the density is not sufficient to ensure the good quality of neighbor images for JCNN-NN.", "mention_start": 13, "mention_end": 90, "dataset_mention": "both the 102 Category Flower Dataset and Caltech-UCSD Birds-200-2011 Dataset"}, {"mentioned_in_paper": "724", "context_id": "35", "dataset_context": "We evaluate DiffusionDet on MS-COCO [51] dataset.", "mention_start": 28, "mention_end": 48, "dataset_mention": "MS-COCO [51] dataset"}, {"mentioned_in_paper": "724", "context_id": "39", "dataset_context": "Moreover, we further conduct experiments on challenging LVIS [31] dataset, and DiffusionDet also performs well on this long-tailed dataset, achieving 42.1 AP with Swin-Base [54] backbone.", "mention_start": 55, "mention_end": 73, "dataset_mention": "LVIS [31] dataset"}, {"mentioned_in_paper": "724", "context_id": "39", "dataset_context": "Moreover, we further conduct experiments on challenging LVIS [31] dataset, and DiffusionDet also performs well on this long-tailed dataset, achieving 42.1 AP with Swin-Base [54] backbone.", "mention_start": 113, "mention_end": 138, "dataset_mention": "this long-tailed dataset"}, {"mentioned_in_paper": "724", "context_id": "120", "dataset_context": "Then we compare DiffusionDet with previous wellestablished detectors on MS-COCO [51] and LVIS [31] dataset.", "mention_start": 72, "mention_end": 106, "dataset_mention": "MS-COCO [51] and LVIS [31] dataset"}, {"mentioned_in_paper": "724", "context_id": "122", "dataset_context": " [51] dataset contains about 118K training images in the train2017 set and 5K validation images in the val2017 set.", "mention_start": 0, "mention_end": 13, "dataset_mention": " [51] dataset"}, {"mentioned_in_paper": "724", "context_id": "125", "dataset_context": "LVIS v1.0 [31] dataset is a large-vocabulary object detection and instance segmentation dataset which has 100K training images and 20K validation images.", "mention_start": 0, "mention_end": 22, "dataset_mention": "LVIS v1.0 [31] dataset"}, {"mentioned_in_paper": "724", "context_id": "125", "dataset_context": "LVIS v1.0 [31] dataset is a large-vocabulary object detection and instance segmentation dataset which has 100K training images and 20K validation images.", "mention_start": 26, "mention_end": 95, "dataset_mention": "a large-vocabulary object detection and instance segmentation dataset"}, {"mentioned_in_paper": "724", "context_id": "175", "dataset_context": "We compare the results on a more challenging LVIS dataset in Table 2.", "mention_start": 45, "mention_end": 57, "dataset_mention": "LVIS dataset"}, {"mentioned_in_paper": "725", "context_id": "133", "dataset_context": "It also performs consistently on distorted dataset while negligible drop in performance on the original clean testset.", "mention_start": 33, "mention_end": 50, "dataset_mention": "distorted dataset"}, {"mentioned_in_paper": "726", "context_id": "10", "dataset_context": "We demonstrate the effectiveness of the approach versus traditional methods with several ablation experiments on synthetic, measured, and satellite image datasets.", "mention_start": 133, "mention_end": 162, "dataset_mention": " and satellite image datasets"}, {"mentioned_in_paper": "726", "context_id": "14", "dataset_context": "Labeling datasets relevant to the sensor domain, however, is often prohibitively expensive due to the abundance of new data available for emerging sensor modalities, and the cost of collecting and labeling appropriate data for all relevant operating conditions (environment, target, and sensor state).", "mention_start": 0, "mention_end": 17, "dataset_mention": "Labeling datasets"}, {"mentioned_in_paper": "726", "context_id": "16", "dataset_context": "Namely, classification performance on the target dataset of interest suffers when the source dataset used for training has a much different probability distribution. 2", "mention_start": 81, "mention_end": 100, "dataset_mention": "the source dataset"}, {"mentioned_in_paper": "726", "context_id": "31", "dataset_context": "The paper is organized as follows: We describe related work in Sec. 2 before providing DiSDAT details in Sec. 3. We demonstrate the effectiveness of the approach versus traditional methods with several ablation experiments on synthetic, measured, and satellite image datasets in Sec. 4. We also provide practical guidelines for training the network while overcoming vanishing gradients which inhibit learning in some adversarial training settings.", "mention_start": 246, "mention_end": 275, "dataset_mention": " and satellite image datasets"}, {"mentioned_in_paper": "726", "context_id": "112", "dataset_context": "To understand the effect of the different features of DiSDAT, we performed a series of ablation studies on various transfer tasks and datasets.", "mention_start": 106, "mention_end": 142, "dataset_mention": "various transfer tasks and datasets"}, {"mentioned_in_paper": "726", "context_id": "137", "dataset_context": "Cross dataset transfer tasks involving taking two distinct datasets as source and target domains.", "mention_start": 0, "mention_end": 67, "dataset_mention": "Cross dataset transfer tasks involving taking two distinct datasets"}, {"mentioned_in_paper": "726", "context_id": "152", "dataset_context": "xView is a particularly large dataset with approximately 1.0 million satellite images taken at 0.3 meter resolution.", "mention_start": 9, "mention_end": 37, "dataset_mention": "a particularly large dataset"}, {"mentioned_in_paper": "726", "context_id": "156", "dataset_context": "A detailed description of the xView dataset is found in a paper by Darius Lam et al. 32 The SAMPLE dataset is a publicly available synthetic and measured SAR imagery dataset of 10 target classes.", "mention_start": 26, "mention_end": 43, "dataset_mention": "the xView dataset"}, {"mentioned_in_paper": "726", "context_id": "156", "dataset_context": "A detailed description of the xView dataset is found in a paper by Darius Lam et al. 32 The SAMPLE dataset is a publicly available synthetic and measured SAR imagery dataset of 10 target classes.", "mention_start": 110, "mention_end": 173, "dataset_mention": "a publicly available synthetic and measured SAR imagery dataset"}, {"mentioned_in_paper": "726", "context_id": "159", "dataset_context": "Table 2 shows the results of using images of dresses and shirts from the FMNIST dataset as the source domain, with FMNIST images of sandals and ankle boots as the target.", "mention_start": 69, "mention_end": 87, "dataset_mention": "the FMNIST dataset"}, {"mentioned_in_paper": "726", "context_id": "174", "dataset_context": "The results of the cross-dataset transfer task consisting of images of 0's and 1's from MNIST as the source domain and images of 2's and 3's from the USPS dataset can be found in Table 4.", "mention_start": 15, "mention_end": 32, "dataset_mention": "the cross-dataset"}, {"mentioned_in_paper": "726", "context_id": "174", "dataset_context": "The results of the cross-dataset transfer task consisting of images of 0's and 1's from MNIST as the source domain and images of 2's and 3's from the USPS dataset can be found in Table 4.", "mention_start": 146, "mention_end": 162, "dataset_mention": "the USPS dataset"}, {"mentioned_in_paper": "726", "context_id": "186", "dataset_context": " 4. MNIST to USPS cross dataset transfer task for digits 0 and 1 to 2 and 3. We experimented with both a 3 and 10-dimensional latent space.", "mention_start": 12, "mention_end": 31, "dataset_mention": "USPS cross dataset"}, {"mentioned_in_paper": "726", "context_id": "187", "dataset_context": "Reversing the study of the previous cross dataset transfer task and trying different number classes, we take images of 4's and 5's from the USPS dataset as the source domain and images of 3's and 9's from MNIST as the target.", "mention_start": 135, "mention_end": 152, "dataset_mention": "the USPS dataset"}, {"mentioned_in_paper": "726", "context_id": "192", "dataset_context": " 5. USPS to MNIST cross dataset transfer task for digits 4 and 5 to 3 and 9.", "mention_start": 11, "mention_end": 31, "dataset_mention": "MNIST cross dataset"}, {"mentioned_in_paper": "726", "context_id": "224", "dataset_context": "We have presented the DiSDAT network for transfer learning from a labeled source dataset to an unlabeled target dataset.", "mention_start": 64, "mention_end": 88, "dataset_mention": "a labeled source dataset"}, {"mentioned_in_paper": "727", "context_id": "8", "dataset_context": "Specifically, a network trained with half of the source dataset using DAMix can achieve even better adaptivity than that trained with the whole source dataset but without DAMix.", "mention_start": 44, "mention_end": 63, "dataset_mention": "the source dataset"}, {"mentioned_in_paper": "727", "context_id": "8", "dataset_context": "Specifically, a network trained with half of the source dataset using DAMix can achieve even better adaptivity than that trained with the whole source dataset but without DAMix.", "mention_start": 133, "mention_end": 158, "dataset_mention": "the whole source dataset"}, {"mentioned_in_paper": "727", "context_id": "21", "dataset_context": "Deep learning-based methods have achieved breakthroughs in terms of image quality and fidelity; however they are subject to demanding training times and large-scale paired datasets consisting of diverse data.", "mention_start": 142, "mention_end": 180, "dataset_mention": "times and large-scale paired datasets"}, {"mentioned_in_paper": "727", "context_id": "62", "dataset_context": "In particular, when there is a domain shift between the source and target domains, the model trained with DAMix using only half of the source dataset can adapt as well as that trained with the whole source dataset but without DAMix.", "mention_start": 130, "mention_end": 149, "dataset_mention": "the source dataset"}, {"mentioned_in_paper": "727", "context_id": "62", "dataset_context": "In particular, when there is a domain shift between the source and target domains, the model trained with DAMix using only half of the source dataset can adapt as well as that trained with the whole source dataset but without DAMix.", "mention_start": 188, "mention_end": 213, "dataset_mention": "the whole source dataset"}, {"mentioned_in_paper": "727", "context_id": "86", "dataset_context": "In addition, [25] applies frequency information to adversarial training with the ag-gregated dataset in combination with Copy-Blend [24] to achieve domain-invariant performance.", "mention_start": 76, "mention_end": 100, "dataset_mention": "the ag-gregated dataset"}, {"mentioned_in_paper": "727", "context_id": "142", "dataset_context": "To evaluate the effectiveness of DAMix, we select three datasets with different haze densities: Dense-Haze [2], NH-Haze [3], O-Haze [5].", "mention_start": 39, "mention_end": 64, "dataset_mention": " we select three datasets"}, {"mentioned_in_paper": "727", "context_id": "153", "dataset_context": "We conduct thorough experiments on all the combinations of the three chosen datasets (Dense-Haze, NH-Haze, and O-Haze) and present quantitative comparisons and qualitative comparisons.", "mention_start": 59, "mention_end": 84, "dataset_mention": "the three chosen datasets"}, {"mentioned_in_paper": "727", "context_id": "184", "dataset_context": "DAMix achieves the best results with respect to all fractions of the Dense-Haze dataset.", "mention_start": 65, "mention_end": 87, "dataset_mention": "the Dense-Haze dataset"}, {"mentioned_in_paper": "727", "context_id": "205", "dataset_context": "We attribute these results to the diversity of the DAMix-ed samples, which endow the synthetic dataset with more variety in terms of haze distribution.", "mention_start": 74, "mention_end": 102, "dataset_mention": "endow the synthetic dataset"}, {"mentioned_in_paper": "729", "context_id": "5", "dataset_context": "(2) GOT-10k is by far the first video trajectory dataset that uses the semantic hierarchy of WordNet to guide class population, which ensures a comprehensive and relatively unbiased coverage of diverse moving objects.", "mention_start": 18, "mention_end": 56, "dataset_mention": "far the first video trajectory dataset"}, {"mentioned_in_paper": "729", "context_id": "20", "dataset_context": "Table 1 compares current public tracking datasets in terms of scale, diversity, experiment setting, and others.", "mention_start": 0, "mention_end": 49, "dataset_mention": "Table 1 compares current public tracking datasets"}, {"mentioned_in_paper": "729", "context_id": "21", "dataset_context": "We mainly compare our work with two recent largescale datasets: LaSOT [20] and TrackingNet [19], since other datasets are of small scales with no available training set.", "mention_start": 32, "mention_end": 62, "dataset_mention": "two recent largescale datasets"}, {"mentioned_in_paper": "729", "context_id": "35", "dataset_context": "UAV123 ( 9) NUS_PRO ( 12) DTB (15) TrackingNet (21) OTB2015 (22) OxUvA (22) YouTube-BB (23) TColor-128 (27) VOT2019 (30) ImageNet-VID (30) NfS (33) ALOV++ (59) LaSOT ( Fig. 2 : BAR CHART: Number of object classes in different tracking (blue) and video image detection (red) datasets.", "mention_start": 215, "mention_end": 282, "dataset_mention": "different tracking (blue) and video image detection (red) datasets"}, {"mentioned_in_paper": "729", "context_id": "36", "dataset_context": "Among all the compared datasets, GOT-10k offers an unprecedentedly wider coverage of moving object classes.", "mention_start": 6, "mention_end": 31, "dataset_mention": "all the compared datasets"}, {"mentioned_in_paper": "729", "context_id": "55", "dataset_context": "Since 2013, a number of object tracking datasets have been proposed and served as unified platforms for tracker evaluation and comparison.", "mention_start": 23, "mention_end": 48, "dataset_mention": "object tracking datasets"}, {"mentioned_in_paper": "729", "context_id": "56", "dataset_context": "The OTB [9], [12], ALOV++ [21] and VOT [6], [7], [8] datasets represent the initial attempts to unify the test data and performance measurements for generic object tracking.", "mention_start": 48, "mention_end": 61, "dataset_mention": " [8] datasets"}, {"mentioned_in_paper": "729", "context_id": "60", "dataset_context": "They include the large-scale people and rigid object tracking dataset NUS PRO [17], longterm aerial tracking dataset UAV123/UAV20L [15], color tracking dataset TColor-128 [16], long-term tracking dataset OxUvA [13], thermal tracking datasets PTB-TIR [22] and VOT-TIR [5], RGBD tracking dataset PTB [18] and high frame-rate tracking dataset NfS [14].", "mention_start": 13, "mention_end": 69, "dataset_mention": "the large-scale people and rigid object tracking dataset"}, {"mentioned_in_paper": "729", "context_id": "60", "dataset_context": "They include the large-scale people and rigid object tracking dataset NUS PRO [17], longterm aerial tracking dataset UAV123/UAV20L [15], color tracking dataset TColor-128 [16], long-term tracking dataset OxUvA [13], thermal tracking datasets PTB-TIR [22] and VOT-TIR [5], RGBD tracking dataset PTB [18] and high frame-rate tracking dataset NfS [14].", "mention_start": 83, "mention_end": 116, "dataset_mention": " longterm aerial tracking dataset"}, {"mentioned_in_paper": "729", "context_id": "60", "dataset_context": "They include the large-scale people and rigid object tracking dataset NUS PRO [17], longterm aerial tracking dataset UAV123/UAV20L [15], color tracking dataset TColor-128 [16], long-term tracking dataset OxUvA [13], thermal tracking datasets PTB-TIR [22] and VOT-TIR [5], RGBD tracking dataset PTB [18] and high frame-rate tracking dataset NfS [14].", "mention_start": 136, "mention_end": 159, "dataset_mention": " color tracking dataset"}, {"mentioned_in_paper": "729", "context_id": "60", "dataset_context": "They include the large-scale people and rigid object tracking dataset NUS PRO [17], longterm aerial tracking dataset UAV123/UAV20L [15], color tracking dataset TColor-128 [16], long-term tracking dataset OxUvA [13], thermal tracking datasets PTB-TIR [22] and VOT-TIR [5], RGBD tracking dataset PTB [18] and high frame-rate tracking dataset NfS [14].", "mention_start": 176, "mention_end": 203, "dataset_mention": " long-term tracking dataset"}, {"mentioned_in_paper": "729", "context_id": "60", "dataset_context": "They include the large-scale people and rigid object tracking dataset NUS PRO [17], longterm aerial tracking dataset UAV123/UAV20L [15], color tracking dataset TColor-128 [16], long-term tracking dataset OxUvA [13], thermal tracking datasets PTB-TIR [22] and VOT-TIR [5], RGBD tracking dataset PTB [18] and high frame-rate tracking dataset NfS [14].", "mention_start": 215, "mention_end": 241, "dataset_mention": " thermal tracking datasets"}, {"mentioned_in_paper": "729", "context_id": "60", "dataset_context": "They include the large-scale people and rigid object tracking dataset NUS PRO [17], longterm aerial tracking dataset UAV123/UAV20L [15], color tracking dataset TColor-128 [16], long-term tracking dataset OxUvA [13], thermal tracking datasets PTB-TIR [22] and VOT-TIR [5], RGBD tracking dataset PTB [18] and high frame-rate tracking dataset NfS [14].", "mention_start": 271, "mention_end": 293, "dataset_mention": " RGBD tracking dataset"}, {"mentioned_in_paper": "729", "context_id": "60", "dataset_context": "They include the large-scale people and rigid object tracking dataset NUS PRO [17], longterm aerial tracking dataset UAV123/UAV20L [15], color tracking dataset TColor-128 [16], long-term tracking dataset OxUvA [13], thermal tracking datasets PTB-TIR [22] and VOT-TIR [5], RGBD tracking dataset PTB [18] and high frame-rate tracking dataset NfS [14].", "mention_start": 271, "mention_end": 339, "dataset_mention": " RGBD tracking dataset PTB [18] and high frame-rate tracking dataset"}, {"mentioned_in_paper": "729", "context_id": "63", "dataset_context": "More recent datasets TrackingNet [19] and LaSOT [20] offer a scale that is on par with our dataset.", "mention_start": 0, "mention_end": 20, "dataset_mention": "More recent datasets"}, {"mentioned_in_paper": "729", "context_id": "67", "dataset_context": "The comparison of GOT-10k with other tracking datasets in terms of scale, diversity, attribute annotations and others are shown in Table 1, Table 2 and Figure 2. GOT-10k is larger than most tracking datasets and it offers a much wider coverage of object classes.", "mention_start": 31, "mention_end": 54, "dataset_mention": "other tracking datasets"}, {"mentioned_in_paper": "729", "context_id": "67", "dataset_context": "The comparison of GOT-10k with other tracking datasets in terms of scale, diversity, attribute annotations and others are shown in Table 1, Table 2 and Figure 2. GOT-10k is larger than most tracking datasets and it offers a much wider coverage of object classes.", "mention_start": 184, "mention_end": 207, "dataset_mention": "most tracking datasets"}, {"mentioned_in_paper": "729", "context_id": "70", "dataset_context": "Nevertheless, most traditional tracking datasets are small-scale and they only offer evaluation videos, which are not suitable for training deep trackers.", "mention_start": 13, "mention_end": 48, "dataset_mention": " most traditional tracking datasets"}, {"mentioned_in_paper": "729", "context_id": "72", "dataset_context": "ImageNet-VID and YouTube-BB are video object detection datasets and they may contain noisy segments such as incomplete objects and shot changes (Figure 3), while other datasets are tracking datasets that always provide continuous long trajectories.", "mention_start": 180, "mention_end": 198, "dataset_mention": "tracking datasets"}, {"mentioned_in_paper": "729", "context_id": "76", "dataset_context": "Fig. 3 : Screenshots taken from the YouTube-BB [26] and ImageNet-VID [23] datasets.", "mention_start": 31, "mention_end": 82, "dataset_mention": "the YouTube-BB [26] and ImageNet-VID [23] datasets"}, {"mentioned_in_paper": "729", "context_id": "80", "dataset_context": "The purpose of this work is to construct a large tracking dataset with a broad coverage of real-world moving objects, motion patterns, and scenes.", "mention_start": 41, "mention_end": 65, "dataset_mention": "a large tracking dataset"}, {"mentioned_in_paper": "729", "context_id": "95", "dataset_context": "The plot shows the final distribution of these groups in GOT-10k, with OTB2015 dataset as a comparison.", "mention_start": 70, "mention_end": 86, "dataset_mention": "OTB2015 dataset"}, {"mentioned_in_paper": "729", "context_id": "104", "dataset_context": "Figure 4 shows the final distribution of video numbers of the 121 object class groups used in the collection stage, with OTB2015 dataset as a comparison.", "mention_start": 120, "mention_end": 136, "dataset_mention": "OTB2015 dataset"}, {"mentioned_in_paper": "729", "context_id": "116", "dataset_context": "As a special case, similar to the YouTube-BB dataset, we give the person class preferential treatment in terms of the total number of videos during data collection (which accounts for around 24% of our entire database).", "mention_start": 29, "mention_end": 52, "dataset_mention": "the YouTube-BB dataset"}, {"mentioned_in_paper": "729", "context_id": "122", "dataset_context": "Note this differs from some visual tracking datasets such as VOT [5], [8], where the optimal object bounding box is defined as a rotated   rectangle containing minimum background pixels.", "mention_start": 23, "mention_end": 52, "dataset_mention": "some visual tracking datasets"}, {"mentioned_in_paper": "729", "context_id": "129", "dataset_context": "We divide visible ratios into 7 ranges with a step of 15%. Figure 5 (a)-(d) compares the perframe occlusion labeling of different tracking datasets while Figure 5 (e) shows the cumulative distribution of visible ratios annotated in our dataset.", "mention_start": 120, "mention_end": 147, "dataset_mention": "different tracking datasets"}, {"mentioned_in_paper": "729", "context_id": "136", "dataset_context": "We split the GOT-10k dataset into unified training, validation and test sets to enable fair comparison of tracking approaches.", "mention_start": 9, "mention_end": 28, "dataset_mention": "the GOT-10k dataset"}, {"mentioned_in_paper": "729", "context_id": "137", "dataset_context": "Unlike many other machine learning applications [24], [26], the splitting of generic object tracking dataset is not straightforward (i.e., by randomly sampling a proportion of data).", "mention_start": 76, "mention_end": 108, "dataset_mention": "generic object tracking dataset"}, {"mentioned_in_paper": "729", "context_id": "169", "dataset_context": "According to the above analysis, the final splits of GOT-10k dataset are summarized in Figure 7.", "mention_start": 52, "mention_end": 68, "dataset_mention": "GOT-10k dataset"}, {"mentioned_in_paper": "729", "context_id": "199", "dataset_context": "The AO is recently proved [10], [11], [12] to be equivalent to the area under curve (AUC) metric employed in OTB [9], [12], NfS [14], UAV [15], TrackingNet [19], and LaSOT [20] datasets.", "mention_start": 161, "mention_end": 185, "dataset_mention": " and LaSOT [20] datasets"}, {"mentioned_in_paper": "729", "context_id": "201", "dataset_context": "The SR metric is also used in the OTB2015 [12] and OxUvA [13] datasets.", "mention_start": 30, "mention_end": 70, "dataset_mention": "the OTB2015 [12] and OxUvA [13] datasets"}, {"mentioned_in_paper": "729", "context_id": "239", "dataset_context": "This is because the resolutions of videos and objects in GOT-10k are much higher (3 \u223c 9 times) than OTB and VOT datasets.", "mention_start": 100, "mention_end": 120, "dataset_mention": "OTB and VOT datasets"}, {"mentioned_in_paper": "729", "context_id": "311", "dataset_context": "Like many large-scale datasets [24], [26], the class distribution of GOT-10k is imbalanced.", "mention_start": 0, "mention_end": 30, "dataset_mention": "Like many large-scale datasets"}, {"mentioned_in_paper": "729", "context_id": "335", "dataset_context": "We conduct experiments to compare the training performance of deep trackers on GOT-10k and commonly used ImageNet-VID datasets.", "mention_start": 105, "mention_end": 126, "dataset_mention": "ImageNet-VID datasets"}, {"mentioned_in_paper": "729", "context_id": "338", "dataset_context": "On OTB2015 dataset, MemTracker and SiamFCv2 trained on GOT-10k achieve around 1% absolute gain in AO score, compared to those trained on ImageNet-VID; while GOTURN and MDNet obtain worse results on OTB2015 when trained on GOT-10k.", "mention_start": 3, "mention_end": 18, "dataset_mention": "OTB2015 dataset"}, {"mentioned_in_paper": "729", "context_id": "341", "dataset_context": "The cross-dataset evaluation results suggest the potential domain gap between OTB2015 and GOT-10k.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The cross-dataset"}, {"mentioned_in_paper": "729", "context_id": "344", "dataset_context": "Therefore, training on a lower diversity dataset (i.e., ImageNet-VID, which contains only 30 object classes) may limit the generalization ability of deep trackers on our test data.", "mention_start": 22, "mention_end": 48, "dataset_mention": "a lower diversity dataset"}, {"mentioned_in_paper": "729", "context_id": "347", "dataset_context": "It is also the first tracking dataset that follows the one-shot protocol to promote generalization in tracker development.", "mention_start": 11, "mention_end": 37, "dataset_mention": "the first tracking dataset"}, {"mentioned_in_paper": "730", "context_id": "36", "dataset_context": "Traditional machine learning models employed towards this end, such as component analysis, sparse coding, and regression functions, treat data points as vectors and datasets as matrices.", "mention_start": 152, "mention_end": 173, "dataset_mention": "vectors and datasets"}, {"mentioned_in_paper": "730", "context_id": "232", "dataset_context": "In practice, data matrices representing visual data sets are not exactly low-rank.", "mention_start": 12, "mention_end": 56, "dataset_mention": " data matrices representing visual data sets"}, {"mentioned_in_paper": "730", "context_id": "626", "dataset_context": "Likewise, the Weizmann face dataset [185] consists of 28 subjects in 5 viewpoints, 4 illuminations, and 3 expressions.", "mention_start": 9, "mention_end": 35, "dataset_mention": " the Weizmann face dataset"}, {"mentioned_in_paper": "730", "context_id": "628", "dataset_context": "The Weizmann dataset, for example, would represented by a 28 \u00d7 5 \u00d7 4 \u00d7 3 \u00d7 7943 tensor X , where each image consists of 7943 pixels.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The Weizmann dataset"}, {"mentioned_in_paper": "731", "context_id": "149", "dataset_context": "The big \u2295 aggregates  representations into a context representation in a permutation-invariant fashion which memorizes the whole dataset  and  .", "mention_start": 108, "mention_end": 136, "dataset_mention": "memorizes the whole dataset"}, {"mentioned_in_paper": "734", "context_id": "56", "dataset_context": "Xu et al. developed a large-scale synthetic image matting dataset and used it to train a twostage deep neural network for alpha matting.", "mention_start": 20, "mention_end": 65, "dataset_mention": "a large-scale synthetic image matting dataset"}, {"mentioned_in_paper": "734", "context_id": "112", "dataset_context": "We train our network using the matting dataset shared by Xu et al. [52].", "mention_start": 27, "mention_end": 46, "dataset_mention": "the matting dataset"}, {"mentioned_in_paper": "734", "context_id": "115", "dataset_context": "Specifically, we composite the foreground image onto a randomly selected background image from MS-COCO dataset [30].", "mention_start": 94, "mention_end": 110, "dataset_mention": "MS-COCO dataset"}, {"mentioned_in_paper": "734", "context_id": "144", "dataset_context": "We experiment with our methods on the synthetic Composition-1K dataset and a real-world matting image dataset, both of which are provided by Xu et al. [52].", "mention_start": 34, "mention_end": 70, "dataset_mention": "the synthetic Composition-1K dataset"}, {"mentioned_in_paper": "734", "context_id": "144", "dataset_context": "We experiment with our methods on the synthetic Composition-1K dataset and a real-world matting image dataset, both of which are provided by Xu et al. [52].", "mention_start": 34, "mention_end": 109, "dataset_mention": "the synthetic Composition-1K dataset and a real-world matting image dataset"}, {"mentioned_in_paper": "734", "context_id": "147", "dataset_context": "Specifically, the Composition-1K testing dataset contains 1000 composited images.", "mention_start": 13, "mention_end": 48, "dataset_mention": " the Composition-1K testing dataset"}, {"mentioned_in_paper": "734", "context_id": "148", "dataset_context": "They were generated by compositing 50 unique foreground images onto each of the 20 images from the PASCAL VOC 2012 dataset [15].", "mention_start": 95, "mention_end": 122, "dataset_mention": "the PASCAL VOC 2012 dataset"}, {"mentioned_in_paper": "734", "context_id": "150", "dataset_context": "The real world image dataset contains 31 real world images pulled from the internet [52].", "mention_start": 0, "mention_end": 28, "dataset_mention": "The real world image dataset"}, {"mentioned_in_paper": "734", "context_id": "155", "dataset_context": "Table 1 reports the results on these methods as well as ours on the Composition-1K dataset.", "mention_start": 64, "mention_end": 90, "dataset_mention": "the Composition-1K dataset"}, {"mentioned_in_paper": "734", "context_id": "174", "dataset_context": "As existing deep learning methods only output alpha maps, we compare to three representative non-deep learning matting methods, namely Global Matting [19], Closed-Form Matting [27] and KNN Matting [6], on how well foreground images can be extracted from single input images on the Composition-1K dataset.", "mention_start": 276, "mention_end": 303, "dataset_mention": "the Composition-1K dataset"}, {"mentioned_in_paper": "734", "context_id": "188", "dataset_context": "As shown in Table 1, data augmentations, such as ReJPEGing and Gaussian blur, can greatly increases the errors of our methods on the Composition-1k testing dataset.", "mention_start": 128, "mention_end": 163, "dataset_mention": "the Composition-1k testing dataset"}, {"mentioned_in_paper": "734", "context_id": "194", "dataset_context": "We also test our methods on the Spectral Matting dataset [28] with the known ground truth.", "mention_start": 28, "mention_end": 56, "dataset_mention": "the Spectral Matting dataset"}, {"mentioned_in_paper": "734", "context_id": "226", "dataset_context": "The user study in the real world image dataset [52].", "mention_start": 18, "mention_end": 46, "dataset_mention": "the real world image dataset"}, {"mentioned_in_paper": "735", "context_id": "27", "dataset_context": "3) Experiments were carried out on the real scientific research results data set, and the relevant scientific research teams were accurately excavated and the research topics of the scientific research team were extracted, and the template was designed to generate an accurate stereo portrait of the scientific research team.", "mention_start": 35, "mention_end": 80, "dataset_mention": "the real scientific research results data set"}, {"mentioned_in_paper": "735", "context_id": "103", "dataset_context": "In the two scientific research project data sets, the module verification results of the accuracy, recall, and F1 of the project research topic extraction are shown in Table 4 -7 and Figure 3-4.", "mention_start": 3, "mention_end": 48, "dataset_mention": "the two scientific research project data sets"}, {"mentioned_in_paper": "735", "context_id": "110", "dataset_context": "This shows the effectiveness of the topic extraction method TF-IR on the research team achievement dataset.", "mention_start": 69, "mention_end": 106, "dataset_mention": "the research team achievement dataset"}, {"mentioned_in_paper": "736", "context_id": "47", "dataset_context": "In this case, a reconstructed image from an event data set with the presence of prompt \u03b3-ray detection (triple-coincidence) reflects the distribution of the positron-\u03b3 emitter.", "mention_start": 40, "mention_end": 58, "dataset_mention": "an event data set"}, {"mentioned_in_paper": "737", "context_id": "207", "dataset_context": "Datasets We conduct experiments on two million-scale datasets collected from real-world applications, Movielens-10M dataset [20] and Netflix Prize dataset [8], and Table 1 lists the statistics of two datasets.", "mention_start": 35, "mention_end": 61, "dataset_mention": "two million-scale datasets"}, {"mentioned_in_paper": "737", "context_id": "207", "dataset_context": "Datasets We conduct experiments on two million-scale datasets collected from real-world applications, Movielens-10M dataset [20] and Netflix Prize dataset [8], and Table 1 lists the statistics of two datasets.", "mention_start": 101, "mention_end": 123, "dataset_mention": " Movielens-10M dataset"}, {"mentioned_in_paper": "737", "context_id": "207", "dataset_context": "Datasets We conduct experiments on two million-scale datasets collected from real-world applications, Movielens-10M dataset [20] and Netflix Prize dataset [8], and Table 1 lists the statistics of two datasets.", "mention_start": 101, "mention_end": 154, "dataset_mention": " Movielens-10M dataset [20] and Netflix Prize dataset"}, {"mentioned_in_paper": "737", "context_id": "209", "dataset_context": "We binarize the datasets by keeping ratings of five stars as one, and others as zero.", "mention_start": 0, "mention_end": 24, "dataset_mention": "We binarize the datasets"}, {"mentioned_in_paper": "737", "context_id": "230", "dataset_context": "\u2022 CausE [9] : This method requires a large biased dataset and a small unbiased dataset.", "mention_start": 34, "mention_end": 57, "dataset_mention": "a large biased dataset"}, {"mentioned_in_paper": "737", "context_id": "230", "dataset_context": "\u2022 CausE [9] : This method requires a large biased dataset and a small unbiased dataset.", "mention_start": 34, "mention_end": 86, "dataset_mention": "a large biased dataset and a small unbiased dataset"}, {"mentioned_in_paper": "737", "context_id": "238", "dataset_context": "For example, DICE makes over 15% improvements with respect to NDCG@50 using MF as backbone on Moveilens-10M dataset, and over 20% improvements with respect to Recall@20 using GCN as backbone on Netflix dataset.", "mention_start": 93, "mention_end": 115, "dataset_mention": "Moveilens-10M dataset"}, {"mentioned_in_paper": "737", "context_id": "238", "dataset_context": "For example, DICE makes over 15% improvements with respect to NDCG@50 using MF as backbone on Moveilens-10M dataset, and over 20% improvements with respect to Recall@20 using GCN as backbone on Netflix dataset.", "mention_start": 193, "mention_end": 209, "dataset_mention": "Netflix dataset"}, {"mentioned_in_paper": "737", "context_id": "265", "dataset_context": "Figure 5 (a) illustrates the results on Movielens-10M dataset.", "mention_start": 40, "mention_end": 61, "dataset_mention": "Movielens-10M dataset"}, {"mentioned_in_paper": "737", "context_id": "302", "dataset_context": "Table 3 shows the results on Movielens-10M dataset.", "mention_start": 29, "mention_end": 50, "dataset_mention": "Movielens-10M dataset"}, {"mentioned_in_paper": "737", "context_id": "359", "dataset_context": "Besides IPS, Bonner et al. [9] proposed CausE that performs two MF on a large biased dataset and a small unbiased dataset respectively.", "mention_start": 69, "mention_end": 92, "dataset_mention": "a large biased dataset"}, {"mentioned_in_paper": "737", "context_id": "359", "dataset_context": "Besides IPS, Bonner et al. [9] proposed CausE that performs two MF on a large biased dataset and a small unbiased dataset respectively.", "mention_start": 69, "mention_end": 121, "dataset_mention": "a large biased dataset and a small unbiased dataset"}, {"mentioned_in_paper": "741", "context_id": "2", "dataset_context": "The performance and generalization ability of the proposed method is experimentally evaluated on two large scale driving datasets: MulRan and Oxford Radar RobotCar.", "mention_start": 97, "mention_end": 129, "dataset_mention": "two large scale driving datasets"}, {"mentioned_in_paper": "741", "context_id": "23", "dataset_context": "This can be attributed to limited availability of sufficiently large and diverse datasets.", "mention_start": 50, "mention_end": 89, "dataset_mention": "sufficiently large and diverse datasets"}, {"mentioned_in_paper": "741", "context_id": "24", "dataset_context": "The situation has improved recently with the release of large-scale MulRan [12] and Radar RobotCar [3] datasets.", "mention_start": 56, "mention_end": 111, "dataset_mention": "large-scale MulRan [12] and Radar RobotCar [3] datasets"}, {"mentioned_in_paper": "741", "context_id": "112", "dataset_context": "MuRan dataset is gathered using a vehicle equipped with Ouster OS1-64 3D LiDAR with 120 m. range and Navtech CIR204-H FMCW scanning radar with 200 operating range.", "mention_start": 0, "mention_end": 13, "dataset_mention": "MuRan dataset"}, {"mentioned_in_paper": "741", "context_id": "120", "dataset_context": "The longest and most diverse trajectory from MulRan dataset, Sejong, is split into disjoint training and evaluation parts.", "mention_start": 45, "mention_end": 59, "dataset_mention": "MulRan dataset"}, {"mentioned_in_paper": "741", "context_id": "124", "dataset_context": "To test generalization ability of our model, we test it using two traversals from Radar RobotCar dataset acquired at different days: traversal 019-01-15-13-06-37-radar-oxford-10k as a map split and 2019-01-18-14-14-42-radar-oxford-10k as a query split.", "mention_start": 81, "mention_end": 104, "dataset_mention": "Radar RobotCar dataset"}, {"mentioned_in_paper": "741", "context_id": "143", "dataset_context": "The model trained on a subset of Sejong traversal, from MulRan dataset, has a top performance when evaluated on Radar RobotCar.", "mention_start": 55, "mention_end": 70, "dataset_mention": "MulRan dataset"}, {"mentioned_in_paper": "741", "context_id": "144", "dataset_context": "It must be noted that these two datasets are acquired using a different suite of sensors, although having similar operational characteristics.", "mention_start": 11, "mention_end": 40, "dataset_mention": "noted that these two datasets"}, {"mentioned_in_paper": "741", "context_id": "145", "dataset_context": "Figure 5 shows Recall@N plots, for N ranging from 1 to 10, of all evaluated methods on different evaluation sets from MulRan dataset.", "mention_start": 117, "mention_end": 132, "dataset_mention": "MulRan dataset"}, {"mentioned_in_paper": "741", "context_id": "153", "dataset_context": "Table 5 compares performance of radar-based and LiDAR-based localization on three different evaluation sets from MulRan dataset.", "mention_start": 113, "mention_end": 127, "dataset_mention": "MulRan dataset"}, {"mentioned_in_paper": "741", "context_id": "154", "dataset_context": "LiDAR has 2-3 times higher scanning frequency than radar and our datasets contain 2-3 more LiDAR 3D point clouds than radar scan images.", "mention_start": 51, "mention_end": 73, "dataset_mention": "radar and our datasets"}, {"mentioned_in_paper": "741", "context_id": "158", "dataset_context": "Second, we evaluate the performance, using a smaller, subsampled LiDAR dataset with the same number of point clouds as radar scans.", "mention_start": 53, "mention_end": 78, "dataset_mention": " subsampled LiDAR dataset"}, {"mentioned_in_paper": "742", "context_id": "75", "dataset_context": "To evaluate these proposals we ran set of experiments on Inria Aerial Labelling Dataset 3.1.", "mention_start": 57, "mention_end": 87, "dataset_mention": "Inria Aerial Labelling Dataset"}, {"mentioned_in_paper": "742", "context_id": "78", "dataset_context": "The Inria Aerial Labelling Dataset [6] is an example of a well-explored labeled dataset for satellite imagery.", "mention_start": 0, "mention_end": 34, "dataset_mention": "The Inria Aerial Labelling Dataset"}, {"mentioned_in_paper": "742", "context_id": "78", "dataset_context": "The Inria Aerial Labelling Dataset [6] is an example of a well-explored labeled dataset for satellite imagery.", "mention_start": 56, "mention_end": 87, "dataset_mention": "a well-explored labeled dataset"}, {"mentioned_in_paper": "742", "context_id": "85", "dataset_context": "The core-training dataset was composed of four out of all five cities in the original dataset: Austin, Chicago, Western Tyrol and Vienna.", "mention_start": 0, "mention_end": 25, "dataset_mention": "The core-training dataset"}, {"mentioned_in_paper": "742", "context_id": "86", "dataset_context": "The last city -Kitsap County was held out for fine-tuning dataset.", "mention_start": 46, "mention_end": 65, "dataset_mention": "fine-tuning dataset"}, {"mentioned_in_paper": "742", "context_id": "87", "dataset_context": "The choice of Kitsap was motivated by selecting the worse building detection performance in the model presented in Inria Dataset paper [6], as this region seemed to be the most challenging for model generalization.", "mention_start": 115, "mention_end": 128, "dataset_mention": "Inria Dataset"}, {"mentioned_in_paper": "742", "context_id": "88", "dataset_context": "We trained U-Net network with Adam algorithm with batch normalization on the core-training dataset for 30 epochs.", "mention_start": 73, "mention_end": 98, "dataset_mention": "the core-training dataset"}, {"mentioned_in_paper": "742", "context_id": "91", "dataset_context": "In the fine-tuning dataset3.1.2", "mention_start": 3, "mention_end": 26, "dataset_mention": "the fine-tuning dataset"}, {"mentioned_in_paper": "742", "context_id": "93", "dataset_context": "From all the patches we created two series of small fine-tuning datasets with increasing number of labeled instances, one with instances selected by random sampling and the other with prioritiessampling.", "mention_start": 46, "mention_end": 72, "dataset_mention": "small fine-tuning datasets"}, {"mentioned_in_paper": "742", "context_id": "95", "dataset_context": "For each size of the fine-tuning dataset we fine-tune two models to compare how choice of training samples affects fine-tuning.", "mention_start": 17, "mention_end": 40, "dataset_mention": "the fine-tuning dataset"}, {"mentioned_in_paper": "742", "context_id": "98", "dataset_context": "This figure shows results for the whole fine-tuning dataset, and we can see that when the number of annotated labels increases to the size of the whole fine-tuning dataset, random samples selection gives better results than any imposed order.", "mention_start": 30, "mention_end": 59, "dataset_mention": "the whole fine-tuning dataset"}, {"mentioned_in_paper": "742", "context_id": "98", "dataset_context": "This figure shows results for the whole fine-tuning dataset, and we can see that when the number of annotated labels increases to the size of the whole fine-tuning dataset, random samples selection gives better results than any imposed order.", "mention_start": 141, "mention_end": 171, "dataset_mention": "the whole fine-tuning dataset"}, {"mentioned_in_paper": "742", "context_id": "102", "dataset_context": "The bar chart shows the differences in average IoU scores between the two experiments per fine-tuning dataset size.", "mention_start": 66, "mention_end": 109, "dataset_mention": "the two experiments per fine-tuning dataset"}, {"mentioned_in_paper": "742", "context_id": "108", "dataset_context": "With selecting samples at random with small fine-tuning datasets the results are highly variable.", "mention_start": 38, "mention_end": 64, "dataset_mention": "small fine-tuning datasets"}, {"mentioned_in_paper": "742", "context_id": "109", "dataset_context": "Our main conclusion is that selection of samples for finetuning does matter if the size of fine-tuning dataset is highly limited.", "mention_start": 91, "mention_end": 110, "dataset_mention": "fine-tuning dataset"}, {"mentioned_in_paper": "742", "context_id": "113", "dataset_context": "This knowledge guides humanitarian efforts in distribution of food, water and other basic resources for people affected by the crisis, Figure 6 : Bar chart with differences between IoU of two sampling strategies for all sizes of fine-tuning dataset.", "mention_start": 228, "mention_end": 248, "dataset_mention": "fine-tuning dataset"}, {"mentioned_in_paper": "742", "context_id": "116", "dataset_context": "Figure 7 : Bar chart with differences between IoU of two sampling strategies for initial seven sizes of fine-tuning dataset: that consecutively contains 250, 350, 450, 550, 650, 750, 850 samples.", "mention_start": 103, "mention_end": 123, "dataset_mention": "fine-tuning dataset"}, {"mentioned_in_paper": "743", "context_id": "157", "dataset_context": "We evaluate different configurations of PAG on the BSDS500 and NYUv2 datasets for boundary detection and semantic segmentation (similar observations hold on other tasks, see supplementary materials).", "mention_start": 47, "mention_end": 77, "dataset_mention": "the BSDS500 and NYUv2 datasets"}, {"mentioned_in_paper": "743", "context_id": "179", "dataset_context": "We show results of semantic segmentation on NYUv2 dataset in Table 2, comparing different baselines and our models with MultiPool at macro block Res5, MP@Res5 (PAG) for short, which are trained with different target computational budgets (specified by \u03c1).", "mention_start": 44, "mention_end": 57, "dataset_mention": "NYUv2 dataset"}, {"mentioned_in_paper": "743", "context_id": "201", "dataset_context": "When inserting the Multi-Pool module, we improve even further and surpass the compared methods for most tasks and datasets.", "mention_start": 98, "mention_end": 122, "dataset_mention": "most tasks and datasets"}, {"mentioned_in_paper": "743", "context_id": "246", "dataset_context": "We will release to public all the transformed local surface normals as an extension to Stanford-2D-3D dataset, as well as our interactive tool for annotation.", "mention_start": 87, "mention_end": 109, "dataset_mention": "Stanford-2D-3D dataset"}, {"mentioned_in_paper": "743", "context_id": "248", "dataset_context": "We conduct experiments on BSDS500 [1] and NYUv2 [12] datasets for boundary detection, depth and surface normal estimation, to complement the analysis in the main paper.", "mention_start": 26, "mention_end": 61, "dataset_mention": "BSDS500 [1] and NYUv2 [12] datasets"}, {"mentioned_in_paper": "743", "context_id": "249", "dataset_context": "In Fig. 8, we show the precision-recall curves for boundary detection on BSDS500 dataset [1].", "mention_start": 72, "mention_end": 88, "dataset_mention": "BSDS500 dataset"}, {"mentioned_in_paper": "743", "context_id": "259", "dataset_context": "We provide complementary ablation study on the task of monocular depth estimation on NYUv2 dataset.", "mention_start": 85, "mention_end": 98, "dataset_mention": "NYUv2 dataset"}, {"mentioned_in_paper": "743", "context_id": "283", "dataset_context": "We also show all the binary maps produced by PAG units in Fig. 13 over a random image from Stanford2D3D dataset for semantic segmentation, surface normal estimation and depth estimation.", "mention_start": 91, "mention_end": 111, "dataset_mention": "Stanford2D3D dataset"}, {"mentioned_in_paper": "743", "context_id": "284", "dataset_context": "In Fig. 14, we visualize the learned binary masks by Per-foratedCNN [16] on NYUv2 dataset for semantic segmentation.", "mention_start": 75, "mention_end": 89, "dataset_mention": "NYUv2 dataset"}, {"mentioned_in_paper": "747", "context_id": "33", "dataset_context": "In this paper, we analyze a large Twitter dataset for incivility detection followed by a detailed post-hoc analysis of the detected tweets.", "mention_start": 14, "mention_end": 49, "dataset_mention": " we analyze a large Twitter dataset"}, {"mentioned_in_paper": "747", "context_id": "94", "dataset_context": "Yin et al. [108] use a supervised learning methodology for cyberbully detection using content and sentiment features, as well as contextual (documents in the vicinity) features of the considered documents on Slashdot and MySpace dataset.", "mention_start": 207, "mention_end": 236, "dataset_mention": "Slashdot and MySpace dataset"}, {"mentioned_in_paper": "747", "context_id": "103", "dataset_context": "Kwak et al. [64] have analyzed a large-scale dataset of over 10 million player reports on 1.46 million toxic players from one of the most popular online game in the world, the League of Legends.", "mention_start": 22, "mention_end": 52, "dataset_mention": "analyzed a large-scale dataset"}, {"mentioned_in_paper": "747", "context_id": "167", "dataset_context": "We further filter the dataset based on the presence of mentions in the tweets.", "mention_start": 0, "mention_end": 29, "dataset_mention": "We further filter the dataset"}, {"mentioned_in_paper": "747", "context_id": "170", "dataset_context": "This reduces the tweet dataset to \u223c300,000 tweets from the earlier \u223c2,000,000 tweets.", "mention_start": 0, "mention_end": 30, "dataset_mention": "This reduces the tweet dataset"}, {"mentioned_in_paper": "747", "context_id": "220", "dataset_context": "Donald Trump's relentless focus on tax cuts, deregulation and draining the swamp is great for job growth... with minorities and so on ... Based on the 8800 incivility incidents in our manually labeled dataset, we obtain the incivility contexts.", "mention_start": 179, "mention_end": 208, "dataset_mention": "our manually labeled dataset"}, {"mentioned_in_paper": "747", "context_id": "240", "dataset_context": "We train it on their Twitter dataset having 6248 sentences and test it on 692 sentences.", "mention_start": 15, "mention_end": 36, "dataset_mention": "their Twitter dataset"}, {"mentioned_in_paper": "747", "context_id": "296", "dataset_context": "It is evident from the figure that our dataset is a good mix of both normal users as well as celebrity users.", "mention_start": 19, "mention_end": 46, "dataset_mention": "the figure that our dataset"}, {"mentioned_in_paper": "747", "context_id": "392", "dataset_context": "We use character-level models in this experiment as the characters can provide more information than the words as a whole for the tweet dataset under consideration.", "mention_start": 126, "mention_end": 143, "dataset_mention": "the tweet dataset"}, {"mentioned_in_paper": "748", "context_id": "33", "dataset_context": "As a proof of concept, we adapted the state-of-theart CNN malware detection model, Malconv [14], into the first and second tiers of Echelon and evaluated it on real malware datasets.", "mention_start": 159, "mention_end": 181, "dataset_mention": "real malware datasets"}, {"mentioned_in_paper": "748", "context_id": "39", "dataset_context": "Section 4 presents our twotier Echelon framework, followed by the details of activation trend identification in Section 5 and the training algorithms in Section 6. Section 7 presents the datasets and experimental results.", "mention_start": 152, "mention_end": 195, "dataset_mention": "Section 6. Section 7 presents the datasets"}, {"mentioned_in_paper": "748", "context_id": "145", "dataset_context": "While it is possible to get these datasets from [24] [23], their policy 2, however, does not allow us to release their datasets.", "mention_start": 104, "mention_end": 127, "dataset_mention": "release their datasets"}, {"mentioned_in_paper": "749", "context_id": "53", "dataset_context": "On SRRS [6] and Snow100K [17] datasets, respectively, we obtain 2.86dB and 2.36dB gains in PSNR metrics.", "mention_start": 3, "mention_end": 38, "dataset_mention": "SRRS [6] and Snow100K [17] datasets"}, {"mentioned_in_paper": "749", "context_id": "137", "dataset_context": "Quantitative comparison of various approaches on the CSD [7], SRRS [6], Snow 100K [17], SnowCityScapes [33] and SnowKITTI [33] datasets.", "mention_start": 87, "mention_end": 135, "dataset_mention": " SnowCityScapes [33] and SnowKITTI [33] datasets"}, {"mentioned_in_paper": "749", "context_id": "160", "dataset_context": "To demonstrate the superior performance of our manner on synthetic datasets, we train and test our Snow-Former on the five snow benchmarks, including CSD [7], SRRS [6], Snow100K [17], SnowCityScapes [33] and SnowKITTI [33] datasets, all training and testing dataset settings follow the latest published single image desnowing [7] to choose 2000 images on each testing dataset for a fair and convincing comparison.", "mention_start": 183, "mention_end": 231, "dataset_mention": " SnowCityScapes [33] and SnowKITTI [33] datasets"}, {"mentioned_in_paper": "749", "context_id": "172", "dataset_context": "On the CSD [7] dataset, SnowFormer achieves the 4.02dB PSNR and 0.2 SSIM gain compared with the   Restormer [31] and surpasses the latest specific desnowing method SMGARN [9] 7.52dB on the PSNR metric.", "mention_start": 3, "mention_end": 22, "dataset_mention": "the CSD [7] dataset"}, {"mentioned_in_paper": "749", "context_id": "174", "dataset_context": "PSNR and 0.98 SSIM on the SRRS [6] dataset, which is higher than the secondbest approach NAFNet [4] 2.75dB and 0.2 SSIM.", "mention_start": 22, "mention_end": 42, "dataset_mention": "the SRRS [6] dataset"}, {"mentioned_in_paper": "752", "context_id": "154", "dataset_context": "The SCADA dataset being used in this paper is quite large so calculating the entire posterior is impractical and would require an excessive amount of computational resource and time.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The SCADA dataset"}, {"mentioned_in_paper": "754", "context_id": "69", "dataset_context": "First, compared to model A, model C achieves a consistently superior accuracy (TAR and TPIR) on both 1:1 face verification and 1:N face identification Second, compared to model B, model C achieved also a consistently better accuracy (TAR and TPIR) on both 1:1 face verification and 1:N face identification Last, more importantly, model C is trained from scratch and achieves comparable results to the state-of-the-art (VGGFace2 [14]) which is first pre-trained on the MS-Celeb-1M dataset [15], which contains roughly 10M face images, and then is fine-tuned on the VGGFace2 dataset.", "mention_start": 463, "mention_end": 487, "dataset_mention": "the MS-Celeb-1M dataset"}, {"mentioned_in_paper": "754", "context_id": "69", "dataset_context": "First, compared to model A, model C achieves a consistently superior accuracy (TAR and TPIR) on both 1:1 face verification and 1:N face identification Second, compared to model B, model C achieved also a consistently better accuracy (TAR and TPIR) on both 1:1 face verification and 1:N face identification Last, more importantly, model C is trained from scratch and achieves comparable results to the state-of-the-art (VGGFace2 [14]) which is first pre-trained on the MS-Celeb-1M dataset [15], which contains roughly 10M face images, and then is fine-tuned on the VGGFace2 dataset.", "mention_start": 559, "mention_end": 580, "dataset_mention": "the VGGFace2 dataset"}, {"mentioned_in_paper": "754", "context_id": "80", "dataset_context": "We used the web-collected face dataset (VGGFace2 [14]).", "mention_start": 8, "mention_end": 38, "dataset_mention": "the web-collected face dataset"}, {"mentioned_in_paper": "754", "context_id": "81", "dataset_context": "All of the faces in the VGGFace2 dataset and their landmark points are detected by the recently proposed face detector [16] and facial landmark point detector [17].", "mention_start": 20, "mention_end": 40, "dataset_mention": "the VGGFace2 dataset"}, {"mentioned_in_paper": "754", "context_id": "86", "dataset_context": "We generated a validation set by selecting randomly about 10% from each subject in refined dataset, and the remains are used as the training set.", "mention_start": 83, "mention_end": 98, "dataset_mention": "refined dataset"}, {"mentioned_in_paper": "754", "context_id": "127", "dataset_context": "We evaluated the proposed method on the LFW dataset [10]", "mention_start": 36, "mention_end": 51, "dataset_mention": "the LFW dataset"}, {"mentioned_in_paper": "754", "context_id": "128", "dataset_context": "We evaluated the proposed method on the YTF dataset [11], which reveals the state-of-the-art of face verification in unconstrained environments.", "mention_start": 36, "mention_end": 51, "dataset_mention": "the YTF dataset"}, {"mentioned_in_paper": "754", "context_id": "129", "dataset_context": "YTF dataset is excellent benchmark dataset for face verification in video and contains 3, 425 videos with large variations in illuminations, facial pose, and facial expressions, from 1, 595 different identities, with an average of 2.15 videos per person.", "mention_start": 0, "mention_end": 11, "dataset_mention": "YTF dataset"}, {"mentioned_in_paper": "754", "context_id": "129", "dataset_context": "YTF dataset is excellent benchmark dataset for face verification in video and contains 3, 425 videos with large variations in illuminations, facial pose, and facial expressions, from 1, 595 different identities, with an average of 2.15 videos per person.", "mention_start": 15, "mention_end": 42, "dataset_mention": "excellent benchmark dataset"}, {"mentioned_in_paper": "754", "context_id": "133", "dataset_context": "We evaluated the proposed method on the IJB-A dataset [12] which contains face images and videos captured from unconstrained environments.", "mention_start": 36, "mention_end": 53, "dataset_mention": "the IJB-A dataset"}, {"mentioned_in_paper": "754", "context_id": "137", "dataset_context": "IJB-A dataset provides 10 split evaluations with two protocols (1:1 face verification and 1:N face identification).", "mention_start": 0, "mention_end": 13, "dataset_mention": "IJB-A dataset"}, {"mentioned_in_paper": "755", "context_id": "2", "dataset_context": "Recent object reasoning datasets, such as CATER, have revealed fundamental shortcomings of current vision-based AI systems, particularly when targeting explicit object representations, object permanence, and object reasoning.", "mention_start": 0, "mention_end": 32, "dataset_mention": "Recent object reasoning datasets"}, {"mentioned_in_paper": "755", "context_id": "15", "dataset_context": "Indeed, recent work on synthetic object reasoning datasets like CLEVR, CLEVRER, or CATER [31; 40; 88] suggests that state-of-the-art video reasoning systems [16; 84; 87] still struggle to model fundamental physical object properties, such as hollowness, blockage, or object permanence-concepts that children develop during the first few months of their lives [2; 59].", "mention_start": 22, "mention_end": 58, "dataset_mention": "synthetic object reasoning datasets"}, {"mentioned_in_paper": "755", "context_id": "110", "dataset_context": "Supervision Loss Finally, for the experiments that are using a supervised target object location signal for fine-tuning, detailed in Equation 6, we added a gating network \u03a6 that operates on the latent Gestalt codes G t k before binarization and predicts a single softmax probability, which is used to decide which entity corresponds to the Snitch in the CATER dataset.", "mention_start": 349, "mention_end": 367, "dataset_mention": "the CATER dataset"}, {"mentioned_in_paper": "755", "context_id": "147", "dataset_context": "Apart from a video footage for the main experiments and further algorithmic details on Loci, we provide additional insights and tests in the supplementary material: We evaluate the real world tracking performance of Loci on a ten hour aquarium footage found on YouTube Additionally, we examine Gestalt preservation and indicators of intuitive physics in closed loop predictions on the CLEVRER dataset [88].", "mention_start": 380, "mention_end": 400, "dataset_mention": "the CLEVRER dataset"}, {"mentioned_in_paper": "755", "context_id": "164", "dataset_context": "The Gestalt preserving performance of Loci for closed loop predictions are also demonstrated exemplary on the CLEVRER dataset in Figure 7. Here, the effects of collisions of different geometric objects are predicted into the future.", "mention_start": 106, "mention_end": 125, "dataset_mention": "the CLEVRER dataset"}, {"mentioned_in_paper": "755", "context_id": "190", "dataset_context": "To do this, we first selected a fixed amount (450) Gestalt codes created by Loci using the CATER dataset.", "mention_start": 86, "mention_end": 104, "dataset_mention": "the CATER dataset"}, {"mentioned_in_paper": "756", "context_id": "38", "dataset_context": "In the rest of this paper, we first recall LM formulas in section 2, present the extension with the translation model in section 3, then the usage of Word Embedding within this model in section 4. We present the implementation of this model in section 5 and results on the Cultural Heritage in CLEF (CHiC) Dataset 3, a sub part of the Europeanna, in section 6.", "mention_start": 293, "mention_end": 313, "dataset_mention": "CLEF (CHiC) Dataset"}, {"mentioned_in_paper": "757", "context_id": "3", "dataset_context": "Furthermore, we introduce a novel architecture that exploits such method and achieves competitive performances on the MS-COCO 2014 data set, yielding 134.6 and 131.4 CIDEr-D on the Karpathy test split in the ensemble and single model configuration respectively and 130 CIDEr-D in the official online evaluation server, despite being neither recurrent nor fully attentive.", "mention_start": 113, "mention_end": 139, "dataset_mention": "the MS-COCO 2014 data set"}, {"mentioned_in_paper": "757", "context_id": "98", "dataset_context": "The image region features [1] are extracted by the Faster R-CNN object detector [28] which is trained on the Visual Genome data set [20] using the ResNet-101 backbone [13].", "mention_start": 105, "mention_end": 131, "dataset_mention": "the Visual Genome data set"}, {"mentioned_in_paper": "757", "context_id": "104", "dataset_context": "In order to enable this work to a wider number of resource instances, our experiments focus on the Nvidia Tesla K80 GPUs which are relatively dated compared to the recent architectures such networks are usually trained on for the COCO data set.", "mention_start": 225, "mention_end": 243, "dataset_mention": "the COCO data set"}, {"mentioned_in_paper": "758", "context_id": "4", "dataset_context": "As graph-structured data sets proliferate in domains such as social networks, biological networks and knowledge graphs, the study of graph dependencies is also of increasing practical interest [3, 7].", "mention_start": 3, "mention_end": 29, "dataset_mention": "graph-structured data sets"}, {"mentioned_in_paper": "760", "context_id": "10", "dataset_context": "We used OpenImages Challenge 2019 Object Detection dataset [11] as the training data for most of cases.", "mention_start": 8, "mention_end": 58, "dataset_mention": "OpenImages Challenge 2019 Object Detection dataset"}, {"mentioned_in_paper": "760", "context_id": "11", "dataset_context": "The dataset is a subset of OpenImages V5 dataset [14].", "mention_start": 27, "mention_end": 48, "dataset_mention": "OpenImages V5 dataset"}, {"mentioned_in_paper": "760", "context_id": "80", "dataset_context": "We only train the models ID 17-28 under this data setting.", "mention_start": 14, "mention_end": 53, "dataset_mention": "the models ID 17-28 under this data set"}, {"mentioned_in_paper": "760", "context_id": "84", "dataset_context": "OpenImages dataset [14] exhibits the long-tail distribution characteristics: the number of categories is not balanced, and some categories of data are scarce.", "mention_start": 0, "mention_end": 18, "dataset_mention": "OpenImages dataset"}, {"mentioned_in_paper": "760", "context_id": "106", "dataset_context": "The definition of \"easy to confuse\" can be derived from three different perspectives: a) Hierarchy tag: OpenImages dataset [14] has hierarchy-tag relationships between different categories.", "mention_start": 103, "mention_end": 122, "dataset_mention": " OpenImages dataset"}, {"mentioned_in_paper": "760", "context_id": "130", "dataset_context": "There is a serious class imbalance issue in the Open-Image object detection dataset.", "mention_start": 44, "mention_end": 83, "dataset_mention": "the Open-Image object detection dataset"}, {"mentioned_in_paper": "760", "context_id": "136", "dataset_context": "There are some special relationships between categories in the OpenImage dataset.", "mention_start": 59, "mention_end": 80, "dataset_mention": "the OpenImage dataset"}, {"mentioned_in_paper": "760", "context_id": "156", "dataset_context": "The 28 final models are trained by PyTorch [23] and Tensorflow and all of the backbones are first pre-trained on Im-ageNet dataset.", "mention_start": 113, "mention_end": 130, "dataset_mention": "Im-ageNet dataset"}, {"mentioned_in_paper": "760", "context_id": "171", "dataset_context": "COCO means that we find total 64 classes co-exists in COCO dataset and OpenImage dataset.", "mention_start": 54, "mention_end": 66, "dataset_mention": "COCO dataset"}, {"mentioned_in_paper": "760", "context_id": "171", "dataset_context": "COCO means that we find total 64 classes co-exists in COCO dataset and OpenImage dataset.", "mention_start": 54, "mention_end": 88, "dataset_mention": "COCO dataset and OpenImage dataset"}, {"mentioned_in_paper": "760", "context_id": "172", "dataset_context": "And so, we straightforwardly adopt the Mask RCNN with ResNet152 and Cascade RCNN with ResNet50 as the 64-classes expert model which are strained on COCO dataset.", "mention_start": 147, "mention_end": 160, "dataset_mention": "COCO dataset"}, {"mentioned_in_paper": "761", "context_id": "42", "dataset_context": "-We demonstrate the effectiveness of the proposed model on two light field datasets [19].", "mention_start": 59, "mention_end": 83, "dataset_mention": "two light field datasets"}, {"mentioned_in_paper": "761", "context_id": "44", "dataset_context": "Moreover, we also validate our model on Mobile Phone dataset [32] which contains focal slices captured by a smart-phone camera.", "mention_start": 39, "mention_end": 60, "dataset_mention": "Mobile Phone dataset"}, {"mentioned_in_paper": "761", "context_id": "122", "dataset_context": "To demonstrate the effectiveness of the proposed approach, we conduct experiments on DUT-LFDD dataset, LFSD dataset and a Mobile Phone dataset.", "mention_start": 84, "mention_end": 101, "dataset_mention": "DUT-LFDD dataset"}, {"mentioned_in_paper": "761", "context_id": "122", "dataset_context": "To demonstrate the effectiveness of the proposed approach, we conduct experiments on DUT-LFDD dataset, LFSD dataset and a Mobile Phone dataset.", "mention_start": 102, "mention_end": 115, "dataset_mention": " LFSD dataset"}, {"mentioned_in_paper": "761", "context_id": "122", "dataset_context": "To demonstrate the effectiveness of the proposed approach, we conduct experiments on DUT-LFDD dataset, LFSD dataset and a Mobile Phone dataset.", "mention_start": 102, "mention_end": 142, "dataset_mention": " LFSD dataset and a Mobile Phone dataset"}, {"mentioned_in_paper": "761", "context_id": "123", "dataset_context": "Note that although there are some synthetic light field (LF) datasets proovided by [12] which consist of multiview images and corresponding depth maps.", "mention_start": 29, "mention_end": 69, "dataset_mention": "some synthetic light field (LF) datasets"}, {"mentioned_in_paper": "761", "context_id": "132", "dataset_context": "Mobile Phone dataset: The Mobile Phone dataset provided by previous researchers [32].", "mention_start": 0, "mention_end": 20, "dataset_mention": "Mobile Phone dataset"}, {"mentioned_in_paper": "761", "context_id": "132", "dataset_context": "Mobile Phone dataset: The Mobile Phone dataset provided by previous researchers [32].", "mention_start": 21, "mention_end": 46, "dataset_mention": " The Mobile Phone dataset"}, {"mentioned_in_paper": "761", "context_id": "162", "dataset_context": "Not only that, we also apply the model parameters trained on our dataset directly to the LFSD dataset for testing, and our method achieves significant advantages, such as Top-1 accuracies (Sq Rel) and Top-1 accuracies (RMSE).", "mention_start": 84, "mention_end": 101, "dataset_mention": "the LFSD dataset"}, {"mentioned_in_paper": "761", "context_id": "163", "dataset_context": "Note that the LFSD dataset only provides the focal slices, RGB images and depth maps, therefore, we only compare our method with 3 focus-based methods on LFSD dataset.", "mention_start": 5, "mention_end": 26, "dataset_mention": "that the LFSD dataset"}, {"mentioned_in_paper": "761", "context_id": "163", "dataset_context": "Note that the LFSD dataset only provides the focal slices, RGB images and depth maps, therefore, we only compare our method with 3 focus-based methods on LFSD dataset.", "mention_start": 153, "mention_end": 166, "dataset_mention": "LFSD dataset"}, {"mentioned_in_paper": "761", "context_id": "166", "dataset_context": "We further illustrate the visual results of our method on LFSD dataset in Figure 6.", "mention_start": 58, "mention_end": 70, "dataset_mention": "LFSD dataset"}, {"mentioned_in_paper": "762", "context_id": "14", "dataset_context": "To illustrate this phenomenon, we retrain the video denoiser of [10] on the Vimeo-90k dataset [16] \u2022 T. Tanay, A. Sootla, M. Maggioni and A. Leonardis are with Huawei Technologies Ltd, Noah's Ark Lab.", "mention_start": 71, "mention_end": 93, "dataset_mention": "the Vimeo-90k dataset"}, {"mentioned_in_paper": "762", "context_id": "168", "dataset_context": "We train our models using the Vimeo-90k septuplet dataset [16], consisting of about 90k 7-frame RGB sequences with a resolution of 448 \u00d7 256 downloaded from vimeo.com.", "mention_start": 26, "mention_end": 57, "dataset_mention": "the Vimeo-90k septuplet dataset"}, {"mentioned_in_paper": "762", "context_id": "173", "dataset_context": "In Table 1, we show the numbers of parameters and processing speeds 5 (fps) of each method, as well as their denoising performances as measured by the PSNR on the first frame (PSNR 1 ), last frame (PSNR 7 ) and averaged over all the frames (PSNR mean ) on the first 1024 validation sequences of the Vimeo-90k septuplet dataset.", "mention_start": 294, "mention_end": 326, "dataset_mention": "the Vimeo-90k septuplet dataset"}, {"mentioned_in_paper": "762", "context_id": "197", "dataset_context": "For each model, we show the performance on the 7th frame of the Vimeo-90k validation dataset (PSNR 7 ), the 1 st and 9 th deciles of the instability onsets on a sequence of about 2h30min (\u221e means no instabilities observed).", "mention_start": 59, "mention_end": 92, "dataset_mention": "the Vimeo-90k validation dataset"}, {"mentioned_in_paper": "762", "context_id": "217", "dataset_context": "For each model, we show the performance on the 7th frame of the Vimeo-90k validation dataset (PSNR 7 ), the 1 st and 9 th deciles of the instability onsets on a sequence of about 2h30min (\u221e means no instabilities observed).", "mention_start": 59, "mention_end": 92, "dataset_mention": "the Vimeo-90k validation dataset"}, {"mentioned_in_paper": "762", "context_id": "258", "dataset_context": "For each model, we show the performance on the 7th frame of the Vimeo-90k validation dataset (PSNR 7 ), the 1 st and 9 th deciles of the instability onsets on a sequence of about 2h30min (\u221e means no instabilities observed).", "mention_start": 59, "mention_end": 92, "dataset_mention": "the Vimeo-90k validation dataset"}, {"mentioned_in_paper": "763", "context_id": "6", "dataset_context": "We perform experiments on two challenging human activity datasets, UCF101 and HMDB51, and achieve the state-of-the-art results with the best network.", "mention_start": 42, "mention_end": 65, "dataset_mention": "human activity datasets"}, {"mentioned_in_paper": "763", "context_id": "156", "dataset_context": "We tested our method on two large action datasets, HMDB51 [21], UCF101 [33].", "mention_start": 24, "mention_end": 49, "dataset_mention": "two large action datasets"}, {"mentioned_in_paper": "763", "context_id": "160", "dataset_context": "The UCF101 dataset consists of 101 action categories with 13,320 videos and at least 100 videos are involved in each classes.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The UCF101 dataset"}, {"mentioned_in_paper": "763", "context_id": "165", "dataset_context": "Both networks are initialized with the pre-trained weights trained on the ImageNet [4] dataset.", "mention_start": 70, "mention_end": 94, "dataset_mention": "the ImageNet [4] dataset"}, {"mentioned_in_paper": "765", "context_id": "8", "dataset_context": "The light-weight NAS architecture with only 1/4 parameter size of SoTA architectures can achieve SoTA performance on semantic segmentation on the Cityscapes dataset without using any backbones.", "mention_start": 142, "mention_end": 164, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "765", "context_id": "117", "dataset_context": "The Cityscapes dataset (Cordts et al. 2016) is a recent largescale urban scene dataset containing a diverse set of stereo video sequences from 50 cities.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The Cityscapes dataset"}, {"mentioned_in_paper": "765", "context_id": "117", "dataset_context": "The Cityscapes dataset (Cordts et al. 2016) is a recent largescale urban scene dataset containing a diverse set of stereo video sequences from 50 cities.", "mention_start": 47, "mention_end": 86, "dataset_mention": "a recent largescale urban scene dataset"}, {"mentioned_in_paper": "765", "context_id": "118", "dataset_context": "Cityscapes dataset contains high quality pixel-level annotations of 5, 000 images with size 1, 024 \u00d7 2, 048.", "mention_start": 0, "mention_end": 18, "dataset_mention": "Cityscapes dataset"}, {"mentioned_in_paper": "765", "context_id": "123", "dataset_context": "Fig. 7 shows the searched cell architectures by NAS on the Cityscapes dataset.", "mention_start": 55, "mention_end": 77, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "765", "context_id": "135", "dataset_context": "The Mapillary Vistas Dataset is a large-scale streetlevel image dataset containing 25,000 high-resolution images annotated into 66/124 object categories of which 37/70 classes are instance-specific labels (v.1.2", "mention_start": 0, "mention_end": 28, "dataset_mention": "The Mapillary Vistas Dataset"}, {"mentioned_in_paper": "765", "context_id": "135", "dataset_context": "The Mapillary Vistas Dataset is a large-scale streetlevel image dataset containing 25,000 high-resolution images annotated into 66/124 object categories of which 37/70 classes are instance-specific labels (v.1.2", "mention_start": 32, "mention_end": 71, "dataset_mention": "a large-scale streetlevel image dataset"}, {"mentioned_in_paper": "765", "context_id": "140", "dataset_context": "We also adopted the Mapillary Vistas Dataset (Neuhold et al. 2017) during our training procedure.", "mention_start": 16, "mention_end": 44, "dataset_mention": "the Mapillary Vistas Dataset"}, {"mentioned_in_paper": "765", "context_id": "141", "dataset_context": "Because of the class number of cityscapes is less than Mapillary Vistas Dataset, we have to map the category into the corresponding ones in Cityscapes.", "mention_start": 55, "mention_end": 79, "dataset_mention": "Mapillary Vistas Dataset"}, {"mentioned_in_paper": "765", "context_id": "142", "dataset_context": "Table 2 shows that our NAS model outperforms the SoTA on the Cityscapes with adopting Mapillary Vistas Dataset (Neuhold et al. 2017).", "mention_start": 77, "mention_end": 110, "dataset_mention": "adopting Mapillary Vistas Dataset"}, {"mentioned_in_paper": "765", "context_id": "145", "dataset_context": "Table 3 shows that our NAS model outperforms the other model with pretraining and Object-Contextual Representations(OCR) (Yuan et al. 2019) on the Cityscapes dataset.", "mention_start": 143, "mention_end": 165, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "766", "context_id": "37", "dataset_context": "We conduct extensive model training experiments on eBay and Tiktok datasets and the proposed framework is successfully embedded in eBay's cross-platform advertising system.", "mention_start": 51, "mention_end": 75, "dataset_mention": "eBay and Tiktok datasets"}, {"mentioned_in_paper": "766", "context_id": "57", "dataset_context": "After getting the eBay categories and Tiktok hashtags datasets as input, the backend database stores these two different data corpuses separately and feeds the data to the TagPick algorithm.", "mention_start": 14, "mention_end": 62, "dataset_mention": "the eBay categories and Tiktok hashtags datasets"}, {"mentioned_in_paper": "768", "context_id": "529", "dataset_context": "For example, for the Digg dataset and k = 100, for boostable PRRgraphs, the average number of uncompressed edges is 1810.32, the average number of compressed edges is 2.41, and the compression ratio is 751.59.", "mention_start": 16, "mention_end": 33, "dataset_mention": "the Digg dataset"}, {"mentioned_in_paper": "769", "context_id": "20", "dataset_context": "Since the revision data sets are XML format and the file that contains the user information is separated from the revision data, we had to parse the given data set and refer to another file.", "mention_start": 6, "mention_end": 28, "dataset_mention": "the revision data sets"}, {"mentioned_in_paper": "769", "context_id": "27", "dataset_context": "Figure 1 depicts the comparison of AUC-ROC among different sampling fraction and shows using 1 50 sampling data set of the latest negative data set results in the best scores.", "mention_start": 119, "mention_end": 147, "dataset_mention": "the latest negative data set"}, {"mentioned_in_paper": "769", "context_id": "43", "dataset_context": "1e-5) and a gradient boosting tree package of a scikitlearn as a feature selection model and select 53 features when we set the random_state, one of the hyper parameters of the gradient boosting tree, as 0. The AUC-ROC of our models with feature selection increase from 0.93731 to 0.95124 using the same model and the data set as Section 2.1.2,", "mention_start": 294, "mention_end": 326, "dataset_mention": "the same model and the data set"}, {"mentioned_in_paper": "770", "context_id": "8", "dataset_context": "Extensive experiments on the SmartHome dataset and the large-scale NTU RGB-D dataset demonstrate that our method outperforms most of RNN-based methods, which verify the complementary property between spatial and temporal information and the robustness to noise.", "mention_start": 25, "mention_end": 46, "dataset_mention": "the SmartHome dataset"}, {"mentioned_in_paper": "770", "context_id": "8", "dataset_context": "Extensive experiments on the SmartHome dataset and the large-scale NTU RGB-D dataset demonstrate that our method outperforms most of RNN-based methods, which verify the complementary property between spatial and temporal information and the robustness to noise.", "mention_start": 25, "mention_end": 84, "dataset_mention": "the SmartHome dataset and the large-scale NTU RGB-D dataset"}, {"mentioned_in_paper": "770", "context_id": "44", "dataset_context": "We use the joint configuration in the NTU RGB+D dataset [17], where M equals to 25.", "mention_start": 34, "mention_end": 55, "dataset_mention": "the NTU RGB+D dataset"}, {"mentioned_in_paper": "770", "context_id": "82", "dataset_context": "2) SmartHome Dataset: SmartHome dataset [16] is collected by our lab, which contains six types of actions: \"box\", \"high wave\", \"horizontal wave\", \"curl\", \"circle\", \"hand up\".", "mention_start": 0, "mention_end": 20, "dataset_mention": "2) SmartHome Dataset"}, {"mentioned_in_paper": "770", "context_id": "82", "dataset_context": "2) SmartHome Dataset: SmartHome dataset [16] is collected by our lab, which contains six types of actions: \"box\", \"high wave\", \"horizontal wave\", \"curl\", \"circle\", \"hand up\".", "mention_start": 21, "mention_end": 39, "dataset_mention": " SmartHome dataset"}, {"mentioned_in_paper": "770", "context_id": "84", "dataset_context": "Skeleton joints in SmartHome dataset contain much noises, due to the effect of occlusions and the unconstrained poses of action performers.", "mention_start": 19, "mention_end": 36, "dataset_mention": "SmartHome dataset"}, {"mentioned_in_paper": "770", "context_id": "94", "dataset_context": "Fig. 5 shows the convergence curves on the NTU RGB+D dataset for spatial stream and temporal stream, where the error rate tends to converge when the training epoch grows to 250.", "mention_start": 39, "mention_end": 60, "dataset_mention": "the NTU RGB+D dataset"}, {"mentioned_in_paper": "770", "context_id": "97", "dataset_context": "By fusing the spatial stream and temporal stream, it has no obvious effect on SmartHome dataset for cross subject evaluation.", "mention_start": 77, "mention_end": 95, "dataset_mention": "SmartHome dataset"}, {"mentioned_in_paper": "770", "context_id": "98", "dataset_context": "Because SmartHome dataset does not contain action pairs that has opposite temporal order.", "mention_start": 8, "mention_end": 25, "dataset_mention": "SmartHome dataset"}, {"mentioned_in_paper": "770", "context_id": "99", "dataset_context": "On the contray, two-stream 3D CNN respectively achieves 5.46% and 5.90% higher than individual stream on NTU RGB+D dataset for cross-view evaluation.", "mention_start": 104, "mention_end": 122, "dataset_mention": "NTU RGB+D dataset"}, {"mentioned_in_paper": "770", "context_id": "107", "dataset_context": "Table II show the recognition accuracies with different values of l from 0 to 3. It also can be observed that our method achieves the best performance on the NTU RGB+D dataset when fusing all levels.", "mention_start": 154, "mention_end": 175, "dataset_mention": "the NTU RGB+D dataset"}, {"mentioned_in_paper": "770", "context_id": "109", "dataset_context": "Compared to other methods(see Table III) on the SmartHome dataset, the proposed two-stream 3D CNN model achieves the best performance, with the accuracy of 79.38%, which is better than Synthesized+Pre-trained [16].", "mention_start": 44, "mention_end": 65, "dataset_mention": "the SmartHome dataset"}, {"mentioned_in_paper": "770", "context_id": "112", "dataset_context": "We compare the performance of our method with stateof-theart methods on the NTU RGB+D dataset for crosssubject and cross-view evaluation and report the results in Table IV.", "mention_start": 72, "mention_end": 93, "dataset_mention": "the NTU RGB+D dataset"}, {"mentioned_in_paper": "771", "context_id": "87", "dataset_context": "Experiments are performed in three different MOT datasets: MOT17 [24], MOT20 [10], and DanceTrack [35].", "mention_start": 29, "mention_end": 57, "dataset_mention": "three different MOT datasets"}, {"mentioned_in_paper": "771", "context_id": "111", "dataset_context": "Overall, FCG presents competitive results compared to other methods on MOT17 and MOT20 datasets, achieving state-of-the-art results for the DanceTrack benchmark.", "mention_start": 70, "mention_end": 95, "dataset_mention": "MOT17 and MOT20 datasets"}, {"mentioned_in_paper": "771", "context_id": "113", "dataset_context": "For all benchmarks, there is a similar performance when forming the tracklets within temporal windows of size W = 2 to W = 6 frames, starting to decrease from W = 7 for the DanceTrack dataset, and from W = 15 for MOT17 and MOT20.", "mention_start": 168, "mention_end": 191, "dataset_mention": "the DanceTrack dataset"}, {"mentioned_in_paper": "771", "context_id": "125", "dataset_context": "On the other hand, temporal (T) coherence provides a considerable improvement in the pedestrian datasets, MOT17 and MOT20, and a small decay for DanceTrack.", "mention_start": 80, "mention_end": 104, "dataset_mention": "the pedestrian datasets"}, {"mentioned_in_paper": "771", "context_id": "136", "dataset_context": "While being a much simpler method than other current MOT trackers, FCG achieves state-of-the-art performance on the DanceTrack benchmark, and presents competitive results in both MOT17 and MOT20 datasets.", "mention_start": 173, "mention_end": 203, "dataset_mention": "both MOT17 and MOT20 datasets"}, {"mentioned_in_paper": "772", "context_id": "3", "dataset_context": "Well separated clusters are identified even in the presence of outliers, whereas for not so well separated dataset, final number of clusters are estimated and the detected clusters are merged to produce the final clusters.", "mention_start": 84, "mention_end": 114, "dataset_mention": "not so well separated dataset"}, {"mentioned_in_paper": "772", "context_id": "4", "dataset_context": "Experiments performed with several large dimensional synthetic and real datasets show good results with robustness to noise and density variation within dataset.", "mention_start": 27, "mention_end": 80, "dataset_mention": "several large dimensional synthetic and real datasets"}, {"mentioned_in_paper": "772", "context_id": "23", "dataset_context": "Human, when exposed to a representation of an intelligible dataset, at once recognizes the clusters present, because some data points appears so close, that they could hardly go to different clusters.", "mention_start": 42, "mention_end": 66, "dataset_mention": "an intelligible dataset"}, {"mentioned_in_paper": "772", "context_id": "31", "dataset_context": "Unlike equation 1, the cost will not always increase with the decrease in number of clusters, indicating that the dataset supports merge.", "mention_start": 93, "mention_end": 121, "dataset_mention": " indicating that the dataset"}, {"mentioned_in_paper": "772", "context_id": "34", "dataset_context": "We have conducted experiments with standard convex datasets, and compared the performance of the algorithm with the existing algorithms, in terms of proposed number of clusters and quality of the obtained clusters.", "mention_start": 35, "mention_end": 59, "dataset_mention": "standard convex datasets"}, {"mentioned_in_paper": "772", "context_id": "98", "dataset_context": "Figure 1 shows affinity histogram taken for d8c8N [18] data set.", "mention_start": 44, "mention_end": 63, "dataset_mention": "d8c8N [18] data set"}, {"mentioned_in_paper": "772", "context_id": "123", "dataset_context": "Fig. 2 shows the distribution of cluster sizes as found in the thyroid dataset [19].", "mention_start": 59, "mention_end": 78, "dataset_mention": "the thyroid dataset"}, {"mentioned_in_paper": "772", "context_id": "128", "dataset_context": "for j = 1 to n do 7:    for j = 1 : n do  Fig. 4 shows in first two dimensions the d8c8N data set clustered by the algorithm, here outliers as desired are not included in any cluster.", "mention_start": 56, "mention_end": 97, "dataset_mention": "first two dimensions the d8c8N data set"}, {"mentioned_in_paper": "772", "context_id": "133", "dataset_context": "Datasets used for cluster analysis generally have known number of clusters and are called ground truth datasets.", "mention_start": 83, "mention_end": 111, "dataset_mention": "called ground truth datasets"}, {"mentioned_in_paper": "772", "context_id": "138", "dataset_context": "Table 1 shows real world and synthetic datasets used.", "mention_start": 14, "mention_end": 47, "dataset_mention": "real world and synthetic datasets"}, {"mentioned_in_paper": "772", "context_id": "143", "dataset_context": "By the nature of the centroid distance function used, the algorithm would not work very well for shape based datasets, thus all synthetic data sets taken, are convex.", "mention_start": 96, "mention_end": 117, "dataset_mention": "shape based datasets"}, {"mentioned_in_paper": "772", "context_id": "148", "dataset_context": "An exact match of reported value with the known number of clusters in the ground truth dataset is considered as success, we define accuracy as the percentage of datasets for which an algorithm correctly predicts the number of clusters.", "mention_start": 70, "mention_end": 94, "dataset_mention": "the ground truth dataset"}, {"mentioned_in_paper": "772", "context_id": "168", "dataset_context": "We have proposed a parameter free clustering algorithm which gives promising results in convex datasets.", "mention_start": 88, "mention_end": 103, "dataset_mention": "convex datasets"}, {"mentioned_in_paper": "773", "context_id": "141", "dataset_context": "Algorithm 1 The Improved Bayesian Optimization Algorithm 1: Randomly initial some control strategy inputs with multi-dimensions ( 0 ,  1 , \u2026 ,   ) 2: Compute the state variables through the control inputs and the given high-dimensional control model 3: Calculate the corresponding objective function values for each control strategy input and same them to compose a dataset  = {( 0 , ( 0 )), ( 1 , ( 1 )), \u2026 , (  , (  )) 4: Train the Gaussian process model by using dataset  5: for i = 1, 2, \u2026,  do 6:", "mention_start": 355, "mention_end": 373, "dataset_mention": "compose a dataset"}, {"mentioned_in_paper": "773", "context_id": "198", "dataset_context": "To compare four algorithms under the same implementation condition, we do compute their mean squared error (MSE) values [39] [40] on the same dataset named diabetes datasets from Python sklearn library [41].", "mention_start": 132, "mention_end": 173, "dataset_mention": "the same dataset named diabetes datasets"}, {"mentioned_in_paper": "774", "context_id": "0", "dataset_context": "This paper takes a problem-oriented perspective and presents a comprehensive review of transfer learning methods, both shallow and deep, for cross-dataset visual recognition.", "mention_start": 140, "mention_end": 154, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "1", "dataset_context": "Specifically, it categorises the cross-dataset recognition into seventeen problems based on a set of carefully chosen data and label attributes.", "mention_start": 13, "mention_end": 46, "dataset_mention": " it categorises the cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "15", "dataset_context": "Unlike these existing survey papers, this paper takes a new problem-oriented perspective and presents a comprehensive review of transfer learning methods for cross-dataset visual recognition.", "mention_start": 157, "mention_end": 171, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "17", "dataset_context": "\u2022 It defines a set of data and label attributes, categorises in a fine-grained way the cross-dataset recognition into seventeen problems based on these attributes, and presents a comprehensive review of the transfer learning methods, both shallow and deep, developed to date for each problem.", "mention_start": 63, "mention_end": 100, "dataset_mention": "a fine-grained way the cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "26", "dataset_context": "Section 2 explains the terminologies used in the paper, defines the problem-oriented taxonomy of cross-dataset recognition, and summarises the transfer learning approaches to crossdataset recognition.", "mention_start": 96, "mention_end": 110, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "26", "dataset_context": "Section 2 explains the terminologies used in the paper, defines the problem-oriented taxonomy of cross-dataset recognition, and summarises the transfer learning approaches to crossdataset recognition.", "mention_start": 174, "mention_end": 187, "dataset_mention": "crossdataset"}, {"mentioned_in_paper": "774", "context_id": "29", "dataset_context": "Section 7 discusses and examines the suitability of the most commonly used datasets for cross-dataset transfer learning for all the problems.", "mention_start": 88, "mention_end": 101, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "37", "dataset_context": "(Dataset) A dataset is defined as S = {N, X, P(x), Y, f (x)}, which is a collection of N data that belong to a specific domain D = {X, P(x)} with a specific task T = {Y, f (x)}.", "mention_start": 0, "mention_end": 8, "dataset_mention": "(Dataset"}, {"mentioned_in_paper": "774", "context_id": "37", "dataset_context": "(Dataset) A dataset is defined as S = {N, X, P(x), Y, f (x)}, which is a collection of N data that belong to a specific domain D = {X, P(x)} with a specific task T = {Y, f (x)}.", "mention_start": 0, "mention_end": 19, "dataset_mention": "(Dataset) A dataset"}, {"mentioned_in_paper": "774", "context_id": "43", "dataset_context": "Specifically, in the context of cross-dataset recognition, the aim of transfer learning is to learn a robust classifier f (x) from a dataset (i.e.", "mention_start": 31, "mention_end": 45, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "44", "dataset_context": "target dataset S T ) by effectively utilising the knowledge offered through other datasets (i.e.", "mention_start": 24, "mention_end": 90, "dataset_mention": "effectively utilising the knowledge offered through other datasets"}, {"mentioned_in_paper": "774", "context_id": "45", "dataset_context": "source datasets S S ).", "mention_start": 0, "mention_end": 15, "dataset_mention": "source datasets"}, {"mentioned_in_paper": "774", "context_id": "46", "dataset_context": "In cross-dataset recognition, there are often two datasets.", "mention_start": 3, "mention_end": 16, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "46", "dataset_context": "In cross-dataset recognition, there are often two datasets.", "mention_start": 39, "mention_end": 58, "dataset_mention": "often two datasets"}, {"mentioned_in_paper": "774", "context_id": "47", "dataset_context": "One, referred to as a source dataset, is used in training and the other, referred to as a target dataset, is to be recognized.", "mention_start": 19, "mention_end": 36, "dataset_mention": "a source dataset"}, {"mentioned_in_paper": "774", "context_id": "50", "dataset_context": "These attributes have led to a comprehensive taxonomy of cross-dataset recognition problems that provides a unique perspective for this survey.", "mention_start": 57, "mention_end": 70, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "70", "dataset_context": "Fig. 1 shows the problem-oriented taxonomy for cross-dataset recognition, which shows seventeen different problems.", "mention_start": 47, "mention_end": 60, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "72", "dataset_context": "A problem-oriented taxonomy for cross-dataset recognition including the number of papers that are found to address the problems.", "mention_start": 32, "mention_end": 45, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "74", "dataset_context": "This section summarises several most typical approaches to transfer learning for cross-dataset recognition, including Statistical approach, Geometric approach, Higher-level Representation, Correspondence approach, Class-based approach, Self Labelling, and Hybrid approach.", "mention_start": 81, "mention_end": 94, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "83", "dataset_context": "Geometric Approach: bridges datasets according to their geometrical properties.", "mention_start": 19, "mention_end": 36, "dataset_mention": " bridges datasets"}, {"mentioned_in_paper": "774", "context_id": "104", "dataset_context": "In the following sections, we present a comprehensive review on what approaches have been or can be used for the cross-dataset recognition problems shown in Figure 1.", "mention_start": 108, "mention_end": 126, "dataset_mention": "the cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "145", "dataset_context": "Inspired by the \"name the dataset\" game of Torralba and Efros [31], we can directly train a domain classifier \u03b8 D to identify whether a training example originates from the source or target domain given its feature representation.", "mention_start": 12, "mention_end": 33, "dataset_mention": "the \"name the dataset"}, {"mentioned_in_paper": "774", "context_id": "159", "dataset_context": "The higher-level representation approach and class-based approach have been used together for better cross-dataset representation.", "mention_start": 94, "mention_end": 114, "dataset_mention": "better cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "241", "dataset_context": "Donahue et al. [50] propose the deep convolutional representations named DeCAF, where a deep CNN model is pre-trained using the source dataset (generally largescale) in a fully supervised fashion.", "mention_start": 123, "mention_end": 142, "dataset_mention": "the source dataset"}, {"mentioned_in_paper": "774", "context_id": "243", "dataset_context": "The deep auto-encoders are also used for the cross-dataset tasks by exploiting more transferable features by reconstruction [27, 75, 77, 104, 109].", "mention_start": 41, "mention_end": 58, "dataset_mention": "the cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "376", "dataset_context": "Thus, multiple source datasets are generally required to learn the dataset invariant knowledge that can be generalized to a new dataset.", "mention_start": 5, "mention_end": 30, "dataset_mention": " multiple source datasets"}, {"mentioned_in_paper": "774", "context_id": "376", "dataset_context": "Thus, multiple source datasets are generally required to learn the dataset invariant knowledge that can be generalized to a new dataset.", "mention_start": 56, "mention_end": 74, "dataset_mention": "learn the dataset"}, {"mentioned_in_paper": "774", "context_id": "383", "dataset_context": "Fang et al. [58] propose an unbiased metric learning approach to learn unbiased metric from multiple biased datasets.", "mention_start": 92, "mention_end": 116, "dataset_mention": "multiple biased datasets"}, {"mentioned_in_paper": "774", "context_id": "471", "dataset_context": "Chen et al. [25] and Li et al. [134] assume the source datasets contain multiple modalities and target dataset only contains one modality and the distribution shift between datasets also exists.", "mention_start": 44, "mention_end": 63, "dataset_mention": "the source datasets"}, {"mentioned_in_paper": "774", "context_id": "487", "dataset_context": "The deep CNNs are pre-trained on the source data (e.g. a large-scale labelled RGB dataset).", "mention_start": 33, "mention_end": 89, "dataset_mention": "the source data (e.g. a large-scale labelled RGB dataset"}, {"mentioned_in_paper": "774", "context_id": "498", "dataset_context": "Though the original purposes of some of these work on translation between domains may not be cross-dataset recognition, the ideas can be borrowed for cross-modality or cross feature spaces recognition.", "mention_start": 93, "mention_end": 106, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "508", "dataset_context": "The pre-trained deep models from a very large source dataset are used either as an initialization (then fine-tune the model according to the target data) or as a fixed feature extractor for the target, which is generally different from the original task (i.e.", "mention_start": 33, "mention_end": 60, "dataset_mention": "a very large source dataset"}, {"mentioned_in_paper": "774", "context_id": "585", "dataset_context": "Most of the methods for this problem rely on the existence of a labelled source dataset of seen categories and the prior knowledge about the semantic relationship between the unseen and seen categories.", "mention_start": 62, "mention_end": 87, "dataset_mention": "a labelled source dataset"}, {"mentioned_in_paper": "774", "context_id": "635", "dataset_context": "Table 1 lists the commonly used visual datasets for transfer learning.", "mention_start": 32, "mention_end": 47, "dataset_mention": "visual datasets"}, {"mentioned_in_paper": "774", "context_id": "637", "dataset_context": "In the table, the \u2713indicates the dataset has been evaluated on the corresponding problem while the # indicates the datasets that have the potential to be used in the evaluation of the algorithms for the problem though reported results are not publicly available to our knowledge.", "mention_start": 13, "mention_end": 40, "dataset_mention": " the \u2713indicates the dataset"}, {"mentioned_in_paper": "774", "context_id": "637", "dataset_context": "In the table, the \u2713indicates the dataset has been evaluated on the corresponding problem while the # indicates the datasets that have the potential to be used in the evaluation of the algorithms for the problem though reported results are not publicly available to our knowledge.", "mention_start": 94, "mention_end": 123, "dataset_mention": "the # indicates the datasets"}, {"mentioned_in_paper": "774", "context_id": "639", "dataset_context": "Transfer learning is a promising and important approach to cross-dataset visual recognition and has been extensively studied in the past decades with much success.", "mention_start": 59, "mention_end": 72, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "643", "dataset_context": "Such a landscape together with the recent fast-advancing deep learning approach has revealed many challenges and opened many future opportunities as elaborated below for cross-dataset visual recognition.", "mention_start": 170, "mention_end": 183, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "648", "dataset_context": "The pre-trained deep models from a very large source dataset are used either as an initialization [269] (then fine-tune the model according to the target data) or a fixed feature extractor for the target task of interest [50, 189].", "mention_start": 33, "mention_end": 60, "dataset_mention": "a very large source dataset"}, {"mentioned_in_paper": "774", "context_id": "656", "dataset_context": "\u2713 Car over time [92] \u2713 STL-10 dataset [34] \u2713 LabelMe \u2192 NUS-WIDE [235] \u2713", "mention_start": 11, "mention_end": 37, "dataset_mention": "time [92] \u2713 STL-10 dataset"}, {"mentioned_in_paper": "774", "context_id": "660", "dataset_context": "KSA [156] \u2713 A combination of KTH, Weizmann, UIUC [143] \u2713 Multiview IXMAS dataset [244] \u2713 \u2713 N-UCLA Multiview Action3D [236] \u2713 ACT4 2 dataset [30, 166] \u2713 \u2713 MSR pair action 3D \u2192 MSR daily [103] \u2713", "mention_start": 43, "mention_end": 80, "dataset_mention": " UIUC [143] \u2713 Multiview IXMAS dataset"}, {"mentioned_in_paper": "774", "context_id": "660", "dataset_context": "KSA [156] \u2713 A combination of KTH, Weizmann, UIUC [143] \u2713 Multiview IXMAS dataset [244] \u2713 \u2713 N-UCLA Multiview Action3D [236] \u2713 ACT4 2 dataset [30, 166] \u2713 \u2713 MSR pair action 3D \u2192 MSR daily [103] \u2713", "mention_start": 43, "mention_end": 139, "dataset_mention": " UIUC [143] \u2713 Multiview IXMAS dataset [244] \u2713 \u2713 N-UCLA Multiview Action3D [236] \u2713 ACT4 2 dataset"}, {"mentioned_in_paper": "774", "context_id": "675", "dataset_context": "Partial domain adaptation aims at adapting from a source dataset to an unlabelled target dataset whose label space is known to be a subspace of that of the source [21, 94, 272] or in a more general and challenging setting where only a subset of the label spaces between the source and target is overlapping [171].", "mention_start": 48, "mention_end": 64, "dataset_mention": "a source dataset"}, {"mentioned_in_paper": "774", "context_id": "676", "dataset_context": "The former may be considered to be a special case of transfer learning between heterogeneous label spaces and a typical and practical example is to transfer from a large source dataset with more classes to a small target dataset with less classes.", "mention_start": 162, "mention_end": 184, "dataset_mention": "a large source dataset"}, {"mentioned_in_paper": "774", "context_id": "702", "dataset_context": "So far, there has been little study on how the existing algorithms for cross-dataset recognition would perform on imbalanced target data or how the imbalance would affect the algorithm performance.", "mention_start": 70, "mention_end": 84, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "736", "dataset_context": "A recent work releases a large scale weakly labelled web image dataset (WebVision [136]).", "mention_start": 0, "mention_end": 70, "dataset_mention": "A recent work releases a large scale weakly labelled web image dataset"}, {"mentioned_in_paper": "774", "context_id": "742", "dataset_context": "Most of the current visual datasets for cross-dataset recognition are small scale in terms of either number of classes or number of samples and they are especially not suitable for evaluating deep learning algorithms.", "mention_start": 8, "mention_end": 35, "dataset_mention": "the current visual datasets"}, {"mentioned_in_paper": "774", "context_id": "742", "dataset_context": "Most of the current visual datasets for cross-dataset recognition are small scale in terms of either number of classes or number of samples and they are especially not suitable for evaluating deep learning algorithms.", "mention_start": 40, "mention_end": 53, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "744", "dataset_context": "suitable for different problems) and realistic dataset would drive the research a significant step forward.", "mention_start": 13, "mention_end": 54, "dataset_mention": "different problems) and realistic dataset"}, {"mentioned_in_paper": "774", "context_id": "746", "dataset_context": "Combinations and re-targeting of existent datasets can be an effective and economical way as demonstrated in [275].", "mention_start": 33, "mention_end": 50, "dataset_mention": "existent datasets"}, {"mentioned_in_paper": "774", "context_id": "747", "dataset_context": "As shown in Table 1, there are few visual recognition datasets designed for online transfer learning (e.g.", "mention_start": 30, "mention_end": 62, "dataset_mention": "few visual recognition datasets"}, {"mentioned_in_paper": "774", "context_id": "752", "dataset_context": "Many transfer learning algorithms for cross-dataset visual recognition have been developed in the last decade as reviewed in this paper.", "mention_start": 38, "mention_end": 51, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "754", "dataset_context": "This paper intends to answer the question by providing a problem-oriented taxonomy of transfer learning for cross-dataset recognition and a comprehensive survey of the recently developed algorithms with respect to the taxonomy.", "mention_start": 108, "mention_end": 121, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "774", "context_id": "758", "dataset_context": "Though it is impossible for this survey to cover all the published papers on this topic, the selected works have well represented the recent advances and in-depth analysis of these works have revealed the future research directions in transfer learning for cross-dataset visual recognition.", "mention_start": 256, "mention_end": 270, "dataset_mention": "cross-dataset"}, {"mentioned_in_paper": "776", "context_id": "43", "dataset_context": "Eigenvalues of the original unpermuted data set that reject the null hypothesis are considered \"signal\".", "mention_start": 15, "mention_end": 47, "dataset_mention": "the original unpermuted data set"}, {"mentioned_in_paper": "776", "context_id": "77", "dataset_context": "In this study, T2018-ML dataset is used in unsupervised experiments only.", "mention_start": 14, "mention_end": 31, "dataset_mention": " T2018-ML dataset"}, {"mentioned_in_paper": "776", "context_id": "90", "dataset_context": "We begin with examining all available content performance signals (count of retweets, replies and favorites) in the extended time-frame datasets.", "mention_start": 111, "mention_end": 144, "dataset_mention": "the extended time-frame datasets"}, {"mentioned_in_paper": "776", "context_id": "94", "dataset_context": "Finally we evaluate the generalizability of our method across topics and cultures, modeling engagement on the multilingual extendedtimeframe dataset T2017-ML.", "mention_start": 105, "mention_end": 148, "dataset_mention": "the multilingual extendedtimeframe dataset"}, {"mentioned_in_paper": "776", "context_id": "97", "dataset_context": "Variances of the un-permuted signals and the 95% quantiles for the three eigenvalues of the permuted data are shown in figure 2. Very similar results are obtained for the 2018 data set (not shown).", "mention_start": 167, "mention_end": 184, "dataset_mention": "the 2018 data set"}, {"mentioned_in_paper": "776", "context_id": "110", "dataset_context": "Features extracted from the quoted content did not provide a significant boost over SOTA, likely due to visual modality dominating in the T2016-IMG dataset, as considered by (Wang et al., 2018).", "mention_start": 133, "mention_end": 155, "dataset_mention": "the T2016-IMG dataset"}, {"mentioned_in_paper": "776", "context_id": "114", "dataset_context": "Table 6 shows the performance of our engagement models on the multilingual extended timeframe dataset.", "mention_start": 58, "mention_end": 101, "dataset_mention": "the multilingual extended timeframe dataset"}, {"mentioned_in_paper": "780", "context_id": "5", "dataset_context": "Experiments on multiple object tracking datasets demonstrate that our method can effectively learning discriminative appearance embeddings in a semi-supervised fashion and outperform state of the art methods on representative benchmarks.", "mention_start": 15, "mention_end": 48, "dataset_mention": "multiple object tracking datasets"}, {"mentioned_in_paper": "780", "context_id": "18", "dataset_context": "This is because such method stems from image-level recognition datasets where the temporal information is not present.", "mention_start": 39, "mention_end": 71, "dataset_mention": "image-level recognition datasets"}, {"mentioned_in_paper": "780", "context_id": "29", "dataset_context": "We benchmark tracking models learned with Semi-TCL on multiple MOT datasets, including MOT15 [15], MOT16 [20], MOT17 [20], and MOT20 [6].", "mention_start": 54, "mention_end": 75, "dataset_mention": "multiple MOT datasets"}, {"mentioned_in_paper": "780", "context_id": "144", "dataset_context": "They are image detection dataset for pre-training, labeled video tracking dataset for supervised joint tracking and embedding learning, unlabeled video dataset for Semisupervised learning.", "mention_start": 50, "mention_end": 81, "dataset_mention": " labeled video tracking dataset"}, {"mentioned_in_paper": "780", "context_id": "145", "dataset_context": "Person detection dataset we employed Crowdhuman [26] for the pre-training.", "mention_start": 0, "mention_end": 24, "dataset_mention": "Person detection dataset"}, {"mentioned_in_paper": "780", "context_id": "146", "dataset_context": "Crowdhuman is a person detection image dataset with more than 20k images and 470K instances.", "mention_start": 14, "mention_end": 46, "dataset_mention": "a person detection image dataset"}, {"mentioned_in_paper": "780", "context_id": "147", "dataset_context": "Labeled video tracking dataset We used MOT15, MOT17, MOT20 training set as our labeled set.", "mention_start": 0, "mention_end": 30, "dataset_mention": "Labeled video tracking dataset"}, {"mentioned_in_paper": "780", "context_id": "150", "dataset_context": "Unlabeled video dataset We employed the AVA-Kinetics [16] and MEVA [5] Dataset to boost the Semi-TCL learning.", "mention_start": 36, "mention_end": 78, "dataset_mention": "the AVA-Kinetics [16] and MEVA [5] Dataset"}, {"mentioned_in_paper": "780", "context_id": "152", "dataset_context": "The AVA-Kinetics dataset has relatively low resolution varying from 144 \u00d7 256 to 454 \u00d7 256, and the total amount of videos is 230k.", "mention_start": 0, "mention_end": 24, "dataset_mention": "The AVA-Kinetics dataset"}, {"mentioned_in_paper": "780", "context_id": "155", "dataset_context": "Compared with AVA-Kinetics, MEVA dataset has a high resolution with 1920 \u00d7 1072.", "mention_start": 27, "mention_end": 40, "dataset_mention": " MEVA dataset"}, {"mentioned_in_paper": "780", "context_id": "163", "dataset_context": "Person detection dataset is first employed as pre-training, and then Semi-TCL training is conducted on the joint set of labeled and unlabeled videos.", "mention_start": 0, "mention_end": 24, "dataset_mention": "Person detection dataset"}, {"mentioned_in_paper": "780", "context_id": "168", "dataset_context": "Semi-TCL is trained on the joint labeled and unlabeled video dataset, while tested on MOT15, MOT16, MOT17, MOT20 benchmarks.", "mention_start": 23, "mention_end": 68, "dataset_mention": "the joint labeled and unlabeled video dataset"}, {"mentioned_in_paper": "780", "context_id": "198", "dataset_context": "To see whether CE or contrastive learning can have better pre-training quality, we use Crowdhuman dataset [26] for training and evaluation is conducted on MOT17 validation.", "mention_start": 86, "mention_end": 105, "dataset_mention": "Crowdhuman dataset"}, {"mentioned_in_paper": "780", "context_id": "208", "dataset_context": "Table 3 shows embedding learned with MEVA dataset (15 videos and 17k frames) outperform all the three AVA series dataset with much smaller data amount.", "mention_start": 37, "mention_end": 49, "dataset_mention": "MEVA dataset"}, {"mentioned_in_paper": "780", "context_id": "208", "dataset_context": "Table 3 shows embedding learned with MEVA dataset (15 videos and 17k frames) outperform all the three AVA series dataset with much smaller data amount.", "mention_start": 37, "mention_end": 120, "dataset_mention": "MEVA dataset (15 videos and 17k frames) outperform all the three AVA series dataset"}, {"mentioned_in_paper": "780", "context_id": "209", "dataset_context": "With this comparison, we are also interested in the case where we combine the AVA and MEVA datasets.", "mention_start": 73, "mention_end": 99, "dataset_mention": "the AVA and MEVA datasets"}, {"mentioned_in_paper": "780", "context_id": "210", "dataset_context": "By combining the MEVA and AVA100 dataset, we found the joint video dataset can boost the MOT17 evaluation results further to IDF1 78.4 and MOTA to 78.0.", "mention_start": 3, "mention_end": 40, "dataset_mention": "combining the MEVA and AVA100 dataset"}, {"mentioned_in_paper": "780", "context_id": "210", "dataset_context": "By combining the MEVA and AVA100 dataset, we found the joint video dataset can boost the MOT17 evaluation results further to IDF1 78.4 and MOTA to 78.0.", "mention_start": 41, "mention_end": 74, "dataset_mention": " we found the joint video dataset"}, {"mentioned_in_paper": "780", "context_id": "214", "dataset_context": "MEVA [5] and AVA are both curated as action recognition datasets but the content type is different 3.", "mention_start": 37, "mention_end": 64, "dataset_mention": "action recognition datasets"}, {"mentioned_in_paper": "780", "context_id": "215", "dataset_context": "With larger resolution than AVA videos and crowded scenes, videos in the MEVA dataset are more akin to the videos presented in MOT dataset, where the video are mostly from surveillance or car mounted cameras.", "mention_start": 68, "mention_end": 85, "dataset_mention": "the MEVA dataset"}, {"mentioned_in_paper": "780", "context_id": "215", "dataset_context": "With larger resolution than AVA videos and crowded scenes, videos in the MEVA dataset are more akin to the videos presented in MOT dataset, where the video are mostly from surveillance or car mounted cameras.", "mention_start": 126, "mention_end": 138, "dataset_mention": "MOT dataset"}, {"mentioned_in_paper": "780", "context_id": "219", "dataset_context": "Based on on primitive prediction results, the mining dataset has average 103 tracks while the overall average tracks number is 36.7.", "mention_start": 41, "mention_end": 60, "dataset_mention": " the mining dataset"}, {"mentioned_in_paper": "780", "context_id": "222", "dataset_context": "To run this experiment, we also build a AVA 100/200/300 dataset by just random selection.", "mention_start": 37, "mention_end": 63, "dataset_mention": "a AVA 100/200/300 dataset"}, {"mentioned_in_paper": "780", "context_id": "225", "dataset_context": "We also compared Semi-TCL with an alternative approach which uses the cross entropy loss in [42] (CE) for semi supervised learning on the AVA and MOT17 training joint dataset.", "mention_start": 161, "mention_end": 174, "dataset_mention": "joint dataset"}, {"mentioned_in_paper": "781", "context_id": "36", "dataset_context": "To evaluate, we implement our method based on the simple and versatile latent mapper from StyleCLIP and conduct experiments on the challenging face editing task, using the large-scale human face dataset CelebA-HQ [14, 25].", "mention_start": 167, "mention_end": 202, "dataset_mention": "the large-scale human face dataset"}, {"mentioned_in_paper": "781", "context_id": "43", "dataset_context": "We conduct extensive experiments on the CelebA-HQ dataset and find that our qualitative and quantitative results are rather impressive.", "mention_start": 36, "mention_end": 57, "dataset_mention": "the CelebA-HQ dataset"}, {"mentioned_in_paper": "781", "context_id": "140", "dataset_context": "Following StyleCLIP, we use the CelebA-HQ dataset [14, 25], which consists of 30,000 images, 27,176 for train-set and 2,824 for test-set; Style-GAN2 [16] pre-trained on FFHQ [15] is used to generate images; e4e [42] is used to invert images into latent embeddings in the latent space of StyleGAN2.", "mention_start": 27, "mention_end": 49, "dataset_mention": "the CelebA-HQ dataset"}, {"mentioned_in_paper": "781", "context_id": "194", "dataset_context": "One common issue in the image manipulation model is that it is biased toward the dataset the model was trained on.", "mention_start": 63, "mention_end": 88, "dataset_mention": "biased toward the dataset"}, {"mentioned_in_paper": "785", "context_id": "80", "dataset_context": "The FFR dataset included 1000 artifact-free FFR trials per tone (i.e., 1000 FFRs in response to 1000 repetitions of each tone).", "mention_start": 0, "mention_end": 15, "dataset_mention": "The FFR dataset"}, {"mentioned_in_paper": "785", "context_id": "208", "dataset_context": "In designing the simulation scenarios, we have tried to closely mimic our motivating 'brain activities distances between tones' data set.", "mention_start": 120, "mention_end": 136, "dataset_mention": "tones' data set"}, {"mentioned_in_paper": "785", "context_id": "236", "dataset_context": "The results presented here are for the tone distance data set.", "mention_start": 35, "mention_end": 61, "dataset_mention": "the tone distance data set"}, {"mentioned_in_paper": "786", "context_id": "0", "dataset_context": "In this paper, we introduce an expert-annotated dataset for detecting real-world environmental claims made by listed companies.", "mention_start": 27, "mention_end": 55, "dataset_mention": "an expert-annotated dataset"}, {"mentioned_in_paper": "786", "context_id": "24", "dataset_context": "We show that models trained on current claim and pledege detection datasets perform poorly at detecting environmental claims, hence the need of this new dataset.", "mention_start": 31, "mention_end": 75, "dataset_mention": "current claim and pledege detection datasets"}, {"mentioned_in_paper": "786", "context_id": "26", "dataset_context": "We will also release the dataset 4 and models 5 via huggingface Transformers [7].", "mention_start": 13, "mention_end": 32, "dataset_mention": "release the dataset"}, {"mentioned_in_paper": "786", "context_id": "29", "dataset_context": "Resulting datasets and methods assist the research community to investigate such topics at scale using computer assistance.", "mention_start": 0, "mention_end": 18, "dataset_mention": "Resulting datasets"}, {"mentioned_in_paper": "786", "context_id": "31", "dataset_context": "NLP tasks and datasets include climate change topic detection [9], claim verification of climate change related claims [10], detecting media stance on global warming [11], and the analysis of regulatory disclosures [12].", "mention_start": 0, "mention_end": 22, "dataset_mention": "NLP tasks and datasets"}, {"mentioned_in_paper": "786", "context_id": "46", "dataset_context": "While it is not a large-scale dataset, this is the result of a conscious decision to prioritize quality over quantity by employing experts to annotate the data.", "mention_start": 12, "mention_end": 37, "dataset_mention": "not a large-scale dataset"}, {"mentioned_in_paper": "786", "context_id": "70", "dataset_context": "However, we find that the model transfers poorly to predicting environmental claims, hence the need for a dedicated dataset and specialized models for this task.", "mention_start": 103, "mention_end": 123, "dataset_mention": "a dedicated dataset"}, {"mentioned_in_paper": "786", "context_id": "71", "dataset_context": "We also train a RoBERTa base model on a Pledge Detection dataset [5].", "mention_start": 38, "mention_end": 64, "dataset_mention": "a Pledge Detection dataset"}, {"mentioned_in_paper": "786", "context_id": "77", "dataset_context": "Table 2 : Main results: We report micro-and macro-F 1 (\u00b5-F 1 and m-F 1 ) on a cross-validation split (CV), the development set split (dev) and the test set split of the environemntal claims dataset.", "mention_start": 164, "mention_end": 197, "dataset_mention": "the environemntal claims dataset"}, {"mentioned_in_paper": "786", "context_id": "90", "dataset_context": "We then perform a small error analysis which we detail in Appendix D. While we introduce a rather small dataset, Figure 2 shows that model performance as a function of dataset size converges quickly. 11", "mention_start": 89, "mention_end": 111, "dataset_mention": "a rather small dataset"}, {"mentioned_in_paper": "786", "context_id": "106", "dataset_context": "It is a small but high quality dataset annotated by domain experts.", "mention_start": 18, "mention_end": 38, "dataset_mention": "high quality dataset"}, {"mentioned_in_paper": "786", "context_id": "107", "dataset_context": "We described the resulting dataset and the construction process.", "mention_start": 0, "mention_end": 34, "dataset_mention": "We described the resulting dataset"}, {"mentioned_in_paper": "786", "context_id": "112", "dataset_context": "Hence, our effort to publish the dataset.", "mention_start": 20, "mention_end": 40, "dataset_mention": "publish the dataset"}, {"mentioned_in_paper": "786", "context_id": "122", "dataset_context": "To conclude, we envision that the dataset and related models bring large positive impact by encouraging truly environmentally friendly actions and less verbose boasting about environmental credentials.", "mention_start": 12, "mention_end": 41, "dataset_mention": " we envision that the dataset"}, {"mentioned_in_paper": "787", "context_id": "61", "dataset_context": "In Section 5, we investigate computations of truss structures for one-dimensional stress-strain data with noise-free and noisy data sets.", "mention_start": 105, "mention_end": 136, "dataset_mention": "noise-free and noisy data sets"}, {"mentioned_in_paper": "787", "context_id": "81", "dataset_context": "One class of data-driven problems consists of finding the internal state z \u2208 C, which minimizes some distance d to the global material data set D = D (1) \u00d7 ... \u00d7 D (m) .", "mention_start": 114, "mention_end": 143, "dataset_mention": "the global material data set"}, {"mentioned_in_paper": "787", "context_id": "139", "dataset_context": "\u2022 Data sets with different data distributions and data densities can be handled.", "mention_start": 0, "mention_end": 11, "dataset_mention": "\u2022 Data sets"}, {"mentioned_in_paper": "787", "context_id": "192", "dataset_context": "Due to the extended data set with", "mention_start": 7, "mention_end": 28, "dataset_mention": "the extended data set"}, {"mentioned_in_paper": "787", "context_id": "211", "dataset_context": "We begin by considering a sequence (D h ) with h = 1, ..., 5 increasingly fine data sets consisting of 25 \u2022 4 h points on the stress-strain curve with a random distribution of \u03b5 \u2208 [\u22120.025, 0.025].", "mention_start": 58, "mention_end": 88, "dataset_mention": " 5 increasingly fine data sets"}, {"mentioned_in_paper": "787", "context_id": "256", "dataset_context": "For the 1% noise data set, the optimal choice would be approximately \u03c3 = 2 \u22121 and for the 5% noise data set \u03c3 = 2 1 , respectively.", "mention_start": 4, "mention_end": 25, "dataset_mention": "the 1% noise data set"}, {"mentioned_in_paper": "787", "context_id": "256", "dataset_context": "For the 1% noise data set, the optimal choice would be approximately \u03c3 = 2 \u22121 and for the 5% noise data set \u03c3 = 2 1 , respectively.", "mention_start": 85, "mention_end": 107, "dataset_mention": "the 5% noise data set"}, {"mentioned_in_paper": "787", "context_id": "260", "dataset_context": "The computations using the tensor voting solution scheme are done with a learning parameter of \u03c3 = 0.5 and a \u03b2 End = 100 for the 1 % noise data set and with \u03c3 = 1.0 and \u03b2 End = 10 for the 5 % noise data set.", "mention_start": 125, "mention_end": 147, "dataset_mention": "the 1 % noise data set"}, {"mentioned_in_paper": "787", "context_id": "260", "dataset_context": "The computations using the tensor voting solution scheme are done with a learning parameter of \u03c3 = 0.5 and a \u03b2 End = 100 for the 1 % noise data set and with \u03c3 = 1.0 and \u03b2 End = 10 for the 5 % noise data set.", "mention_start": 184, "mention_end": 206, "dataset_mention": "the 5 % noise data set"}, {"mentioned_in_paper": "787", "context_id": "266", "dataset_context": "Considering the max-ent tensor voting solution scheme in Fig. 9a, the performance for the 1% noise data set is remarkably good, especially for small data set sizes.", "mention_start": 85, "mention_end": 107, "dataset_mention": "the 1% noise data set"}, {"mentioned_in_paper": "787", "context_id": "267", "dataset_context": "For the coarsest data set size, the error more or less coincides with the solution of the min-dist tensor voting solution scheme.", "mention_start": 4, "mention_end": 25, "dataset_mention": "the coarsest data set"}, {"mentioned_in_paper": "787", "context_id": "274", "dataset_context": "The smallest data set size of 100 points is certainly well feasible in the context of the truss example.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The smallest data set"}, {"mentioned_in_paper": "787", "context_id": "296", "dataset_context": "The quality of the results of the tensor voting scheme based on the sparsest sampling size can well compete with the one of the classical solution scheme based on the finest data set sampling (see Fig. 11).", "mention_start": 163, "mention_end": 182, "dataset_mention": "the finest data set"}, {"mentioned_in_paper": "787", "context_id": "309", "dataset_context": "The analysis of the remaining local distances suggests a criterion for improving data sets adaptively.", "mention_start": 71, "mention_end": 90, "dataset_mention": "improving data sets"}, {"mentioned_in_paper": "787", "context_id": "316", "dataset_context": "Hence, it is implied that the data has a specific underlying structure, which is used to receive additional information in the case of sparse data sets.", "mention_start": 134, "mention_end": 151, "dataset_mention": "sparse data sets"}, {"mentioned_in_paper": "787", "context_id": "333", "dataset_context": "As seen in this work, different mathematical objects like data sets with extended tangent spaces can be incorporated without facing convergence problems of the solver as in the classical computing paradigm.", "mention_start": 21, "mention_end": 67, "dataset_mention": " different mathematical objects like data sets"}, {"mentioned_in_paper": "787", "context_id": "350", "dataset_context": "The self-consistent data-driven identification approach of Leygue et al. [11] was introduced to create such goaloriented data sets.", "mention_start": 103, "mention_end": 130, "dataset_mention": "such goaloriented data sets"}, {"mentioned_in_paper": "788", "context_id": "5", "dataset_context": "This review presents the most popular public datasets, proposes a categorization of the strategies employed by recent contributions, evaluates the performance of the current state-of-the-art, and discusses the remaining challenges and promising directions for future works.", "mention_start": 0, "mention_end": 53, "dataset_mention": "This review presents the most popular public datasets"}, {"mentioned_in_paper": "788", "context_id": "13", "dataset_context": "Section III analyses the main datasets used in RGB-D segmentation papers.", "mention_start": 0, "mention_end": 38, "dataset_mention": "Section III analyses the main datasets"}, {"mentioned_in_paper": "788", "context_id": "37", "dataset_context": "In this section, we introduce the most popular semantic segmentation RGB-D datasets and analyze the main challenges related to these datasets (and indoor datasets in general).", "mention_start": 29, "mention_end": 83, "dataset_mention": "the most popular semantic segmentation RGB-D datasets"}, {"mentioned_in_paper": "788", "context_id": "37", "dataset_context": "In this section, we introduce the most popular semantic segmentation RGB-D datasets and analyze the main challenges related to these datasets (and indoor datasets in general).", "mention_start": 126, "mention_end": 162, "dataset_mention": "these datasets (and indoor datasets"}, {"mentioned_in_paper": "788", "context_id": "56", "dataset_context": "Matterport3D [14] : Similar to Stanford 2D-3D-S, this dataset is a recent large dataset composed of 194 400 The main problem to mention is the important unbalanced distribution of classes in indoor datasets.", "mention_start": 64, "mention_end": 87, "dataset_mention": "a recent large dataset"}, {"mentioned_in_paper": "788", "context_id": "56", "dataset_context": "Matterport3D [14] : Similar to Stanford 2D-3D-S, this dataset is a recent large dataset composed of 194 400 The main problem to mention is the important unbalanced distribution of classes in indoor datasets.", "mention_start": 190, "mention_end": 206, "dataset_mention": "indoor datasets"}, {"mentioned_in_paper": "788", "context_id": "58", "dataset_context": "'Wall' or 'Floor') cover almost the whole dataset while labels have very few samples [7].", "mention_start": 10, "mention_end": 49, "dataset_mention": "'Floor') cover almost the whole dataset"}, {"mentioned_in_paper": "788", "context_id": "61", "dataset_context": "Compared to the current depth sensor's performance, the depth maps collected by less recent datasets (NYUv2 or SUN-RGBD) are not as accurate.", "mention_start": 79, "mention_end": 100, "dataset_mention": "less recent datasets"}, {"mentioned_in_paper": "788", "context_id": "104", "dataset_context": "We reviewed the most popular RGBD datasets and discussed their main challenges.", "mention_start": 0, "mention_end": 42, "dataset_mention": "We reviewed the most popular RGBD datasets"}, {"mentioned_in_paper": "788", "context_id": "107", "dataset_context": "Finally, during this review, we observe that many recent state-of-the-art models still focus on smaller, older datasets of lower resolution.", "mention_start": 104, "mention_end": 119, "dataset_mention": " older datasets"}, {"mentioned_in_paper": "788", "context_id": "108", "dataset_context": "We believe that future works must exploit the advantages of recent large-scale datasets in order to achieve better results by a large margin.", "mention_start": 60, "mention_end": 87, "dataset_mention": "recent large-scale datasets"}, {"mentioned_in_paper": "789", "context_id": "117", "dataset_context": "For visual recognition datasets, we adopt the following methods for preprocessing:", "mention_start": 4, "mention_end": 31, "dataset_mention": "visual recognition datasets"}, {"mentioned_in_paper": "789", "context_id": "150", "dataset_context": "The reference dataset, CiFAR-10, is denoted with subscript , e.g.,   ,   ,  , , and  , .", "mention_start": 0, "mention_end": 21, "dataset_mention": "The reference dataset"}, {"mentioned_in_paper": "790", "context_id": "5", "dataset_context": "Our BRNet is simple but effective, which significantly outperforms the state-of-the-art methods on two large-scale point cloud datasets, ScanNet V2 (+7.5% in terms of mAP@0.50)", "mention_start": 98, "mention_end": 135, "dataset_mention": "two large-scale point cloud datasets"}, {"mentioned_in_paper": "790", "context_id": "135", "dataset_context": "We evaluate our method on two large-scale indoor scene datasets, i.e.", "mention_start": 26, "mention_end": 63, "dataset_mention": "two large-scale indoor scene datasets"}, {"mentioned_in_paper": "790", "context_id": "140", "dataset_context": "ScanNet V2 is a 3D mesh dataset about 1, 500 3D reconstructed indoor scenes.", "mention_start": 14, "mention_end": 31, "dataset_mention": "a 3D mesh dataset"}, {"mentioned_in_paper": "790", "context_id": "142", "dataset_context": "The scans in the ScanNet V2 dataset are more complete with more objects than those in the SUN RGB-D dataset.", "mention_start": 13, "mention_end": 35, "dataset_mention": "the ScanNet V2 dataset"}, {"mentioned_in_paper": "790", "context_id": "142", "dataset_context": "The scans in the ScanNet V2 dataset are more complete with more objects than those in the SUN RGB-D dataset.", "mention_start": 86, "mention_end": 107, "dataset_mention": "the SUN RGB-D dataset"}, {"mentioned_in_paper": "790", "context_id": "145", "dataset_context": "The input of our method is a point cloud randomly sub-sampled from the raw data of each dataset, i.e., 20, 000 points from a point cloud in the SUN RGB-D dataset, and 40, 000 points from a 3D mesh in the ScanNet V2 dataset.", "mention_start": 139, "mention_end": 161, "dataset_mention": "the SUN RGB-D dataset"}, {"mentioned_in_paper": "790", "context_id": "145", "dataset_context": "The input of our method is a point cloud randomly sub-sampled from the raw data of each dataset, i.e., 20, 000 points from a point cloud in the SUN RGB-D dataset, and 40, 000 points from a 3D mesh in the ScanNet V2 dataset.", "mention_start": 199, "mention_end": 222, "dataset_mention": "the ScanNet V2 dataset"}, {"mentioned_in_paper": "790", "context_id": "150", "dataset_context": "The base learning rates are 0.001 for the SUN RGB-D [31] dataset and 0.005 for the ScanNet V2 [4] dataset.", "mention_start": 38, "mention_end": 64, "dataset_mention": "the SUN RGB-D [31] dataset"}, {"mentioned_in_paper": "790", "context_id": "150", "dataset_context": "The base learning rates are 0.001 for the SUN RGB-D [31] dataset and 0.005 for the ScanNet V2 [4] dataset.", "mention_start": 79, "mention_end": 105, "dataset_mention": "the ScanNet V2 [4] dataset"}, {"mentioned_in_paper": "790", "context_id": "153", "dataset_context": "Based on PyTorch platform equipped with one NVIDIA GeForce RTX 2080 Ti GPU card, it takes around 4 hours to train the model on the ScanNet V2 dataset, while it takes around 12 hours on the SUN RGB-D dataset.", "mention_start": 126, "mention_end": 149, "dataset_mention": "the ScanNet V2 dataset"}, {"mentioned_in_paper": "790", "context_id": "153", "dataset_context": "Based on PyTorch platform equipped with one NVIDIA GeForce RTX 2080 Ti GPU card, it takes around 4 hours to train the model on the ScanNet V2 dataset, while it takes around 12 hours on the SUN RGB-D dataset.", "mention_start": 184, "mention_end": 206, "dataset_mention": "the SUN RGB-D dataset"}, {"mentioned_in_paper": "790", "context_id": "161", "dataset_context": "*Note for fair comparison, we report the results of H3DNet on the ScanNet V2 dataset under both 1 and 4 PointNet++ backbones (BB) settings.", "mention_start": 61, "mention_end": 84, "dataset_mention": "the ScanNet V2 dataset"}, {"mentioned_in_paper": "790", "context_id": "162", "dataset_context": "While we only report the result of H3DNet with 4 PointNet++ backbones (BB) on the SUN RGB-D dataset, as the work [41] only reports the result under this setting.", "mention_start": 78, "mention_end": 99, "dataset_mention": "the SUN RGB-D dataset"}, {"mentioned_in_paper": "790", "context_id": "173", "dataset_context": "Notably, ML-CVNet [36] works well on the ScanNet dataset but achieves relatively poor performance on the SUN RGB-D dataset, while HGNet [2] works well on the SUN RGB-D dataset but achieves poor result on the ScanNet dataset, especially in terms of the mAP@0.50", "mention_start": 36, "mention_end": 56, "dataset_mention": "the ScanNet dataset"}, {"mentioned_in_paper": "790", "context_id": "173", "dataset_context": "Notably, ML-CVNet [36] works well on the ScanNet dataset but achieves relatively poor performance on the SUN RGB-D dataset, while HGNet [2] works well on the SUN RGB-D dataset but achieves poor result on the ScanNet dataset, especially in terms of the mAP@0.50", "mention_start": 100, "mention_end": 122, "dataset_mention": "the SUN RGB-D dataset"}, {"mentioned_in_paper": "790", "context_id": "173", "dataset_context": "Notably, ML-CVNet [36] works well on the ScanNet dataset but achieves relatively poor performance on the SUN RGB-D dataset, while HGNet [2] works well on the SUN RGB-D dataset but achieves poor result on the ScanNet dataset, especially in terms of the mAP@0.50", "mention_start": 153, "mention_end": 175, "dataset_mention": "the SUN RGB-D dataset"}, {"mentioned_in_paper": "790", "context_id": "173", "dataset_context": "Notably, ML-CVNet [36] works well on the ScanNet dataset but achieves relatively poor performance on the SUN RGB-D dataset, while HGNet [2] works well on the SUN RGB-D dataset but achieves poor result on the ScanNet dataset, especially in terms of the mAP@0.50", "mention_start": 203, "mention_end": 223, "dataset_mention": "the ScanNet dataset"}, {"mentioned_in_paper": "790", "context_id": "177", "dataset_context": "Moreover, H3DNet [41] ensembles 4 PointNet++ [23] backbones to achieve the reported result on the SUN RGB-D dataset, while our model only needs one backbone as the base feature extractor.", "mention_start": 93, "mention_end": 115, "dataset_mention": "the SUN RGB-D dataset"}, {"mentioned_in_paper": "790", "context_id": "179", "dataset_context": "As shown in Table 2, our method performs the best on 12 classes among 18 total classes from the ScanNet dataset in terms of mAP@0.50.", "mention_start": 91, "mention_end": 111, "dataset_mention": "the ScanNet dataset"}, {"mentioned_in_paper": "790", "context_id": "197", "dataset_context": "gains of this alternative method over VoteNet on the SUN RGB-D dataset are positively related to size variances.", "mention_start": 49, "mention_end": 70, "dataset_mention": "the SUN RGB-D dataset"}, {"mentioned_in_paper": "790", "context_id": "244", "dataset_context": "compared with H3DNet [41] using 4 Point-Net++ backbones and doubled input point clouds (i.e., 20, 000 points by our BRNet , and 40, 000 points by H3DNet) on the ScanNetV2 dataset.", "mention_start": 156, "mention_end": 178, "dataset_mention": "the ScanNetV2 dataset"}, {"mentioned_in_paper": "790", "context_id": "247", "dataset_context": "We show the per-category results on ScanNet V2 dataset with 3D IoU threshold 0.25 in Table S3, and the per-category results on SUN RGB-D with both 3D IoU thresholds 0.25 and 0.50 in Table S4 and S5.", "mention_start": 36, "mention_end": 54, "dataset_mention": "ScanNet V2 dataset"}, {"mentioned_in_paper": "790", "context_id": "249", "dataset_context": "3D object detection results on ScanNetV2 dataset with multiple IoU thresholds.", "mention_start": 31, "mention_end": 48, "dataset_mention": "ScanNetV2 dataset"}, {"mentioned_in_paper": "790", "context_id": "258", "dataset_context": "For objects in the SUN RGB-D dataset, our approach can gain 7.9%, 10.9%, 6.7%, 7.6%, 6.3% increase on Bathtub, Bed, Dresser, Nightstand and Sofa compared with H3DNet [41].", "mention_start": 15, "mention_end": 36, "dataset_mention": "the SUN RGB-D dataset"}, {"mentioned_in_paper": "790", "context_id": "260", "dataset_context": "We provide more qualitative comparisons between our method and the top-performing reference methods, such as VoteNet [20], MLCVNet [36] and H3DNet [41], on the ScanNet V2 and SUN RGB-D datasets, as shown in Fig. S2 and Fig. S3, respectively.", "mention_start": 155, "mention_end": 193, "dataset_mention": "the ScanNet V2 and SUN RGB-D datasets"}, {"mentioned_in_paper": "790", "context_id": "277", "dataset_context": "After revisiting seed points, we get a 32 di-Table S3. 3D object detection results on ScanNet V2 validation dataset.", "mention_start": 85, "mention_end": 115, "dataset_mention": "ScanNet V2 validation dataset"}, {"mentioned_in_paper": "790", "context_id": "280", "dataset_context": "More qualitative results on SUN RGB-D dataset [31].", "mention_start": 28, "mention_end": 45, "dataset_mention": "SUN RGB-D dataset"}, {"mentioned_in_paper": "791", "context_id": "45", "dataset_context": "The experiments are conducted on the MS COCO dataset Lin et al. (2014) and the models are trained for 12 epochs, following the default 1\u00d7 setting Liu et al. (2021).", "mention_start": 33, "mention_end": 52, "dataset_mention": "the MS COCO dataset"}, {"mentioned_in_paper": "791", "context_id": "56", "dataset_context": "We employ simple max pooling layers in the last pooling layer with kernel size 2 and stride 2. As shown in Table 3, the transformed model improves the baseline model by 0.8 mIoU on the ADE20K dataset and 1.2 mIoU on the Cityscapes dataset.", "mention_start": 180, "mention_end": 199, "dataset_mention": "the ADE20K dataset"}, {"mentioned_in_paper": "791", "context_id": "56", "dataset_context": "We employ simple max pooling layers in the last pooling layer with kernel size 2 and stride 2. As shown in Table 3, the transformed model improves the baseline model by 0.8 mIoU on the ADE20K dataset and 1.2 mIoU on the Cityscapes dataset.", "mention_start": 215, "mention_end": 238, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "792", "context_id": "45", "dataset_context": "With the advent for large volumes of easily shareable labeled-datasets, learning-based approaches have gained popularity recently.", "mention_start": 37, "mention_end": 70, "dataset_mention": "easily shareable labeled-datasets"}, {"mentioned_in_paper": "792", "context_id": "145", "dataset_context": "We also tested our approach on two real world data sets.", "mention_start": 31, "mention_end": 55, "dataset_mention": "two real world data sets"}, {"mentioned_in_paper": "792", "context_id": "148", "dataset_context": "We use scans from the Oakland dataset for this experiment [24].", "mention_start": 18, "mention_end": 37, "dataset_mention": "the Oakland dataset"}, {"mentioned_in_paper": "793", "context_id": "98", "dataset_context": "This has generally remained unobtainable for semantic queries because the use cases have either not been document-based or were unable to be processed by analytical algorithms that don't use rule-logic within semantic data sets.", "mention_start": 209, "mention_end": 227, "dataset_mention": "semantic data sets"}, {"mentioned_in_paper": "793", "context_id": "99", "dataset_context": "The internet is an HTML representation of multiple knowledge domains overlayed upon a set of segmented documents with unique URLs, while a document-oriented semantic dataset is an RDF/OWL representation of multiple knowledge domains overlayed upon a set of segmented named graphs with unique URIs.", "mention_start": 136, "mention_end": 173, "dataset_mention": "a document-oriented semantic dataset"}, {"mentioned_in_paper": "793", "context_id": "101", "dataset_context": "Many web-oriented popularity, similarity, and clustering analytics appear to be well suited for semantic datasets.", "mention_start": 95, "mention_end": 113, "dataset_mention": "semantic datasets"}, {"mentioned_in_paper": "795", "context_id": "2", "dataset_context": "We test our model under the metric of the top-1 accuracy on the CIFAR-10 dataset.", "mention_start": 60, "mention_end": 80, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "796", "context_id": "92", "dataset_context": "An ultrasound breast image dataset (UDIAT) is analyzed in this study [9].", "mention_start": 0, "mention_end": 34, "dataset_mention": "An ultrasound breast image dataset"}, {"mentioned_in_paper": "797", "context_id": "211", "dataset_context": "Table II shows that SABNA is able to process all test datasets and it outperforms both BFS and the A-star strategy of URLearning, irrespective of the input dataset.", "mention_start": 37, "mention_end": 62, "dataset_mention": "process all test datasets"}, {"mentioned_in_paper": "797", "context_id": "220", "dataset_context": "For example, because the URLearning heuristic is not well tuned to the data in \"Water\" and \"Soybean\" datasets, the open and closed lists of A-star explode and the method runs out of memory.", "mention_start": 78, "mention_end": 109, "dataset_mention": "\"Water\" and \"Soybean\" datasets"}, {"mentioned_in_paper": "797", "context_id": "226", "dataset_context": "For the \"Mushroom\" dataset SABNA explores more nodes, yet it remains faster.", "mention_start": 4, "mention_end": 26, "dataset_mention": "the \"Mushroom\" dataset"}, {"mentioned_in_paper": "797", "context_id": "229", "dataset_context": "Because the \"Mushroom\" dataset is relatively small the overhead becomes the major component of the overall runtime.", "mention_start": 8, "mention_end": 30, "dataset_mention": "the \"Mushroom\" dataset"}, {"mentioned_in_paper": "798", "context_id": "107", "dataset_context": "Regarding the predominance and characteristics of the data sets most used in the investigated studies, we highlight the customized initiatives designed mainly for a research need and the PlantVillage [34] data set.", "mention_start": 178, "mention_end": 213, "dataset_mention": "and the PlantVillage [34] data set"}, {"mentioned_in_paper": "798", "context_id": "109", "dataset_context": "The customized Data Sets are present in 66 studies (S1, S2, S3, S6, S7, S8, S9, S12, S15, S17, S18, S20, S22, S24, S25, S28, S32, S33, S34, S36 , S37, S38, S40, S43, S44, S45, S46, S47, S49, S50, S52, S58, S64, S68, S69, S70, S71, S73, S74, S75, S77, S79, S80, S82, S84 , S87, S88, S89, S90, S91, S92, S95, S96, S97, S98, S100, S102, S103, S104, S107, S112, S113, S115, S116, S119, S121) and PlantVillage is used in your full or partial version in 45 studies (S3, S4, S9, S11, S13, S14, S18, S21, S23, S26, S27, S28, S29, S35, S41, S48, S51, S53, S55, S56, S57, S60, S61, S62, S63, S65, S66, S67, S68, S69, S72, S73, S76, S78, S81, S85, S86, S93, S99, S101, S105, S108, S110, S114, S117).", "mention_start": 0, "mention_end": 24, "dataset_mention": "The customized Data Sets"}, {"mentioned_in_paper": "798", "context_id": "113", "dataset_context": "In particular, this is a predominant feature in the PlantVillage data set.", "mention_start": 47, "mention_end": 73, "dataset_mention": "the PlantVillage data set"}, {"mentioned_in_paper": "798", "context_id": "125", "dataset_context": "Using the data summarized in B, we analyzed 54 datasets with 142 types of crops, associating each disease-causing pathogen to its large group, namely: Virus, Bacteria, Fungi, Algae, Plague, Nematodes and, Abiotics.", "mention_start": 31, "mention_end": 55, "dataset_mention": " we analyzed 54 datasets"}, {"mentioned_in_paper": "798", "context_id": "158", "dataset_context": "The answer to question SQ2 from Section 4.2, groups the data sets with the highest recurrence in the studies, and demonstrates that the approaches try to reduce the limitations by creating customized sets that mix images from controlled environments with images captured under real growing conditions.", "mention_start": 44, "mention_end": 65, "dataset_mention": " groups the data sets"}, {"mentioned_in_paper": "798", "context_id": "161", "dataset_context": "According to [43], who proposed the approach of the S28 study, when applying a customized CNN architecture, they obtained high precision with training and testing using the PlantVillage data set.", "mention_start": 168, "mention_end": 194, "dataset_mention": "the PlantVillage data set"}, {"mentioned_in_paper": "799", "context_id": "54", "dataset_context": "In this study, we create and release two corpora comparing Amazon Alexa, Google Assistant, and Apple Siri: (1) a large corpus of user reviews to compare user perceptions of both personification and genderisation of the assistants, and (2) a corpus of system responses to questions from the Per-sonaChat dataset (Zhang et al., 2018). 5", "mention_start": 285, "mention_end": 310, "dataset_mention": "the Per-sonaChat dataset"}, {"mentioned_in_paper": "799", "context_id": "104", "dataset_context": "To elicit these responses, we extracted 300 unique questions selected at random from dialogues from the Persona-Chat dataset (Zhang et al., 2018), which contains crowdsourced human conversations about an assigned 'persona', i.e. personal characteristics and preferences.", "mention_start": 99, "mention_end": 124, "dataset_mention": "the Persona-Chat dataset"}, {"mentioned_in_paper": "801", "context_id": "19", "dataset_context": "An exemplar is the publicly-available Yelp dataset where 5.2M reviews are aggregated into 1.3M user sessions [53].", "mention_start": 15, "mention_end": 50, "dataset_mention": "the publicly-available Yelp dataset"}, {"mentioned_in_paper": "801", "context_id": "314", "dataset_context": "For each Si, we aggregate all the data held by vertices of Si to vi; that is every vertex in Si (except vi itself) transfers its dataset to vi.", "mention_start": 92, "mention_end": 136, "dataset_mention": "Si (except vi itself) transfers its dataset"}, {"mentioned_in_paper": "801", "context_id": "366", "dataset_context": "Is GRASP faster than aggregation based on repartitioning and LOOM on TPC-H and real datasets?", "mention_start": 69, "mention_end": 92, "dataset_mention": "TPC-H and real datasets"}, {"mentioned_in_paper": "801", "context_id": "408", "dataset_context": "The fourth dataset is the Amazon review dataset [19].", "mention_start": 22, "mention_end": 47, "dataset_mention": "the Amazon review dataset"}, {"mentioned_in_paper": "801", "context_id": "409", "dataset_context": "The review dataset has more than 82 million reviews from about 21 million users.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The review dataset"}, {"mentioned_in_paper": "801", "context_id": "415", "dataset_context": "The fifth dataset is the Yelp review dataset [53].", "mention_start": 0, "mention_end": 17, "dataset_mention": "The fifth dataset"}, {"mentioned_in_paper": "801", "context_id": "415", "dataset_context": "The fifth dataset is the Yelp review dataset [53].", "mention_start": 21, "mention_end": 44, "dataset_mention": "the Yelp review dataset"}, {"mentioned_in_paper": "801", "context_id": "416", "dataset_context": "The review dataset has more than 5 million reviews from about 1.3 million users.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The review dataset"}, {"mentioned_in_paper": "801", "context_id": "417", "dataset_context": "The Yelp dataset has similar attributes as the Amazon dataset and we use a similar query to calculate the average stars a customer gives.", "mention_start": 0, "mention_end": 16, "dataset_mention": "The Yelp dataset"}, {"mentioned_in_paper": "801", "context_id": "417", "dataset_context": "The Yelp dataset has similar attributes as the Amazon dataset and we use a similar query to calculate the average stars a customer gives.", "mention_start": 43, "mention_end": 61, "dataset_mention": "the Amazon dataset"}, {"mentioned_in_paper": "801", "context_id": "493", "dataset_context": "In this experiment we force GRASP to use a modified bandwidth matrix while running the aggregation query on the     MODIS dataset.", "mention_start": 104, "mention_end": 129, "dataset_mention": "the     MODIS dataset"}, {"mentioned_in_paper": "801", "context_id": "527", "dataset_context": "These experiments evaluate the performance of the GRASP plans with the TPC-H workload and three real datasets.", "mention_start": 67, "mention_end": 109, "dataset_mention": "the TPC-H workload and three real datasets"}, {"mentioned_in_paper": "801", "context_id": "532", "dataset_context": "GRASP is 2\u00d7 faster than LOOM and 3.5\u00d7 faster than Preagg+Repart in the MODIS dataset.", "mention_start": 67, "mention_end": 84, "dataset_mention": "the MODIS dataset"}, {"mentioned_in_paper": "801", "context_id": "533", "dataset_context": "Network utilization: Figure 18 shows the network utilization plot for the MODIS dataset.", "mention_start": 69, "mention_end": 87, "dataset_mention": "the MODIS dataset"}, {"mentioned_in_paper": "801", "context_id": "555", "dataset_context": "We also evaluate the accuracy of the minhash estimation with the MODIS dataset.", "mention_start": 61, "mention_end": 78, "dataset_mention": "the MODIS dataset"}, {"mentioned_in_paper": "801", "context_id": "559", "dataset_context": "This section evaluates GRASP on the MODIS dataset on Amazon EC2.", "mention_start": 32, "mention_end": 49, "dataset_mention": "the MODIS dataset"}, {"mentioned_in_paper": "801", "context_id": "609", "dataset_context": "Distribution-aware algorithms have also been proposed to deal with skewed datasets.", "mention_start": 67, "mention_end": 82, "dataset_mention": "skewed datasets"}, {"mentioned_in_paper": "801", "context_id": "612", "dataset_context": "Xu et al. [50] addressed skew in parallel joins by first scanning the dataset to identify the skewed values, then keeping the skewed rows locally and duplicating the matching rows.", "mention_start": 51, "mention_end": 77, "dataset_mention": "first scanning the dataset"}, {"mentioned_in_paper": "802", "context_id": "47", "dataset_context": "However, for any of the aforementioned tasks, it is challenging to obtain a large-scale dataset with the ground-truth label or formulate the correct measurement model that represents the wide range of possible sensor characteristics.", "mention_start": 66, "mention_end": 95, "dataset_mention": "obtain a large-scale dataset"}, {"mentioned_in_paper": "802", "context_id": "121", "dataset_context": "Figure 3 evaluates the SSIM of the reconstructed intensity image compared against the ground truth intensity image with the office zigzag scene in the IJRR dataset [34].", "mention_start": 147, "mention_end": 163, "dataset_mention": "the IJRR dataset"}, {"mentioned_in_paper": "802", "context_id": "137", "dataset_context": "We use three available real-world datasets that are widely used for eventbased image reconstruction, namely IJRR [34], HQF [53], and Stereo DAVIS dataset [65] for quantitative comparison.", "mention_start": 128, "mention_end": 153, "dataset_mention": " and Stereo DAVIS dataset"}, {"mentioned_in_paper": "802", "context_id": "138", "dataset_context": "For the stereo DAVIS dataset, we only use the measurements from a single event camera.", "mention_start": 4, "mention_end": 28, "dataset_mention": "the stereo DAVIS dataset"}, {"mentioned_in_paper": "802", "context_id": "139", "dataset_context": "Unless otherwise noted, we use four sequences (dynamic 6dof, office spiral, office zigzag, hdr boxes) from the IJRR dataset, three sequences (reflective materials, high texture plants, still life) from HQF dataset and two sequences (monitor, reader) from Stereo DAVIS dataset.", "mention_start": 106, "mention_end": 123, "dataset_mention": "the IJRR dataset"}, {"mentioned_in_paper": "802", "context_id": "139", "dataset_context": "Unless otherwise noted, we use four sequences (dynamic 6dof, office spiral, office zigzag, hdr boxes) from the IJRR dataset, three sequences (reflective materials, high texture plants, still life) from HQF dataset and two sequences (monitor, reader) from Stereo DAVIS dataset.", "mention_start": 201, "mention_end": 213, "dataset_mention": "HQF dataset"}, {"mentioned_in_paper": "802", "context_id": "139", "dataset_context": "Unless otherwise noted, we use four sequences (dynamic 6dof, office spiral, office zigzag, hdr boxes) from the IJRR dataset, three sequences (reflective materials, high texture plants, still life) from HQF dataset and two sequences (monitor, reader) from Stereo DAVIS dataset.", "mention_start": 254, "mention_end": 275, "dataset_mention": "Stereo DAVIS dataset"}, {"mentioned_in_paper": "802", "context_id": "148", "dataset_context": "We further validate intensity image reconstruction results on the CED Dataset [7] with a different resolution (346\u00d7240) in the supplementary material.", "mention_start": 62, "mention_end": 77, "dataset_mention": "the CED Dataset"}, {"mentioned_in_paper": "802", "context_id": "149", "dataset_context": " 1. Quantitative comparison of image reconstruction on scenes from the IJRR [34], HQF [53] and Stereo DAVIS [65] dataset.", "mention_start": 81, "mention_end": 120, "dataset_mention": " HQF [53] and Stereo DAVIS [65] dataset"}, {"mentioned_in_paper": "803", "context_id": "152", "dataset_context": "All models are trained with ILSVRC-2012 ImageNet dataset (Deng et al., 2009) with input image size of 224\u00d7224.", "mention_start": 28, "mention_end": 56, "dataset_mention": "ILSVRC-2012 ImageNet dataset"}, {"mentioned_in_paper": "804", "context_id": "7", "dataset_context": "Within computer vision, these models trained on ever larger datasets, such as Instagram-1B (Mahajan et al., 2018) or JFT-3B (Dosovitskiy et al., 2021), have been shown to successfully distinguish between semantic categories at high accuracies.", "mention_start": 47, "mention_end": 68, "dataset_mention": "ever larger datasets"}, {"mentioned_in_paper": "804", "context_id": "17", "dataset_context": "Yet, this paradigm is gaining importance as many recent advancements have been made possible due to extremely large proprietary datasets that are kept private.", "mention_start": 99, "mention_end": 136, "dataset_mention": "extremely large proprietary datasets"}, {"mentioned_in_paper": "804", "context_id": "21", "dataset_context": "yield datasets of synthetic images that maximally activate neurons in the final layer of teacher.", "mention_start": 0, "mention_end": 14, "dataset_mention": "yield datasets"}, {"mentioned_in_paper": "804", "context_id": "22", "dataset_context": "Related to this, there are works which conduct \"dataset\" distillation, where the objective is to distill large-scale datasets into much smaller ones, such that models trained on it reach similar levels of performance as on the original data.", "mention_start": 46, "mention_end": 55, "dataset_mention": "\"dataset"}, {"mentioned_in_paper": "804", "context_id": "22", "dataset_context": "Related to this, there are works which conduct \"dataset\" distillation, where the objective is to distill large-scale datasets into much smaller ones, such that models trained on it reach similar levels of performance as on the original data.", "mention_start": 96, "mention_end": 125, "dataset_mention": "distill large-scale datasets"}, {"mentioned_in_paper": "804", "context_id": "24", "dataset_context": "In contrast to GAN-and inversion-based methods, as well as to dataset distillation, our approach does not require the knowledge of the weights and architecture of the teacher model, and instead works with black-box \"API\"-style teacher models and much smaller \"datasets\" of just a single datum plus augmentations.", "mention_start": 204, "mention_end": 268, "dataset_mention": "black-box \"API\"-style teacher models and much smaller \"datasets"}, {"mentioned_in_paper": "804", "context_id": "32", "dataset_context": "i. Dataset generation.", "mention_start": 0, "mention_end": 10, "dataset_mention": "i. Dataset"}, {"mentioned_in_paper": "804", "context_id": "33", "dataset_context": "In (Asano et al., 2020), a single \"source\" image is augmented many times to generate a static dataset of fixed size.", "mention_start": 84, "mention_end": 101, "dataset_mention": "a static dataset"}, {"mentioned_in_paper": "804", "context_id": "46", "dataset_context": "However, in contrast to (Beyer et al., 2022), we neither have access to TPUs nor can train 10K epochs on ImageNet-sized datasets.", "mention_start": 104, "mention_end": 128, "dataset_mention": "ImageNet-sized datasets"}, {"mentioned_in_paper": "804", "context_id": "61", "dataset_context": "We find that while distillation using the source dataset always works best (95.26% and 78.06%) on CIFAR-10/100, using a single image can yield models which almost reach this upper bar (94.1% and 73.8%).", "mention_start": 38, "mention_end": 56, "dataset_mention": "the source dataset"}, {"mentioned_in_paper": "804", "context_id": "62", "dataset_context": "Moreover, we find that one image distillation even outperforms using 10K images of CIFAR-10 when teaching CIFAR-100 classes even though  these two datasets are remarkably similar.", "mention_start": 127, "mention_end": 155, "dataset_mention": "though  these two datasets"}, {"mentioned_in_paper": "804", "context_id": "81", "dataset_context": "In Table 2, we find that there is something unique about using a single image, as our method outperforms several synthetic datasets, such as FractalDB Kataoka et al. (2020), randomly initialized StyleGAN Baradad et al. (2021), as well as the GAN-based approach of (Micaelli & Storkey, 2019).", "mention_start": 81, "mention_end": 131, "dataset_mention": "our method outperforms several synthetic datasets"}, {"mentioned_in_paper": "804", "context_id": "82", "dataset_context": "This is despite the fact that these synthetic datasets contain \u223c 50K images.", "mention_start": 8, "mention_end": 54, "dataset_mention": "despite the fact that these synthetic datasets"}, {"mentioned_in_paper": "804", "context_id": "83", "dataset_context": "We provide insights on the characteristics of this 1-image dataset in Section 4.4.", "mention_start": 46, "mention_end": 66, "dataset_mention": "this 1-image dataset"}, {"mentioned_in_paper": "804", "context_id": "84", "dataset_context": "Next, using the insights gained in this section, we scale the experiments towards other network architectures, dataset domains and dataset sizes.", "mention_start": 110, "mention_end": 138, "dataset_mention": " dataset domains and dataset"}, {"mentioned_in_paper": "804", "context_id": "94", "dataset_context": "In Table 5, we compare the results against the performance of the teacher model and distilling directly using source dataset.", "mention_start": 109, "mention_end": 124, "dataset_mention": "source dataset"}, {"mentioned_in_paper": "804", "context_id": "96", "dataset_context": "In particular, we see significant improvement in distillation performance when a single audio has a wide variety of sounds to boost accuracy on challenging Voxforge dataset from 72.8% to 78.4%.", "mention_start": 155, "mention_end": 172, "dataset_mention": "Voxforge dataset"}, {"mentioned_in_paper": "804", "context_id": "106", "dataset_context": "From this section on, we scale our experiments to larger models utilizing 224\u00d7224-sized images, and evaluate these on various vision datasets.", "mention_start": 117, "mention_end": 141, "dataset_mention": "various vision datasets"}, {"mentioned_in_paper": "804", "context_id": "111", "dataset_context": "Nevertheless, we find a surprisingly high accuracy of 69% on ImageNet's validation set, even though this dataset comprises 1000 classes, and the student only having seen heavily augmented crops of a single image.", "mention_start": 92, "mention_end": 112, "dataset_mention": "though this dataset"}, {"mentioned_in_paper": "804", "context_id": "159", "dataset_context": "For this we use the common UCF-101 (Soomro et al., 2012) dataset, more specifically the first split.", "mention_start": 50, "mention_end": 64, "dataset_mention": " 2012) dataset"}, {"mentioned_in_paper": "804", "context_id": "173", "dataset_context": "We find that while for CIFAR-10 the choice of the distillation method does not impact the performance, on the more difficult ImageNet dataset, there is a small difference of around \u22120.4% and \u22120.5% for L1 and L2 losses as compared to the KD loss at epoch 30.", "mention_start": 105, "mention_end": 141, "dataset_mention": "the more difficult ImageNet dataset"}, {"mentioned_in_paper": "804", "context_id": "183", "dataset_context": "In Table 12, we report the performance of five independent runs on CIFAR-10 dataset to further highlight the robustness of our experimental results.", "mention_start": 66, "mention_end": 83, "dataset_mention": "CIFAR-10 dataset"}, {"mentioned_in_paper": "804", "context_id": "186", "dataset_context": "We compare the performance of five independent runs on CIFAR-10 dataset.", "mention_start": 55, "mention_end": 71, "dataset_mention": "CIFAR-10 dataset"}, {"mentioned_in_paper": "804", "context_id": "196", "dataset_context": "In Fig. 15, we visualize similarity between different layers of the models trained in a general supervised manner and with distillation using patches of 1-image for CIFAR-10 and ImageNet datasets.", "mention_start": 164, "mention_end": 195, "dataset_mention": "CIFAR-10 and ImageNet datasets"}, {"mentioned_in_paper": "804", "context_id": "201", "dataset_context": "To make the comparison of our training data with the visual inputs of toddlers more concrete, we compare low-level GIST (Oliva & Torralba, 2001) features of our 1-image dataset against those computed from visual inputs of a toddler in (Bambach et al., 2018).", "mention_start": 156, "mention_end": 176, "dataset_mention": "our 1-image dataset"}, {"mentioned_in_paper": "804", "context_id": "203", "dataset_context": "While the GIST distances for ImageNet are very different with a mean around 0.75 (Bambach et al., 2018), we find that our single image dataset's distribution in Fig. 16a closely resembles that of the visual inputs of a toddler.", "mention_start": 104, "mention_end": 142, "dataset_mention": " we find that our single image dataset"}, {"mentioned_in_paper": "804", "context_id": "214", "dataset_context": "CIFAR-100 datasets.", "mention_start": 0, "mention_end": 18, "dataset_mention": "CIFAR-100 datasets"}, {"mentioned_in_paper": "804", "context_id": "221", "dataset_context": "In this section, we compare the performances on downstream tasks of our teacher network (ResNet-18), our single-image distilled network (ResNet-50) and another ResNet-50, purely trained in a selfsupervised manner on our augmented single-image dataset.", "mention_start": 215, "mention_end": 250, "dataset_mention": "our augmented single-image dataset"}, {"mentioned_in_paper": "805", "context_id": "65", "dataset_context": "We present a total of all the classes that can be identified in the coco dataset, including people, bicycles, cars, motorbikes, airplanes, buses, trains, trucks, boats, traffic, lights, fires, hydrants, and stop lights.", "mention_start": 64, "mention_end": 80, "dataset_mention": "the coco dataset"}, {"mentioned_in_paper": "805", "context_id": "102", "dataset_context": "Fig. 12 and Fig. 13 represents the ratio and number of dropped data after clustering using the k-means method on the point cloud dataset.", "mention_start": 113, "mention_end": 136, "dataset_mention": "the point cloud dataset"}, {"mentioned_in_paper": "806", "context_id": "4", "dataset_context": "We validate our findings in various experiments on a benchmark dataset in computer vision and real-world datasets.", "mention_start": 74, "mention_end": 113, "dataset_mention": "computer vision and real-world datasets"}, {"mentioned_in_paper": "806", "context_id": "27", "dataset_context": "(2) Our evaluation is tailored to real-world industry datasets to analyze the impact of instance selection mechanisms in real-world settings and inform practitioners about optimal instance selection strategies for efficient model adaptation.", "mention_start": 34, "mention_end": 62, "dataset_mention": "real-world industry datasets"}, {"mentioned_in_paper": "806", "context_id": "55", "dataset_context": "Besides a reduced number of instance selection mechanisms and less complex benchmark datasets, this work does not include real-world industry datasets or cost considerations.", "mention_start": 28, "mention_end": 93, "dataset_mention": "instance selection mechanisms and less complex benchmark datasets"}, {"mentioned_in_paper": "806", "context_id": "55", "dataset_context": "Besides a reduced number of instance selection mechanisms and less complex benchmark datasets, this work does not include real-world industry datasets or cost considerations.", "mention_start": 121, "mention_end": 150, "dataset_mention": "real-world industry datasets"}, {"mentioned_in_paper": "806", "context_id": "58", "dataset_context": "Further, in contrast to recent related literature, we evaluate all experiments on real-world industry datasets to inform practitioners on meaningful instance selection mechanisms.", "mention_start": 81, "mention_end": 110, "dataset_mention": "real-world industry datasets"}, {"mentioned_in_paper": "806", "context_id": "114", "dataset_context": "Following [2], we evaluate all instance selection mechanisms on mini-ImageNet [28]-a benchmark dataset often used for few-shot learning.", "mention_start": 63, "mention_end": 102, "dataset_mention": "mini-ImageNet [28]-a benchmark dataset"}, {"mentioned_in_paper": "806", "context_id": "116", "dataset_context": "Specifically, we use the German Traffic Signs dataset 1 and the Food-101 dataset 2.", "mention_start": 20, "mention_end": 53, "dataset_mention": "the German Traffic Signs dataset"}, {"mentioned_in_paper": "806", "context_id": "116", "dataset_context": "Specifically, we use the German Traffic Signs dataset 1 and the Food-101 dataset 2.", "mention_start": 20, "mention_end": 80, "dataset_mention": "the German Traffic Signs dataset 1 and the Food-101 dataset"}, {"mentioned_in_paper": "806", "context_id": "118", "dataset_context": "Hereby, we follow current literature that works with the mini-ImageNet dataset (train: 64 classes, validation:", "mention_start": 52, "mention_end": 78, "dataset_mention": "the mini-ImageNet dataset"}, {"mentioned_in_paper": "806", "context_id": "120", "dataset_context": "We split the classes manually for the German Traffic Sign dataset (e. g., red signs are part of the train set and blue signs appear only in the validation and test sets resulting in 23 / 10 / 10).", "mention_start": 34, "mention_end": 65, "dataset_mention": "the German Traffic Sign dataset"}, {"mentioned_in_paper": "806", "context_id": "146", "dataset_context": "In the case of the Food-101 dataset, ensemble-based techniques achieve the model performance of the single instance oracle.", "mention_start": 15, "mention_end": 35, "dataset_mention": "the Food-101 dataset"}, {"mentioned_in_paper": "806", "context_id": "155", "dataset_context": "We find consistent patterns for the instance selection mechanisms on the German traffic sign dataset.", "mention_start": 69, "mention_end": 100, "dataset_mention": "the German traffic sign dataset"}, {"mentioned_in_paper": "806", "context_id": "193", "dataset_context": "For the German traffic sign dataset, Minimum Confidence achieves the maximum performance at 42.1%, and BALD at 48.7%.", "mention_start": 4, "mention_end": 35, "dataset_mention": "the German traffic sign dataset"}, {"mentioned_in_paper": "806", "context_id": "198", "dataset_context": "As these settings become increasingly important in real-world settings, our HITL few-shot system has desirable properties: For instance, for the German traffic sign dataset, the combination of human experts and the few-shot learning model achieves an increase of up to 48.4% model performance which translates to a significantly improved method accuracy.", "mention_start": 140, "mention_end": 172, "dataset_mention": "the German traffic sign dataset"}, {"mentioned_in_paper": "807", "context_id": "51", "dataset_context": "For example, using batch normalization (Ioffe and Szegedy, 2015) or residual connection (He et al., 2016) in image classification generally leads to improvement, almost regardless of the choice of model and dataset.", "mention_start": 196, "mention_end": 214, "dataset_mention": "model and dataset"}, {"mentioned_in_paper": "807", "context_id": "67", "dataset_context": "In order to minimize the influence of other factors such as a complex background, we experimented on the smallNORB (Krizhevsky and Hinton, 2009) dataset, which contains gray-scale images of a few objects taken at many different angles, with a carefully designed pure shape recognition task not disturbed by context or color (Hinton et al., 2018).", "mention_start": 138, "mention_end": 152, "dataset_mention": " 2009) dataset"}, {"mentioned_in_paper": "807", "context_id": "73", "dataset_context": "LeCun and Haffner, 1998) dataset; In this experiment, we trained the model by rotating the training data randomly within the [-30, 30] degree range.", "mention_start": 18, "mention_end": 32, "dataset_mention": " 1998) dataset"}, {"mentioned_in_paper": "807", "context_id": "86", "dataset_context": "We trained CapsNet (Sabour et al., 2017) on MNIST dataset (Y.", "mention_start": 43, "mention_end": 57, "dataset_mention": "MNIST dataset"}, {"mentioned_in_paper": "807", "context_id": "103", "dataset_context": "Table 3 : The accuracy of the routing algorithms for rotational MNIST dataset.", "mention_start": 52, "mention_end": 77, "dataset_mention": "rotational MNIST dataset"}, {"mentioned_in_paper": "807", "context_id": "107", "dataset_context": "Table 4 : The accuracy of the routing algorithms for translated MNIST dataset.", "mention_start": 52, "mention_end": 77, "dataset_mention": "translated MNIST dataset"}, {"mentioned_in_paper": "807", "context_id": "128", "dataset_context": "For the CIFAR10 (LeCun et al., 2004) dataset, we applied random cropping with a padding size of 4 and random horizontal flip.", "mention_start": 30, "mention_end": 44, "dataset_mention": " 2004) dataset"}, {"mentioned_in_paper": "807", "context_id": "131", "dataset_context": "LeCun and Haffner, 1998) dataset, we added padding to make 32 \u00d7 32 images.", "mention_start": 18, "mention_end": 32, "dataset_mention": " 1998) dataset"}, {"mentioned_in_paper": "808", "context_id": "56", "dataset_context": "In [40], a navigating spreading-out graph is proposed that enables billion-scale data set searches for online sales applications.", "mention_start": 44, "mention_end": 89, "dataset_mention": "proposed that enables billion-scale data set"}, {"mentioned_in_paper": "808", "context_id": "64", "dataset_context": "Our present work focuses on the suitability and performance of different nearest-neighbor search algorithms in the context of Data-Driven computing with noise-free material data sets of up to a billion points.", "mention_start": 153, "mention_end": 182, "dataset_mention": "noise-free material data sets"}, {"mentioned_in_paper": "808", "context_id": "82", "dataset_context": "Finally, we show that computations with billion-point data sets are possible within seconds.", "mention_start": 39, "mention_end": 63, "dataset_mention": "billion-point data sets"}, {"mentioned_in_paper": "808", "context_id": "122", "dataset_context": "Nevertheless, the phenomena, which will be discussed here, also occur with problems of lower-dimensional data sets like trusses or continua in plane strain or plane stress conditions.", "mention_start": 86, "mention_end": 114, "dataset_mention": "lower-dimensional data sets"}, {"mentioned_in_paper": "808", "context_id": "133", "dataset_context": "Initially, we compare the DD-solver's computation times for increasingly fine data sets of 10 3 , 10 4 , 10 5 , and 10 6 points with 20 random samples each.", "mention_start": 59, "mention_end": 87, "dataset_mention": "increasingly fine data sets"}, {"mentioned_in_paper": "808", "context_id": "141", "dataset_context": "On the other hand, with an increasing fine data set, the remaining squared distance decreases, as shown in Fig. 3b.", "mention_start": 23, "mention_end": 51, "dataset_mention": "an increasing fine data set"}, {"mentioned_in_paper": "808", "context_id": "284", "dataset_context": "For higherdimensional data sets, this type of search algorithm seems to be superior.", "mention_start": 4, "mention_end": 31, "dataset_mention": "higherdimensional data sets"}, {"mentioned_in_paper": "808", "context_id": "375", "dataset_context": "However, experimetnal data sets are inevitably noisy and may contain outliers.", "mention_start": 8, "mention_end": 31, "dataset_mention": " experimetnal data sets"}, {"mentioned_in_paper": "808", "context_id": "390", "dataset_context": "In this data infrastructure, queries from a finite-element calculation performed on a local computer would be sent to the material data warehouse, which would perform near-neighbor searches and other operations on pre-structured data sets and send the results to the local user.", "mention_start": 213, "mention_end": 238, "dataset_mention": "pre-structured data sets"}, {"mentioned_in_paper": "810", "context_id": "55", "dataset_context": "We conduct experiments on two VCMR datasets, TVR and How2R, And two VR datasets, YC2R and VATEX-EN-R.", "mention_start": 26, "mention_end": 43, "dataset_mention": "two VCMR datasets"}, {"mentioned_in_paper": "810", "context_id": "55", "dataset_context": "We conduct experiments on two VCMR datasets, TVR and How2R, And two VR datasets, YC2R and VATEX-EN-R.", "mention_start": 59, "mention_end": 79, "dataset_mention": " And two VR datasets"}, {"mentioned_in_paper": "810", "context_id": "57", "dataset_context": "The results of VCMR task on TVR and How2R datasets are reported on Table 1.", "mention_start": 28, "mention_end": 50, "dataset_mention": "TVR and How2R datasets"}, {"mentioned_in_paper": "810", "context_id": "59", "dataset_context": "Our method outperforms all the other methods on TVR dataset, and has slight advantage over Recall@1 metric on How2R dataset.", "mention_start": 48, "mention_end": 59, "dataset_mention": "TVR dataset"}, {"mentioned_in_paper": "810", "context_id": "59", "dataset_context": "Our method outperforms all the other methods on TVR dataset, and has slight advantage over Recall@1 metric on How2R dataset.", "mention_start": 109, "mention_end": 123, "dataset_mention": "How2R dataset"}, {"mentioned_in_paper": "810", "context_id": "64", "dataset_context": "Note that although HERO conducted experiments on How2R dataset, the version used by HERO is not the same as the one used by VALUE benchmark and us, so we cannot extract their results on How2R.", "mention_start": 49, "mention_end": 62, "dataset_mention": "How2R dataset"}, {"mentioned_in_paper": "810", "context_id": "65", "dataset_context": "Method YC2R VATEX-EN-R R@1 R@5 R@10 AveR R@1 R@5 R@10 AveR Table 2 shows the model performances on two VR datasets.", "mention_start": 99, "mention_end": 114, "dataset_mention": "two VR datasets"}, {"mentioned_in_paper": "810", "context_id": "66", "dataset_context": "By experiment we find that CLIP-ViT+S3D does not work on YC2R, therefore we use ResNet+S3D feature on YC2R dataset.", "mention_start": 101, "mention_end": 114, "dataset_mention": "YC2R dataset"}, {"mentioned_in_paper": "810", "context_id": "67", "dataset_context": "The large gap between VALUE and ours on validation set and slight performace difference on test set demonstrate that model is easy to overfit on VATEX-EN-R dataset, as well as mentioned in [4].", "mention_start": 145, "mention_end": 163, "dataset_mention": "VATEX-EN-R dataset"}, {"mentioned_in_paper": "810", "context_id": "68", "dataset_context": "R@1 R@5 R@10 AveR R@1 R@5 R@10 AveR Base  Since our fusion method focus on VCMR task, we only study ablations on two VCMR datasets.", "mention_start": 112, "mention_end": 130, "dataset_mention": "two VCMR datasets"}, {"mentioned_in_paper": "810", "context_id": "71", "dataset_context": "And data augmentions including shuffling, cutoff and dropout contribute to overall metrics, despite of decreasing Recall@1 on How2R dataset.", "mention_start": 125, "mention_end": 139, "dataset_mention": "How2R dataset"}, {"mentioned_in_paper": "812", "context_id": "5", "dataset_context": "However, CNAPs cannot be robust to randomly selected support sets and perform poorly on some datasets of Meta-Dataset because of its scattered mean embeddings responded by the simple mean operator.", "mention_start": 104, "mention_end": 117, "dataset_mention": "Meta-Dataset"}, {"mentioned_in_paper": "812", "context_id": "22", "dataset_context": "CNAPs are evaluated on a large-scale and complex dataset containing 10 cross-domain common datasets while other metric-based methods mainly focus on in-domain transfer ability.", "mention_start": 68, "mention_end": 99, "dataset_mention": "10 cross-domain common datasets"}, {"mentioned_in_paper": "812", "context_id": "25", "dataset_context": "The test Meta-Dataset [5] containing 10 sub-datasets is employed.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The test Meta-Dataset"}, {"mentioned_in_paper": "812", "context_id": "26", "dataset_context": "In AZS-I, one single random but fixed support set is chosen from each sub-dataset for all tasks in it to stop the adaptation, Only slight performance drop was observed.", "mention_start": 64, "mention_end": 81, "dataset_mention": "each sub-dataset"}, {"mentioned_in_paper": "812", "context_id": "27", "dataset_context": "In AZS-II, a single random but fixed support set is chosen from a particular sub-dataset but is used across all sub-datasets to further stop all the adaptation for CNAPs at the inference stage.", "mention_start": 63, "mention_end": 88, "dataset_mention": "a particular sub-dataset"}, {"mentioned_in_paper": "812", "context_id": "30", "dataset_context": "However, a dramatic performance drops was observed on few sub-datasets: Omniglot [6], Quick Draw, and Aircraft sub-datasets.", "mention_start": 97, "mention_end": 123, "dataset_mention": " and Aircraft sub-datasets"}, {"mentioned_in_paper": "812", "context_id": "31", "dataset_context": "Thus, a question arises: are the task-specific support sets still necessary for multidataset few-shot learning or the feature extractor of CNAPs relies too much on the support sets?", "mention_start": 79, "mention_end": 92, "dataset_mention": "multidataset"}, {"mentioned_in_paper": "812", "context_id": "34", "dataset_context": "Consequently, a support set taken from a particular sub-dataset may perform poor across sub-datasets.", "mention_start": 38, "mention_end": 63, "dataset_mention": "a particular sub-dataset"}, {"mentioned_in_paper": "812", "context_id": "50", "dataset_context": "In AZS-II tasks, the adaptation can be completely stopped, which only needs a randomly chosen support set from any arbitrary sub-dataset in the test stage.", "mention_start": 110, "mention_end": 136, "dataset_mention": "any arbitrary sub-dataset"}, {"mentioned_in_paper": "812", "context_id": "85", "dataset_context": "Let X C and X T denote context image dataset and target image dataset, and Y C and Y T denote their labels.", "mention_start": 23, "mention_end": 44, "dataset_mention": "context image dataset"}, {"mentioned_in_paper": "812", "context_id": "86", "dataset_context": "CNPs use Neural Networks (NNs) to build the mean mapping parameterized by \u03b8 from the context image dataset to feature space, F (\u2022, \u03b8) :", "mention_start": 81, "mention_end": 106, "dataset_mention": "the context image dataset"}, {"mentioned_in_paper": "812", "context_id": "138", "dataset_context": "In this section, CNAP-CMF is compared with the state-of-the-art CNAPs for multi-task classification tasks on Meta-Datasets.", "mention_start": 108, "mention_end": 122, "dataset_mention": "Meta-Datasets"}, {"mentioned_in_paper": "812", "context_id": "140", "dataset_context": "Similar to CNAPs, the models are fully trained on two different training datasets, 1) all sub-datasets of Meta-Datasets, and 2) ImageNet-1K.", "mention_start": 82, "mention_end": 102, "dataset_mention": " 1) all sub-datasets"}, {"mentioned_in_paper": "812", "context_id": "140", "dataset_context": "Similar to CNAPs, the models are fully trained on two different training datasets, 1) all sub-datasets of Meta-Datasets, and 2) ImageNet-1K.", "mention_start": 105, "mention_end": 119, "dataset_mention": "Meta-Datasets"}, {"mentioned_in_paper": "812", "context_id": "141", "dataset_context": "The trained models are then tested on all sub-datasets of Meta-Datasets as well as MNIST, CIFAR-10, and CIFAR-100, in order to evaluate the cross-domain performance.", "mention_start": 58, "mention_end": 71, "dataset_mention": "Meta-Datasets"}, {"mentioned_in_paper": "812", "context_id": "143", "dataset_context": "This section reports the evaluation results of AZS-II and one-shot tasks on the whole test dataset of Meta-Dataset when the models are trained on Meta-Dataset.", "mention_start": 102, "mention_end": 114, "dataset_mention": "Meta-Dataset"}, {"mentioned_in_paper": "812", "context_id": "143", "dataset_context": "This section reports the evaluation results of AZS-II and one-shot tasks on the whole test dataset of Meta-Dataset when the models are trained on Meta-Dataset.", "mention_start": 146, "mention_end": 158, "dataset_mention": "Meta-Dataset"}, {"mentioned_in_paper": "812", "context_id": "147", "dataset_context": "Due to the limit of computation resource, the auto-regressive (AR) proposed in CNAPs is not used when the model is trained on Meta-Dataset, but used when the model is trained on ImageNet-1K.", "mention_start": 125, "mention_end": 138, "dataset_mention": "Meta-Dataset"}, {"mentioned_in_paper": "812", "context_id": "151", "dataset_context": "For the multi-dataset classification task, a model is considered to perform better than current model if it yields higher classification accuracy on over half of all sub-datasets.", "mention_start": 4, "mention_end": 21, "dataset_mention": "the multi-dataset"}, {"mentioned_in_paper": "812", "context_id": "153", "dataset_context": "On Omniglot dataset, AZS-II, CNAP-CMF outperforms CNAPs 37.2%.", "mention_start": 3, "mention_end": 19, "dataset_mention": "Omniglot dataset"}, {"mentioned_in_paper": "812", "context_id": "154", "dataset_context": "Simi-larly, for AZS-II, CNAP-CMF outperforms CNAPs by 38.3%, 20.7%, and 10.4% on Aircraft, Quick Draw, and MNIST datasets respectively.", "mention_start": 102, "mention_end": 121, "dataset_mention": " and MNIST datasets"}, {"mentioned_in_paper": "812", "context_id": "155", "dataset_context": "When CNAPs are directly applied to AZS-II tasks, where the fixed support set is randomly chosen in ImageNet-1k sub-dataset, the classification performance decreases significantly on Omniglot [6], Aircraft [25], Quick Draw, and MNIST.", "mention_start": 98, "mention_end": 122, "dataset_mention": "ImageNet-1k sub-dataset"}, {"mentioned_in_paper": "812", "context_id": "160", "dataset_context": "To simply see the fluctuation, four support sets are randomly chosen from Omniglot dataset in the same task, and Table 3 gives the classification results.", "mention_start": 73, "mention_end": 90, "dataset_mention": "Omniglot dataset"}, {"mentioned_in_paper": "812", "context_id": "177", "dataset_context": "For each sub-dataset of Meta-Dataset, the running time of CNAPs and CNAP-CMF for the sampled 600 tasks is computed.", "mention_start": 4, "mention_end": 20, "dataset_mention": "each sub-dataset"}, {"mentioned_in_paper": "812", "context_id": "177", "dataset_context": "For each sub-dataset of Meta-Dataset, the running time of CNAPs and CNAP-CMF for the sampled 600 tasks is computed.", "mention_start": 24, "mention_end": 36, "dataset_mention": "Meta-Dataset"}, {"mentioned_in_paper": "812", "context_id": "182", "dataset_context": "In AZS-I, a support set is randomly chosen from each sub-dataset and used on all tasks within it; in AZS-II, a support set is randomly chosen from a sub-dataset but used on all tasks of all sub-dataset.", "mention_start": 47, "mention_end": 64, "dataset_mention": "each sub-dataset"}, {"mentioned_in_paper": "812", "context_id": "182", "dataset_context": "In AZS-I, a support set is randomly chosen from each sub-dataset and used on all tasks within it; in AZS-II, a support set is randomly chosen from a sub-dataset but used on all tasks of all sub-dataset.", "mention_start": 146, "mention_end": 160, "dataset_mention": "a sub-dataset"}, {"mentioned_in_paper": "812", "context_id": "182", "dataset_context": "In AZS-I, a support set is randomly chosen from each sub-dataset and used on all tasks within it; in AZS-II, a support set is randomly chosen from a sub-dataset but used on all tasks of all sub-dataset.", "mention_start": 185, "mention_end": 201, "dataset_mention": "all sub-dataset"}, {"mentioned_in_paper": "813", "context_id": "9", "dataset_context": "To validate the effectiveness of our method, we conduct experiments on several commonly used scene text benchmarks, including both curved and multi-oriented text datasets.", "mention_start": 125, "mention_end": 170, "dataset_mention": "both curved and multi-oriented text datasets"}, {"mentioned_in_paper": "813", "context_id": "132", "dataset_context": "MSRA-TD500 [45] is a multi-oriented text dataset which contains 300 training images and 200 testing images with text-line level annotation.", "mention_start": 19, "mention_end": 48, "dataset_mention": "a multi-oriented text dataset"}, {"mentioned_in_paper": "813", "context_id": "138", "dataset_context": "(1) learning from scratch; (2) fine-tuning models pre-trained on the SynthText dataset.", "mention_start": 64, "mention_end": 86, "dataset_mention": "the SynthText dataset"}, {"mentioned_in_paper": "813", "context_id": "145", "dataset_context": "To analyze our designs in depth, we conduct a series of ablation studies on both curve and multioriented text datasets (Total-Text and MSRA-TD500).", "mention_start": 75, "mention_end": 118, "dataset_mention": "both curve and multioriented text datasets"}, {"mentioned_in_paper": "813", "context_id": "153", "dataset_context": "86.0 70.0 77.0 8.9 PixelLink [3] 83.0 73.2 77.8 3.0 TextSnake [23] 83.2 73.9 78.3 1.1 RRD [17] 87.0 73.0 79.0 10.0 TextField [42] 87.4 75.9 81.3 -CRAFT [1] 88.2 78.2 82.9 8.6 MCN [21] 88.0 79.0 83.0 -PAN [38] 84.4 83.8 84.1 30.2 DB [16] 91.5 79.2 84.9 32.0 DRRG [49] 88.1 82.3 85.1 -CT-736 90.0 82.5 86. 1 34.8 Curved text detection We first evaluate our CT on the Total-Text and CTW1500 datasets to examine its capability for curved text detection.", "mention_start": 361, "mention_end": 396, "dataset_mention": "the Total-Text and CTW1500 datasets"}, {"mentioned_in_paper": "813", "context_id": "183", "dataset_context": "To further demonstrate the rotation robustness of our method, we evaluate our CPN-based text spotter on the Rotated ICDAR2013 dataset.", "mention_start": 103, "mention_end": 133, "dataset_mention": "the Rotated ICDAR2013 dataset"}, {"mentioned_in_paper": "813", "context_id": "184", "dataset_context": "Rotated ICDAR2013 [13] is an augmented text dataset that is generated from ICDAR2013 [10].", "mention_start": 26, "mention_end": 51, "dataset_mention": "an augmented text dataset"}, {"mentioned_in_paper": "813", "context_id": "185", "dataset_context": "To form the Rotated ICDAR2013 dataset, all the images and annotations in the test set of the ICDAR2013 benchmark are rotated with some specific angles, including 15 \u2022 , 30 \u2022 , 45 \u2022 , 60 \u2022 , 75 \u2022 and 90 \u2022 .", "mention_start": 8, "mention_end": 37, "dataset_mention": "the Rotated ICDAR2013 dataset"}, {"mentioned_in_paper": "813", "context_id": "188", "dataset_context": "Since the annotations are extended from horizontal rectangles to multi-oriented ones, we adopt the evaluation protocols in the ICDAR2015 dataset [9].", "mention_start": 122, "mention_end": 144, "dataset_mention": "the ICDAR2015 dataset"}, {"mentioned_in_paper": "814", "context_id": "40", "dataset_context": "We evaluate the performance of ELSA on a benchmark Amazon review dataset that has been used in various cross-lingual sentiment classification studies [50, 58, 62].", "mention_start": 39, "mention_end": 72, "dataset_mention": "a benchmark Amazon review dataset"}, {"mentioned_in_paper": "814", "context_id": "167", "dataset_context": "The labeled data (L S for training and L T for testing) used in our work are from the Amazon review dataset [3] created by Prettenhofer and Stein [50].", "mention_start": 82, "mention_end": 107, "dataset_mention": "the Amazon review dataset"}, {"mentioned_in_paper": "814", "context_id": "185", "dataset_context": "The processed emoji-Tweets provide the E S and E T datasets, whose statistics are presented in Table 1.", "mention_start": 35, "mention_end": 59, "dataset_mention": "the E S and E T datasets"}, {"mentioned_in_paper": "814", "context_id": "318", "dataset_context": "Most previous cross-lingual sentiment studies [50, 58, 62] used the Amazon review dataset for evaluation.", "mention_start": 63, "mention_end": 89, "dataset_mention": "the Amazon review dataset"}, {"mentioned_in_paper": "815", "context_id": "333", "dataset_context": "Building specifications follow the NYC Department of City Planning Documentation, and building data is from the PLUTO dataset [95].", "mention_start": 107, "mention_end": 125, "dataset_mention": "the PLUTO dataset"}, {"mentioned_in_paper": "816", "context_id": "5", "dataset_context": "To demonstrate this effect, we create the Dataset of Curt\u00f3 & Zarza, the first GAN augmented dataset of faces.", "mention_start": 67, "mention_end": 99, "dataset_mention": " the first GAN augmented dataset"}, {"mentioned_in_paper": "816", "context_id": "9", "dataset_context": "A novel bias-free dataset, Curt\u00f3 & Zarza 1 2 , containing human faces from different ethnical groups in a wide variety of illumination conditions and image resolutions is introduced.", "mention_start": 0, "mention_end": 25, "dataset_mention": "A novel bias-free dataset"}, {"mentioned_in_paper": "816", "context_id": "10", "dataset_context": "Curt\u00f3 is enhanced with HDCGAN synthetic images, thus being the first GAN augmented dataset of faces.", "mention_start": 52, "mention_end": 90, "dataset_mention": "being the first GAN augmented dataset"}, {"mentioned_in_paper": "816", "context_id": "41", "dataset_context": "\u2022 New dataset targeted for GAN training, Curt\u00f3, that introduces a wide space of learning attributes.", "mention_start": 0, "mention_end": 13, "dataset_mention": "\u2022 New dataset"}, {"mentioned_in_paper": "816", "context_id": "145", "dataset_context": "Further, we present a bias-free dataset of faces containing wellbalanced ethnical groups, Curt\u00f3 & Zarza, that poses a very difficult challenge and is rich on learning attributes to sample from.", "mention_start": 19, "mention_end": 39, "dataset_mention": "a bias-free dataset"}, {"mentioned_in_paper": "816", "context_id": "146", "dataset_context": "Moreover, we enhance Curt\u00f3 with 4,239 unlabeled synthetic images generated by HDCGAN, being therefore the first GAN augmented dataset of faces.", "mention_start": 101, "mention_end": 133, "dataset_mention": "the first GAN augmented dataset"}, {"mentioned_in_paper": "818", "context_id": "95", "dataset_context": "In [44], authors have produced a dataset of personalized saliency maps (PSMs), although this dataset has been modified and more correct explanation of the dataset are in the paper [45].", "mention_start": 21, "mention_end": 40, "dataset_mention": "produced a dataset"}, {"mentioned_in_paper": "818", "context_id": "122", "dataset_context": "One of the main limitations in personalized saliency predic-tion is the lack of large saliency datasets for each individual.", "mention_start": 80, "mention_end": 103, "dataset_mention": "large saliency datasets"}, {"mentioned_in_paper": "818", "context_id": "124", "dataset_context": "First they construct and train a multi-task CNN from the PSM dataset for predicting PSMs of individuals included in the PSM dataset.", "mention_start": 53, "mention_end": 68, "dataset_mention": "the PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "124", "dataset_context": "First they construct and train a multi-task CNN from the PSM dataset for predicting PSMs of individuals included in the PSM dataset.", "mention_start": 116, "mention_end": 131, "dataset_mention": "the PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "126", "dataset_context": "Before this procedure, some images should be selected from the PSM dataset for calculating person similarities between the target person and those included in the PSM dataset.", "mention_start": 58, "mention_end": 74, "dataset_mention": "the PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "126", "dataset_context": "Before this procedure, some images should be selected from the PSM dataset for calculating person similarities between the target person and those included in the PSM dataset.", "mention_start": 158, "mention_end": 174, "dataset_mention": "the PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "129", "dataset_context": "Next, the person similarity is calculated by using selected images included in the PSM dataset.", "mention_start": 78, "mention_end": 94, "dataset_mention": "the PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "131", "dataset_context": "For guaranteeing the high diversity of the selected images, AIS focuses on the kinds of objects included in the training images in the PSM dataset by using a deep learning-based object detection method.", "mention_start": 130, "mention_end": 146, "dataset_mention": "the PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "133", "dataset_context": "Finally, FPSP of a target image for the new target person is realized on the basis of the person similarity and PSMs predicted by the multi-task CNN trained for the persons in the PSM dataset.", "mention_start": 175, "mention_end": 191, "dataset_mention": "the PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "148", "dataset_context": "Among 1600 images in this dataset, 1100 images are chosen from existing saliency detection datasets including SALICON, [2], ImageNet, [54], iSUN, [55], OSIE, [56], PASCAL-S, [57], 125 images are captured by the authors of the paper [45], and 375 images are gathered from the Internet.", "mention_start": 62, "mention_end": 99, "dataset_mention": "existing saliency detection datasets"}, {"mentioned_in_paper": "818", "context_id": "150", "dataset_context": "The four saliency maps of the same picture observed by the same individual are then combined, and the result is used as the observer's ground truth for the personalized saliency maps (PSM) dataset.", "mention_start": 151, "mention_end": 196, "dataset_mention": "the personalized saliency maps (PSM) dataset"}, {"mentioned_in_paper": "818", "context_id": "159", "dataset_context": "We will refer to this dataset in other sections as PSM dataset.", "mention_start": 51, "mention_end": 62, "dataset_mention": "PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "208", "dataset_context": "In each network, node numbers are the subject ID in the PSM dataset.", "mention_start": 51, "mention_end": 67, "dataset_mention": "the PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "250", "dataset_context": "Assume that in PSM dataset we have personalized saliency maps for a set of images I.", "mention_start": 15, "mention_end": 26, "dataset_mention": "PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "253", "dataset_context": "For a cluster C i , for each image x \u2208 I, we obtain the average of personalized saliency maps for image x (taken from PSM dataset) for all individuals in C i , and call this image AvgSal x Ci .", "mention_start": 117, "mention_end": 129, "dataset_mention": "PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "260", "dataset_context": "In this part we describe how to predict clustered saliency maps for a new individual, having a dataset D of personalized saliency data.", "mention_start": 85, "mention_end": 102, "dataset_mention": " having a dataset"}, {"mentioned_in_paper": "818", "context_id": "263", "dataset_context": "For setting \"All individuals\" we have included the \"Average of pairwise similarities\" and \"Median of pairwise similarities\" for all pairs of individuals in PSM dataset.", "mention_start": 156, "mention_end": 167, "dataset_mention": "PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "326", "dataset_context": "In order to test our proposed approach in part III-G, for each of the 30 individuals in the PSM dataset, we hold this individual out and consider it as a new person.", "mention_start": 87, "mention_end": 103, "dataset_mention": "the PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "352", "dataset_context": "Even though the findings in this paper unravel the effects of different factors in saliency prediction for groups of people in the PSM dataset, the limited features and diversity in the demographics of individuals in this dataset has a negative effect on finding more explanatory clusters.", "mention_start": 127, "mention_end": 142, "dataset_mention": "the PSM dataset"}, {"mentioned_in_paper": "818", "context_id": "356", "dataset_context": "With an improved dataset with more descriptive personal features, our methods might even appear more effective.", "mention_start": 5, "mention_end": 24, "dataset_mention": "an improved dataset"}, {"mentioned_in_paper": "818", "context_id": "357", "dataset_context": "For future work, building a personal saliency maps dataset with an improved set of personal features and more diversity in the demographics of the subjects can be very helpful for unraveling different effects in saliency prediction.", "mention_start": 25, "mention_end": 58, "dataset_mention": "a personal saliency maps dataset"}, {"mentioned_in_paper": "819", "context_id": "85", "dataset_context": "Then, we discard the matches whose angle with the horizontal direction Stereo correspondences between two different images from the EuRoC dataset.", "mention_start": 127, "mention_end": 145, "dataset_mention": "the EuRoC dataset"}, {"mentioned_in_paper": "819", "context_id": "107", "dataset_context": "In addition, we also have employed the Tsukuba Stereo Dataset [26], a synthetic dataset rendered under 4 different illuminations, i.e. fluorescent, lamps, flashlight, and daylight.", "mention_start": 34, "mention_end": 61, "dataset_mention": "the Tsukuba Stereo Dataset"}, {"mentioned_in_paper": "819", "context_id": "114", "dataset_context": "As for the Tsukuba dataset, we observe that the number of features successfully tracked dramatically decreases as the response of the detectors is not capable of producing a compatible set of lines from the same images.", "mention_start": 7, "mention_end": 26, "dataset_mention": "the Tsukuba dataset"}, {"mentioned_in_paper": "819", "context_id": "116", "dataset_context": "In this set of experiments, we test the performance of the compared algorithms in the EuRoC [27] dataset.", "mention_start": 81, "mention_end": 104, "dataset_mention": "the EuRoC [27] dataset"}, {"mentioned_in_paper": "819", "context_id": "117", "dataset_context": "In order to simulate changes in exposure time or illumination within the EuRoC dataset [27] (we will refer to simulated sequences with an asterisk) we change the gain and bias of the image with two uniform distribution, i.e. \u03b1 = U(0.5, 2.5) and \u03b2 = U(0, 20) pixels every 30 seconds.", "mention_start": 69, "mention_end": 86, "dataset_mention": "the EuRoC dataset"}, {"mentioned_in_paper": "820", "context_id": "57", "dataset_context": "Through extensive experiments, PiCO+ exhibits significant improvement to state-of-the-art PLL approaches on noisy PLL datasets.", "mention_start": 107, "mention_end": 126, "dataset_mention": "noisy PLL datasets"}, {"mentioned_in_paper": "820", "context_id": "68", "dataset_context": "Moreover, we make the first attempt to conduct experiments on fine-grained classification datasets, where we show classification performance improvement by up to 9.61% compared with the best baseline on the CUB-200 dataset.", "mention_start": 202, "mention_end": 222, "dataset_mention": "the CUB-200 dataset"}, {"mentioned_in_paper": "820", "context_id": "69", "dataset_context": "We also evaluate our new PiCO+ framework on the noisy variants of these datasets, where PiCO+ outperforms the best baseline by up to 12.52% on the noisy PLL version of the CIFAR-10 dataset.", "mention_start": 167, "mention_end": 188, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "820", "context_id": "159", "dataset_context": "Empirically, we observe that the current PLL methods, including PiCO, exhibit a significant performance drop on noisy PLL datasets (Section 5.3).", "mention_start": 111, "mention_end": 130, "dataset_mention": "noisy PLL datasets"}, {"mentioned_in_paper": "820", "context_id": "209", "dataset_context": "Adopting the identical experimental settings in previous work [18], [19], we generate conventional partially labeled datasets by flipping negative labels \u0233 = y to false positive labels with a probability q = P (\u0233 \u2208 Y |\u0233 = y).", "mention_start": 85, "mention_end": 125, "dataset_mention": "conventional partially labeled datasets"}, {"mentioned_in_paper": "820", "context_id": "231", "dataset_context": "For pseudo target Ablation study of PiCO on standard partial label learning datasets CIFAR-10 (q = 0.5) and CIFAR-100 (q = 0.05).", "mention_start": 67, "mention_end": 84, "dataset_mention": "learning datasets"}, {"mentioned_in_paper": "820", "context_id": "246", "dataset_context": "Similar to the standard PLL setup, we warm up the model by fitting uniform targets for 5 and 50 epochs on CIFAR-10 and CIFAR-100 datasets respectively.", "mention_start": 105, "mention_end": 137, "dataset_mention": "CIFAR-10 and CIFAR-100 datasets"}, {"mentioned_in_paper": "820", "context_id": "249", "dataset_context": "Specifically, on CIFAR-10 dataset, we improve upon the best baseline by 4.09%, 4.80%, and 5.80% where q is set to 0.1, 0.3, 0.5 respectively.", "mention_start": 16, "mention_end": 33, "dataset_mention": "CIFAR-10 dataset"}, {"mentioned_in_paper": "820", "context_id": "260", "dataset_context": "We use the CIFAR-10 dataset with q = 0.5.", "mention_start": 7, "mention_end": 27, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "820", "context_id": "266", "dataset_context": "We ablate the contributions of two key components of PiCO: contrastive Accuracy comparisons on noisy PLL datasets.", "mention_start": 94, "mention_end": 113, "dataset_mention": "noisy PLL datasets"}, {"mentioned_in_paper": "820", "context_id": "277", "dataset_context": "Ablation study of PiCO on noisy partial label learning datasets CIFAR-10 (q = 0.5, \u03b7 = 0.2) and CIFAR-100 (q = 0.05, \u03b7 = 0.2).", "mention_start": 46, "mention_end": 63, "dataset_mention": "learning datasets"}, {"mentioned_in_paper": "820", "context_id": "286", "dataset_context": "In Table 3, we compare PiCO+ with competitive PLL methods on CIFAR datasets, where PiCO+ significantly outperforms baselines.", "mention_start": 60, "mention_end": 75, "dataset_mention": "CIFAR datasets"}, {"mentioned_in_paper": "820", "context_id": "287", "dataset_context": "In specific, on CIFAR-10 dataset with q = 0.5, PiCO+ improves upon the best competitor by 6.66% and 12.52% when \u03b7 is set to 0.1, 0.2 respectively.", "mention_start": 15, "mention_end": 32, "dataset_mention": "CIFAR-10 dataset"}, {"mentioned_in_paper": "820", "context_id": "290", "dataset_context": "In Figure 5, we visualize the feature representations of PiCO+ on the noisy PLL CIFAR-10 dataset with q = 0.5, \u03b7 = 0.2.", "mention_start": 65, "mention_end": 96, "dataset_mention": "the noisy PLL CIFAR-10 dataset"}, {"mentioned_in_paper": "820", "context_id": "301", "dataset_context": "We compare PiCO+ with three variants: 1) PiCO+ w/o L n-cls removes the label guessing technique; 2) PiCO+ w/o L n-cont removes the label-driven contrastive loss; 3) PiCO+ w/o kNN disables the neighbor-augmented contrastive loss; Accuracy comparisons on noisy PLL datasets with more noisy samples.", "mention_start": 252, "mention_end": 271, "dataset_mention": "noisy PLL datasets"}, {"mentioned_in_paper": "820", "context_id": "313", "dataset_context": "On the CIFAR-10 dataset, we observe a severe performance degradation with \u03b2 being larger.", "mention_start": 3, "mention_end": 23, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "820", "context_id": "317", "dataset_context": "Finally, we conduct experiments on noisy PLL datasets that contain much more severe noise to show the robustness of our PiCO+ method.", "mention_start": 34, "mention_end": 53, "dataset_mention": "noisy PLL datasets"}, {"mentioned_in_paper": "820", "context_id": "324", "dataset_context": "To verify this, we conduct experiments on two datasets: 1) CUB-200 dataset [26] contains 200 bird species; 2) CIFAR-100 with hierarchical labels (CIFAR-100-H), where we generate candidate labels that belong to the same superclass 1 .", "mention_start": 55, "mention_end": 74, "dataset_mention": " 1) CUB-200 dataset"}, {"mentioned_in_paper": "820", "context_id": "327", "dataset_context": "In addition, we test the performance of PiCO+ on both standard and noisy PLL versions of finegrained datasets.", "mention_start": 88, "mention_end": 109, "dataset_mention": "finegrained datasets"}, {"mentioned_in_paper": "820", "context_id": "331", "dataset_context": "1. CIFAR-100 dataset consists of 20 superclasses, with 5 classes in each superclass.", "mention_start": 0, "mention_end": 20, "dataset_mention": "1. CIFAR-100 dataset"}, {"mentioned_in_paper": "820", "context_id": "486", "dataset_context": "On the CIFAR-10 dataset, the performance is stable with varying \u03b3.", "mention_start": 3, "mention_end": 23, "dataset_mention": "the CIFAR-10 dataset"}, {"mentioned_in_paper": "821", "context_id": "131", "dataset_context": "The Princeton Benchmark library comprises a data set with 4,300 manually generated segmentations for 380 surface meshes of 19 different object categories.", "mention_start": 0, "mention_end": 52, "dataset_mention": "The Princeton Benchmark library comprises a data set"}, {"mentioned_in_paper": "823", "context_id": "6", "dataset_context": "We validate our approach on the challenging Something-Else tasks from the Something-Something-V2 dataset.", "mention_start": 70, "mention_end": 104, "dataset_mention": "the Something-Something-V2 dataset"}, {"mentioned_in_paper": "823", "context_id": "7", "dataset_context": "We further show that our framework is flexible and can generalize to a new domain by showing competitive results on the Charades-Fewshot dataset.", "mention_start": 116, "mention_end": 144, "dataset_mention": "the Charades-Fewshot dataset"}, {"mentioned_in_paper": "823", "context_id": "32", "dataset_context": "With this ability, our approach can recognize unseen verbnoun compositions at test time better than other approaches and establishes a new state of the art on the Something-Else task [39] from the Something-Something-V2 dataset [21].", "mention_start": 192, "mention_end": 227, "dataset_mention": "the Something-Something-V2 dataset"}, {"mentioned_in_paper": "823", "context_id": "35", "dataset_context": "Finally, we show that our model generalizes to a different domain of actions found in the Charades dataset [48] where our approach performs competitively on the Charades-Fewshot benchmark [30].", "mention_start": 85, "mention_end": 106, "dataset_mention": "the Charades dataset"}, {"mentioned_in_paper": "823", "context_id": "121", "dataset_context": "The Something-Else task is an extension to the Something-Something-V2 [21] dataset with new object annotations and compositional action recognition splits.", "mention_start": 43, "mention_end": 82, "dataset_mention": "the Something-Something-V2 [21] dataset"}, {"mentioned_in_paper": "823", "context_id": "131", "dataset_context": "The VI pathways is based on the TSM [36] model because of its efficiency and validated performance on the Something-Something-V2 dataset.", "mention_start": 102, "mention_end": 136, "dataset_mention": "the Something-Something-V2 dataset"}, {"mentioned_in_paper": "823", "context_id": "218", "dataset_context": "To demonstrate that our framework for recognizing compositional actions is flexible, we evaluate our method on the Charades [48] dataset which contains 9,848 videos and 157 action categories with an average length of 30 seconds which may contain one or more action instances.", "mention_start": 110, "mention_end": 136, "dataset_mention": "the Charades [48] dataset"}, {"mentioned_in_paper": "823", "context_id": "219", "dataset_context": "Many action categories found in the Charades dataset are defined as a verb+noun composition (ie.", "mention_start": 32, "mention_end": 52, "dataset_mention": "the Charades dataset"}, {"mentioned_in_paper": "823", "context_id": "223", "dataset_context": "Our goal with the Charades dataset is to demonstrate the flexibility of our framework when applied to a different problem domain.", "mention_start": 14, "mention_end": 34, "dataset_mention": "the Charades dataset"}, {"mentioned_in_paper": "823", "context_id": "225", "dataset_context": "For the VI pathway, we use the SlowFast [16] model given its competitive performance on the Charades dataset.", "mention_start": 87, "mention_end": 108, "dataset_mention": "the Charades dataset"}, {"mentioned_in_paper": "823", "context_id": "230", "dataset_context": "Problem Formulation The Charades dataset contains 37 annotated objects.", "mention_start": 0, "mention_end": 40, "dataset_mention": "Problem Formulation The Charades dataset"}, {"mentioned_in_paper": "824", "context_id": "5", "dataset_context": "Furthermore, the current public available infrared and visible light datasets are mainly used for object detection or tracking, and some are composed of discontinuous images which are not suitable for video tasks.", "mention_start": 12, "mention_end": 77, "dataset_mention": " the current public available infrared and visible light datasets"}, {"mentioned_in_paper": "824", "context_id": "9", "dataset_context": "Moreover, additional experimental results on the flower-to-flower dataset indicate I2V-GAN is also applicable to other video translation tasks.", "mention_start": 44, "mention_end": 73, "dataset_mention": "the flower-to-flower dataset"}, {"mentioned_in_paper": "824", "context_id": "10", "dataset_context": "The code and IRVI dataset are available at https://github.com/BIT-DA/I2V-GAN.", "mention_start": 0, "mention_end": 25, "dataset_mention": "The code and IRVI dataset"}, {"mentioned_in_paper": "824", "context_id": "54", "dataset_context": "Additionally and the same important as above, the current mostused public IR and VI datasets are unsuited for I2V video translation tasks.", "mention_start": 45, "mention_end": 92, "dataset_mention": " the current mostused public IR and VI datasets"}, {"mentioned_in_paper": "824", "context_id": "171", "dataset_context": "We comprehensively select three public available infrared and visible light video datasets for comparison, which are VOT2019-RGBTIR [33], FLIR [18] and KAIST [36] in day road scene.", "mention_start": 0, "mention_end": 90, "dataset_mention": "We comprehensively select three public available infrared and visible light video datasets"}, {"mentioned_in_paper": "824", "context_id": "177", "dataset_context": "Our IRVI dataset contains 6 clips for training and another 6 clips for testing.", "mention_start": 0, "mention_end": 16, "dataset_mention": "Our IRVI dataset"}, {"mentioned_in_paper": "824", "context_id": "182", "dataset_context": "IRVI Dataset.", "mention_start": 0, "mention_end": 12, "dataset_mention": "IRVI Dataset"}, {"mentioned_in_paper": "824", "context_id": "183", "dataset_context": "Our experiments are mainly based on IRVI dataset.", "mention_start": 36, "mention_end": 48, "dataset_mention": "IRVI dataset"}, {"mentioned_in_paper": "824", "context_id": "188", "dataset_context": "Flower Video Dataset.", "mention_start": 0, "mention_end": 20, "dataset_mention": "Flower Video Dataset"}, {"mentioned_in_paper": "825", "context_id": "32", "dataset_context": "We apply our experiments on three facial expression datasets in order to recognize the six basic expressions along with a neutral state.", "mention_start": 28, "mention_end": 60, "dataset_mention": "three facial expression datasets"}, {"mentioned_in_paper": "827", "context_id": "36", "dataset_context": "The availability of big sleep datasets [59], [60] has enabled deep learning methods capability of learning high-dimensional features from sleep databases directly to solve the sleep stage scoring problem [49], [56], [61], [62].", "mention_start": 20, "mention_end": 38, "dataset_mention": "big sleep datasets"}, {"mentioned_in_paper": "827", "context_id": "124", "dataset_context": "Three PSG datasets were used in this study including Physionet Sleep-EDF [98], [99], Physionet EDF-20 and DREAMS [100].", "mention_start": 0, "mention_end": 18, "dataset_mention": "Three PSG datasets"}, {"mentioned_in_paper": "827", "context_id": "125", "dataset_context": "Physionet Sleep-EDF dataset is considered as the main dataset in", "mention_start": 0, "mention_end": 27, "dataset_mention": "Physionet Sleep-EDF dataset"}, {"mentioned_in_paper": "827", "context_id": "125", "dataset_context": "Physionet Sleep-EDF dataset is considered as the main dataset in", "mention_start": 45, "mention_end": 61, "dataset_mention": "the main dataset"}, {"mentioned_in_paper": "827", "context_id": "130", "dataset_context": "The Cassette records of Physionet Sleep EDF Expanded dataset were utilized including 153 PSG recordings from 78 healthy subjects aged 25-101, without any sleepimpacting medication.", "mention_start": 24, "mention_end": 60, "dataset_mention": "Physionet Sleep EDF Expanded dataset"}, {"mentioned_in_paper": "827", "context_id": "146", "dataset_context": "EDF-20 dataset _ The EDF-20 dataset includes 20 PSG records from the Cassette records of Physionet Sleep EDF Expanded dataset with the same properties of sampling rate and included signals.", "mention_start": 0, "mention_end": 14, "dataset_mention": "EDF-20 dataset"}, {"mentioned_in_paper": "827", "context_id": "146", "dataset_context": "EDF-20 dataset _ The EDF-20 dataset includes 20 PSG records from the Cassette records of Physionet Sleep EDF Expanded dataset with the same properties of sampling rate and included signals.", "mention_start": 0, "mention_end": 35, "dataset_mention": "EDF-20 dataset _ The EDF-20 dataset"}, {"mentioned_in_paper": "827", "context_id": "146", "dataset_context": "EDF-20 dataset _ The EDF-20 dataset includes 20 PSG records from the Cassette records of Physionet Sleep EDF Expanded dataset with the same properties of sampling rate and included signals.", "mention_start": 89, "mention_end": 125, "dataset_mention": "Physionet Sleep EDF Expanded dataset"}, {"mentioned_in_paper": "827", "context_id": "150", "dataset_context": "DREAMS dataset _ The DREAMS database includes eight different datasets but in this study the DREAMS subjects dataset was used.", "mention_start": 0, "mention_end": 14, "dataset_mention": "DREAMS dataset"}, {"mentioned_in_paper": "827", "context_id": "150", "dataset_context": "DREAMS dataset _ The DREAMS database includes eight different datasets but in this study the DREAMS subjects dataset was used.", "mention_start": 89, "mention_end": 116, "dataset_mention": "the DREAMS subjects dataset"}, {"mentioned_in_paper": "827", "context_id": "151", "dataset_context": "the DREAMS subjects dataset includes whole-night PSG recordings of 20 healthy subjects and manually scored in sleep stages according to both the R&K and AASM criteria.", "mention_start": 0, "mention_end": 27, "dataset_mention": "the DREAMS subjects dataset"}, {"mentioned_in_paper": "827", "context_id": "156", "dataset_context": "Therefore, LOO validation method was used to evaluate the performance same as EDF-20 dataset.", "mention_start": 77, "mention_end": 92, "dataset_mention": "EDF-20 dataset"}, {"mentioned_in_paper": "827", "context_id": "158", "dataset_context": "First, using EDF-Expanded dataset, the single-and multi-epoch networks were trained by training sets in the night-holdout and subject-holdout approaches.", "mention_start": 12, "mention_end": 33, "dataset_mention": "EDF-Expanded dataset"}, {"mentioned_in_paper": "827", "context_id": "166", "dataset_context": "The proposed architecture achieved an average accuracy of 92.33%, Kappa of 0.85 and MF1 of 85.41% in night-holdout cross validation approach and average accuracy of 90.08%, Kappa of 0.81, and MF1 of 80.81% in the subject-holdout cross validation approach in EDF-Expanded dataset.", "mention_start": 257, "mention_end": 278, "dataset_mention": "EDF-Expanded dataset"}, {"mentioned_in_paper": "827", "context_id": "167", "dataset_context": "The results of the sleep stage scoring using EDF-Expanded dataset with just the single-epoch networks are presented in Table S1 and S2.", "mention_start": 45, "mention_end": 65, "dataset_mention": "EDF-Expanded dataset"}, {"mentioned_in_paper": "827", "context_id": "177", "dataset_context": "The proposed architecture was trained using the EDF-20 and DREAMS datasets and evaluated by LOO cross validation method separately.", "mention_start": 44, "mention_end": 74, "dataset_mention": "the EDF-20 and DREAMS datasets"}, {"mentioned_in_paper": "827", "context_id": "178", "dataset_context": "The proposed method achieved accuracy of 93.9%, Kappa of 0.88 and MF1 of 83.34% for the EDF-20 dataset (The confusion matrix of the results for the EDF-20 dataset is presented in Table S3).", "mention_start": 83, "mention_end": 102, "dataset_mention": "the EDF-20 dataset"}, {"mentioned_in_paper": "827", "context_id": "178", "dataset_context": "The proposed method achieved accuracy of 93.9%, Kappa of 0.88 and MF1 of 83.34% for the EDF-20 dataset (The confusion matrix of the results for the EDF-20 dataset is presented in Table S3).", "mention_start": 143, "mention_end": 162, "dataset_mention": "the EDF-20 dataset"}, {"mentioned_in_paper": "827", "context_id": "179", "dataset_context": "By using the DREAMS dataset, after training and evaluating the proposed architecture, the network achieved accuracy of 88.09%, Kappa of 0.84 and MF1 of 86.96% in LOO cross validation (The confusion matrix of the results for the DREAMS dataset is presented in Table S4).", "mention_start": 9, "mention_end": 27, "dataset_mention": "the DREAMS dataset"}, {"mentioned_in_paper": "827", "context_id": "179", "dataset_context": "By using the DREAMS dataset, after training and evaluating the proposed architecture, the network achieved accuracy of 88.09%, Kappa of 0.84 and MF1 of 86.96% in LOO cross validation (The confusion matrix of the results for the DREAMS dataset is presented in Table S4).", "mention_start": 223, "mention_end": 242, "dataset_mention": "the DREAMS dataset"}, {"mentioned_in_paper": "827", "context_id": "180", "dataset_context": "Table 6 shows the overall performance comparison of the proposed method and some recent deep learning studies that used the Physionet EDF datasets including EDF-Expanded and EDF-20.", "mention_start": 120, "mention_end": 146, "dataset_mention": "the Physionet EDF datasets"}, {"mentioned_in_paper": "827", "context_id": "182", "dataset_context": "In the current study, for EDF-Expanded dataset this ratio is about 0.2.", "mention_start": 25, "mention_end": 46, "dataset_mention": "EDF-Expanded dataset"}, {"mentioned_in_paper": "827", "context_id": "187", "dataset_context": "To increase results comparability, the EDF-20 dataset was used which includes 20 records and evaluated using LOO cross validation method same as many other mentioned studies in Table 6.", "mention_start": 34, "mention_end": 53, "dataset_mention": " the EDF-20 dataset"}, {"mentioned_in_paper": "827", "context_id": "189", "dataset_context": "The results of using EDF-20 dataset in both condition of using EEG and EOG as well as single EEG channel are proposed in Table 6 (The confusion matrix of the results for using single channel EEG of the EDF-20 dataset is presented in Table S5).", "mention_start": 21, "mention_end": 35, "dataset_mention": "EDF-20 dataset"}, {"mentioned_in_paper": "827", "context_id": "189", "dataset_context": "The results of using EDF-20 dataset in both condition of using EEG and EOG as well as single EEG channel are proposed in Table 6 (The confusion matrix of the results for using single channel EEG of the EDF-20 dataset is presented in Table S5).", "mention_start": 198, "mention_end": 216, "dataset_mention": "the EDF-20 dataset"}, {"mentioned_in_paper": "827", "context_id": "191", "dataset_context": "The same experiments of using both EEG-EOG signals and just EEG signals were repeated using the DREAMS dataset (The confusion matrices of the both experiments are presented in Table S4  and S6).", "mention_start": 92, "mention_end": 110, "dataset_mention": "the DREAMS dataset"}, {"mentioned_in_paper": "827", "context_id": "192", "dataset_context": "Table 7 shows the results of the proposed method using DREAMS dataset in comparison to some other studies which used the same dataset.", "mention_start": 55, "mention_end": 69, "dataset_mention": "DREAMS dataset"}, {"mentioned_in_paper": "827", "context_id": "200", "dataset_context": "The proposed system was trained and tested using EDF-Expanded dataset and the record-holdout evaluation approach to increase the system's learned knowledge and interpretability by the user.", "mention_start": 49, "mention_end": 69, "dataset_mention": "EDF-Expanded dataset"}, {"mentioned_in_paper": "827", "context_id": "258", "dataset_context": "To study the effect of the Gabor layers on the proposed method, the Gabor layers replaced by one-dimensional convolution layers with the same length of 200 (2 seconds with sampling rate of 100 Hz) then the networks were trained and evaluated with the LOO cross validation using EDF-20 dataset.", "mention_start": 277, "mention_end": 292, "dataset_mention": "EDF-20 dataset"}, {"mentioned_in_paper": "827", "context_id": "272", "dataset_context": "Different cross validation approaches and datasets were considered to evaluate the performance of the proposed method and its generalizability.", "mention_start": 0, "mention_end": 50, "dataset_mention": "Different cross validation approaches and datasets"}, {"mentioned_in_paper": "828", "context_id": "191", "dataset_context": "To evaluate the performance of our method, three representative benchmark datasets are employed, namely, Yoochoose, Diginetica, Nowplaying:", "mention_start": 42, "mention_end": 82, "dataset_mention": " three representative benchmark datasets"}, {"mentioned_in_paper": "828", "context_id": "192", "dataset_context": "\u2022 Yoochoose 2 : The Yoochoose dataset is obtained from the RecSys Challenge 2015, which consists of six mouth click-streams of an E-commerce website.", "mention_start": 15, "mention_end": 37, "dataset_mention": " The Yoochoose dataset"}, {"mentioned_in_paper": "828", "context_id": "193", "dataset_context": "\u2022 Diginetica 3 : The Diginetica dataset comes from CIKM Cup 2016, containing anonymous transaction data within five months of an E-commerce platform, which is suitable for sessionbased recommendation.", "mention_start": 16, "mention_end": 39, "dataset_mention": " The Diginetica dataset"}, {"mentioned_in_paper": "828", "context_id": "194", "dataset_context": "\u2022 Nowplaying 4 : The Nowplaying dataset comes from music-related tweets [52], which describes the music listening behavior sequences of users.", "mention_start": 16, "mention_end": 39, "dataset_mention": " The Nowplaying dataset"}, {"mentioned_in_paper": "828", "context_id": "198", "dataset_context": "Further, for a session  =   1 ,   2 , ...,    , we use a sequence splitting preprocessing [25, 48] [48], we use the most recent portions 1/64 and 1/4 of the training sequences, denoted as \"\u210e1/64\" and \"\u210e1/4\" datasets, respectively.", "mention_start": 187, "mention_end": 215, "dataset_mention": "\"\u210e1/64\" and \"\u210e1/4\" datasets"}, {"mentioned_in_paper": "828", "context_id": "312", "dataset_context": "We conduct extensive experiments on Yoochoose, Diginetica and Nowplaying datasets to validated the effectiveness of the proposed RNMSR.", "mention_start": 46, "mention_end": 81, "dataset_mention": " Diginetica and Nowplaying datasets"}, {"mentioned_in_paper": "830", "context_id": "99", "dataset_context": "In addition, the temporal variation of action instances is significant across different videos, in particular for ActivityNet dataset.", "mention_start": 113, "mention_end": 133, "dataset_mention": "ActivityNet dataset"}, {"mentioned_in_paper": "830", "context_id": "117", "dataset_context": "Specifically, following the common practice, we employ a sliding window strategy with a fixed overlap ratio on THUMOS14 dataset and a re-scaling operation on ActivityNet-1.3 dataset.", "mention_start": 110, "mention_end": 127, "dataset_mention": "THUMOS14 dataset"}, {"mentioned_in_paper": "830", "context_id": "117", "dataset_context": "Specifically, following the common practice, we employ a sliding window strategy with a fixed overlap ratio on THUMOS14 dataset and a re-scaling operation on ActivityNet-1.3 dataset.", "mention_start": 157, "mention_end": 181, "dataset_mention": "ActivityNet-1.3 dataset"}, {"mentioned_in_paper": "830", "context_id": "204", "dataset_context": "Table 3 shows AR@AN on THUMOS14 dataset under different settings of the scaling factor.", "mention_start": 23, "mention_end": 39, "dataset_mention": "THUMOS14 dataset"}, {"mentioned_in_paper": "830", "context_id": "325", "dataset_context": "HACS Segments dataset [47] contain 50,000 untrimmed videos and share the same 200 action categories with ActivityNet-1.3 dataset [18].", "mention_start": 0, "mention_end": 21, "dataset_mention": "HACS Segments dataset"}, {"mentioned_in_paper": "830", "context_id": "325", "dataset_context": "HACS Segments dataset [47] contain 50,000 untrimmed videos and share the same 200 action categories with ActivityNet-1.3 dataset [18].", "mention_start": 105, "mention_end": 128, "dataset_mention": "ActivityNet-1.3 dataset"}, {"mentioned_in_paper": "830", "context_id": "326", "dataset_context": "To evaluate the quality of proposals, we calculate Average Recall with Average Number of proposals per video (AR@AN), and the Area under the AR vs AN curve (AUC) as metrics on HACS Segments dataset, which are the same as ActivityNet-1.3 dataset.", "mention_start": 175, "mention_end": 197, "dataset_mention": "HACS Segments dataset"}, {"mentioned_in_paper": "830", "context_id": "326", "dataset_context": "To evaluate the quality of proposals, we calculate Average Recall with Average Number of proposals per video (AR@AN), and the Area under the AR vs AN curve (AUC) as metrics on HACS Segments dataset, which are the same as ActivityNet-1.3 dataset.", "mention_start": 220, "mention_end": 244, "dataset_mention": "ActivityNet-1.3 dataset"}, {"mentioned_in_paper": "831", "context_id": "233", "dataset_context": "In Fig. 5a, the expected value E[f (x)] = 0.708 is the baseline SA value produced from the model and the dataset.", "mention_start": 86, "mention_end": 112, "dataset_mention": "the model and the dataset"}, {"mentioned_in_paper": "833", "context_id": "76", "dataset_context": "She classifies non-ordered multivariate data sets from which she creates discrete auditory events.", "mention_start": 15, "mention_end": 49, "dataset_mention": "non-ordered multivariate data sets"}, {"mentioned_in_paper": "835", "context_id": "90", "dataset_context": "As a precursor to the later exposition, suppose that the random swap occurs with a fixed probability p (not necessarily 0.5), yielding the logged datasets C i 1 the rate of clicks that documents get when they remain in position 1 and let \u01091,k k = 1 n 2 C j k be the rate of clicks when they get swapped to position k.", "mention_start": 125, "mention_end": 154, "dataset_mention": " yielding the logged datasets"}, {"mentioned_in_paper": "835", "context_id": "183", "dataset_context": "We used three ranking functions { f 1 , f 2 , f 3 } for defining the interventional sets, which were generated by using different learning methods and datasets.", "mention_start": 119, "mention_end": 159, "dataset_mention": "different learning methods and datasets"}, {"mentioned_in_paper": "836", "context_id": "3", "dataset_context": "Tested on real life molecule property datasets, our model consistently improves performance on large multitasked datasets over all baselines, both globally and on a per-task setting.", "mention_start": 10, "mention_end": 46, "dataset_mention": "real life molecule property datasets"}, {"mentioned_in_paper": "836", "context_id": "3", "dataset_context": "Tested on real life molecule property datasets, our model consistently improves performance on large multitasked datasets over all baselines, both globally and on a per-task setting.", "mention_start": 94, "mention_end": 121, "dataset_mention": "large multitasked datasets"}, {"mentioned_in_paper": "836", "context_id": "14", "dataset_context": "Either they contain only a few thousands or even hundreds of molecules, or they are heavily multi-tasked and scarcely labelled, so that even apparently large datasets can contain only a few handful of positively labelled molecules.", "mention_start": 140, "mention_end": 166, "dataset_mention": "apparently large datasets"}, {"mentioned_in_paper": "836", "context_id": "33", "dataset_context": "In particular, at the time this article is written, our model ranks first on the Open Graph Database leaderboard for the PCBA dataset, which is one of the largest benchmark for molecular bio-activity prediction.", "mention_start": 116, "mention_end": 133, "dataset_mention": "the PCBA dataset"}, {"mentioned_in_paper": "836", "context_id": "125", "dataset_context": "We performed experiments on five molecule classification datasets, from MoleculeNet (Wu et al., 2018) and Open Graph Benchmark (OGB) (Hu et al., 2020) collections, which cover a wide range of tasks:", "mention_start": 28, "mention_end": 65, "dataset_mention": "five molecule classification datasets"}, {"mentioned_in_paper": "836", "context_id": "128", "dataset_context": "\u2022 HIV is a single classification task dataset corresponding to the experimentally measured ability of a compound to inhibit the HIV virus replication.", "mention_start": 9, "mention_end": 45, "dataset_mention": "a single classification task dataset"}, {"mentioned_in_paper": "836", "context_id": "130", "dataset_context": "Table 2. Test performance of the different models and datasets.", "mention_start": 29, "mention_end": 62, "dataset_mention": "the different models and datasets"}, {"mentioned_in_paper": "836", "context_id": "132", "dataset_context": "Apart from our results, performances for the HIV and PCBA datasets are taken directly from the OGB leaderboard (Hu et al., 2020).", "mention_start": 40, "mention_end": 66, "dataset_mention": "the HIV and PCBA datasets"}, {"mentioned_in_paper": "836", "context_id": "136", "dataset_context": "For comparability with litterature, we use PRC-AUC as the metric for the two most skewed datasets, MUV and PCBA, while we use ROC-AUC on the others.", "mention_start": 68, "mention_end": 97, "dataset_mention": "the two most skewed datasets"}, {"mentioned_in_paper": "836", "context_id": "137", "dataset_context": "Finally, we augment the smallest datasets with PCBA, making them a supplementary task of the larger PCBA dataset.", "mention_start": 19, "mention_end": 41, "dataset_mention": "the smallest datasets"}, {"mentioned_in_paper": "836", "context_id": "137", "dataset_context": "Finally, we augment the smallest datasets with PCBA, making them a supplementary task of the larger PCBA dataset.", "mention_start": 88, "mention_end": 112, "dataset_mention": "the larger PCBA dataset"}, {"mentioned_in_paper": "836", "context_id": "144", "dataset_context": "The longest experiments are those on the PCBA dataset or datasets augmented by PCBA, which take approximately 6 hours.", "mention_start": 37, "mention_end": 53, "dataset_mention": "the PCBA dataset"}, {"mentioned_in_paper": "836", "context_id": "147", "dataset_context": "The results are displayed in table 2. Our model ranks first on the large PCBA dataset, by a clear margin, making it the leading model for PCBA in the OGB leaderboard when this paper was written.", "mention_start": 63, "mention_end": 85, "dataset_mention": "the large PCBA dataset"}, {"mentioned_in_paper": "836", "context_id": "150", "dataset_context": "We believe this is due to the fact that the smaller datasets are easier to overfit, so that the addition in expressive power in our model was ill-used to memorize the dataset instead of learning useful structural features.", "mention_start": 26, "mention_end": 60, "dataset_mention": "the fact that the smaller datasets"}, {"mentioned_in_paper": "836", "context_id": "150", "dataset_context": "We believe this is due to the fact that the smaller datasets are easier to overfit, so that the addition in expressive power in our model was ill-used to memorize the dataset instead of learning useful structural features.", "mention_start": 153, "mention_end": 174, "dataset_mention": "memorize the dataset"}, {"mentioned_in_paper": "836", "context_id": "152", "dataset_context": "As detailed in section 5.1, the training is performed on the training data of both datasets, but evaluation is performed only on the validation set of the nonaugmented dataset.", "mention_start": 150, "mention_end": 175, "dataset_mention": "the nonaugmented dataset"}, {"mentioned_in_paper": "836", "context_id": "153", "dataset_context": "Obviously, we observe a performance boost over the training on the non-augmented datasets.", "mention_start": 62, "mention_end": 89, "dataset_mention": "the non-augmented datasets"}, {"mentioned_in_paper": "836", "context_id": "154", "dataset_context": "Nonetheless, our model performs better on all augmented datasets compared to GINE.", "mention_start": 41, "mention_end": 64, "dataset_mention": "all augmented datasets"}, {"mentioned_in_paper": "836", "context_id": "162", "dataset_context": "In particular, our model ranks first on the PCBA dataset when this article is written.", "mention_start": 39, "mention_end": 56, "dataset_mention": "the PCBA dataset"}, {"mentioned_in_paper": "837", "context_id": "4", "dataset_context": "Extensive experiments demonstrate that our method can reduce latency by 30%-60% on KITTI and Waymo datasets.", "mention_start": 83, "mention_end": 107, "dataset_mention": "KITTI and Waymo datasets"}, {"mentioned_in_paper": "837", "context_id": "5", "dataset_context": "Specifically, the inference speed of our detector can reach 162 FPS and 30 FPS with negligible performance degradation on KITTI and Waymo datasets, respectively.", "mention_start": 121, "mention_end": 146, "dataset_mention": "KITTI and Waymo datasets"}, {"mentioned_in_paper": "837", "context_id": "32", "dataset_context": "To verify the efficiency of our method, we conduct extensive experiments on two typical datasets, i.e., KITTI [1] and Waymo [8].", "mention_start": 75, "mention_end": 96, "dataset_mention": "two typical datasets"}, {"mentioned_in_paper": "837", "context_id": "124", "dataset_context": "Dataset We evaluate our detector on two representative datasets: KITTI dataset [1] and Waymo dataset [8].", "mention_start": 36, "mention_end": 63, "dataset_mention": "two representative datasets"}, {"mentioned_in_paper": "837", "context_id": "124", "dataset_context": "Dataset We evaluate our detector on two representative datasets: KITTI dataset [1] and Waymo dataset [8].", "mention_start": 64, "mention_end": 78, "dataset_mention": " KITTI dataset"}, {"mentioned_in_paper": "837", "context_id": "124", "dataset_context": "Dataset We evaluate our detector on two representative datasets: KITTI dataset [1] and Waymo dataset [8].", "mention_start": 64, "mention_end": 100, "dataset_mention": " KITTI dataset [1] and Waymo dataset"}, {"mentioned_in_paper": "837", "context_id": "125", "dataset_context": "KITTI dataset includes  Evaluation metrics.", "mention_start": 0, "mention_end": 13, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "837", "context_id": "167", "dataset_context": "To verify the generalization of our method, we further evaluate the performance on Waymo [8] dataset.", "mention_start": 82, "mention_end": 100, "dataset_mention": "Waymo [8] dataset"}, {"mentioned_in_paper": "840", "context_id": "18", "dataset_context": "For instance, for scene classification on the MIT Indoor dataset, multi-scale VLAD, with only 4096 features, comfortably outperforms the mixture of FV and bag-of-parts, which relies on 221550 features (Gong et al., 2014).", "mention_start": 41, "mention_end": 64, "dataset_mention": "the MIT Indoor dataset"}, {"mentioned_in_paper": "840", "context_id": "180", "dataset_context": "As a first experiment, we consider the problem of classifying head orientation using the QMUL and HOCoffee datasets (Tosato et al., 2013).", "mention_start": 84, "mention_end": 115, "dataset_mention": "the QMUL and HOCoffee datasets"}, {"mentioned_in_paper": "840", "context_id": "181", "dataset_context": "The QMUL head dataset contains 19292 images of size 50 \u00d7 50, captured in an airport terminal.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The QMUL head dataset"}, {"mentioned_in_paper": "840", "context_id": "182", "dataset_context": "The HOCoffee dataset (see Fig. 1 for examples) contains 18117 head images of size 50 \u00d7 50.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The HOCoffee dataset"}, {"mentioned_in_paper": "840", "context_id": "186", "dataset_context": "2, we report the performance of kVLAD, sVLAD and nVLAD, as well as of WARCO and lE-VLAD, on the QMUL and HOCoffee datasets.", "mention_start": 91, "mention_end": 122, "dataset_mention": "the QMUL and HOCoffee datasets"}, {"mentioned_in_paper": "840", "context_id": "196", "dataset_context": "As a second task on the SPD manifold, we consider the problem of determining body orientation from images using the Human Orientation Classification (HOC) dataset (Tosato et al., 2013).", "mention_start": 111, "mention_end": 162, "dataset_mention": "the Human Orientation Classification (HOC) dataset"}, {"mentioned_in_paper": "840", "context_id": "197", "dataset_context": "The HOC dataset contains 11881 images of size 64 \u00d7 32 (see Fig. 2 for examples) and comprises 4 orientation classes (Front, Back, Left, and Right).", "mention_start": 0, "mention_end": 15, "dataset_mention": "The HOC dataset"}, {"mentioned_in_paper": "840", "context_id": "223", "dataset_context": "As a first experiment on the Grassmannian, we make use of the Ballet dataset (Wang and Mori, 2009).", "mention_start": 57, "mention_end": 76, "dataset_mention": "the Ballet dataset"}, {"mentioned_in_paper": "840", "context_id": "224", "dataset_context": "The Ballet dataset consists of 8 complex motion patterns performed by 3 subjects (see Fig. 3 for examples).", "mention_start": 0, "mention_end": 18, "dataset_mention": "The Ballet dataset"}, {"mentioned_in_paper": "840", "context_id": "237", "dataset_context": "For the task of object recognition from image-sets, we used the CIFAR dataset (Krizhevsky and Hinton, 2009).", "mention_start": 59, "mention_end": 77, "dataset_mention": "the CIFAR dataset"}, {"mentioned_in_paper": "840", "context_id": "238", "dataset_context": "The CIFAR dataset contains 60000 images (32 \u00d7 32 pixels) from 10 different object categories.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The CIFAR dataset"}, {"mentioned_in_paper": "840", "context_id": "248", "dataset_context": "As a last experiment on the Grassmannian, we evaluated the performance of our algorithms on the task of pose categorization using the CMU-PIE face dataset (Sim et al., 2003).", "mention_start": 129, "mention_end": 154, "dataset_mention": "the CMU-PIE face dataset"}, {"mentioned_in_paper": "840", "context_id": "249", "dataset_context": "The CMU-PIE face dataset contains images of 67 subjects under 13 different poses and 21 different illuminations (see Fig. 4 for examples).", "mention_start": 0, "mention_end": 24, "dataset_mention": "The CMU-PIE face dataset"}, {"mentioned_in_paper": "840", "context_id": "260", "dataset_context": "Interestingly, however, we also evaluated sVLAD using local SPD matrices instead of local subspace, and achieved an accuracy of 93.4% on the Ballet dataset.", "mention_start": 136, "mention_end": 155, "dataset_mention": "the Ballet dataset"}, {"mentioned_in_paper": "840", "context_id": "273", "dataset_context": "Table 5 compares the recognition accuracies of nVLAD, fVLAD and sVLAD against VLAD and the state-of-the-art methods augmented Latent Dirichlet Allocation (aLDA) (Sharan et al., 2013), Multi-Scale Spike-and-Slab Sparse Coding (MS4C) (Li, 2014), and Describable attributes (DTD RBF ) (Cimpoi et al., 2014) on the FMD dataset.", "mention_start": 306, "mention_end": 322, "dataset_mention": "the FMD dataset"}, {"mentioned_in_paper": "841", "context_id": "7", "dataset_context": "Experiments demonstrate that the proposed method outperforms the state-of-the-art online methods (taking image-level input) on the challenging Youtube-VIS dataset [43].", "mention_start": 143, "mention_end": 162, "dataset_mention": "Youtube-VIS dataset"}, {"mentioned_in_paper": "841", "context_id": "38", "dataset_context": "We evaluate our method on the challenging video instance segmentation datasets, Youtube-VIS [43], against the state-of-the-art trackby-detect methods [4], [43], [34], [41], [44], and state-of-theart offline methods, [2], [1].", "mention_start": 42, "mention_end": 78, "dataset_mention": "video instance segmentation datasets"}, {"mentioned_in_paper": "841", "context_id": "41", "dataset_context": "\u2022 A new bottom-up video instance segmentation model equipped with multi-scale temporal context fusion modules, achieving superior performance over most online methods on Youtube-VIS dataset.", "mention_start": 169, "mention_end": 189, "dataset_mention": "Youtube-VIS dataset"}, {"mentioned_in_paper": "841", "context_id": "158", "dataset_context": "IV. EXPERIMENTS In this section, we present our results on YouTube-VIS [43] dataset, which contains 2238 training, 302 validation and 343 test video clips.", "mention_start": 58, "mention_end": 83, "dataset_mention": "YouTube-VIS [43] dataset"}, {"mentioned_in_paper": "841", "context_id": "190", "dataset_context": "The small (area < 32 2 ), medium (32 2 < area < 96 2 ), large (96 2 < area) instance are defined as the COCO dataset [24].", "mention_start": 99, "mention_end": 116, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "841", "context_id": "277", "dataset_context": "The comprehensive experiments on the Youtube-VIS dataset verify the effectiveness of the proposed approach and show that proposed method outperforms current image-level state-of-the-art works on video instance segmentation task.", "mention_start": 33, "mention_end": 56, "dataset_mention": "the Youtube-VIS dataset"}, {"mentioned_in_paper": "843", "context_id": "0", "dataset_context": "Feature-based image matching has been significantly improved through the use of deep learning and new large datasets.", "mention_start": 80, "mention_end": 116, "dataset_mention": "deep learning and new large datasets"}, {"mentioned_in_paper": "843", "context_id": "77", "dataset_context": "The HardNet model variants were all implemented in PyTorch [32] using the same training procedure 1 and the HardNet++ weights were trained on the Liberty, Yosemite, and Notredame datasets, while all the models proposed in this paper are trained solely on the Liberty dataset from random initialisation.", "mention_start": 164, "mention_end": 187, "dataset_mention": " and Notredame datasets"}, {"mentioned_in_paper": "843", "context_id": "77", "dataset_context": "The HardNet model variants were all implemented in PyTorch [32] using the same training procedure 1 and the HardNet++ weights were trained on the Liberty, Yosemite, and Notredame datasets, while all the models proposed in this paper are trained solely on the Liberty dataset from random initialisation.", "mention_start": 254, "mention_end": 274, "dataset_mention": "the Liberty dataset"}, {"mentioned_in_paper": "843", "context_id": "98", "dataset_context": "The model is jointly trained using the pseudo ground truth labels on the MS-COCO [36] dataset, while the evaluation is performed using HPatches [31] benchmark.", "mention_start": 69, "mention_end": 93, "dataset_mention": "the MS-COCO [36] dataset"}, {"mentioned_in_paper": "844", "context_id": "2", "dataset_context": "Hence, it becomes hard to train recognition systems owing to lack of proper dataset.", "mention_start": 68, "mention_end": 83, "dataset_mention": "proper dataset"}, {"mentioned_in_paper": "844", "context_id": "15", "dataset_context": "A lot of Indic scripts remain, for which no formal datasets have been created.", "mention_start": 40, "mention_end": 59, "dataset_mention": "no formal datasets"}, {"mentioned_in_paper": "844", "context_id": "28", "dataset_context": "Although, a number of work [23, 35] have been developed in this area to expand the natural dataset but they still require natural dataset to work upon.", "mention_start": 71, "mention_end": 98, "dataset_mention": "expand the natural dataset"}, {"mentioned_in_paper": "844", "context_id": "28", "dataset_context": "Although, a number of work [23, 35] have been developed in this area to expand the natural dataset but they still require natural dataset to work upon.", "mention_start": 121, "mention_end": 137, "dataset_mention": "natural dataset"}, {"mentioned_in_paper": "844", "context_id": "37", "dataset_context": "This kind of dataset can significantly reduce the time and resources spent on creating a natural dataset.", "mention_start": 87, "mention_end": 104, "dataset_mention": "a natural dataset"}, {"mentioned_in_paper": "844", "context_id": "38", "dataset_context": "Fig. 1 illustrates the block diagram of text recognition system using synthetic/natural dataset.", "mention_start": 70, "mention_end": 95, "dataset_mention": "synthetic/natural dataset"}, {"mentioned_in_paper": "844", "context_id": "68", "dataset_context": "This method though keeps the dataset close to natural handwriting, requires a great amount of sample from a single user.", "mention_start": 0, "mention_end": 36, "dataset_mention": "This method though keeps the dataset"}, {"mentioned_in_paper": "844", "context_id": "78", "dataset_context": "One of the main reasons is not availability of proper dataset for training recognition system.", "mention_start": 47, "mention_end": 61, "dataset_mention": "proper dataset"}, {"mentioned_in_paper": "844", "context_id": "295", "dataset_context": "To compare the performance, we have used IAM dataset [25] for evaluation.", "mention_start": 40, "mention_end": 52, "dataset_mention": "IAM dataset"}, {"mentioned_in_paper": "844", "context_id": "301", "dataset_context": "Table VIII shows the recognition performance using synthetic and handwritten dataset.", "mention_start": 51, "mention_end": 84, "dataset_mention": "synthetic and handwritten dataset"}, {"mentioned_in_paper": "844", "context_id": "303", "dataset_context": "In [35], authors showed an improvement of 2.69% in IAM dataset by adding synthetic data which were generated by perturbation model from handwritten data only.", "mention_start": 50, "mention_end": 62, "dataset_mention": "IAM dataset"}, {"mentioned_in_paper": "844", "context_id": "304", "dataset_context": "The authors of [23] used a subset of IAM dataset and applied synthetic data using n-tuples of characters and obtained 0.3% improvement.", "mention_start": 37, "mention_end": 48, "dataset_mention": "IAM dataset"}, {"mentioned_in_paper": "845", "context_id": "7", "dataset_context": "Experiment results show that our method outperforms previous methods on Scene Flow, KITTI 2012, and KITTI 2015 datasets.", "mention_start": 95, "mention_end": 119, "dataset_mention": " and KITTI 2015 datasets"}, {"mentioned_in_paper": "845", "context_id": "37", "dataset_context": "3) Our method achieves better performance than previous methods on Scene Flow, KITTI 2012, and KITTI 2015 datasets.", "mention_start": 90, "mention_end": 114, "dataset_mention": " and KITTI 2015 datasets"}, {"mentioned_in_paper": "845", "context_id": "132", "dataset_context": "In this section, we evaluate our proposed stereo models on Scene Flow datasets [14] and the KITTI dataset [3, 16].", "mention_start": 58, "mention_end": 78, "dataset_mention": "Scene Flow datasets"}, {"mentioned_in_paper": "845", "context_id": "132", "dataset_context": "In this section, we evaluate our proposed stereo models on Scene Flow datasets [14] and the KITTI dataset [3, 16].", "mention_start": 58, "mention_end": 105, "dataset_mention": "Scene Flow datasets [14] and the KITTI dataset"}, {"mentioned_in_paper": "845", "context_id": "138", "dataset_context": "For Scene Flow datasets, the evaluation metrics is usually the end-point error (EPE), which is the mean average disparity error in pixels.", "mention_start": 4, "mention_end": 23, "dataset_mention": "Scene Flow datasets"}, {"mentioned_in_paper": "845", "context_id": "146", "dataset_context": "For Scene Flow datasets, we train the stereo networks for 16 epochs in total.", "mention_start": 4, "mention_end": 23, "dataset_mention": "Scene Flow datasets"}, {"mentioned_in_paper": "845", "context_id": "149", "dataset_context": "To test on Scene Flow datasets, the full images of size 960\u00d7540 are input to the network for disparity prediction.", "mention_start": 11, "mention_end": 30, "dataset_mention": "Scene Flow datasets"}, {"mentioned_in_paper": "845", "context_id": "150", "dataset_context": "We set the maximum disparity value as D max = 192 following PSMNet [2] for Scene Flow datasets.", "mention_start": 75, "mention_end": 94, "dataset_mention": "Scene Flow datasets"}, {"mentioned_in_paper": "845", "context_id": "153", "dataset_context": "For KITTI 2015 and KITTI 2012, we fine-tune the network pre-trained on Scene Flow datasets for another 300 epochs.", "mention_start": 70, "mention_end": 90, "dataset_mention": "Scene Flow datasets"}, {"mentioned_in_paper": "845", "context_id": "155", "dataset_context": "For testing on KITTI datasets, we first pad zeros on the top and the right side of the images to make the inputs in size 1248\u00d7384.", "mention_start": 15, "mention_end": 29, "dataset_mention": "KITTI datasets"}, {"mentioned_in_paper": "845", "context_id": "165", "dataset_context": "From Table 2 Flow datasets and 5.8% on KITTI 2015 compared with the model Cat64-original-hg (with the hourglass module in [2]).", "mention_start": 5, "mention_end": 26, "dataset_mention": "Table 2 Flow datasets"}, {"mentioned_in_paper": "845", "context_id": "188", "dataset_context": "Experiments demonstrated the effectiveness of our proposed method on the Scene Flow datasets and the KITTI dataset.", "mention_start": 69, "mention_end": 92, "dataset_mention": "the Scene Flow datasets"}, {"mentioned_in_paper": "845", "context_id": "188", "dataset_context": "Experiments demonstrated the effectiveness of our proposed method on the Scene Flow datasets and the KITTI dataset.", "mention_start": 69, "mention_end": 114, "dataset_mention": "the Scene Flow datasets and the KITTI dataset"}, {"mentioned_in_paper": "847", "context_id": "295", "dataset_context": "The model performs the best on DBook and MovieLens when  is 0.8, while  is 1 on  Yelp dataset.", "mention_start": 77, "mention_end": 93, "dataset_mention": " Yelp dataset"}, {"mentioned_in_paper": "847", "context_id": "298", "dataset_context": "Towards this end, we randomly select 5 users in the same task from the Yelp dataset along with their interacted items and visualize the learned user embedding vectors derived from NGCF and TMAG with t-SNE algorithm [46].", "mention_start": 66, "mention_end": 83, "dataset_mention": "the Yelp dataset"}, {"mentioned_in_paper": "849", "context_id": "12", "dataset_context": "Hypersphere embeddings [34] of different faces from the IJB-C dataset (colored clusters) visualized as circles with area proportional to recognizability, using t-SNE [22].", "mention_start": 52, "mention_end": 69, "dataset_mention": "the IJB-C dataset"}, {"mentioned_in_paper": "849", "context_id": "84", "dataset_context": "The observation that UIs cluster together can be illustrated using face embeddings [34] on images of 8 randomly sampled celebrities from the IJB-C dataset.", "mention_start": 137, "mention_end": 154, "dataset_mention": "the IJB-C dataset"}, {"mentioned_in_paper": "849", "context_id": "93", "dataset_context": "For natural LR images, we run clustering on face crops detected by a state-of-the-art FD model on the WIDERFace [42] dataset.", "mention_start": 97, "mention_end": 124, "dataset_mention": "the WIDERFace [42] dataset"}, {"mentioned_in_paper": "849", "context_id": "107", "dataset_context": "These UI images can be obtained by either clustering artificial degraded training data of the embedding model or large-scale in-the-wild face detection datasets such as WIDERFace [42] and taking the resultant large cluster.", "mention_start": 113, "mention_end": 160, "dataset_mention": "large-scale in-the-wild face detection datasets"}, {"mentioned_in_paper": "849", "context_id": "110", "dataset_context": "We illustrate the correlation between ERS and recognizability in Fig. 4 by taking images from the IJB-C dataset and grouping them by their ERS scores.", "mention_start": 94, "mention_end": 111, "dataset_mention": "the IJB-C dataset"}, {"mentioned_in_paper": "849", "context_id": "145", "dataset_context": "We train a face embedding model using CosFace [34] loss, ResNet-101 (R101) [9] backbone and DeepGlint-Face dataset (including MS1M-DeepGlint and Asian-DeepGlint) [5].", "mention_start": 56, "mention_end": 114, "dataset_mention": " ResNet-101 (R101) [9] backbone and DeepGlint-Face dataset"}, {"mentioned_in_paper": "849", "context_id": "150", "dataset_context": "IJB-C dataset suite contains in-the-wild celebrity media, including photos and videos, and multiple predefined benchmark protocols.", "mention_start": 0, "mention_end": 13, "dataset_mention": "IJB-C dataset"}, {"mentioned_in_paper": "849", "context_id": "188", "dataset_context": "We compare UIC generated from the Perturbed DeepGlint Subset (PDS) and face detection datasets WIDERFace and FDDB [13].", "mention_start": 30, "mention_end": 94, "dataset_mention": "the Perturbed DeepGlint Subset (PDS) and face detection datasets"}, {"mentioned_in_paper": "849", "context_id": "190", "dataset_context": "It can be seen that ERS is not sensitive to the sources dataset.", "mention_start": 44, "mention_end": 63, "dataset_mention": "the sources dataset"}, {"mentioned_in_paper": "849", "context_id": "211", "dataset_context": "We perform reid embedding clustering on Market1501 [44] dataset which contains low recognizability examples labeled \"junk\" and \"distractors\" in its gallery set.", "mention_start": 40, "mention_end": 63, "dataset_mention": "Market1501 [44] dataset"}, {"mentioned_in_paper": "849", "context_id": "212", "dataset_context": "And likewise for partially perturbed Deepfashion [19] In-Shop dataset (most image retrieval datasets do not contain natural quality corruption, so we manually perturb the recognizability, similar to Fig. 3).", "mention_start": 17, "mention_end": 69, "dataset_mention": "partially perturbed Deepfashion [19] In-Shop dataset"}, {"mentioned_in_paper": "849", "context_id": "212", "dataset_context": "And likewise for partially perturbed Deepfashion [19] In-Shop dataset (most image retrieval datasets do not contain natural quality corruption, so we manually perturb the recognizability, similar to Fig. 3).", "mention_start": 17, "mention_end": 100, "dataset_mention": "partially perturbed Deepfashion [19] In-Shop dataset (most image retrieval datasets"}, {"mentioned_in_paper": "849", "context_id": "220", "dataset_context": "While we acknowledge the presence and importance of statistical and algorithmic biases, in this paper we focus on additional issues that exist even before any dataset is collected, and affects systems that are not trained on data.", "mention_start": 147, "mention_end": 166, "dataset_mention": "before any dataset"}, {"mentioned_in_paper": "849", "context_id": "249", "dataset_context": "We use images from the TinyFace [37] dataset to generate a 1v1 face verification protocol, test on it using ERS as the recognizability measure to select the best threshold (Table. 11).", "mention_start": 19, "mention_end": 44, "dataset_mention": "the TinyFace [37] dataset"}, {"mentioned_in_paper": "850", "context_id": "57", "dataset_context": "These works divide the multiple source domains into multiple meta-train data sets and one meta-test data set, which mimics the domain gap encountered during testing.", "mention_start": 52, "mention_end": 81, "dataset_mention": "multiple meta-train data sets"}, {"mentioned_in_paper": "850", "context_id": "57", "dataset_context": "These works divide the multiple source domains into multiple meta-train data sets and one meta-test data set, which mimics the domain gap encountered during testing.", "mention_start": 52, "mention_end": 108, "dataset_mention": "multiple meta-train data sets and one meta-test data set"}, {"mentioned_in_paper": "850", "context_id": "179", "dataset_context": "We conduct extensive experiments on public ReID datasets, namely Market1501 [61], DukeMTMC-ReID [62], CUHK03 [23] and MSMT17 [50].", "mention_start": 36, "mention_end": 56, "dataset_mention": "public ReID datasets"}, {"mentioned_in_paper": "850", "context_id": "182", "dataset_context": "The same as [4, 3, 56], all images in each source dataset are used for training regardless of the train or test splits in its own protocol.", "mention_start": 37, "mention_end": 57, "dataset_mention": "each source dataset"}, {"mentioned_in_paper": "850", "context_id": "188", "dataset_context": "This setting selects one dataset from the four for testing and uses the remaining datasets for training.", "mention_start": 0, "mention_end": 32, "dataset_mention": "This setting selects one dataset"}, {"mentioned_in_paper": "850", "context_id": "190", "dataset_context": "This protocol includes the M and D datasets.", "mention_start": 23, "mention_end": 43, "dataset_mention": "the M and D datasets"}, {"mentioned_in_paper": "850", "context_id": "246", "dataset_context": "As Cam-Style only provides synthesized images on the Market1501 and DukeMTMC datasets, we perform comparisons on D\u2192M and M\u2192D tasks.", "mention_start": 49, "mention_end": 85, "dataset_mention": "the Market1501 and DukeMTMC datasets"}, {"mentioned_in_paper": "851", "context_id": "5", "dataset_context": "Based on the extensive YouTube8M dataset, we provide an in-depth evaluation and analysis of their behaviour.", "mention_start": 9, "mention_end": 40, "dataset_mention": "the extensive YouTube8M dataset"}, {"mentioned_in_paper": "851", "context_id": "7", "dataset_context": "The performance of the ensemble of classifiers was also evaluated on the HMDB51 and UCF101 datasets, and show that the resulting method achieves comparable accuracy with state-ofthe-art methods using similar input features.", "mention_start": 69, "mention_end": 99, "dataset_mention": "the HMDB51 and UCF101 datasets"}, {"mentioned_in_paper": "851", "context_id": "18", "dataset_context": "The associated YouTube-8M (v.2) dataset contains approximately 7 million individual video clips, corresponding to almost half a million hours (totalling 50 years!),", "mention_start": 15, "mention_end": 39, "dataset_mention": "YouTube-8M (v.2) dataset"}, {"mentioned_in_paper": "851", "context_id": "36", "dataset_context": "Based on the extensive YouTube8M dataset, we study and comparatively evaluate a broad range of deep architectures, including designs based on recurrent networks arXiv:1807.01026v3", "mention_start": 9, "mention_end": 40, "dataset_mention": "the extensive YouTube8M dataset"}, {"mentioned_in_paper": "851", "context_id": "154", "dataset_context": "The complete Youtube-8M dataset consists of approximately 7 million Youtube videos, each approximately 2-5 minutes in length, with at least 1000 views each.", "mention_start": 0, "mention_end": 31, "dataset_mention": "The complete Youtube-8M dataset"}, {"mentioned_in_paper": "851", "context_id": "221", "dataset_context": "Nonpairwise measures combine performances for all classifiers throughout the dataset into a single measure.", "mention_start": 46, "mention_end": 84, "dataset_mention": "all classifiers throughout the dataset"}, {"mentioned_in_paper": "851", "context_id": "310", "dataset_context": "For each of these approaches, we describe results on our validation dataset and the Kaggle test dataset.", "mention_start": 52, "mention_end": 103, "dataset_mention": "our validation dataset and the Kaggle test dataset"}, {"mentioned_in_paper": "851", "context_id": "320", "dataset_context": "The ensembling coefficients will be learnt on the ensemble-train dataset and evaluated on the ensemble-test partition.", "mention_start": 46, "mention_end": 72, "dataset_mention": "the ensemble-train dataset"}, {"mentioned_in_paper": "851", "context_id": "328", "dataset_context": "This was confirmed by removing the LSTM model from the ensemble, and observing that the resulting performances on the Youtube8M dataset reported in Sections VI-B and VI-C were found to be very similar.", "mention_start": 113, "mention_end": 135, "dataset_mention": "the Youtube8M dataset"}, {"mentioned_in_paper": "851", "context_id": "362", "dataset_context": "A baseline on the unseen Kaggle test dataset is obtained by averaging the base DNN results and uploading to the Kaggle website.", "mention_start": 14, "mention_end": 44, "dataset_mention": "the unseen Kaggle test dataset"}, {"mentioned_in_paper": "851", "context_id": "368", "dataset_context": "The improvements obtained in the ensemble dataset can be verified by ensembling the unseen Kaggle test data using the learnt weights.", "mention_start": 29, "mention_end": 49, "dataset_mention": "the ensemble dataset"}, {"mentioned_in_paper": "851", "context_id": "384", "dataset_context": "UCF101 AND HMDB51 In order to allow for comparison with existing work, we have performed transfer learning on the DNNs used for the Youtube8M dataset on two benchmark datasets: UCF101 [33] and HMDB51 [20].", "mention_start": 127, "mention_end": 149, "dataset_mention": "the Youtube8M dataset"}, {"mentioned_in_paper": "851", "context_id": "386", "dataset_context": "The UCF101 dataset consists of 101 action categories with 13320 videos.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The UCF101 dataset"}, {"mentioned_in_paper": "851", "context_id": "387", "dataset_context": "The HMDB51 dataset contains 51 action categories, with 7000 videos.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The HMDB51 dataset"}, {"mentioned_in_paper": "851", "context_id": "389", "dataset_context": "For both the UCF101 and HMDB51 datasets, we have selected the best DNNs from the Youtube8M dataset for use in classification: gated VLAD, gated RVLAD, gated netFV, gated BOW, Gated FC 10K, Gated ResNet 8K and Gated ResNet 10K.", "mention_start": 4, "mention_end": 39, "dataset_mention": "both the UCF101 and HMDB51 datasets"}, {"mentioned_in_paper": "851", "context_id": "389", "dataset_context": "For both the UCF101 and HMDB51 datasets, we have selected the best DNNs from the Youtube8M dataset for use in classification: gated VLAD, gated RVLAD, gated netFV, gated BOW, Gated FC 10K, Gated ResNet 8K and Gated ResNet 10K.", "mention_start": 76, "mention_end": 98, "dataset_mention": "the Youtube8M dataset"}, {"mentioned_in_paper": "851", "context_id": "391", "dataset_context": "The remaining layers prior to the gating layer were initialised with the weights trained using the Youtube8M dataset.", "mention_start": 95, "mention_end": 116, "dataset_mention": "the Youtube8M dataset"}, {"mentioned_in_paper": "851", "context_id": "397", "dataset_context": "The ensembled system had an accuracy of 84% on the UCF101 dataset.", "mention_start": 47, "mention_end": 65, "dataset_mention": "the UCF101 dataset"}, {"mentioned_in_paper": "851", "context_id": "401", "dataset_context": "The performance of 84% for the UCF101 dataset is comparable to existing state-of-the art methods when RGB full-frame features are used: 83.35%(MVSV) [5], 84.78%(Conv.", "mention_start": 27, "mention_end": 45, "dataset_mention": "the UCF101 dataset"}, {"mentioned_in_paper": "851", "context_id": "403", "dataset_context": "In terms of the HMDB51 dataset, we have found that the ensemble of DNNs had an class accuracy of 52.6%.", "mention_start": 12, "mention_end": 30, "dataset_mention": "the HMDB51 dataset"}, {"mentioned_in_paper": "851", "context_id": "405", "dataset_context": "As with the UCF101 dataset, the ResNet archictures again obtains the highest accuracies.", "mention_start": 8, "mention_end": 26, "dataset_mention": "the UCF101 dataset"}, {"mentioned_in_paper": "851", "context_id": "409", "dataset_context": "Interestingly, we find that for both the UCF101 and HMDB51 datasets, using a compact representation of 1152-    D feature vectors for each 1-second duration of visual information in a video can result in classification accuracies comparable to when the entire RGB image of every video frame is used.", "mention_start": 31, "mention_end": 67, "dataset_mention": "both the UCF101 and HMDB51 datasets"}, {"mentioned_in_paper": "851", "context_id": "413", "dataset_context": "This allows the methods and their ensemble used in this paper to scale well to extremely large datasets such as Youtube8M, both in terms of computational and storage requirements.", "mention_start": 79, "mention_end": 103, "dataset_mention": "extremely large datasets"}, {"mentioned_in_paper": "851", "context_id": "417", "dataset_context": "Since the temporal agnostic models are clusterbased and trained on the much longer Youtube8M dataset, the number of clusters used will likely be too many for a clips that are 1/50 the length.", "mention_start": 67, "mention_end": 100, "dataset_mention": "the much longer Youtube8M dataset"}, {"mentioned_in_paper": "851", "context_id": "436", "dataset_context": "Additionally, we have performed transfer learning on existing DNNs from the above on the UCF101 and HMDB51 datasets, and demonstrated that our methods are similar in performance to state-of-the-art methods despite using featuresvectors that are considerably more compact when compared with full RGB frames.", "mention_start": 84, "mention_end": 115, "dataset_mention": "the UCF101 and HMDB51 datasets"}, {"mentioned_in_paper": "853", "context_id": "165", "dataset_context": "MSCOCO [39] is the most widely-used object detection evaluation dataset, which consists of 118,287 training samples (trainval35k), 5,000 validation samples (val5k) and 40,670 test samples (test-dev) in its 2017 version.", "mention_start": 15, "mention_end": 71, "dataset_mention": "the most widely-used object detection evaluation dataset"}, {"mentioned_in_paper": "854", "context_id": "33", "dataset_context": "In a second phase, once we have obtained the raw data from the references, we processed the datasets.", "mention_start": 74, "mention_end": 100, "dataset_mention": " we processed the datasets"}, {"mentioned_in_paper": "857", "context_id": "173", "dataset_context": "It is not easy to directly evaluate visual pattern mining works due to the lack of well-annotated datasets for this task.", "mention_start": 83, "mention_end": 106, "dataset_mention": "well-annotated datasets"}, {"mentioned_in_paper": "857", "context_id": "185", "dataset_context": "Instead, we notice that the currently available datasets for object detection can be used to evaluate pattern mining works.", "mention_start": 8, "mention_end": 56, "dataset_mention": " we notice that the currently available datasets"}, {"mentioned_in_paper": "857", "context_id": "214", "dataset_context": "We use the MITIndoor dataset for scene classification, which has 67 classes of indoor images.", "mention_start": 7, "mention_end": 28, "dataset_mention": "the MITIndoor dataset"}, {"mentioned_in_paper": "857", "context_id": "237", "dataset_context": "The CUB-200 dataset has 200 classes of different birds, and the Stanford Dogs dataset has 120 categories of different dogs.", "mention_start": 0, "mention_end": 19, "dataset_mention": "The CUB-200 dataset"}, {"mentioned_in_paper": "857", "context_id": "237", "dataset_context": "The CUB-200 dataset has 200 classes of different birds, and the Stanford Dogs dataset has 120 categories of different dogs.", "mention_start": 55, "mention_end": 85, "dataset_mention": " and the Stanford Dogs dataset"}, {"mentioned_in_paper": "857", "context_id": "242", "dataset_context": "We notice that some recent works reported significantly high performance by leveraging the manually labeled bounding box information, such as 82.0% reported by PD [15] on the CUB-200 dataset.", "mention_start": 170, "mention_end": 190, "dataset_mention": "the CUB-200 dataset"}, {"mentioned_in_paper": "860", "context_id": "133", "dataset_context": "It is observed from the reviewed papers that publicly available retinal datasets are frequently used to prove the effectiveness of the proposed method or make comparisons between different approaches.", "mention_start": 20, "mention_end": 80, "dataset_mention": "the reviewed papers that publicly available retinal datasets"}, {"mentioned_in_paper": "860", "context_id": "195", "dataset_context": "On a 821-cell dataset, an accuracy value of 86.45% is finally achieved.", "mention_start": 3, "mention_end": 21, "dataset_mention": "a 821-cell dataset"}, {"mentioned_in_paper": "860", "context_id": "248", "dataset_context": "The best results are achieved on Herlev public dataset, yielding 0.93 Zijdenbos similarity index (ZSI) [61] of the nuclei segmentation.", "mention_start": 33, "mention_end": 54, "dataset_mention": "Herlev public dataset"}, {"mentioned_in_paper": "860", "context_id": "376", "dataset_context": "On the open source ICPR MITOSIS 2014 dataset, the proposed classification methodology is found to have F-score 0.437.", "mention_start": 3, "mention_end": 44, "dataset_mention": "the open source ICPR MITOSIS 2014 dataset"}, {"mentioned_in_paper": "860", "context_id": "381", "dataset_context": "The effectiveness of the model is verified on micro image sequence data set of pluripotent stem cells from University of California, Riverside, USA, yielding an accuracy value of 0.9346.", "mention_start": 46, "mention_end": 75, "dataset_mention": "micro image sequence data set"}, {"mentioned_in_paper": "860", "context_id": "387", "dataset_context": "On the Camelyon16 dataset, including 270 WSIs for training, 130 tumor WSIs for testing, an average free response receiver operating characteristic (FROC) score of 0.8096 is achieved finally.", "mention_start": 3, "mention_end": 25, "dataset_mention": "the Camelyon16 dataset"}, {"mentioned_in_paper": "860", "context_id": "399", "dataset_context": "In the experiment, lung cancer and MultiOrgan dataset are used for testing, both achieving accuracy over 98%.", "mention_start": 18, "mention_end": 53, "dataset_mention": " lung cancer and MultiOrgan dataset"}, {"mentioned_in_paper": "860", "context_id": "418", "dataset_context": "Researchers test the performance of HCRF on a public gastric histopathological image dataset containing 560 images, achieving 78.91% segmentation accuracy.", "mention_start": 44, "mention_end": 92, "dataset_mention": "a public gastric histopathological image dataset"}, {"mentioned_in_paper": "860", "context_id": "455", "dataset_context": "On the DRIVE dataset, this method yields sensitivity 78.5%, specificity 96.7%.", "mention_start": 3, "mention_end": 20, "dataset_mention": "the DRIVE dataset"}, {"mentioned_in_paper": "860", "context_id": "458", "dataset_context": "The authors manage to achieve over 96% accuracy and over 72% on DRIVE, STARE, HRF and CHASEDB1 four datasets.", "mention_start": 77, "mention_end": 108, "dataset_mention": " HRF and CHASEDB1 four datasets"}, {"mentioned_in_paper": "860", "context_id": "486", "dataset_context": "This method is verified on several retina databases and yields an accuracy of 100% in DRIVE, MESSID, DIARETDB, and DRION dataset.", "mention_start": 110, "mention_end": 128, "dataset_mention": " and DRION dataset"}, {"mentioned_in_paper": "860", "context_id": "492", "dataset_context": "No matter tested in private dataset or DRIONS-DB, RIM-ONE v.3, and DRISHTI-GS public dataset, Dice coefficient values all achieve over 95%.", "mention_start": 62, "mention_end": 92, "dataset_mention": " and DRISHTI-GS public dataset"}, {"mentioned_in_paper": "860", "context_id": "496", "dataset_context": "In the experiment, ISIC 2017 [104] and PH2 [105] public datasets are used for testing, and a mean Dice coefficient of 94.14% is finally achieved.", "mention_start": 18, "mention_end": 64, "dataset_mention": " ISIC 2017 [104] and PH2 [105] public datasets"}, {"mentioned_in_paper": "860", "context_id": "503", "dataset_context": "The classification accuracy, recall and AUC scores of 98%, 98.5%, and 99% are achieved on HAM10000 dataset of over 10000 images.", "mention_start": 89, "mention_end": 106, "dataset_mention": "HAM10000 dataset"}, {"mentioned_in_paper": "860", "context_id": "573", "dataset_context": "This method is evaluated on the MR dataset, yielding mean Dice coefficients of nearly 90%.", "mention_start": 28, "mention_end": 42, "dataset_mention": "the MR dataset"}, {"mentioned_in_paper": "860", "context_id": "582", "dataset_context": "They finally achieve state-of-the-art performance on the MSRC-1 and CorelB datasets.", "mention_start": 53, "mention_end": 83, "dataset_mention": "the MSRC-1 and CorelB datasets"}, {"mentioned_in_paper": "861", "context_id": "13", "dataset_context": "Sample skeletal joints (the red dots joined by blue lines for visualization) for two actions, draw circle (top) and cross arms in the chest (bottom) from the UTD dataset [5].", "mention_start": 153, "mention_end": 169, "dataset_mention": "the UTD dataset"}, {"mentioned_in_paper": "861", "context_id": "23", "dataset_context": "[cs.CV] 1 Nov 2016 Kinect and MSRC 12 datasets and comparisons with the state-of-the-art shows the effectiveness of the proposed approach for both offline and online action recognition tasks.", "mention_start": 0, "mention_end": 46, "dataset_mention": "[cs.CV] 1 Nov 2016 Kinect and MSRC 12 datasets"}, {"mentioned_in_paper": "861", "context_id": "111", "dataset_context": "Now, we perform extensive evaluation of the proposed approach on 3 publicly available datasets :UTD-MHAD, UT Kinect Action dataset and MSRC-12 Gesture Dataset.", "mention_start": 105, "mention_end": 130, "dataset_mention": " UT Kinect Action dataset"}, {"mentioned_in_paper": "861", "context_id": "111", "dataset_context": "Now, we perform extensive evaluation of the proposed approach on 3 publicly available datasets :UTD-MHAD, UT Kinect Action dataset and MSRC-12 Gesture Dataset.", "mention_start": 105, "mention_end": 158, "dataset_mention": " UT Kinect Action dataset and MSRC-12 Gesture Dataset"}, {"mentioned_in_paper": "861", "context_id": "112", "dataset_context": "The UT Kinect dataset [18] consists of 10 subjects performing 10 actions and each subject performs every action two times.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The UT Kinect dataset"}, {"mentioned_in_paper": "861", "context_id": "122", "dataset_context": "The UTD-MHAD dataset [5] is a very new dataset and is gathered using both Microsoft Kinect sensor and a wearable inertial sensor.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The UTD-MHAD dataset"}, {"mentioned_in_paper": "861", "context_id": "126", "dataset_context": "It is a comparatively difficult dataset as large number of actions are pooled together.", "mention_start": 6, "mention_end": 39, "dataset_mention": "a comparatively difficult dataset"}, {"mentioned_in_paper": "861", "context_id": "134", "dataset_context": "The MSRC-12 Kinect gesture dataset [6] contains sequences of human movements, represented as body-part locations, and the associated gesture is to be recognized by the system.", "mention_start": 0, "mention_end": 34, "dataset_mention": "The MSRC-12 Kinect gesture dataset"}, {"mentioned_in_paper": "861", "context_id": "151", "dataset_context": "In this work, we evaluate the usefulness of the proposed algorithm for online action recognition on the offline datasets itself, UT Kinect and UT Dallas, under the constraint that only partial data is available.", "mention_start": 99, "mention_end": 120, "dataset_mention": "the offline datasets"}, {"mentioned_in_paper": "861", "context_id": "157", "dataset_context": "We achieve a recognition accuracy of 88.89% on UTK Dataset and 79.07% on UTD Dataset using the covariance descriptor as the feature to the dictionary.", "mention_start": 47, "mention_end": 58, "dataset_mention": "UTK Dataset"}, {"mentioned_in_paper": "861", "context_id": "157", "dataset_context": "We achieve a recognition accuracy of 88.89% on UTK Dataset and 79.07% on UTD Dataset using the covariance descriptor as the feature to the dictionary.", "mention_start": 73, "mention_end": 84, "dataset_mention": "UTD Dataset"}, {"mentioned_in_paper": "861", "context_id": "160", "dataset_context": "Since the proposed approach is based on incremental update of confidence scores, first we perform an experiment to see how the scores gets updated for some example actions of the UT Kinect Action dataset.", "mention_start": 174, "mention_end": 203, "dataset_mention": "the UT Kinect Action dataset"}, {"mentioned_in_paper": "861", "context_id": "163", "dataset_context": "We report results on the UT Kinect Action dataset, but we have observed similar behaviour for the other datasets as well.", "mention_start": 21, "mention_end": 49, "dataset_mention": "the UT Kinect Action dataset"}, {"mentioned_in_paper": "861", "context_id": "166", "dataset_context": "Figure 5 shows the recognition accuracies of the UTK dataset as a function of the total number of available frames in the whole video sequence.", "mention_start": 45, "mention_end": 60, "dataset_mention": "the UTK dataset"}, {"mentioned_in_paper": "863", "context_id": "22", "dataset_context": "We evaluate our method on image pairs and in a multi-view setting on ScanNet [14], Matterport3D [10], and MegaDepth [29] datasets and show that our joint approach to feature matching and pose estimation improves over prior work on learned feature matching, enabled by the following contributions:", "mention_start": 101, "mention_end": 129, "dataset_mention": " and MegaDepth [29] datasets"}, {"mentioned_in_paper": "863", "context_id": "175", "dataset_context": "For multi-view evaluation, we sample test images with the same overlap criterion as used by prior work to sample image pairs [34, 44, 46] Table 3. Baseline comparison on multi-view indoor pose estimation on ScanNet; \"cross-dataset\" indicates that COTR was trained on MegaDepth.", "mention_start": 215, "mention_end": 230, "dataset_mention": " \"cross-dataset"}, {"mentioned_in_paper": "863", "context_id": "278", "dataset_context": "Unmatched keypoints must have a minimum reprojection error greater than 15 pixels on the indoor datasets and greater than 10 pixels on MegaDepth.", "mention_start": 85, "mention_end": 104, "dataset_mention": "the indoor datasets"}, {"mentioned_in_paper": "863", "context_id": "285", "dataset_context": "On MegaDepth, the overlap between images is the portion of co-visible 3D points of the sparse reconstruction [17, 44], thus the overlap definition is different from the indoor datasets and not comparable.", "mention_start": 164, "mention_end": 184, "dataset_mention": "the indoor datasets"}, {"mentioned_in_paper": "863", "context_id": "290", "dataset_context": "On the indoor datasets we use 400 keypoints per image during training time: first, keypoints above a confidence threshold of 0.001 are sampled, second, if there are fewer than 400, the remainder is filled with random image points and confidence 0 as a data augmentation.", "mention_start": 3, "mention_end": 22, "dataset_mention": "the indoor datasets"}, {"mentioned_in_paper": "863", "context_id": "294", "dataset_context": "On ScanNet and Matterport3D, we use the official dataset split.", "mention_start": 35, "mention_end": 56, "dataset_mention": "the official dataset"}, {"mentioned_in_paper": "863", "context_id": "300", "dataset_context": "We additionally train a SuperGlue model on Matterport3D and a SuperGlue model on MegaDepth using the above described dataset split, which is necessary as the provided model was trained on a train set that contains our test set.", "mention_start": 97, "mention_end": 124, "dataset_mention": "the above described dataset"}, {"mentioned_in_paper": "865", "context_id": "8", "dataset_context": "The efficacy of the proposed PacingPseudo is validated on three public medical image datasets, including the segmentation tasks of abdominal multi-organs, cardiac structures, and myocardium.", "mention_start": 58, "mention_end": 93, "dataset_mention": "three public medical image datasets"}, {"mentioned_in_paper": "865", "context_id": "39", "dataset_context": "We benchmark PacingPseudo on three public medical image datasets: CHAOS T1 and T2 (abdominal multi-organs) (Kavur et al., 2021), ACDC (cardiac structures) (Bernard et al., 2018), and LVSC (myocardium) (Suinesiaputra et al., 2014).", "mention_start": 29, "mention_end": 64, "dataset_mention": "three public medical image datasets"}, {"mentioned_in_paper": "865", "context_id": "128", "dataset_context": "The CHAOS dataset (Kavur et al., 2021) provides 20 patients for multi-organ segmentation.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The CHAOS dataset"}, {"mentioned_in_paper": "865", "context_id": "136", "dataset_context": "The ACDC dataset (Bernard et al., 2018) provides 100 patients for heart structure segmentation.", "mention_start": 0, "mention_end": 16, "dataset_mention": "The ACDC dataset"}, {"mentioned_in_paper": "865", "context_id": "143", "dataset_context": "The LVSC dataset (Suinesiaputra et al., 2014) provides 100 patients for myocardium (MYO) segmentation.", "mention_start": 0, "mention_end": 16, "dataset_mention": "The LVSC dataset"}, {"mentioned_in_paper": "865", "context_id": "149", "dataset_context": "Overall, the scribbles occupy \u223c10% of the foreground and \u223c0.5% of the background on the above datasets.", "mention_start": 83, "mention_end": 102, "dataset_mention": "the above datasets"}, {"mentioned_in_paper": "867", "context_id": "33", "dataset_context": "We apply NLIL on Visual Genome (Krishna et al., 2016) dataset for learning explanations for 150 object classes over 1M entities.", "mention_start": 47, "mention_end": 61, "dataset_mention": " 2016) dataset"}, {"mentioned_in_paper": "867", "context_id": "256", "dataset_context": "To show this, we conduct experiments on Visual Genome dataset (Krishna et al., 2016).", "mention_start": 39, "mention_end": 61, "dataset_mention": "Visual Genome dataset"}, {"mentioned_in_paper": "867", "context_id": "257", "dataset_context": "The original dataset is highly noisy (Zellers et al., 2018), so we use a pre-processed version available as the GQA dataset (Hudson & Manning, 2019).", "mention_start": 107, "mention_end": 123, "dataset_mention": "the GQA dataset"}, {"mentioned_in_paper": "867", "context_id": "263", "dataset_context": "As none of the ILP works scale to this benchmark, we compare NLIL with two supervised baselines: (i) MLP-RCNN: a MLP classifier with RCNN features of the object (available in GQA dataset) as input; and (ii) Freq: a frequency-based baseline that predicts object label by looking at the mostly occurred object class in the relation that contains the target.", "mention_start": 174, "mention_end": 186, "dataset_mention": "GQA dataset"}, {"mentioned_in_paper": "867", "context_id": "345", "dataset_context": "For VG dataset, we set T = 3, L = 2 and C = 4.", "mention_start": 4, "mention_end": 14, "dataset_mention": "VG dataset"}, {"mentioned_in_paper": "868", "context_id": "114", "dataset_context": "The data in autonomous driving datasets is highly imbalanced.", "mention_start": 12, "mention_end": 39, "dataset_mention": "autonomous driving datasets"}, {"mentioned_in_paper": "870", "context_id": "251", "dataset_context": "The goal of transfer learning [35] is to improve the performance of the target learner by training a previous model on a large (or similar) dataset and then transferring the knowledge from the previous model to the target learner.", "mention_start": 119, "mention_end": 147, "dataset_mention": "a large (or similar) dataset"}, {"mentioned_in_paper": "871", "context_id": "4", "dataset_context": "Our approach achieves the new state of the art on a challenging urban VLN dataset TOUCHDOWN, outperforming the baseline by 6.89% (absolute improvement) on Success weighted by Edit Distance (SED).", "mention_start": 64, "mention_end": 81, "dataset_mention": "urban VLN dataset"}, {"mentioned_in_paper": "871", "context_id": "23", "dataset_context": "We conduct experiments on a language-grounded street-view navigation dataset TOUCHDOWN 1 (Chen et al., 2019).", "mention_start": 26, "mention_end": 76, "dataset_mention": "a language-grounded street-view navigation dataset"}, {"mentioned_in_paper": "871", "context_id": "24", "dataset_context": "Extensive results show that our proposed approach significantly improves the performance over the baseline model on all metrics and achieves the new state-of-the-art on the TOUCHDOWN dataset. 2", "mention_start": 169, "mention_end": 190, "dataset_mention": "the TOUCHDOWN dataset"}, {"mentioned_in_paper": "872", "context_id": "21", "dataset_context": "Our experiments on a large supervised ranking dataset demonstrate the effectiveness and efficiency of MORES.", "mention_start": 19, "mention_end": 53, "dataset_mention": "a large supervised ranking dataset"}, {"mentioned_in_paper": "872", "context_id": "157", "dataset_context": "This experiment trains MORES using the MS MARCO dataset, and adapts the model to two datasets: ClueWeb09-B and Robust04.", "mention_start": 35, "mention_end": 55, "dataset_mention": "the MS MARCO dataset"}, {"mentioned_in_paper": "874", "context_id": "185", "dataset_context": "However, both clean and adversarial accuracies of DN4-AT are still far from the normal accuracy on the miniImageNet dataset (70.84%).", "mention_start": 98, "mention_end": 123, "dataset_mention": "the miniImageNet dataset"}, {"mentioned_in_paper": "875", "context_id": "59", "dataset_context": "Subsequently, we define the eigenvector and acquire a new data set to train the Support Vector Machine.", "mention_start": 13, "mention_end": 66, "dataset_mention": " we define the eigenvector and acquire a new data set"}, {"mentioned_in_paper": "876", "context_id": "112", "dataset_context": "We conduct extensive experiments to evaluate the efficacy of the proposed DTQ approach against several state-of-the-art shallow and deep hashing methods on three image retrieval benchmark datasets, NUS-WIDE, CIFAR-10, and MS-COCO.", "mention_start": 156, "mention_end": 196, "dataset_mention": "three image retrieval benchmark datasets"}, {"mentioned_in_paper": "876", "context_id": "114", "dataset_context": "The evaluation is conducted on three widely used image retrieval benchmark xdatasets: NUS-WIDE, CIFAR-10, and MS-COCO.", "mention_start": 49, "mention_end": 84, "dataset_mention": "image retrieval benchmark xdatasets"}, {"mentioned_in_paper": "876", "context_id": "126", "dataset_context": "We follow [1] [2] [3] and adopt MAP@5000 for NUS-WIDE dataset, MAP@5000 for MS-COCO dataset, and MAP@54000 for CIFAR-10 dataset.", "mention_start": 45, "mention_end": 61, "dataset_mention": "NUS-WIDE dataset"}, {"mentioned_in_paper": "876", "context_id": "126", "dataset_context": "We follow [1] [2] [3] and adopt MAP@5000 for NUS-WIDE dataset, MAP@5000 for MS-COCO dataset, and MAP@54000 for CIFAR-10 dataset.", "mention_start": 75, "mention_end": 91, "dataset_mention": "MS-COCO dataset"}, {"mentioned_in_paper": "876", "context_id": "126", "dataset_context": "We follow [1] [2] [3] and adopt MAP@5000 for NUS-WIDE dataset, MAP@5000 for MS-COCO dataset, and MAP@54000 for CIFAR-10 dataset.", "mention_start": 110, "mention_end": 127, "dataset_mention": "CIFAR-10 dataset"}, {"mentioned_in_paper": "877", "context_id": "220", "dataset_context": "We used CUDA 8 to implement the  antitative comparison on the SHREC dataset, measuring, from le to right: conformal distortion, compatibility with symmetries and distance from ground truth landmarks.", "mention_start": 57, "mention_end": 75, "dataset_mention": "the SHREC dataset"}, {"mentioned_in_paper": "877", "context_id": "253", "dataset_context": "We use the BIM benchmark [Kim et al. 2011] that provides more than 200 pairs of highly non-isometric shapes from the SHREC dataset [Giorgi et al. 2007] with user-veri ed landmarks.", "mention_start": 113, "mention_end": 130, "dataset_mention": "the SHREC dataset"}, {"mentioned_in_paper": "877", "context_id": "282", "dataset_context": "We ran the experiment on the \"quadrupeds\" class (20 pairs) from the SHREC dataset and did the following modi cations: (a) moved every landmark randomly to a vertex in its 5-ring neighborhood, (b) switched between the two eyes, or mapped both eyes on M 1 to a single eye on M 2 , and (c) mapped both feet on M 1 to the same foot on M 2 .", "mention_start": 64, "mention_end": 81, "dataset_mention": "the SHREC dataset"}, {"mentioned_in_paper": "877", "context_id": "285", "dataset_context": "5.4 Dataset: SHREC two pairs, input: functional map e functional map [Ovsjanikov et al. 2012] machinery is quite versatile, and allows to compute generalized maps in a variety of cases.", "mention_start": 0, "mention_end": 11, "dataset_mention": "5.4 Dataset"}, {"mentioned_in_paper": "877", "context_id": "287", "dataset_context": "We use the SHREC dataset with its landmark data from the BIM benchmark [Kim et al. 2011], and use the landmarks to compute a functional map using the Wave Kernel Map and the Wave Kernel Signature [Aubry et al. 2011].", "mention_start": 7, "mention_end": 24, "dataset_mention": "the SHREC dataset"}, {"mentioned_in_paper": "878", "context_id": "7", "dataset_context": "Achieving automatic sign localisation enables a diverse range of practical applications: construction of sign language dictionaries to support language learners, indexing of signing content to enable efficient search and \"intelligent fast-forward\" to topics of interest, automatic sign language dataset construction, \"wakeword\" recognition for signers [34] and tools to assist linguistic analysis of large-scale signing corpora.", "mention_start": 270, "mention_end": 302, "dataset_mention": " automatic sign language dataset"}, {"mentioned_in_paper": "878", "context_id": "12", "dataset_context": "Widely used datasets such as RWTH-PHOENIX [9, 26] and the CSL dataset [23] provide continuous sign annotations in the form of glosses 1 or free-form sentences, but lack precise temporal annotations and are limited in content diversity, vocabulary, and scale.", "mention_start": 45, "mention_end": 69, "dataset_mention": " 26] and the CSL dataset"}, {"mentioned_in_paper": "878", "context_id": "36", "dataset_context": "These approaches [9, 10, 11] have shown improvements towards translation in the restricted domain of discourse of the RWTH-PHOENIX-Weather-2014T German Sign Language (DGS) dataset [9].", "mention_start": 113, "mention_end": 179, "dataset_mention": "the RWTH-PHOENIX-Weather-2014T German Sign Language (DGS) dataset"}, {"mentioned_in_paper": "878", "context_id": "40", "dataset_context": "Sign language datasets either offer isolated gloss-level annotations of single signs, e.g., MSASL [24], WLASL [27], or are heavily constrained in visual domain and vocabulary, e.g., RWTH-PHOENIX [9, 26], KETI [25] (only 105 sentences).", "mention_start": 0, "mention_end": 22, "dataset_mention": "Sign language datasets"}, {"mentioned_in_paper": "878", "context_id": "41", "dataset_context": "Large-scale continuous sign language datasets, on the other hand, are not exhaustively annotated [2, 35].", "mention_start": 0, "mention_end": 45, "dataset_mention": "Large-scale continuous sign language datasets"}, {"mentioned_in_paper": "878", "context_id": "42", "dataset_context": "The recent efforts of Albanie et al. [2] scale up the automatic annotation of sign language data, and construct the BSL-1K dataset with the help of a visual keyword spotter [30, 41] trained on lip reading to detect instances of mouthed words as a proxy for spotting signs.", "mention_start": 111, "mention_end": 130, "dataset_mention": "the BSL-1K dataset"}, {"mentioned_in_paper": "878", "context_id": "124", "dataset_context": "We use BSL-1K [2], a large-scale, subtitled and sparsely annotated dataset (for a vocabulary of 1,064 signs) of more than 1000 hours of continuous signing from sign language interpreted BBC television broadcasts.", "mention_start": 33, "mention_end": 74, "dataset_mention": " subtitled and sparsely annotated dataset"}, {"mentioned_in_paper": "878", "context_id": "185", "dataset_context": "To form a new annotated set for sign recognition training, we apply the trained Transformer models on the whole 685K training video-subtitle pairs of the BSL-1K dataset.", "mention_start": 149, "mention_end": 168, "dataset_mention": "the BSL-1K dataset"}, {"mentioned_in_paper": "878", "context_id": "228", "dataset_context": "Second, the alignment between text and video is far from perfect in large-scale sign language datasets which inserts significant amount of noise in training.", "mention_start": 67, "mention_end": 102, "dataset_mention": "large-scale sign language datasets"}, {"mentioned_in_paper": "878", "context_id": "238", "dataset_context": "We find that state-of-the-art translation models have very low recall on a large-vocabulary dataset, but a satisfactory localisation accuracy through attention that allows us to annotate sign timings.", "mention_start": 73, "mention_end": 99, "dataset_mention": "a large-vocabulary dataset"}, {"mentioned_in_paper": "878", "context_id": "261", "dataset_context": "When applying the method of [30] to localise signs through similarity matching with dictionary videos, we query 9K signs from the full BSLDict dataset with search windows of \u00b14 seconds padding around the subtitle timestamps.", "mention_start": 125, "mention_end": 150, "dataset_mention": "the full BSLDict dataset"}, {"mentioned_in_paper": "879", "context_id": "1", "dataset_context": "In this paper, we first propose a semi-automatic labeling method to create the HDAD-pair dataset of which each HDAD-pair consists of one HDAD map and its binarized HDAD map.", "mention_start": 74, "mention_end": 96, "dataset_mention": "the HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "2", "dataset_context": "Based on the created training HDAD-pair dataset, we propose a convolutional neural network-based (CNN-based) binarization method to produce high-quality binarized HDAD maps.", "mention_start": 30, "mention_end": 47, "dataset_mention": "HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "47", "dataset_context": "Because the size of each HDAD map is huge and is ranged from 2436\u00d71738 to 10124\u00d76962, to reduce human efforts on labeling each pixel annotation as a background pixel or a foreground pixel, our first motivation is to design a fast and effective labeling method to create the HDAD-pair dataset, in which each HDAD-pair consists of one input HDAD map and its ground truth binarized HDAD map.", "mention_start": 269, "mention_end": 291, "dataset_mention": "the HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "50", "dataset_context": "Based on our created HDAD-pair dataset, our third motivation is to retrain the two state-of-the-art methods [5], [41], and then report the performance merit of our method.", "mention_start": 21, "mention_end": 38, "dataset_mention": "HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "51", "dataset_context": "To address our three motivations, in this paper, we first propose a semi-automatic labeling method to create the HDAD-pair dataset.", "mention_start": 108, "mention_end": 130, "dataset_mention": "the HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "52", "dataset_context": "Then, based on the created HDAD dataset, we propose an effective CNN-based binarization method for HDAD maps.", "mention_start": 26, "mention_end": 39, "dataset_mention": "HDAD dataset"}, {"mentioned_in_paper": "879", "context_id": "54", "dataset_context": "In the first contribution, we propose a semi-automatic labeling method to create the HDAD-pair dataset.", "mention_start": 80, "mention_end": 102, "dataset_mention": "the HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "60", "dataset_context": "Furthermore, a slight handmade adjustment is applied to produce the ground truth binarized HDAD map, creating the HDAD-pair dataset.", "mention_start": 109, "mention_end": 131, "dataset_mention": "the HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "61", "dataset_context": "In the second contribution, based on the created HDADpair dataset, we propose a novel and effective CNN-based binarization method for HDAD maps, and the proposed method achieves substantial accuracy, PSNR, and the perceptual effect improvements relative to the nine comparative methods.", "mention_start": 48, "mention_end": 65, "dataset_mention": "HDADpair dataset"}, {"mentioned_in_paper": "879", "context_id": "66", "dataset_context": "In Section II, we propose a semi-automatic labeling method to create the new HDAD-pair dataset effectively.", "mention_start": 68, "mention_end": 94, "dataset_mention": "the new HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "70", "dataset_context": "In this section, a new semi-automatic labeling method is proposed to label each pixel of the HDAD map to be a foreground pixel or a background pixel, thus producing the HDAD-pair dataset.", "mention_start": 154, "mention_end": 186, "dataset_mention": "producing the HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "72", "dataset_context": "In the first stage, a fusion-based approach is proposed to produce the rough HDAD-pair dataset.", "mention_start": 66, "mention_end": 94, "dataset_mention": "the rough HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "73", "dataset_context": "In the second stage, the center weighted median filter technique for removing noise and a slight handmade adjustment are applied to refine the binarized HDAD maps, creating the resultant HDAD-pair dataset.", "mention_start": 172, "mention_end": 204, "dataset_mention": "the resultant HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "77", "dataset_context": "Furthermore, a fusion approach, which integrates MLT and IHEGT together, is proposed to generate the rough HDAD-pair dataset automatically.", "mention_start": 96, "mention_end": 124, "dataset_mention": "the rough HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "95", "dataset_context": "In this subsection, the proposed fusion-based approach is used to integrate MLT and IHEGT together for producing a rough HDAD-pair dataset.", "mention_start": 102, "mention_end": 138, "dataset_mention": "producing a rough HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "101", "dataset_context": "After a slight handmade adjustment to refine Fig. 3 (c), Fig. 3(d) illustrates the finally binarized HDAD map which will be included in the fine HDAD-pair dataset.", "mention_start": 135, "mention_end": 162, "dataset_mention": "the fine HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "103", "dataset_context": "Next, based on the newly created HDAD-pair dataset, the training step is described and the three novelties in our CNN framework are highlighted to explain why the number of parameters used is much less than that in the two state-of-the-art methods in [5], [41], leading to the execution-time reduction merit of our method.", "mention_start": 32, "mention_end": 50, "dataset_mention": "HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "125", "dataset_context": "In the second set of experiments, we retrain the two state-of-the-art methods [5], [41] based on the same HDAD-pair dataset, and then based on the same testing dataset, with the similar binarization accuracy and quality, the experimental data demonstrates the significant execution-time and parameters reduction merits of our method relative to the retrained version of the two methods [5], [41].", "mention_start": 96, "mention_end": 123, "dataset_mention": "the same HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "140", "dataset_context": "Since the available codes in the two state-of-the-art methods [5], [41] are provided for the DIBCO dataset [10], for completeness and fairness, we retrain the two methods.", "mention_start": 88, "mention_end": 106, "dataset_mention": "the DIBCO dataset"}, {"mentioned_in_paper": "879", "context_id": "151", "dataset_context": "First, we propose a semi-automatic labeling method to produce the ground truth HDAD-pair dataset effectively.", "mention_start": 61, "mention_end": 96, "dataset_mention": "the ground truth HDAD-pair dataset"}, {"mentioned_in_paper": "879", "context_id": "152", "dataset_context": "Secondly, based on the newly created HDAD-pair dataset, we propose an effective CNN-based binarization method for HDAD maps.", "mention_start": 36, "mention_end": 54, "dataset_mention": "HDAD-pair dataset"}, {"mentioned_in_paper": "880", "context_id": "80", "dataset_context": "The data required for PdM is typically the sensor measurements, warranty logs, maintenance logs, engineering drawing etc. Machine learning algorithms analyze these 'big' data sets and identify useful patterns (like novelties or anomalies) to build an effective PdM system.", "mention_start": 96, "mention_end": 179, "dataset_mention": " engineering drawing etc. Machine learning algorithms analyze these 'big' data sets"}, {"mentioned_in_paper": "880", "context_id": "198", "dataset_context": "DRL has been employed to learn anomaly detection models in situations where a partially labeled dataset is available alongwith a large-scale unlabeled dataset [94].", "mention_start": 76, "mention_end": 103, "dataset_mention": "a partially labeled dataset"}, {"mentioned_in_paper": "880", "context_id": "198", "dataset_context": "DRL has been employed to learn anomaly detection models in situations where a partially labeled dataset is available alongwith a large-scale unlabeled dataset [94].", "mention_start": 107, "mention_end": 158, "dataset_mention": "available alongwith a large-scale unlabeled dataset"}, {"mentioned_in_paper": "880", "context_id": "201", "dataset_context": "DRL approach attempts to identify anomalies that lie beyond the partially labeled datasets.", "mention_start": 34, "mention_end": 90, "dataset_mention": "anomalies that lie beyond the partially labeled datasets"}, {"mentioned_in_paper": "880", "context_id": "293", "dataset_context": "We have investigated several standard datasets including MIMII (a sound-based industrial machine fault detection dataset [87]), AI4I [131], and a predictive maintenance (PdM) dataset from Azure [132].", "mention_start": 8, "mention_end": 46, "dataset_mention": "investigated several standard datasets"}, {"mentioned_in_paper": "880", "context_id": "293", "dataset_context": "We have investigated several standard datasets including MIMII (a sound-based industrial machine fault detection dataset [87]), AI4I [131], and a predictive maintenance (PdM) dataset from Azure [132].", "mention_start": 57, "mention_end": 120, "dataset_mention": "MIMII (a sound-based industrial machine fault detection dataset"}, {"mentioned_in_paper": "880", "context_id": "293", "dataset_context": "We have investigated several standard datasets including MIMII (a sound-based industrial machine fault detection dataset [87]), AI4I [131], and a predictive maintenance (PdM) dataset from Azure [132].", "mention_start": 139, "mention_end": 182, "dataset_mention": " and a predictive maintenance (PdM) dataset"}, {"mentioned_in_paper": "880", "context_id": "294", "dataset_context": "Among these, the Azure predictive maintenance dataset is the most interesting, as its multivariate training data includes healthy data that changes over time to develop a failure.", "mention_start": 12, "mention_end": 53, "dataset_mention": " the Azure predictive maintenance dataset"}, {"mentioned_in_paper": "880", "context_id": "297", "dataset_context": "The Azure dataset is a simulated dataset containing one year of hourly data for 100 devices with vibrations, rotation speeds, voltage, and pressure data, as well as time series records of when various parts were replaced on the devices.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The Azure dataset"}, {"mentioned_in_paper": "880", "context_id": "312", "dataset_context": "The starting dataset is a natural one to use without too much preprocessing if the goal is to identify faults as they happen, but in our case the goal is to identify the faults before they happen (PdM).", "mention_start": 0, "mention_end": 20, "dataset_mention": "The starting dataset"}, {"mentioned_in_paper": "880", "context_id": "353", "dataset_context": "For our study, we have used the Backblaze dataset [134], discussed in subsection 5.2 as it is one of the largest open-source datasets in this domain.", "mention_start": 27, "mention_end": 49, "dataset_mention": "the Backblaze dataset"}, {"mentioned_in_paper": "880", "context_id": "353", "dataset_context": "For our study, we have used the Backblaze dataset [134], discussed in subsection 5.2 as it is one of the largest open-source datasets in this domain.", "mention_start": 100, "mention_end": 133, "dataset_mention": "the largest open-source datasets"}, {"mentioned_in_paper": "880", "context_id": "358", "dataset_context": "As the Backblaze dataset is a large-scale dataset consisting of 228 million records, we have used the PySpark distributed computing framework which is supported by the Dataproc service of GCP.", "mention_start": 3, "mention_end": 24, "dataset_mention": "the Backblaze dataset"}, {"mentioned_in_paper": "880", "context_id": "360", "dataset_context": "Although Backblaze dataset is humongous, it is important to note that it is severely imbalanced with only 9000 records out of 228 million records having positive labels.", "mention_start": 9, "mention_end": 26, "dataset_mention": "Backblaze dataset"}, {"mentioned_in_paper": "880", "context_id": "366", "dataset_context": "In this section, we provide different variations of variational autoencoder (VAE), as a Bayesian modeling which captures the uncertainty in the decisions, for failure prediction in the UC Berkeley milling data set [135].", "mention_start": 180, "mention_end": 213, "dataset_mention": "the UC Berkeley milling data set"}, {"mentioned_in_paper": "880", "context_id": "367", "dataset_context": "The milling dataset is includes 16 cases of milling tools making cuts in metal.", "mention_start": 0, "mention_end": 19, "dataset_mention": "The milling dataset"}, {"mentioned_in_paper": "880", "context_id": "379", "dataset_context": "For the Milling dataset, the pre-processing pipeline developed in [136] is used.", "mention_start": 4, "mention_end": 23, "dataset_mention": "the Milling dataset"}, {"mentioned_in_paper": "880", "context_id": "389", "dataset_context": "The architecture, shown in 14, includes three blocks of \"TCN block + batch-normalization + 2 \u00d7 2 max-pooling layer\", with activation functions being scaled Figure 13 : A sample of a single cut (cut 99) from Milling dataset exponential linear units (SELU).", "mention_start": 206, "mention_end": 222, "dataset_mention": "Milling dataset"}, {"mentioned_in_paper": "880", "context_id": "403", "dataset_context": "better anomaly score which leads to higher AUC. Figure 15 demonstrates a comparison of these three variants of VAE applied on the Milling dataset.", "mention_start": 126, "mention_end": 145, "dataset_mention": "the Milling dataset"}, {"mentioned_in_paper": "880", "context_id": "406", "dataset_context": "In such scenarios, it is important to have an end-to-end machine learning (ML) system which can scale well in presence of large-scale historical datasets and classify or predict anomalies in a time-efficient manner amenable for a real-world system.", "mention_start": 121, "mention_end": 153, "dataset_mention": "large-scale historical datasets"}, {"mentioned_in_paper": "880", "context_id": "552", "dataset_context": "Experimentations on both artificial and real datasets demonstrate that the accuracy of the proposed method surpasses various advanced approaches and can efficiently select the best service in different scenarios.", "mention_start": 20, "mention_end": 53, "dataset_mention": "both artificial and real datasets"}, {"mentioned_in_paper": "880", "context_id": "553", "dataset_context": "The UK-DALE dataset [141, 142] -domestic appliance-level electricity demand and whole-house demand from five UK homes.", "mention_start": 0, "mention_end": 19, "dataset_mention": "The UK-DALE dataset"}, {"mentioned_in_paper": "880", "context_id": "556", "dataset_context": "Training desegregation algorithms using supervised learning approaches, researchers require labelled datasets with energy usage from individual appliances.", "mention_start": 91, "mention_end": 109, "dataset_mention": "labelled datasets"}, {"mentioned_in_paper": "880", "context_id": "557", "dataset_context": "The UK-DALE dataset, an open-access dataset from the UK recording Domestic Appliance-Level Electricity at a sample rate of 16kHz for the whole-house and at 1/6 Hz for individual appliances.", "mention_start": 0, "mention_end": 19, "dataset_mention": "The UK-DALE dataset"}, {"mentioned_in_paper": "880", "context_id": "557", "dataset_context": "The UK-DALE dataset, an open-access dataset from the UK recording Domestic Appliance-Level Electricity at a sample rate of 16kHz for the whole-house and at 1/6 Hz for individual appliances.", "mention_start": 20, "mention_end": 43, "dataset_mention": " an open-access dataset"}, {"mentioned_in_paper": "880", "context_id": "559", "dataset_context": "[143] lists several public data sets.", "mention_start": 0, "mention_end": 36, "dataset_mention": "[143] lists several public data sets"}, {"mentioned_in_paper": "880", "context_id": "561", "dataset_context": "A new dataset of industrial machine sounds for malfunctioning industrial machine investigation and inspection (MIMII dataset) is discussed in [87].", "mention_start": 47, "mention_end": 124, "dataset_mention": "malfunctioning industrial machine investigation and inspection (MIMII dataset"}, {"mentioned_in_paper": "880", "context_id": "564", "dataset_context": "The open source MIMII dataset is to assist the machine-learning and signal-processing community with their development of automated facility maintenance.", "mention_start": 0, "mention_end": 29, "dataset_mention": "The open source MIMII dataset"}, {"mentioned_in_paper": "880", "context_id": "566", "dataset_context": " [144] is another rich acoustic dataset with recorded sound representing operation of miniature machines.", "mention_start": 9, "mention_end": 39, "dataset_mention": "another rich acoustic dataset"}, {"mentioned_in_paper": "880", "context_id": "567", "dataset_context": "Motor Current data set (MOTOR).", "mention_start": 0, "mention_end": 22, "dataset_mention": "Motor Current data set"}, {"mentioned_in_paper": "880", "context_id": "576", "dataset_context": "Outlier Detection Data Sets (ODDS) [146] : This collection provides open access to a number of data sets meant for outlier detection.", "mention_start": 0, "mention_end": 27, "dataset_mention": "Outlier Detection Data Sets"}, {"mentioned_in_paper": "880", "context_id": "577", "dataset_context": "In ODDS provides open access to a large collection of outlier detection datasets with ground truth (if available).", "mention_start": 54, "mention_end": 80, "dataset_mention": "outlier detection datasets"}, {"mentioned_in_paper": "880", "context_id": "580", "dataset_context": "The Backblaze dataset [134] consists of millions of records pertaining to hardware data for years spanning from 2013 to 2021.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The Backblaze dataset"}, {"mentioned_in_paper": "880", "context_id": "582", "dataset_context": "The Backblaze dataset is a large-scale dataset with over 228 million rows of data and a size of approximately 60 GB.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The Backblaze dataset"}, {"mentioned_in_paper": "880", "context_id": "583", "dataset_context": "It contains data from four principal hard drive manufacturers-Seagate, Hitachi, Western Digital and Toshiba.The complete dataset consists of several CSV files and each file reports all working hard drives on each day.", "mention_start": 79, "mention_end": 128, "dataset_mention": " Western Digital and Toshiba.The complete dataset"}, {"mentioned_in_paper": "880", "context_id": "586", "dataset_context": "A comprehensive set of open source datasets useful for developing PdM is listed in [143].", "mention_start": 23, "mention_end": 43, "dataset_mention": "open source datasets"}, {"mentioned_in_paper": "880", "context_id": "588", "dataset_context": "Inline with the existing technologies 3.1, 3.2, 3.3, 3.4, detailed case studies based on open source data sets 4, and an initial exploratory analysis of the data, we adopt a three step approach to derive best business value.", "mention_start": 88, "mention_end": 110, "dataset_mention": "open source data sets"}, {"mentioned_in_paper": "882", "context_id": "3", "dataset_context": "By implementing our approach using the VGG-face and ExpNet architectures with extensive experiments on the Oulu-CASIA and SFEW datasets, we show that the proposed approach achieves performance at the state of the art for facial expression recognition.", "mention_start": 103, "mention_end": 135, "dataset_mention": "the Oulu-CASIA and SFEW datasets"}, {"mentioned_in_paper": "882", "context_id": "59", "dataset_context": "Following Ding et al. [9], we first fine-tune the VGG-face network on expression datasets by minimizing the cross entropy loss.", "mention_start": 69, "mention_end": 89, "dataset_mention": "expression datasets"}, {"mentioned_in_paper": "882", "context_id": "103", "dataset_context": "The effectiveness of the proposed approach in recognizing basic facial expressions has been evaluated in constrained and unconstrained (i.e., in-the-wild) settings using two publicly available datasets with different challenges: Oulu-CASIA dataset [28] : Includes 480 image sequences of 80 subjects taken in a constrained environment with normal illumination conditions.", "mention_start": 228, "mention_end": 247, "dataset_mention": " Oulu-CASIA dataset"}, {"mentioned_in_paper": "882", "context_id": "108", "dataset_context": "Static Facial Expression in the Wild (SFEW) dataset [5] : Consists of 1, 322 static images labeled with seven facial expressions (the six basic plus the neutral one).", "mention_start": 28, "mention_end": 51, "dataset_mention": "the Wild (SFEW) dataset"}, {"mentioned_in_paper": "882", "context_id": "120", "dataset_context": "On the SFEW dataset, one model is trained using the provided training data.", "mention_start": 3, "mention_end": 19, "dataset_mention": "the SFEW dataset"}, {"mentioned_in_paper": "882", "context_id": "124", "dataset_context": "For data augmentation, we used horizontal flipping on the original data without any other supplementary datasets.", "mention_start": 53, "mention_end": 112, "dataset_mention": "the original data without any other supplementary datasets"}, {"mentioned_in_paper": "882", "context_id": "144", "dataset_context": "The best performance were achieved for weighted sum fusion with w global , w eyes , w cheek\u2212le f t , w cheek\u2212right equal to 1 and w mouth = 0.2, for the Oulu-CASIA dataset, and w global = 1, and w eyes , w mouth , w cheek\u2212le f t , w cheek\u2212right equal to 0.1 for the SFEW dataset.", "mention_start": 148, "mention_end": 171, "dataset_mention": "the Oulu-CASIA dataset"}, {"mentioned_in_paper": "882", "context_id": "144", "dataset_context": "The best performance were achieved for weighted sum fusion with w global , w eyes , w cheek\u2212le f t , w cheek\u2212right equal to 1 and w mouth = 0.2, for the Oulu-CASIA dataset, and w global = 1, and w eyes , w mouth , w cheek\u2212le f t , w cheek\u2212right equal to 0.1 for the SFEW dataset.", "mention_start": 261, "mention_end": 278, "dataset_mention": "the SFEW dataset"}, {"mentioned_in_paper": "882", "context_id": "147", "dataset_context": "Note that for testing the Oulu-CASIA dataset, we represented each video by its three peak frames as in Ding et al. [9].", "mention_start": 14, "mention_end": 44, "dataset_mention": "testing the Oulu-CASIA dataset"}, {"mentioned_in_paper": "882", "context_id": "152", "dataset_context": "Though less marked, an increment of 0.69% for the VGG-face and of 0.92% for ExpNet has been also obtained on the SFEW dataset.", "mention_start": 108, "mention_end": 125, "dataset_mention": "the SFEW dataset"}, {"mentioned_in_paper": "882", "context_id": "155", "dataset_context": "Table 1 also shows that the fusion of the local (R-FMs) and global (G-FMs) approaches achieves a clear superiority on the Oulu-CASIA dataset surpassing by 1.25% the global approach, while no improvement is observed on the SFEW dataset.", "mention_start": 118, "mention_end": 140, "dataset_mention": "the Oulu-CASIA dataset"}, {"mentioned_in_paper": "882", "context_id": "155", "dataset_context": "Table 1 also shows that the fusion of the local (R-FMs) and global (G-FMs) approaches achieves a clear superiority on the Oulu-CASIA dataset surpassing by 1.25% the global approach, while no improvement is observed on the SFEW dataset.", "mention_start": 217, "mention_end": 234, "dataset_mention": "the SFEW dataset"}, {"mentioned_in_paper": "882", "context_id": "161", "dataset_context": "We motivate this result by the fact that, in the wild acquisitions as for the SFEW dataset, the region of the eyes can be affected by occlusions, and the landmarks detection can be less accurate (see Section 3 of the supplementary material for failure cases of landmark detection in this dataset).", "mention_start": 73, "mention_end": 90, "dataset_mention": "the SFEW dataset"}, {"mentioned_in_paper": "882", "context_id": "172", "dataset_context": "On the SFEW dataset, the global approach achieves the second highest accuracy, surpassing various state of the art methods with significant gains.", "mention_start": 3, "mention_end": 19, "dataset_mention": "the SFEW dataset"}, {"mentioned_in_paper": "882", "context_id": "178", "dataset_context": "By implementing our approach using different architectures, i.e., VGG-face and ExpNet, in extensive experiments on the Oulu-CASIA and SFEW datasets, we have shown that the proposed approach achieves state-of-the-art performance for facial expression recognition.", "mention_start": 114, "mention_end": 147, "dataset_mention": "the Oulu-CASIA and SFEW datasets"}, {"mentioned_in_paper": "882", "context_id": "182", "dataset_context": "Global DCNN features and their covariance descriptors: Figure 3 shows four selected feature maps (chosen from 512 FMs) extracted with the ExpNet model for two subjects of the Oulu-CASIA dataset (happy and surprise expressions).", "mention_start": 170, "mention_end": 193, "dataset_mention": "the Oulu-CASIA dataset"}, {"mentioned_in_paper": "884", "context_id": "12", "dataset_context": "KTH dataset, Schuldt et al. 2004) and robustly recognize actions in realistic settings such as Hollywood movies (Marszalek et al. 2009), videos from YouTube (Liu et al. 2009), or sport scenes (Rodriguez et al. 2008).", "mention_start": 0, "mention_end": 11, "dataset_mention": "KTH dataset"}, {"mentioned_in_paper": "884", "context_id": "41", "dataset_context": "In this work, which is an extension of our original publications (Rohrbach et al. 2012a) and (Rohrbach et al. 2012b), we recorded, annotated, and publicly released a large-scale dataset in a kitchen scenario which addresses the discussed limitations.", "mention_start": 141, "mention_end": 185, "dataset_mention": " and publicly released a large-scale dataset"}, {"mentioned_in_paper": "884", "context_id": "73", "dataset_context": "Third, we recorded and annotated a video dataset called MPII Cooking 2. It provides challenges for classification and detection of fine-grained activities and their participants, human pose estimation, and composite activity recognition (optionally) using script data.", "mention_start": 6, "mention_end": 48, "dataset_mention": " we recorded and annotated a video dataset"}, {"mentioned_in_paper": "884", "context_id": "76", "dataset_context": "We first make an extensive review of related datasets, activity recognition approaches, and the use of text data for visual recognition in Section 2. Then we introduce our MPII Cooking 2 dataset in Section 3 which we benchmark in the subsequent sections.", "mention_start": 167, "mention_end": 194, "dataset_mention": "our MPII Cooking 2 dataset"}, {"mentioned_in_paper": "884", "context_id": "81", "dataset_context": "We first present an overview of the different video activity recognition datasets (Section 2.1) and then review recent approaches to activity recognition (Section 2.2), putting a focus on works which use human pose as a cue.", "mention_start": 32, "mention_end": 81, "dataset_mention": "the different video activity recognition datasets"}, {"mentioned_in_paper": "884", "context_id": "84", "dataset_context": "Even when excluding single image action datasets such as the Stanford-40 Action Dataset (Yao et al. 2011b or the Pascal Action Classification Challenge (Everingham et al. 2011), the number of proposed activity datasets is quite large (Chaquet et al. (2013) survey 68 datasets).", "mention_start": 10, "mention_end": 48, "dataset_mention": "excluding single image action datasets"}, {"mentioned_in_paper": "884", "context_id": "84", "dataset_context": "Even when excluding single image action datasets such as the Stanford-40 Action Dataset (Yao et al. 2011b or the Pascal Action Classification Challenge (Everingham et al. 2011), the number of proposed activity datasets is quite large (Chaquet et al. (2013) survey 68 datasets).", "mention_start": 57, "mention_end": 87, "dataset_mention": "the Stanford-40 Action Dataset"}, {"mentioned_in_paper": "884", "context_id": "84", "dataset_context": "Even when excluding single image action datasets such as the Stanford-40 Action Dataset (Yao et al. 2011b or the Pascal Action Classification Challenge (Everingham et al. 2011), the number of proposed activity datasets is quite large (Chaquet et al. (2013) survey 68 datasets).", "mention_start": 191, "mention_end": 218, "dataset_mention": "proposed activity datasets"}, {"mentioned_in_paper": "884", "context_id": "84", "dataset_context": "Even when excluding single image action datasets such as the Stanford-40 Action Dataset (Yao et al. 2011b or the Pascal Action Classification Challenge (Everingham et al. 2011), the number of proposed activity datasets is quite large (Chaquet et al. (2013) survey 68 datasets).", "mention_start": 221, "mention_end": 275, "dataset_mention": "quite large (Chaquet et al. (2013) survey 68 datasets"}, {"mentioned_in_paper": "884", "context_id": "86", "dataset_context": "We distinguish four broad categories of datasets: full body pose, movie and web, surveillance, and assisted daily living datasetsour dataset falls in the last category.", "mention_start": 94, "mention_end": 129, "dataset_mention": " and assisted daily living datasets"}, {"mentioned_in_paper": "884", "context_id": "86", "dataset_context": "We distinguish four broad categories of datasets: full body pose, movie and web, surveillance, and assisted daily living datasetsour dataset falls in the last category.", "mention_start": 94, "mention_end": 140, "dataset_mention": " and assisted daily living datasetsour dataset"}, {"mentioned_in_paper": "884", "context_id": "87", "dataset_context": "The full body pose datasets are defined by actors performing full body actions.", "mention_start": 0, "mention_end": 27, "dataset_mention": "The full body pose datasets"}, {"mentioned_in_paper": "884", "context_id": "90", "dataset_context": "In contrast to these full body pose datasets, our dataset contains more and in particular fine-grained activities.", "mention_start": 15, "mention_end": 44, "dataset_mention": "these full body pose datasets"}, {"mentioned_in_paper": "884", "context_id": "92", "dataset_context": "UCF50 1 and similar datasets (Liu et al. 2009; Niebles et al. 2010; Rodriguez et al. 2008) focus on sport activities.", "mention_start": 0, "mention_end": 28, "dataset_mention": "UCF50 1 and similar datasets"}, {"mentioned_in_paper": "884", "context_id": "95", "dataset_context": "The Sports-1M dataset exceeds all datasets with respect to number of clips (1.1 million) and categories (487 different sports), which are, however, only weakly labeled.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The Sports-1M dataset"}, {"mentioned_in_paper": "884", "context_id": "95", "dataset_context": "The Sports-1M dataset exceeds all datasets with respect to number of clips (1.1 million) and categories (487 different sports), which are, however, only weakly labeled.", "mention_start": 0, "mention_end": 42, "dataset_mention": "The Sports-1M dataset exceeds all datasets"}, {"mentioned_in_paper": "884", "context_id": "107", "dataset_context": "The datasets Coffee and Cigarettes (Laptev and P\u00e9rez 2007) and High Five (Patron-Perez et al. 2010) are different to the other movie datasets by promoting activity detection rather than classification.", "mention_start": 117, "mention_end": 141, "dataset_mention": "the other movie datasets"}, {"mentioned_in_paper": "884", "context_id": "110", "dataset_context": "The recent MPII Movie Description dataset (Rohrbach et al. 2015) does not label clips with labels but with natural sentences which are sourced from movie scripts and audio descriptions for the blind.", "mention_start": 0, "mention_end": 41, "dataset_mention": "The recent MPII Movie Description dataset"}, {"mentioned_in_paper": "884", "context_id": "112", "dataset_context": "The PETS (Ferryman 2007) or SDHA2010 2 workshop datasets contain real world situations from surveillance cameras in shops, subway stations, or airports.", "mention_start": 28, "mention_end": 56, "dataset_mention": "SDHA2010 2 workshop datasets"}, {"mentioned_in_paper": "884", "context_id": "115", "dataset_context": "The VIRAT (Oh et al. 2011) dataset is a recent attempt to provide a large scale dataset with 23 activities on nearly 30 hours of video.", "mention_start": 0, "mention_end": 34, "dataset_mention": "The VIRAT (Oh et al. 2011) dataset"}, {"mentioned_in_paper": "884", "context_id": "118", "dataset_context": "Next we discuss the domain of Assisted daily living (ADL) datasets, which also includes our dataset.", "mention_start": 30, "mention_end": 66, "dataset_mention": "Assisted daily living (ADL) datasets"}, {"mentioned_in_paper": "884", "context_id": "119", "dataset_context": "The University of Rochester Activities of Daily Living Dataset (URADL) (Messing et al. 2009) provides high-resolution videos of 10 different activities such as answer phone, chop banana, or peel banana.", "mention_start": 42, "mention_end": 62, "dataset_mention": "Daily Living Dataset"}, {"mentioned_in_paper": "884", "context_id": "121", "dataset_context": "In the TUM Kitchen dataset (Tenorth et al. 2009) all subjects perform the same composite activity (setting a table) and rather similar actions with limited variation.", "mention_start": 3, "mention_end": 26, "dataset_mention": "the TUM Kitchen dataset"}, {"mentioned_in_paper": "884", "context_id": "125", "dataset_context": "In the CMU-MMAC dataset (la Torre et al. 2009) all subjects prepare the identical five dishes with very similar ingredients and tools.", "mention_start": 3, "mention_end": 23, "dataset_mention": "the CMU-MMAC dataset"}, {"mentioned_in_paper": "884", "context_id": "131", "dataset_context": "Overall our dataset fills the gap of a large database with on the one hand a detection challenge of finegrained activities and on the other hand a recognition challenge of highly variable composite activities.", "mention_start": 0, "mention_end": 19, "dataset_mention": "Overall our dataset"}, {"mentioned_in_paper": "884", "context_id": "154", "dataset_context": " Jhuang et al. (2013) study the benefits of pose estimation for activity recognition on a subset of the HMDB dataset (Kuehne et al. 2011).", "mention_start": 99, "mention_end": 116, "dataset_mention": "the HMDB dataset"}, {"mentioned_in_paper": "884", "context_id": "163", "dataset_context": "On two fine-grained visual recognition datasets, CU200 Birds (Welinder et al. 2010) and Oxford Flower-102 (Nilsback and Zisserman 2008), they show the benefit of their constraint optimization approach.", "mention_start": 3, "mention_end": 47, "dataset_mention": "two fine-grained visual recognition datasets"}, {"mentioned_in_paper": "884", "context_id": "168", "dataset_context": "A similar idea is pursued by Motwani and Mooney (2012), who mine and cluster verbs from descriptions of the video snippets in the MSVD dataset (Chen and Dolan 2011).", "mention_start": 125, "mention_end": 142, "dataset_mention": "the MSVD dataset"}, {"mentioned_in_paper": "884", "context_id": "174", "dataset_context": " Das et al. (2013) compose descriptions for kitchen videos of their YouCook dataset showing YouTube cooking videos.", "mention_start": 61, "mention_end": 83, "dataset_mention": "their YouCook dataset"}, {"mentioned_in_paper": "884", "context_id": "176", "dataset_context": "Most of the activity recognition approaches and datasets have been evaluated on full-body motion or challenging web or movie datasets but not on fine-grained motions with low inter-class variability.", "mention_start": 8, "mention_end": 56, "dataset_mention": "the activity recognition approaches and datasets"}, {"mentioned_in_paper": "884", "context_id": "176", "dataset_context": "Most of the activity recognition approaches and datasets have been evaluated on full-body motion or challenging web or movie datasets but not on fine-grained motions with low inter-class variability.", "mention_start": 119, "mention_end": 133, "dataset_mention": "movie datasets"}, {"mentioned_in_paper": "884", "context_id": "177", "dataset_context": "We therefore evaluate the holistic Dense Trajectories approach from Wang et al. (2013a) as well as two pose-based and two hand centric approaches on our MPII Cooking 2 dataset.", "mention_start": 149, "mention_end": 175, "dataset_mention": "our MPII Cooking 2 dataset"}, {"mentioned_in_paper": "884", "context_id": "203", "dataset_context": "First, we updated the dataset by correcting and unifying some of the annotations and adding a few more videos.", "mention_start": 6, "mention_end": 29, "dataset_mention": " we updated the dataset"}, {"mentioned_in_paper": "884", "context_id": "204", "dataset_context": "We refer to this new version as MPII Cooking 2. It supersedes both previous datasets, see Table 3.", "mention_start": 32, "mention_end": 84, "dataset_mention": "MPII Cooking 2. It supersedes both previous datasets"}, {"mentioned_in_paper": "884", "context_id": "223", "dataset_context": "We refer to this new dataset version as MPII Cooking 2. It supersedes both previous datasets.", "mention_start": 40, "mention_end": 92, "dataset_mention": "MPII Cooking 2. It supersedes both previous datasets"}, {"mentioned_in_paper": "885", "context_id": "24", "dataset_context": "To show the effectiveness of our proposed approach, we conduct experiments on PCQM4M-LSC dataset [Hu et al., 2021b], inlcluded in the 2021 KDD Cup on OGB Large-Scale Challenge 1, to predict the HOMO-LUMO energy gap of molecules without given 3D equilibrium structures.", "mention_start": 77, "mention_end": 96, "dataset_mention": "PCQM4M-LSC dataset"}, {"mentioned_in_paper": "887", "context_id": "142", "dataset_context": "We also report our results on the sequestered dataset of the 3D Face Alignment in the Wild Challenge (3DFAW), done in conjunction with the 2016 European Conference on Computer Vision (ECCV), where the herein derived algorithm was a top performer.", "mention_start": 30, "mention_end": 53, "dataset_mention": "the sequestered dataset"}, {"mentioned_in_paper": "887", "context_id": "153", "dataset_context": "During Fig. 3 : Result of our algorithm on images of the humans in-the-wild dataset [35].", "mention_start": 52, "mention_end": 83, "dataset_mention": "the humans in-the-wild dataset"}, {"mentioned_in_paper": "887", "context_id": "156", "dataset_context": "effectiveness and generality of our method, we randomly selected several 2D images from the human-in-the-wild dataset [35] and used the herein derived algorithm to recover the 3D shape of the human bodies in these images.", "mention_start": 87, "mention_end": 117, "dataset_mention": "the human-in-the-wild dataset"}, {"mentioned_in_paper": "887", "context_id": "182", "dataset_context": "Reconstruction error was reported by the organizers of the competition on a sequestered dataset to which we did not have prior access.", "mention_start": 74, "mention_end": 95, "dataset_mention": "a sequestered dataset"}, {"mentioned_in_paper": "888", "context_id": "8", "dataset_context": "Finally, we conduct a series of experiments on the DTU dataset and Tanks & Temples dataset that demonstrate the efficiency and robustness of our DS-MVSNet compared with the state-of-the-art methods.", "mention_start": 46, "mention_end": 62, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "888", "context_id": "8", "dataset_context": "Finally, we conduct a series of experiments on the DTU dataset and Tanks & Temples dataset that demonstrate the efficiency and robustness of our DS-MVSNet compared with the state-of-the-art methods.", "mention_start": 46, "mention_end": 90, "dataset_mention": "the DTU dataset and Tanks & Temples dataset"}, {"mentioned_in_paper": "888", "context_id": "38", "dataset_context": "\u2022 We propose a simplified unsupervised MVS structure, DS-MVSNet, which is trained in an end-to-end manner and performs the dense reconstruction experiments on MVS datasets and achieves the best overall performance.", "mention_start": 158, "mention_end": 171, "dataset_mention": "MVS datasets"}, {"mentioned_in_paper": "888", "context_id": "163", "dataset_context": "The DTU dataset is a large-scale indoor multi-view stereo dataset collected under well-controlled laboratory conditions with a fixed camera trajectory.", "mention_start": 0, "mention_end": 15, "dataset_mention": "The DTU dataset"}, {"mentioned_in_paper": "888", "context_id": "163", "dataset_context": "The DTU dataset is a large-scale indoor multi-view stereo dataset collected under well-controlled laboratory conditions with a fixed camera trajectory.", "mention_start": 19, "mention_end": 65, "dataset_mention": "a large-scale indoor multi-view stereo dataset"}, {"mentioned_in_paper": "888", "context_id": "167", "dataset_context": "The proposed DS-MVSNet is trained on the DTU dataset.", "mention_start": 37, "mention_end": 52, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "888", "context_id": "176", "dataset_context": "In this section, we compare our proposed DS-MVSNet with the state-of-the-art methods including supervised and unsupervised methods on the DTU dataset.", "mention_start": 133, "mention_end": 149, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "888", "context_id": "189", "dataset_context": "Furthermore, we evaluate the generalization ability of our DS-MVSNet method by using the model trained on the DTU dataset without fine-tuning process and reconstructing point clouds on the intermediate Tanks and Temples dataset.", "mention_start": 105, "mention_end": 121, "dataset_mention": "the DTU dataset"}, {"mentioned_in_paper": "888", "context_id": "189", "dataset_context": "Furthermore, we evaluate the generalization ability of our DS-MVSNet method by using the model trained on the DTU dataset without fine-tuning process and reconstructing point clouds on the intermediate Tanks and Temples dataset.", "mention_start": 184, "mention_end": 227, "dataset_mention": "the intermediate Tanks and Temples dataset"}, {"mentioned_in_paper": "888", "context_id": "191", "dataset_context": "We use the camera parameters provided by MVSNet [37] as the input and use the Fscore [15] as the evaluation metric to measure the accuracy and completeness of the Tanks and Temples dataset.", "mention_start": 159, "mention_end": 188, "dataset_mention": "the Tanks and Temples dataset"}, {"mentioned_in_paper": "888", "context_id": "197", "dataset_context": "Unless specified, all the following studies are done on DTU evaluation dataset.", "mention_start": 55, "mention_end": 78, "dataset_mention": "DTU evaluation dataset"}, {"mentioned_in_paper": "889", "context_id": "266", "dataset_context": "Specifically, we randomly generated 500 pointwise maps between a human shape from the FAUST-remeshed dataset, and apply different refinement methods including recent state-of-the-art methods RHM [Ezuz et al. 2019] and ZoomOut [Melzi et al. 2019b].", "mention_start": 81, "mention_end": 108, "dataset_mention": "the FAUST-remeshed dataset"}, {"mentioned_in_paper": "889", "context_id": "277", "dataset_context": "We compiled a small test dataset consisting of 7 interesting shapes for the recovery of maps from the shape to itself and 5 shape pairs for the recovery of maps between two shapes.", "mention_start": 0, "mention_end": 32, "dataset_mention": "We compiled a small test dataset"}, {"mentioned_in_paper": "889", "context_id": "309", "dataset_context": "We apply our map tree exploration Algorithm 2 to the SHREC'19 Challenge dataset [Melzi et al. 2019a], which involves 430 shape pairs from 44 shapes with different mesh resolution, triangulation, and partiality.", "mention_start": 49, "mention_end": 79, "dataset_mention": "the SHREC'19 Challenge dataset"}, {"mentioned_in_paper": "889", "context_id": "343", "dataset_context": "Table 5 reports a quantitative evaluation on self-symmetry detection on the SHREC'19 dataset that consists of 44 shapes with different poses.", "mention_start": 72, "mention_end": 92, "dataset_mention": "the SHREC'19 dataset"}, {"mentioned_in_paper": "889", "context_id": "361", "dataset_context": "Specifically, the SHREC'19 dataset consists of 44 shapes with different mesh resolution and triangulation.", "mention_start": 13, "mention_end": 34, "dataset_mention": " the SHREC'19 dataset"}, {"mentioned_in_paper": "889", "context_id": "462", "dataset_context": "We test 210 shape pairs from SCAPE dataset and report the average accuracy and runtime for different baseline methods.", "mention_start": 29, "mention_end": 42, "dataset_mention": "SCAPE dataset"}, {"mentioned_in_paper": "890", "context_id": "7", "dataset_context": "We introduce a CP-Net for 3D object classification that achieves high accuracy for the ModelNet40 dataset among point cloud-based methods, which validates the effectiveness of the CPL.", "mention_start": 83, "mention_end": 105, "dataset_mention": "the ModelNet40 dataset"}, {"mentioned_in_paper": "890", "context_id": "70", "dataset_context": "This differs with methods such as [5], in which a novel method is proposed to down-sample a specific dataset.", "mention_start": 77, "mention_end": 108, "dataset_mention": "down-sample a specific dataset"}, {"mentioned_in_paper": "890", "context_id": "154", "dataset_context": "\u2022 Finally, fully connected layers of size 512, 256 and 40 are applied to transform the feature vector of size 1024 to the number of classes in the ModelNet40 dataset [27], which is 40.", "mention_start": 142, "mention_end": 165, "dataset_mention": "the ModelNet40 dataset"}, {"mentioned_in_paper": "890", "context_id": "157", "dataset_context": "We evaluate our model on ModelNet40 3D object classification dataset [27].", "mention_start": 25, "mention_end": 68, "dataset_mention": "ModelNet40 3D object classification dataset"}, {"mentioned_in_paper": "890", "context_id": "203", "dataset_context": "Experimental results using a common dataset show the competitiveness of the proposed method in terms of classification accuracy in comparison to previous state-of-the-art methods.", "mention_start": 27, "mention_end": 43, "dataset_mention": "a common dataset"}, {"mentioned_in_paper": "892", "context_id": "0", "dataset_context": "Dealing with sparse, long-tailed datasets, and coldstart problems is always a challenge for recommender systems.", "mention_start": 20, "mention_end": 41, "dataset_mention": " long-tailed datasets"}, {"mentioned_in_paper": "892", "context_id": "21", "dataset_context": "Our goal in doing so is to improve upon existing models, especially when dealing with user cold-start issues in sparse datasets.", "mention_start": 111, "mention_end": 127, "dataset_mention": "sparse datasets"}, {"mentioned_in_paper": "892", "context_id": "23", "dataset_context": "We make several simplifying assumptions to deal with the otherwise prohibitive number of parameters introduced by such a general formulation, especially when dealing with sparse datasets.", "mention_start": 170, "mention_end": 186, "dataset_mention": "sparse datasets"}, {"mentioned_in_paper": "892", "context_id": "58", "dataset_context": "The objective of our task is to predict the sequential behavior of users given the above information on sparse datasets, where dealing with user cold-start issues is paramount.", "mention_start": 104, "mention_end": 119, "dataset_mention": "sparse datasets"}, {"mentioned_in_paper": "892", "context_id": "109", "dataset_context": "\u02dcjamalim/datasets/ modeling users' preferences and the strength of Markov chains at capturing sequential continuity.", "mention_start": 0, "mention_end": 17, "dataset_mention": "\u02dcjamalim/datasets"}, {"mentioned_in_paper": "892", "context_id": "119", "dataset_context": "To this end, for each of the four datasets introduced earlier, we obtain a series of user cold-start datasets by varying a threshold N .", "mention_start": 84, "mention_end": 109, "dataset_mention": "user cold-start datasets"}, {"mentioned_in_paper": "893", "context_id": "7", "dataset_context": "Furthermore, we evaluated the approach on real-world medical datasets.", "mention_start": 41, "mention_end": 69, "dataset_mention": "real-world medical datasets"}, {"mentioned_in_paper": "893", "context_id": "8", "dataset_context": "On ten publicly available lung CT images from the DIR-Lab 4DCT dataset, we achieve the best mean landmark error of 0.93 mm compared to other submissions on the DIR-Lab website, with an average runtime of only 9.23 s.", "mention_start": 46, "mention_end": 70, "dataset_mention": "the DIR-Lab 4DCT dataset"}, {"mentioned_in_paper": "893", "context_id": "515", "dataset_context": "The maximum inhale and exhale images come with  Average landmark errors in millimeters and execution times on CPU in seconds for the DIR-Lab 4DCT datasets after affine-linear pre-alignment (Initial) and after deformable registration (Proposed).", "mention_start": 128, "mention_end": 154, "dataset_mention": "the DIR-Lab 4DCT datasets"}, {"mentioned_in_paper": "894", "context_id": "217", "dataset_context": "Of these, MNIST, EMNIST, FASHION-MNIST, and QMNIST were commonly used high-dimensional datasets [31, 32, 33], while ADULT, BANK, YEAST, LETTER, IMDB were low-dimensional datasets.", "mention_start": 69, "mention_end": 95, "dataset_mention": "high-dimensional datasets"}, {"mentioned_in_paper": "894", "context_id": "217", "dataset_context": "Of these, MNIST, EMNIST, FASHION-MNIST, and QMNIST were commonly used high-dimensional datasets [31, 32, 33], while ADULT, BANK, YEAST, LETTER, IMDB were low-dimensional datasets.", "mention_start": 143, "mention_end": 178, "dataset_mention": " IMDB were low-dimensional datasets"}, {"mentioned_in_paper": "894", "context_id": "244", "dataset_context": "Experiments were conducted on the MNIST, EMNIST, FASHION-MNIST, and QMNIST datasets, since these need to be processed by multi-grained scanning.", "mention_start": 63, "mention_end": 83, "dataset_mention": " and QMNIST datasets"}, {"mentioned_in_paper": "894", "context_id": "248", "dataset_context": "For example, from Table 3, it can be seen that H-gcForestcs has a time cost that is 482.5 s (41.83%) lower than for gcForestcs on the MNIST dataset.", "mention_start": 129, "mention_end": 147, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "894", "context_id": "266", "dataset_context": "For example, for the MNIST dataset in Table 8, the accuracy of H-gcForest is not significantly different from that of gcForest, since T 0.05,4 is 2.13, which is larger than the t-test statistic of 0.88.", "mention_start": 16, "mention_end": 34, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "894", "context_id": "267", "dataset_context": "Furthermore, for the MNIST dataset, Table 7 shows that the time costs of H-Forest and HW-Forest are significantly different, since the t-test statistic is 192.15, which is much larger than 2.13.", "mention_start": 16, "mention_end": 34, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "894", "context_id": "272", "dataset_context": "We conducted experiments with windows of size 2\u00d72, 4\u00d74, 6\u00d76, and 8\u00d78, and selected 10 instances from the FASHION-MNIST dataset.", "mention_start": 100, "mention_end": 126, "dataset_mention": "the FASHION-MNIST dataset"}, {"mentioned_in_paper": "894", "context_id": "277", "dataset_context": "The reason for this is that on the FASHION-MNSIT dataset, the difference between the number of redundant feature vectors and important feature vectors is small.", "mention_start": 31, "mention_end": 56, "dataset_mention": "the FASHION-MNSIT dataset"}, {"mentioned_in_paper": "894", "context_id": "300", "dataset_context": "We selected DBC-Forest as a competitive algorithm and used the MNIST, EMNIST, FASHION-MNIST, QMNIST, and IMDB datasets, since our model and competitive algorithm produced more layers for these datasets.", "mention_start": 100, "mention_end": 118, "dataset_mention": " and IMDB datasets"}, {"mentioned_in_paper": "894", "context_id": "303", "dataset_context": "For example, it can be seen from Figure 13 that for the IMDB dataset, the accuracy of HW-Forest is 0.09% higher than DBC-Forest at the second level, and 0.19% higher than DBC-Forest at the last level.", "mention_start": 51, "mention_end": 68, "dataset_mention": "the IMDB dataset"}, {"mentioned_in_paper": "894", "context_id": "307", "dataset_context": "For example, on the MNIST dataset, HW-Forest has 1,213 fewer instances than DBC-Forest at the second level, and 146 fewer instances than DBC-Forest at the last level.", "mention_start": 15, "mention_end": 33, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "894", "context_id": "312", "dataset_context": "Comparisons of the thresholds for the MNIST, IMDB, LETTER, and BANK datasets are shown in Figures 14 to 17, respectively.", "mention_start": 58, "mention_end": 76, "dataset_mention": " and BANK datasets"}, {"mentioned_in_paper": "894", "context_id": "314", "dataset_context": "For example, on the MNIST dataset, DBC-Forest cannot avoid accuracy fluctuations, and some high-confidence instances screened with this threshold have low accuracy.", "mention_start": 15, "mention_end": 33, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "894", "context_id": "322", "dataset_context": "For example, on the QMNIST dataset, HW-Forest achieves the highest accuracy of 99.07%.", "mention_start": 15, "mention_end": 34, "dataset_mention": "the QMNIST dataset"}, {"mentioned_in_paper": "894", "context_id": "326", "dataset_context": "As shown in Table 11, for high-dimensional datasets, HW-Forest runs faster than the competitive models in most cases; for low-dimensional datasets, HW-Forest runs slower than gcForestcs, but faster than the other competitive models.", "mention_start": 25, "mention_end": 51, "dataset_mention": "high-dimensional datasets"}, {"mentioned_in_paper": "894", "context_id": "326", "dataset_context": "As shown in Table 11, for high-dimensional datasets, HW-Forest runs faster than the competitive models in most cases; for low-dimensional datasets, HW-Forest runs slower than gcForestcs, but faster than the other competitive models.", "mention_start": 121, "mention_end": 146, "dataset_mention": "low-dimensional datasets"}, {"mentioned_in_paper": "894", "context_id": "327", "dataset_context": "For example, on the MNIST dataset, the time cost for HW-gcForest is 1135.42", "mention_start": 15, "mention_end": 33, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "894", "context_id": "329", "dataset_context": "On the ADULT dataset, the time cost of HW-gcForest is 10.15 s, which is slower than gcForestcs but faster than the other competitive models.", "mention_start": 3, "mention_end": 20, "dataset_mention": "the ADULT dataset"}, {"mentioned_in_paper": "894", "context_id": "330", "dataset_context": "The reason for this is that for high-dimensional datasets, deep forest models take a long time to find the classification models.", "mention_start": 32, "mention_end": 57, "dataset_mention": "high-dimensional datasets"}, {"mentioned_in_paper": "894", "context_id": "332", "dataset_context": "HW-Forest employs hashing screening to effectively eliminate redundant feature vectors, and therefore runs faster than the competitive models for high-dimensional datasets.", "mention_start": 145, "mention_end": 171, "dataset_mention": "high-dimensional datasets"}, {"mentioned_in_paper": "894", "context_id": "333", "dataset_context": "However, for low-dimensional datasets, the deep forest models take only a short time to find the classification models, and these datasets do not need to be processed by multigrained scanning.", "mention_start": 12, "mention_end": 37, "dataset_mention": "low-dimensional datasets"}, {"mentioned_in_paper": "894", "context_id": "336", "dataset_context": "For example, from Table 10, we can see that for the QMNIST dataset, HW-Forest and W-Forest achieve the same accuracy of 99.13%.", "mention_start": 47, "mention_end": 66, "dataset_mention": "the QMNIST dataset"}, {"mentioned_in_paper": "894", "context_id": "337", "dataset_context": "The same effect can be seen for the ADULT, BANK, YEAST, IMDB, and LETTER datasets.", "mention_start": 61, "mention_end": 81, "dataset_mention": " and LETTER datasets"}, {"mentioned_in_paper": "894", "context_id": "338", "dataset_context": "For the MNIST, FASHION-MNIST, and EMNIST datasets, the results for the accuracy are close.", "mention_start": 29, "mention_end": 49, "dataset_mention": " and EMNIST datasets"}, {"mentioned_in_paper": "894", "context_id": "339", "dataset_context": "The reason is for this is that low-dimensional datasets do not need to be processed by multi-grained scanning.", "mention_start": 26, "mention_end": 55, "dataset_mention": "that low-dimensional datasets"}, {"mentioned_in_paper": "894", "context_id": "341", "dataset_context": "For high-dimensional datasets, as demonstrated in our analysis of the effect of hashing screening in Section 5.3, the hashing screening mechanism does not influence the accuracy of the model.", "mention_start": 4, "mention_end": 29, "dataset_mention": "high-dimensional datasets"}, {"mentioned_in_paper": "895", "context_id": "19", "dataset_context": "These three types of methods show satisfactory performance but suffer from two main disadvantages: (1) These models are generally shallow models with limited capacity to reveal the relations in complex multi-view data; (2) It is inefficient to employ them for large-scale and high-dimensional datasets.", "mention_start": 259, "mention_end": 301, "dataset_mention": "large-scale and high-dimensional datasets"}, {"mentioned_in_paper": "895", "context_id": "120", "dataset_context": "Especially on the BDGP dataset with a missing-view rate of 30%, ACTIVE obtains about 91% ACC and 76% NMI, which are about 12% and 17% higher than those of the second-best method, respectively.", "mention_start": 14, "mention_end": 30, "dataset_mention": "the BDGP dataset"}, {"mentioned_in_paper": "895", "context_id": "123", "dataset_context": "To more intuitively show the superiority of the representation obtained by our ACTIVE, t-SNE [Maaten and Hinton, 2008] is used to visualize the results of different methods on the BDGP dataset with a paired-view ratio of 30% in Fig. 3.", "mention_start": 175, "mention_end": 192, "dataset_mention": "the BDGP dataset"}, {"mentioned_in_paper": "895", "context_id": "134", "dataset_context": "To examine the effect of K, we design a K-sensitivity experiment on the BDGP datasets with K \u2208 {1, 10}.", "mention_start": 67, "mention_end": 85, "dataset_mention": "the BDGP datasets"}, {"mentioned_in_paper": "895", "context_id": "137", "dataset_context": "We then investigate the sensitivity of parameters \u03b1 and \u03b2, and the ACC and NMI results on Coil20 datasets are shown in Fig. 4(c).", "mention_start": 89, "mention_end": 105, "dataset_mention": "Coil20 datasets"}, {"mentioned_in_paper": "896", "context_id": "12", "dataset_context": "A solution presented as a logical tree is a very convenient way to analyze a dataset and interpret a solution.", "mention_start": 67, "mention_end": 84, "dataset_mention": "analyze a dataset"}, {"mentioned_in_paper": "897", "context_id": "7", "dataset_context": "Subsequently, we train this network end to end on a large visible video detection dataset to learn the similarity between paired objects before we transfer the network into the TIR domain.", "mention_start": 49, "mention_end": 89, "dataset_mention": "a large visible video detection dataset"}, {"mentioned_in_paper": "897", "context_id": "43", "dataset_context": "Therefore, to handle the lack of training data, we train the proposed Siamese network on a large visible video dataset to learn the similarity between paired objects before we transfer the learned network into the TIR domain.", "mention_start": 88, "mention_end": 118, "dataset_mention": "a large visible video dataset"}, {"mentioned_in_paper": "897", "context_id": "168", "dataset_context": "Given the limited scale of the existing TIR tracking and detection dataset, we choose the RGB detection video dataset: ILSVRC2015 because we believe that TIR objects and RGB objects have something in common.", "mention_start": 27, "mention_end": 74, "dataset_mention": "the existing TIR tracking and detection dataset"}, {"mentioned_in_paper": "897", "context_id": "168", "dataset_context": "Given the limited scale of the existing TIR tracking and detection dataset, we choose the RGB detection video dataset: ILSVRC2015 because we believe that TIR objects and RGB objects have something in common.", "mention_start": 85, "mention_end": 117, "dataset_mention": "the RGB detection video dataset"}, {"mentioned_in_paper": "897", "context_id": "171", "dataset_context": "Once the proposed network has been learned from the ILSVRC2015 dataset, we transfer it into the TIR domain.", "mention_start": 48, "mention_end": 70, "dataset_mention": "the ILSVRC2015 dataset"}, {"mentioned_in_paper": "897", "context_id": "174", "dataset_context": "In order to train the network, we prepare the training pairs (a target and a corresponding search region) from a large visible images video detection dataset from ImageNet [54] and the corresponding labels as in [21].", "mention_start": 110, "mention_end": 157, "dataset_mention": "a large visible images video detection dataset"}, {"mentioned_in_paper": "898", "context_id": "37", "dataset_context": "Following this idea, we formulate the general problem of finding dataset invariants and show that, when constrained to a linear setting, this formulation reduces to the MahaAD method, which unknowingly embodies a dataset invariant characterization.", "mention_start": 56, "mention_end": 72, "dataset_mention": "finding dataset"}, {"mentioned_in_paper": "898", "context_id": "37", "dataset_context": "Following this idea, we formulate the general problem of finding dataset invariants and show that, when constrained to a linear setting, this formulation reduces to the MahaAD method, which unknowingly embodies a dataset invariant characterization.", "mention_start": 189, "mention_end": 220, "dataset_mention": "unknowingly embodies a dataset"}, {"mentioned_in_paper": "898", "context_id": "39", "dataset_context": "In summary, the contributions of this paper include (1) a thorough evaluation of numerous state-of-the-art U-OOD methods on different tasks and datasets, whereby highlighting that most methods perform erratically and inconsistently, (2) a novel interpretation of U-OOD using training set invariants, which allows for an appropriate definition of U-OOD and (3) a new U-OOD benchmark derived from our novel interpretation with invariants.", "mention_start": 123, "mention_end": 152, "dataset_mention": "different tasks and datasets"}, {"mentioned_in_paper": "898", "context_id": "46", "dataset_context": "Supervised OOD detection approaches require either an explicitly trained classifier or a labelled dataset to work.", "mention_start": 87, "mention_end": 105, "dataset_mention": "a labelled dataset"}, {"mentioned_in_paper": "898", "context_id": "69", "dataset_context": "Finally, Rippel et al. [60] combined Mahalanobis distances in the space of ImageNet features for state-of-the-art results on the MVTec dataset.", "mention_start": 124, "mention_end": 142, "dataset_mention": "the MVTec dataset"}, {"mentioned_in_paper": "898", "context_id": "131", "dataset_context": "We refer to a specific experiment by the notation in-dataset:out-dataset.", "mention_start": 37, "mention_end": 60, "dataset_mention": "the notation in-dataset"}, {"mentioned_in_paper": "898", "context_id": "131", "dataset_context": "We refer to a specific experiment by the notation in-dataset:out-dataset.", "mention_start": 61, "mention_end": 72, "dataset_mention": "out-dataset"}, {"mentioned_in_paper": "898", "context_id": "153", "dataset_context": "These performance instabilities were not only observed across the different tasks reported in Table 1, but also within the tasks with fixed in-distribution across different OOD datasets.", "mention_start": 162, "mention_end": 185, "dataset_mention": "different OOD datasets"}, {"mentioned_in_paper": "898", "context_id": "154", "dataset_context": "For example, for the shift-high-res task, performance of most methods fluctuated depending on the chosen OOD dataset (see Fig. 4).", "mention_start": 93, "mention_end": 116, "dataset_mention": "the chosen OOD dataset"}, {"mentioned_in_paper": "898", "context_id": "158", "dataset_context": "( * ) Taken from original publication; ( + ) taken from [58]; ( \u2212 ) taken from [76]; ( \u2020 ) taken from [70] Method of stability, as it performs well regardless of the in and the out datasets selected.", "mention_start": 168, "mention_end": 189, "dataset_mention": "and the out datasets"}, {"mentioned_in_paper": "898", "context_id": "161", "dataset_context": "For example, in the CIFAR10:SVHN experiment (task shift-low-res), using two GeForce RTX 3090s, MahaAD was the fastest to train, taking roughly 90 seconds to process the entire CIFAR10 dataset.", "mention_start": 156, "mention_end": 191, "dataset_mention": "process the entire CIFAR10 dataset"}, {"mentioned_in_paper": "898", "context_id": "170", "dataset_context": "We report here additional results that support the importance of data invariants, both for the quality of U-OOD detection and as a tool to analyse U-OOD predictions and evaluation datasets.", "mention_start": 138, "mention_end": 188, "dataset_mention": "analyse U-OOD predictions and evaluation datasets"}, {"mentioned_in_paper": "898", "context_id": "218", "dataset_context": "(In) The NIH Clinical Center ChestX-ray dataset containing 85524 training images.", "mention_start": 0, "mention_end": 47, "dataset_mention": "(In) The NIH Clinical Center ChestX-ray dataset"}, {"mentioned_in_paper": "898", "context_id": "222", "dataset_context": "(In) A collection of 4261 healthy X-ray scans of the NIH Clinical Center ChestX-ray dataset.", "mention_start": 49, "mention_end": 91, "dataset_mention": "the NIH Clinical Center ChestX-ray dataset"}, {"mentioned_in_paper": "898", "context_id": "232", "dataset_context": "We only use it as an OOD dataset, where the test set is reduced to 10000 samples.", "mention_start": 18, "mention_end": 32, "dataset_mention": "an OOD dataset"}, {"mentioned_in_paper": "898", "context_id": "236", "dataset_context": "(Out) 10 domainclass combinations are used as OOD datasets.", "mention_start": 46, "mention_end": 58, "dataset_mention": "OOD datasets"}, {"mentioned_in_paper": "898", "context_id": "248", "dataset_context": "HierAD [70] computes the ratio between the Glow generative model likelihood and a general background likelihood consisting of a Glow model trained on the 80 Million Tiny Images dataset [79], provided at 2.", "mention_start": 150, "mention_end": 184, "dataset_mention": "the 80 Million Tiny Images dataset"}, {"mentioned_in_paper": "899", "context_id": "20", "dataset_context": "He has received no large dataset, and the only direct communication with him has been composed solely of rather empty pleasantries and the one perfunctory declaration that he has been served an apertif.", "mention_start": 7, "mention_end": 32, "dataset_mention": "received no large dataset"}, {"mentioned_in_paper": "900", "context_id": "106", "dataset_context": "This collection contains a labelled dataset of MR images collected on 242 patients with VS undergoing Gamma Knife stereotactic radiosurgery (GK SRS).", "mention_start": 0, "mention_end": 43, "dataset_mention": "This collection contains a labelled dataset"}, {"mentioned_in_paper": "902", "context_id": "88", "dataset_context": "In order for our analysis to be valid the dataset must be large enough to catch cascades from a spectrum of sizes.", "mention_start": 32, "mention_end": 49, "dataset_mention": "valid the dataset"}, {"mentioned_in_paper": "903", "context_id": "7", "dataset_context": "Under the ISPRS 3D Semantic Labeling Contest dataset, the DAPnet outperforms the benchmark by 85.2% with an overall accuracy of 90.7%.", "mention_start": 0, "mention_end": 52, "dataset_mention": "Under the ISPRS 3D Semantic Labeling Contest dataset"}, {"mentioned_in_paper": "903", "context_id": "71", "dataset_context": "The processed blocks then reconstitute a new dataset.", "mention_start": 0, "mention_end": 52, "dataset_mention": "The processed blocks then reconstitute a new dataset"}, {"mentioned_in_paper": "903", "context_id": "197", "dataset_context": "The Vaihingen dataset has been proposed within the scope of the ISPRS test project on urban classification [51], [52], and it is the benchmark dataset for ISPRS 3D semantic labeling.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The Vaihingen dataset"}, {"mentioned_in_paper": "903", "context_id": "199", "dataset_context": "In detail, the ALS point cloud dataset consists of 1,165,598 points and is divided into two areas for training and testing.", "mention_start": 10, "mention_end": 38, "dataset_mention": " the ALS point cloud dataset"}, {"mentioned_in_paper": "903", "context_id": "207", "dataset_context": "This dataset is also an ALS point cloud dataset, captured in Jacksonville, Florida, and Omaha, Nebraska.", "mention_start": 21, "mention_end": 47, "dataset_mention": "an ALS point cloud dataset"}, {"mentioned_in_paper": "903", "context_id": "208", "dataset_context": "The DFC 3D dataset has 6 classes, which are Ground, High Vegetation/Trees (high veg), Building Roof (building), Water, Elevated Road/Bridge, and Unlabeled.", "mention_start": 0, "mention_end": 18, "dataset_mention": "The DFC 3D dataset"}, {"mentioned_in_paper": "903", "context_id": "217", "dataset_context": "The evaluation metrics of per-class accuracy, precision, recall, and F1-score are calculated using the ISPRS 3D Semantic Labeling Contest dataset.", "mention_start": 98, "mention_end": 145, "dataset_mention": "the ISPRS 3D Semantic Labeling Contest dataset"}, {"mentioned_in_paper": "903", "context_id": "251", "dataset_context": "The DAPnet trained on the Vaihingen 3D dataset is directly used to predict semantic labels for the DFC 3D dataset without retraining.", "mention_start": 22, "mention_end": 46, "dataset_mention": "the Vaihingen 3D dataset"}, {"mentioned_in_paper": "903", "context_id": "251", "dataset_context": "The DAPnet trained on the Vaihingen 3D dataset is directly used to predict semantic labels for the DFC 3D dataset without retraining.", "mention_start": 95, "mention_end": 113, "dataset_mention": "the DFC 3D dataset"}, {"mentioned_in_paper": "903", "context_id": "253", "dataset_context": "Here we mainly analyze the classification results of these three classes on the DFC 3D dataset.", "mention_start": 76, "mention_end": 94, "dataset_mention": "the DFC 3D dataset"}, {"mentioned_in_paper": "903", "context_id": "260", "dataset_context": "Compared to the GAPHNet, the improvement of DAPnet is not as significant as on the Vaihingen 3D dataset.", "mention_start": 78, "mention_end": 103, "dataset_mention": "the Vaihingen 3D dataset"}, {"mentioned_in_paper": "903", "context_id": "262", "dataset_context": "So even though the overall performance of the two models is close, we believe the DAPnet can be applied to more scenarios, and its best performance on the DFC 3D dataset attests to its good generalization ability.", "mention_start": 150, "mention_end": 169, "dataset_mention": "the DFC 3D dataset"}, {"mentioned_in_paper": "905", "context_id": "143", "dataset_context": "We used the FAUST [4] dataset containing scanned human shapes in different poses and the TOSCA [7] dataset containing synthetic models of humans in a variety of nearisometric deformations.", "mention_start": 8, "mention_end": 29, "dataset_mention": "the FAUST [4] dataset"}, {"mentioned_in_paper": "905", "context_id": "143", "dataset_context": "We used the FAUST [4] dataset containing scanned human shapes in different poses and the TOSCA [7] dataset containing synthetic models of humans in a variety of nearisometric deformations.", "mention_start": 65, "mention_end": 106, "dataset_mention": "different poses and the TOSCA [7] dataset"}, {"mentioned_in_paper": "905", "context_id": "158", "dataset_context": "On the FAUST dataset, we used subjects 1-7 for training, subject 8 for validation, and subject 9-10 for testing.", "mention_start": 3, "mention_end": 20, "dataset_mention": "the FAUST dataset"}, {"mentioned_in_paper": "905", "context_id": "166", "dataset_context": "Figure 5 (first row) shows the performance of different descriptors on the FAUST dataset.", "mention_start": 71, "mention_end": 88, "dataset_mention": "the FAUST dataset"}, {"mentioned_in_paper": "905", "context_id": "168", "dataset_context": "In order to test the generalization capability of the learned descriptors, we applied OSD and GCNN learned on the FAUST dataset to TOSCA shapes (Figure 5, second row).", "mention_start": 109, "mention_end": 127, "dataset_mention": "the FAUST dataset"}, {"mentioned_in_paper": "905", "context_id": "170", "dataset_context": "To show the application of GCNN for computing intrinsic correspondence, we reproduced the experiment of Rodol\u00e0 et al. [37] on the FAUST dataset, replacing their random forest with a GCNN architecture GCNN3 containing three convolutional layers (input: 150-dimensional geometry vectors, LIN16+ReLU, GC32+AMP+ReLU, GC64+AMP+ReLU, GC128+AMP+ReLU, LIN256, LIN6890).", "mention_start": 125, "mention_end": 143, "dataset_mention": "the FAUST dataset"}, {"mentioned_in_paper": "905", "context_id": "175", "dataset_context": "In our final experiment, we performed pose-invariant shape retrieval on the FAUST dataset.", "mention_start": 71, "mention_end": 89, "dataset_mention": "the FAUST dataset"}, {"mentioned_in_paper": "908", "context_id": "7", "dataset_context": "In the experimental analysis, we used a hospital dataset of 210,000 appointments collected in 2019.", "mention_start": 37, "mention_end": 56, "dataset_mention": "a hospital dataset"}, {"mentioned_in_paper": "908", "context_id": "41", "dataset_context": "We used an appointment scheduling dataset from our hospital.", "mention_start": 8, "mention_end": 41, "dataset_mention": "an appointment scheduling dataset"}, {"mentioned_in_paper": "913", "context_id": "44", "dataset_context": "Deep-Ten outperforms existing modular methods and achieves the state-of-the-art results on material/texture datasets such as MINC-2500 and KTH-TIPS-2b.", "mention_start": 91, "mention_end": 116, "dataset_mention": "material/texture datasets"}, {"mentioned_in_paper": "913", "context_id": "45", "dataset_context": "Additionally, this Deep Encoding Network performs well in general recognition tasks beyond texture and material as demonstrated with results on MIT-Indoor and Caltech-101 datasets.", "mention_start": 143, "mention_end": 179, "dataset_mention": "MIT-Indoor and Caltech-101 datasets"}, {"mentioned_in_paper": "913", "context_id": "129", "dataset_context": "Datasets The evaluation considers five material and texture datasets.", "mention_start": 0, "mention_end": 68, "dataset_mention": "Datasets The evaluation considers five material and texture datasets"}, {"mentioned_in_paper": "913", "context_id": "130", "dataset_context": "Materials in Context Database (MINC) [2] is a large scale material in the wild dataset.", "mention_start": 70, "mention_end": 86, "dataset_mention": "the wild dataset"}, {"mentioned_in_paper": "913", "context_id": "132", "dataset_context": "Flickr Material Dataset (FMD) [35], a popular benchmark for material recognition containing 10 material classes, 90 images per-class used for training and 10 for test.", "mention_start": 0, "mention_end": 23, "dataset_mention": "Flickr Material Dataset"}, {"mentioned_in_paper": "913", "context_id": "133", "dataset_context": "Ground Terrain in Outdoor Scenes Dataset (GTOS) [45] is a dataset of ground materials in outdoor scene with 40 categories.", "mention_start": 18, "mention_end": 40, "dataset_mention": "Outdoor Scenes Dataset"}, {"mentioned_in_paper": "913", "context_id": "137", "dataset_context": "4D-Light-Field-Material (4D-Light) [43] is a recent light-field material dataset containing 12 material categories with 100 samples per-category.", "mention_start": 43, "mention_end": 80, "dataset_mention": "a recent light-field material dataset"}, {"mentioned_in_paper": "913", "context_id": "140", "dataset_context": "MIT-Indoor [33] dataset is an indoor scene categorization dataset with 67 categories, a standard subset of 80 images per-category for training and 20 for test is used in this work.", "mention_start": 0, "mention_end": 23, "dataset_mention": "MIT-Indoor [33] dataset"}, {"mentioned_in_paper": "913", "context_id": "140", "dataset_context": "MIT-Indoor [33] dataset is an indoor scene categorization dataset with 67 categories, a standard subset of 80 images per-category for training and 20 for test is used in this work.", "mention_start": 27, "mention_end": 65, "dataset_mention": "an indoor scene categorization dataset"}, {"mentioned_in_paper": "913", "context_id": "141", "dataset_context": "Caltech 101 [16] is a 102 category (1 for background) object classification dataset; 10% randomly picked samples are used for test and the others for training.", "mention_start": 42, "mention_end": 83, "dataset_mention": "background) object classification dataset"}, {"mentioned_in_paper": "913", "context_id": "175", "dataset_context": "We evaluate the performance of Deep-TEN, FV-SIFT and FV-CNN on aforementioned golden-standard material and texture datasets, such as MINC-2500, FMD, KTH and two new material datasets: 4D-Light and GTOS.", "mention_start": 62, "mention_end": 123, "dataset_mention": "aforementioned golden-standard material and texture datasets"}, {"mentioned_in_paper": "913", "context_id": "175", "dataset_context": "We evaluate the performance of Deep-TEN, FV-SIFT and FV-CNN on aforementioned golden-standard material and texture datasets, such as MINC-2500, FMD, KTH and two new material datasets: 4D-Light and GTOS.", "mention_start": 148, "mention_end": 182, "dataset_mention": " KTH and two new material datasets"}, {"mentioned_in_paper": "913", "context_id": "180", "dataset_context": "Therefore, Deep-TEN works well on GTOS and KTH datasets.", "mention_start": 33, "mention_end": 55, "dataset_mention": "GTOS and KTH datasets"}, {"mentioned_in_paper": "913", "context_id": "185", "dataset_context": "For the MIT-Indoor dataset, the Encoding Layer works well on scene categorization due to the need for a certain level of orderless and invariance.", "mention_start": 4, "mention_end": 26, "dataset_mention": "the MIT-Indoor dataset"}, {"mentioned_in_paper": "913", "context_id": "190", "dataset_context": "Impact of Multi-size For in-the-wild datasets, such as MINC-2500 and MIT-Indoor, the performance of all the approaches are improved by adopting multi-size as expected.", "mention_start": 25, "mention_end": 45, "dataset_mention": "in-the-wild datasets"}, {"mentioned_in_paper": "913", "context_id": "191", "dataset_context": "Remarkably, as shown in Table 4, Deep-TEN shows a performance boost of 4.9% using multi-size training and outperforms the best baseline by 7.4% on MIT-Indoor dataset.", "mention_start": 146, "mention_end": 165, "dataset_mention": "MIT-Indoor dataset"}, {"mentioned_in_paper": "913", "context_id": "193", "dataset_context": "Figure 3 Comparison with State-of-the-Art As shown in Table 5, Deep-TEN outperforms the state-of-the-art on four material/texture recognition datasets: MINC-2500, KTH, GTOS and 4D-Light.", "mention_start": 107, "mention_end": 150, "dataset_mention": "four material/texture recognition datasets"}, {"mentioned_in_paper": "913", "context_id": "204", "dataset_context": "For the STL-10 dataset only the labeled images are used for training.", "mention_start": 4, "mention_end": 22, "dataset_mention": "the STL-10 dataset"}, {"mentioned_in_paper": "913", "context_id": "214", "dataset_context": "The experimental results show that the recognition result of STL-10 dataset is significantly improved by joint training the Deep TEN with CIFAR-10 dataset.", "mention_start": 61, "mention_end": 75, "dataset_mention": "STL-10 dataset"}, {"mentioned_in_paper": "913", "context_id": "214", "dataset_context": "The experimental results show that the recognition result of STL-10 dataset is significantly improved by joint training the Deep TEN with CIFAR-10 dataset.", "mention_start": 138, "mention_end": 154, "dataset_mention": "CIFAR-10 dataset"}, {"mentioned_in_paper": "913", "context_id": "218", "dataset_context": "Deep-TEN outperforms traditional off-the-shelf methods and achieves state-of-the-art results on MINC-2500, KTH and two recent material datasets: GTOS and 4D-Lightfield.", "mention_start": 106, "mention_end": 143, "dataset_mention": " KTH and two recent material datasets"}, {"mentioned_in_paper": "914", "context_id": "99", "dataset_context": "For simplicity, we refer to this dataset as desktop client dataset.", "mention_start": 43, "mention_end": 66, "dataset_mention": "desktop client dataset"}, {"mentioned_in_paper": "914", "context_id": "101", "dataset_context": "From this set, we further select users who also use the iOS app over the same period and collect their corresponding usage from the mobile logs, which is referred to as the mobile dataset.", "mention_start": 168, "mention_end": 187, "dataset_mention": "the mobile dataset"}, {"mentioned_in_paper": "915", "context_id": "2", "dataset_context": "Moreover, we construct a novel Package Theft Detection dataset to facilitate the research on this task.", "mention_start": 22, "mention_end": 62, "dataset_mention": "a novel Package Theft Detection dataset"}, {"mentioned_in_paper": "915", "context_id": "3", "dataset_context": "Our method achieves 80% AUC performance on the newly proposed dataset, showing the effectiveness of the proposed GLF-PTDE framework and its robustness in different real scenes for package theft detection.", "mention_start": 43, "mention_end": 69, "dataset_mention": "the newly proposed dataset"}, {"mentioned_in_paper": "915", "context_id": "23", "dataset_context": "\u2022 Build a novel package theft detection dataset to facilitate PTD research.", "mention_start": 8, "mention_end": 47, "dataset_mention": "a novel package theft detection dataset"}, {"mentioned_in_paper": "915", "context_id": "42", "dataset_context": "We use OpenPose [6] to obtain the human pose information, which is trained on COCO dataset to generate the human pose features.", "mention_start": 77, "mention_end": 90, "dataset_mention": "COCO dataset"}, {"mentioned_in_paper": "915", "context_id": "79", "dataset_context": "Here we use 18 joints so the human pose feature size is 18 \u00d7 3. The pose features are extracted by the COCO dataset pre-trained model since we do not have the human pose ground truth in our package theft dataset.", "mention_start": 99, "mention_end": 115, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "915", "context_id": "79", "dataset_context": "Here we use 18 joints so the human pose feature size is 18 \u00d7 3. The pose features are extracted by the COCO dataset pre-trained model since we do not have the human pose ground truth in our package theft dataset.", "mention_start": 186, "mention_end": 211, "dataset_mention": "our package theft dataset"}, {"mentioned_in_paper": "916", "context_id": "6", "dataset_context": "Furthermore, a novel reordering of the MARS dataset, called X-MARS is introduced to allow cross-validation of models trained for single-image re-id on tracklet data.", "mention_start": 34, "mention_end": 51, "dataset_mention": "the MARS dataset"}, {"mentioned_in_paper": "916", "context_id": "22", "dataset_context": "With the increase in available computational power and large datasets fueling the rise of Convolutional Neural Networks (CNNs), large improvements have been seen within the field computer vision in general and this challenging matching problem in particular.", "mention_start": 21, "mention_end": 69, "dataset_mention": "available computational power and large datasets"}, {"mentioned_in_paper": "916", "context_id": "49", "dataset_context": "Unfortunately, MARS cannot be used for cross-evaluations with the related single-image Market-1501 dataset [ZST + 15], because the training and test sets of both datasets overlap.", "mention_start": 61, "mention_end": 106, "dataset_mention": "the related single-image Market-1501 dataset"}, {"mentioned_in_paper": "916", "context_id": "50", "dataset_context": "To alleviate this problem the X-MARS dataset is proposed, which is a reordering of the MARS dataset to remove the overlap between Market-1501's and MARS' training and test sets.", "mention_start": 13, "mention_end": 44, "dataset_mention": "this problem the X-MARS dataset"}, {"mentioned_in_paper": "916", "context_id": "50", "dataset_context": "To alleviate this problem the X-MARS dataset is proposed, which is a reordering of the MARS dataset to remove the overlap between Market-1501's and MARS' training and test sets.", "mention_start": 82, "mention_end": 99, "dataset_mention": "the MARS dataset"}, {"mentioned_in_paper": "916", "context_id": "243", "dataset_context": "For example, the widely used Market-1501 [ZST + 15], MARS [Spr16] and DukeMTMC-reID [RSZ + 16a] datasets for person re-id do not contain any information on the person's angle towards the camera.", "mention_start": 52, "mention_end": 104, "dataset_mention": " MARS [Spr16] and DukeMTMC-reID [RSZ + 16a] datasets"}, {"mentioned_in_paper": "916", "context_id": "244", "dataset_context": "To still be able to utilize view information, the model's view predictor is pretrained on the RAP dataset [LZC + 16].", "mention_start": 89, "mention_end": 105, "dataset_mention": "the RAP dataset"}, {"mentioned_in_paper": "916", "context_id": "245", "dataset_context": "Although the RAP dataset does contain labels for back, front, right and left, the classes right and left are combined to one side class serving two purposes: At first, random horizontal flips are applied for data augmentation (see Section 3.4.1),", "mention_start": 9, "mention_end": 24, "dataset_mention": "the RAP dataset"}, {"mentioned_in_paper": "916", "context_id": "250", "dataset_context": "To train the view model, at first, the model's view predictor side-branch is trained on the RAP dataset while the layers of the main network remain fixed.", "mention_start": 87, "mention_end": 103, "dataset_mention": "the RAP dataset"}, {"mentioned_in_paper": "916", "context_id": "262", "dataset_context": "In the second step, the RAP dataset is used since it provides view labels to train the view predictor side-branch (marked in blue).", "mention_start": 19, "mention_end": 35, "dataset_mention": " the RAP dataset"}, {"mentioned_in_paper": "916", "context_id": "284", "dataset_context": "Examples for the Duke dataset can be seen in Figure 4.2", "mention_start": 13, "mention_end": 29, "dataset_mention": "the Duke dataset"}, {"mentioned_in_paper": "916", "context_id": "285", "dataset_context": "The MARS dataset [Spr16] is based on the same raw data as the Market dataset and the same persons have been assigned the same labels.", "mention_start": 0, "mention_end": 16, "dataset_mention": "The MARS dataset"}, {"mentioned_in_paper": "916", "context_id": "285", "dataset_context": "The MARS dataset [Spr16] is based on the same raw data as the Market dataset and the same persons have been assigned the same labels.", "mention_start": 58, "mention_end": 76, "dataset_mention": "the Market dataset"}, {"mentioned_in_paper": "916", "context_id": "289", "dataset_context": "In Figure 4.3 images from two tracklets of the MARS dataset are shown.", "mention_start": 43, "mention_end": 59, "dataset_mention": "the MARS dataset"}, {"mentioned_in_paper": "916", "context_id": "292", "dataset_context": "The X-MARS reordering of the MARS dataset is introduced and the evaluation of image re-id systems on video data are discussed in Section 4.2.", "mention_start": 25, "mention_end": 41, "dataset_mention": "the MARS dataset"}, {"mentioned_in_paper": "916", "context_id": "303", "dataset_context": "These experiments are performed across the Market and Duke datasets.", "mention_start": 39, "mention_end": 67, "dataset_mention": "the Market and Duke datasets"}, {"mentioned_in_paper": "916", "context_id": "319", "dataset_context": "Figure 4.4 shows the retrieved gallery images for several exemplary query images from the Market dataset.", "mention_start": 86, "mention_end": 104, "dataset_mention": "the Market dataset"}, {"mentioned_in_paper": "916", "context_id": "324", "dataset_context": "Due to the fact that the evaluated person re-id datasets do not provide view angle information, the models' view predictor is pre-learned on the RAP dataset as described in Section 3.4.2.", "mention_start": 140, "mention_end": 156, "dataset_mention": "the RAP dataset"}, {"mentioned_in_paper": "916", "context_id": "334", "dataset_context": "Secondly, in the RAP dataset all images not being totally frontal or backwards are labeled with the side view classes (left and right).", "mention_start": 12, "mention_end": 28, "dataset_mention": "the RAP dataset"}, {"mentioned_in_paper": "916", "context_id": "340", "dataset_context": "Furthermore, the same effect of a much blurrier side image is observed in the annotated case, confirming that this effect is based on the RAP dataset's view labeling.", "mention_start": 133, "mention_end": 149, "dataset_mention": "the RAP dataset"}, {"mentioned_in_paper": "916", "context_id": "349", "dataset_context": "Table 4.3 shows a comparison between the best results of PIE and PSE on the Market dataset, both using a ResNet-50 architecture as base model.", "mention_start": 72, "mention_end": 90, "dataset_mention": "the Market dataset"}, {"mentioned_in_paper": "916", "context_id": "358", "dataset_context": "E.g. the MARS dataset providing tracklets is about 36 times larger than the Market dataset, while they are both based on the same data source and MARS even contains less identities than Market (see Table 4.1).", "mention_start": 5, "mention_end": 21, "dataset_mention": "the MARS dataset"}, {"mentioned_in_paper": "916", "context_id": "358", "dataset_context": "E.g. the MARS dataset providing tracklets is about 36 times larger than the Market dataset, while they are both based on the same data source and MARS even contains less identities than Market (see Table 4.1).", "mention_start": 72, "mention_end": 90, "dataset_mention": "the Market dataset"}, {"mentioned_in_paper": "916", "context_id": "360", "dataset_context": "Therefore, cross evaluating a person detection system trained on the Market dataset with the MARS dataset could give insights into these aspects.", "mention_start": 64, "mention_end": 83, "dataset_mention": "the Market dataset"}, {"mentioned_in_paper": "916", "context_id": "360", "dataset_context": "Therefore, cross evaluating a person detection system trained on the Market dataset with the MARS dataset could give insights into these aspects.", "mention_start": 88, "mention_end": 105, "dataset_mention": "the MARS dataset"}, {"mentioned_in_paper": "916", "context_id": "361", "dataset_context": "In Section 4.2.1 it is discussed why the MARS dataset cannot be used for such an evaluation and a novel reordering of it called X-MARS is introduce to solve this shortcoming.", "mention_start": 37, "mention_end": 53, "dataset_mention": "the MARS dataset"}, {"mentioned_in_paper": "916", "context_id": "363", "dataset_context": "Unfortunately, although the MARS and Market datasets are based on the same data source and labels are assigned consistently, they cannot be used for cross-evaluation since their test and training sets overlap largely (see The same procedure is applied for the test set of X-MARS.", "mention_start": 23, "mention_end": 52, "dataset_mention": "the MARS and Market datasets"}, {"mentioned_in_paper": "916", "context_id": "365", "dataset_context": "To evaluate the robustness in real-world deployments with very large gallery sizes, the scalability of the PSE model is investigated with the Market-1501+500k (Market500k) dataset.", "mention_start": 137, "mention_end": 179, "dataset_mention": "the Market-1501+500k (Market500k) dataset"}, {"mentioned_in_paper": "916", "context_id": "366", "dataset_context": "This dataset is an extension to the Market dataset offering an additional 500,000 distractor images that can be added to the gallery to evaluate the impact of very large gallery sizes.", "mention_start": 32, "mention_end": 50, "dataset_mention": "the Market dataset"}, {"mentioned_in_paper": "916", "context_id": "367", "dataset_context": "To examine the impact of added distractors, the accuracy of the proposed models is evaluated with increasing gallery sizes by adding 100k, 200k, 300k, 400k and 500k distractor images randomly sampled from the Market500k dataset's distractors.", "mention_start": 204, "mention_end": 227, "dataset_mention": "the Market500k dataset"}, {"mentioned_in_paper": "916", "context_id": "372", "dataset_context": "The average number of actual persons per frame in the PRW dataset is about three.", "mention_start": 50, "mention_end": 65, "dataset_mention": "the PRW dataset"}, {"mentioned_in_paper": "916", "context_id": "386", "dataset_context": "Additionally, X-MARS, a reordering of the MARS dataset has been proposed allowing meaningful cross-evaluation of single-image trained models on tracklet data giving insights into real world considerations.", "mention_start": 37, "mention_end": 54, "dataset_mention": "the MARS dataset"}, {"mentioned_in_paper": "916", "context_id": "390", "dataset_context": "To be able to train the model's internal view prediction for datasets not offering view labels, it has been shown that pre-training the view predictor on the RAP dataset is a viable option.", "mention_start": 153, "mention_end": 169, "dataset_mention": "the RAP dataset"}, {"mentioned_in_paper": "916", "context_id": "394", "dataset_context": "The introduction of the X-MARS reordering of the MARS dataset allows a meaningful evaluation of image based re-id systems on tracklet data.", "mention_start": 45, "mention_end": 61, "dataset_mention": "the MARS dataset"}, {"mentioned_in_paper": "916", "context_id": "396", "dataset_context": "The evaluation on the Market500k dataset shows that another strength of the pose sensitive reid embedding is its improved scalability for very large gallery sizes.", "mention_start": 18, "mention_end": 40, "dataset_mention": "the Market500k dataset"}, {"mentioned_in_paper": "916", "context_id": "398", "dataset_context": "On the PRW dataset, investigation of the resistance of the PSE model against false detections and badly aligned person images showed its benefits over state of the art methods.", "mention_start": 3, "mention_end": 18, "dataset_mention": "the PRW dataset"}, {"mentioned_in_paper": "916", "context_id": "400", "dataset_context": "However, the evaluation shows that the pose only model does not achieve significant improvements over the baseline on the PRW dataset.", "mention_start": 117, "mention_end": 133, "dataset_mention": "the PRW dataset"}, {"mentioned_in_paper": "918", "context_id": "4", "dataset_context": "Extensive experiments on benchmark RGB-D saliency datasets illustrate the effectiveness of our framework.", "mention_start": 25, "mention_end": 58, "dataset_mention": "benchmark RGB-D saliency datasets"}, {"mentioned_in_paper": "918", "context_id": "5", "dataset_context": "Further, to prosper the development of this field, we contribute the largest (7\u00d7 larger than NJU2K) COME15K dataset, which contains 15,625 image pairs with high quality polygon-/scribble-/object-/instance-/rank-level annotations.", "mention_start": 92, "mention_end": 115, "dataset_mention": "NJU2K) COME15K dataset"}, {"mentioned_in_paper": "918", "context_id": "7", "dataset_context": "Source code and dataset are available at https://github.com/", "mention_start": 0, "mention_end": 23, "dataset_mention": "Source code and dataset"}, {"mentioned_in_paper": "918", "context_id": "25", "dataset_context": "In Table 1 we compare the widely used RGB-D saliency datasets, in terms of the size, [35] 1,200 Indoor/Outdoor Light-field cameras Tr, Te NLPR [34] 1,000 Indoor/Outdoor Microsoft Kinect Tr, Te SSB [32] 1,000 Internet Stereo cameras Te SIP [11] 929 Person in outside Huawei Mate10 Te DES [7] 135 Indoor Microsoft Kinect Te LFSD [27] 80 Indoor/Outdoor Lytro Illum cameras Te Ours 15,625 Indoor/Outdoor Holopix Social Platform Tr, Te types of data, the sources of depth data, and their roles (for training \"Tr\" or for testing \"Te\") in RGB-D saliency detection.", "mention_start": 38, "mention_end": 61, "dataset_mention": "RGB-D saliency datasets"}, {"mentioned_in_paper": "918", "context_id": "26", "dataset_context": "We note that the conventional training set for RGB-D saliency detection is a combination of samples from the NJU2K [21] dataset and NLPR [34], which includes only 2,200 image pairs in total.", "mention_start": 105, "mention_end": 127, "dataset_mention": "the NJU2K [21] dataset"}, {"mentioned_in_paper": "918", "context_id": "27", "dataset_context": "Although another 800 training images from the DUT dataset [35] can serve as the third part of the training set, the total number of training images is 3,000, which is not big enough, and may lead to biased model.", "mention_start": 42, "mention_end": 57, "dataset_mention": "the DUT dataset"}, {"mentioned_in_paper": "918", "context_id": "31", "dataset_context": "To provide an RGB-D saliency detection dataset for robust model training, and a sufficient size of testing data for model evaluation, we contribute the largest RGB-D saliency detection dataset, relabeled from Holo50K dataset [18], with 8,025 image pairs for training and 7,600 image pairs for testing.", "mention_start": 11, "mention_end": 46, "dataset_mention": "an RGB-D saliency detection dataset"}, {"mentioned_in_paper": "918", "context_id": "31", "dataset_context": "To provide an RGB-D saliency detection dataset for robust model training, and a sufficient size of testing data for model evaluation, we contribute the largest RGB-D saliency detection dataset, relabeled from Holo50K dataset [18], with 8,025 image pairs for training and 7,600 image pairs for testing.", "mention_start": 133, "mention_end": 192, "dataset_mention": " we contribute the largest RGB-D saliency detection dataset"}, {"mentioned_in_paper": "918", "context_id": "31", "dataset_context": "To provide an RGB-D saliency detection dataset for robust model training, and a sufficient size of testing data for model evaluation, we contribute the largest RGB-D saliency detection dataset, relabeled from Holo50K dataset [18], with 8,025 image pairs for training and 7,600 image pairs for testing.", "mention_start": 208, "mention_end": 224, "dataset_mention": "Holo50K dataset"}, {"mentioned_in_paper": "918", "context_id": "36", "dataset_context": "3) We contribute the largest RGB-D saliency detection dataset, with a 15,625 labeled set and a 5,000 unlabeled set to achieve fully-/weakly-/un-supervised RGB-D saliency detection.", "mention_start": 0, "mention_end": 61, "dataset_mention": "3) We contribute the largest RGB-D saliency detection dataset"}, {"mentioned_in_paper": "918", "context_id": "47", "dataset_context": "For the RGB-D dataset, the RGB image and depth data share similar semantic information, which can be defined as the common parts.", "mention_start": 4, "mention_end": 21, "dataset_mention": "the RGB-D dataset"}, {"mentioned_in_paper": "918", "context_id": "58", "dataset_context": "The widely used RGB-D saliency detection datasets include NJU2K [21], NLPR [34], SSB [32], DES [7], LFSD [27], SIP [11], DUT [35], etc., as shown in Table 1.", "mention_start": 16, "mention_end": 49, "dataset_mention": "RGB-D saliency detection datasets"}, {"mentioned_in_paper": "918", "context_id": "60", "dataset_context": "Piao et al. [35] introduces the DUT dataset, with 800 images for training and 400 images for testing.", "mention_start": 28, "mention_end": 43, "dataset_mention": "the DUT dataset"}, {"mentioned_in_paper": "918", "context_id": "103", "dataset_context": "Further, as the training dataset is a combination of samples from NJU2K [21] and NLPR dataset [34], different splits of the training set often lead to inconsistent performance evaluation.", "mention_start": 65, "mention_end": 93, "dataset_mention": "NJU2K [21] and NLPR dataset"}, {"mentioned_in_paper": "918", "context_id": "105", "dataset_context": "To boost RGB-D saliency detection, we contribute the largest RGB-D saliency detection dataset.", "mention_start": 34, "mention_end": 93, "dataset_mention": " we contribute the largest RGB-D saliency detection dataset"}, {"mentioned_in_paper": "918", "context_id": "108", "dataset_context": "Our new COME15K dataset is based on Holo50K [18], which is a stereo dataset, including scenarios from both indoor and outdoor.", "mention_start": 0, "mention_end": 23, "dataset_mention": "Our new COME15K dataset"}, {"mentioned_in_paper": "918", "context_id": "108", "dataset_context": "Our new COME15K dataset is based on Holo50K [18], which is a stereo dataset, including scenarios from both indoor and outdoor.", "mention_start": 58, "mention_end": 75, "dataset_mention": "a stereo dataset"}, {"mentioned_in_paper": "918", "context_id": "109", "dataset_context": "We first filter 2 the Holo50K dataset and then obtain 16,000 stereo image pairs for labelling (the candidate labeled set) and another 5,000 image pairs as the unlabeled set.", "mention_start": 0, "mention_end": 37, "dataset_mention": "We first filter 2 the Holo50K dataset"}, {"mentioned_in_paper": "918", "context_id": "110", "dataset_context": "Note that the stereo pairs in Holo50K dataset are directly captured by a stereo camera without rectification, we use a modified version of a SOTA off-the-shelf stereo matching algorithm [55] to compute the depth for both the candidate labeled set and unlabeled set with the left-right view images as input.", "mention_start": 30, "mention_end": 45, "dataset_mention": "Holo50K dataset"}, {"mentioned_in_paper": "918", "context_id": "114", "dataset_context": "Note that, we delete those samples with no common salient regions, and obtain our final labeled dataset of size 15,625.", "mention_start": 66, "mention_end": 103, "dataset_mention": " and obtain our final labeled dataset"}, {"mentioned_in_paper": "918", "context_id": "115", "dataset_context": "Further, based on the scribble annotations and instance-level saliency maps, we rank each saliency instance according to the initial scribble annotations to form our RGB-D saliency ranking dataset.", "mention_start": 161, "mention_end": 196, "dataset_mention": "our RGB-D saliency ranking dataset"}, {"mentioned_in_paper": "918", "context_id": "128", "dataset_context": "Dataset: For fair comparisons with existing RGB-D saliency detection models, we follow the conventional training setting, in which the training set is a combination of 1,485 images from the NJU2K dataset [21] and 700 images from the NLPR dataset [34].", "mention_start": 186, "mention_end": 203, "dataset_mention": "the NJU2K dataset"}, {"mentioned_in_paper": "918", "context_id": "128", "dataset_context": "Dataset: For fair comparisons with existing RGB-D saliency detection models, we follow the conventional training setting, in which the training set is a combination of 1,485 images from the NJU2K dataset [21] and 700 images from the NLPR dataset [34].", "mention_start": 229, "mention_end": 245, "dataset_mention": "the NLPR dataset"}, {"mentioned_in_paper": "918", "context_id": "136", "dataset_context": "The whole training takes 4.5 hours with batch size 5 on an NVIDIA GeForce RTX 2080Ti GPU for the conventional training (NJU2K-train+NLPR-train) dataset, and 16 hours with our new training (COME15K-train) dataset.", "mention_start": 119, "mention_end": 151, "dataset_mention": "(NJU2K-train+NLPR-train) dataset"}, {"mentioned_in_paper": "918", "context_id": "136", "dataset_context": "The whole training takes 4.5 hours with batch size 5 on an NVIDIA GeForce RTX 2080Ti GPU for the conventional training (NJU2K-train+NLPR-train) dataset, and 16 hours with our new training (COME15K-train) dataset.", "mention_start": 187, "mention_end": 211, "dataset_mention": "(COME15K-train) dataset"}, {"mentioned_in_paper": "918", "context_id": "141", "dataset_context": "Performance on DUT [35] dataset: Some existing RGB-D saliency detection approaches [35, 30] fine-tune their models on the DUT training dataset [35] to evaluate their performance on the DUT testing set.", "mention_start": 15, "mention_end": 31, "dataset_mention": "DUT [35] dataset"}, {"mentioned_in_paper": "918", "context_id": "175", "dataset_context": "The effectiveness of mutual information minimization as regularizer: We computed the mean absolute cosine similarities of the highest stage feature embeddings (z 4 a and z 4 g ) for \"W0\" from Table 4 (without mutual information minimization as regularizer) and ours, which are cosine(z M 0 a,g ) = 0.90 and cosine(z Ours a,g ) = 0.11 on the NLPR testing dataset.", "mention_start": 337, "mention_end": 361, "dataset_mention": "the NLPR testing dataset"}, {"mentioned_in_paper": "918", "context_id": "190", "dataset_context": "Depth generation: We generate our COME15K dataset with Holo50K [18], where the stereo pairs are not strictly rectified, which may cause severe matching failures even with state-of-the-art stereo algorithms [6].", "mention_start": 29, "mention_end": 49, "dataset_mention": "our COME15K dataset"}, {"mentioned_in_paper": "918", "context_id": "202", "dataset_context": "As our RGB-D saliency dataset is constructed on a stereo dataset [18], we directly train a stereo image pair based saliency object detection model, where the depth is implicitly instead of explicitly obtained from the stereo image pairs.", "mention_start": 3, "mention_end": 29, "dataset_mention": "our RGB-D saliency dataset"}, {"mentioned_in_paper": "918", "context_id": "202", "dataset_context": "As our RGB-D saliency dataset is constructed on a stereo dataset [18], we directly train a stereo image pair based saliency object detection model, where the depth is implicitly instead of explicitly obtained from the stereo image pairs.", "mention_start": 48, "mention_end": 64, "dataset_mention": "a stereo dataset"}, {"mentioned_in_paper": "918", "context_id": "210", "dataset_context": "NJU2K [21] NJU400 [20] COME15K-Normal COME15K-Difficult We explained the architecture and the other stereo saliency datasets in detail in the supplementary material.", "mention_start": 0, "mention_end": 124, "dataset_mention": "NJU2K [21] NJU400 [20] COME15K-Normal COME15K-Difficult We explained the architecture and the other stereo saliency datasets"}, {"mentioned_in_paper": "918", "context_id": "225", "dataset_context": "Further, we introduced the largest RGB-D saliency detection dataset with five types of annotations to prosper the development of fully-/weakly-/un-supervised RGB-D saliency detection tasks.", "mention_start": 22, "mention_end": 67, "dataset_mention": "the largest RGB-D saliency detection dataset"}, {"mentioned_in_paper": "919", "context_id": "37", "dataset_context": "One of the few exceptions is the well known PETS dataset [16], targeted primarily at surveillance applications.", "mention_start": 29, "mention_end": 56, "dataset_mention": "the well known PETS dataset"}, {"mentioned_in_paper": "919", "context_id": "47", "dataset_context": "A well-established and useful way of organizing datasets is through standardized challenges.", "mention_start": 37, "mention_end": 56, "dataset_mention": "organizing datasets"}, {"mentioned_in_paper": "919", "context_id": "78", "dataset_context": "In the following we define a clear protocol that was obeyed throughout the entire dataset to guarantee consistency.", "mention_start": 3, "mention_end": 89, "dataset_mention": "the following we define a clear protocol that was obeyed throughout the entire dataset"}, {"mentioned_in_paper": "919", "context_id": "228", "dataset_context": "One incentive behind compiling this benchmark was to reduce dataset bias by keeping the data as diverse as possible.", "mention_start": 53, "mention_end": 67, "dataset_mention": "reduce dataset"}, {"mentioned_in_paper": "920", "context_id": "1", "dataset_context": "This paper examines the possible reasons for low accuracy by comparing English datasets with non-Latin languages.", "mention_start": 71, "mention_end": 87, "dataset_mention": "English datasets"}, {"mentioned_in_paper": "920", "context_id": "6", "dataset_context": "The English synthetic datasets utilize over 1400 fonts while Arabic and other non-Latin datasets utilize less than 100 fonts for data generation.", "mention_start": 0, "mention_end": 30, "dataset_mention": "The English synthetic datasets"}, {"mentioned_in_paper": "920", "context_id": "6", "dataset_context": "The English synthetic datasets utilize over 1400 fonts while Arabic and other non-Latin datasets utilize less than 100 fonts for data generation.", "mention_start": 61, "mention_end": 96, "dataset_mention": "Arabic and other non-Latin datasets"}, {"mentioned_in_paper": "920", "context_id": "8", "dataset_context": "We improve the Word Recognition Rates (WRRs) on Arabic MLT-17 and MLT-19 datasets by 24.54% and 2.32% compared to previous works or baselines.", "mention_start": 48, "mention_end": 81, "dataset_mention": "Arabic MLT-17 and MLT-19 datasets"}, {"mentioned_in_paper": "920", "context_id": "9", "dataset_context": "We achieve WRR gains of 7.88% and 3.72% for IIIT-ILST and MLT-19 Devanagari datasets.", "mention_start": 44, "mention_end": 84, "dataset_mention": "IIIT-ILST and MLT-19 Devanagari datasets"}, {"mentioned_in_paper": "920", "context_id": "15", "dataset_context": " 1 : Comparing STAR-Net's performance on IIIT5K [13] dataset when trained on synthetic data created using a varying number of fonts and training samples.", "mention_start": 40, "mention_end": 60, "dataset_mention": "IIIT5K [13] dataset"}, {"mentioned_in_paper": "920", "context_id": "19", "dataset_context": "Like English, the recognition algorithms proposed for Latin datasets have not successfully recorded similar accuracies on non-Latin datasets.", "mention_start": 53, "mention_end": 68, "dataset_mention": "Latin datasets"}, {"mentioned_in_paper": "920", "context_id": "19", "dataset_context": "Like English, the recognition algorithms proposed for Latin datasets have not successfully recorded similar accuracies on non-Latin datasets.", "mention_start": 121, "mention_end": 140, "dataset_mention": "non-Latin datasets"}, {"mentioned_in_paper": "920", "context_id": "21", "dataset_context": "In Fig. 1, we illustrate the analysis of Word Recognition Rates (WRR) on the IIIT5K English dataset [13] by varying the number of training samples and fonts used in the synthetic data.", "mention_start": 72, "mention_end": 99, "dataset_mention": "the IIIT5K English dataset"}, {"mentioned_in_paper": "920", "context_id": "24", "dataset_context": "The motivation behind this work is described in Section 3. The methodology to train the deep neural network on the Arabic and Devanagari datasets is detailed in Section 4.", "mention_start": 111, "mention_end": 145, "dataset_mention": "the Arabic and Devanagari datasets"}, {"mentioned_in_paper": "920", "context_id": "27", "dataset_context": "1. We study the two parameters for synthetic datasets crucial to the performance of the reading models on the IIIT5K English dataset; i) the number of training examples and ii) the number of diverse fonts 2. We share 55 additional fonts in Arabic, and 97 new fonts in Devanagari, which we found using a region-wise online search.", "mention_start": 106, "mention_end": 132, "dataset_mention": "the IIIT5K English dataset"}, {"mentioned_in_paper": "920", "context_id": "32", "dataset_context": "As shown in Table 1, Mathew et al. [12] release the IIIT-ILST Dataset containing around 1K images each three non-Latin languages.", "mention_start": 20, "mention_end": 69, "dataset_mention": " Mathew et al. [12] release the IIIT-ILST Dataset"}, {"mentioned_in_paper": "920", "context_id": "33", "dataset_context": "The MLT dataset from the IC-DAR'17 RRC contains images from Arabic, Bangla, Chinese, English, French, German, Italian, Japanese, and Korean [15].", "mention_start": 0, "mention_end": 15, "dataset_mention": "The MLT dataset"}, {"mentioned_in_paper": "920", "context_id": "35", "dataset_context": "Recent OCR-on-the-go and CATALIST 2 datasets include around 1000 and 2322 annotated videos in Marathi, Hindi, and English [19].", "mention_start": 0, "mention_end": 44, "dataset_mention": "Recent OCR-on-the-go and CATALIST 2 datasets"}, {"mentioned_in_paper": "920", "context_id": "36", "dataset_context": "Arabic scene-text recognition datasets involve ARASTEC and MLT-17,19 [26].", "mention_start": 0, "mention_end": 38, "dataset_mention": "Arabic scene-text recognition datasets"}, {"mentioned_in_paper": "920", "context_id": "37", "dataset_context": "Chinese datasets cover RCTW, very high word recognition rates (> 90%) when we tested our non-Latin models on the held-out synthetic datasets, which shows that learning to read the non-Latin glyphs is trivial for the existing deep models.", "mention_start": 0, "mention_end": 16, "dataset_mention": "Chinese datasets"}, {"mentioned_in_paper": "920", "context_id": "37", "dataset_context": "Chinese datasets cover RCTW, very high word recognition rates (> 90%) when we tested our non-Latin models on the held-out synthetic datasets, which shows that learning to read the non-Latin glyphs is trivial for the existing deep models.", "mention_start": 108, "mention_end": 140, "dataset_mention": "the held-out synthetic datasets"}, {"mentioned_in_paper": "920", "context_id": "41", "dataset_context": "Korean and Japanese scene-text recognition datasets involve KAIST and DOST [9, 7].", "mention_start": 0, "mention_end": 51, "dataset_mention": "Korean and Japanese scene-text recognition datasets"}, {"mentioned_in_paper": "920", "context_id": "42", "dataset_context": "Different English datasets are listed in the last row of Table 1 [30, 28, 20, 13, 16, 10, 17, 27, 3, 15, 14].", "mention_start": 0, "mention_end": 26, "dataset_mention": "Different English datasets"}, {"mentioned_in_paper": "920", "context_id": "50", "dataset_context": "Bu\u0161ta et al. [2] propose a CNN (and CTC) based method for text localization, script identification, and text recognition and is tested on 11 languages (including Arabic) of MLT-17 dataset.", "mention_start": 172, "mention_end": 187, "dataset_mention": "MLT-17 dataset"}, {"mentioned_in_paper": "920", "context_id": "60", "dataset_context": "By changing the two parameters, we train our model (refer Section 4) on the above synthetic datasets and test them on the IIIT5K dataset.", "mention_start": 71, "mention_end": 100, "dataset_mention": "the above synthetic datasets"}, {"mentioned_in_paper": "920", "context_id": "60", "dataset_context": "By changing the two parameters, we train our model (refer Section 4) on the above synthetic datasets and test them on the IIIT5K dataset.", "mention_start": 117, "mention_end": 136, "dataset_mention": "the IIIT5K dataset"}, {"mentioned_in_paper": "920", "context_id": "61", "dataset_context": "We observe that the Word Recognition Rate (WRR) of the dataset with around 20M samples and over 1400 fonts achieves state-of-the-art accuracy on the IIIT5K dataset [11].", "mention_start": 145, "mention_end": 163, "dataset_mention": "the IIIT5K dataset"}, {"mentioned_in_paper": "920", "context_id": "74", "dataset_context": "We run our models on Arabic and Devnagari test sets from MLT-17, IIIT-ILST, and MLT-19 datasets 4.", "mention_start": 75, "mention_end": 95, "dataset_mention": " and MLT-19 datasets"}, {"mentioned_in_paper": "920", "context_id": "88", "dataset_context": "We train each model for 10 epochs and test on Arabic and Devanagari word images from IIIT-ILST, MLT-17, and MLT-19 datasets.", "mention_start": 103, "mention_end": 123, "dataset_mention": " and MLT-19 datasets"}, {"mentioned_in_paper": "920", "context_id": "89", "dataset_context": "Only for the Arabic MLT-17 dataset, we fine-tune our models on training images and test them on validation images to fairly compare with Bu\u0161ta et al. [1].", "mention_start": 9, "mention_end": 34, "dataset_mention": "the Arabic MLT-17 dataset"}, {"mentioned_in_paper": "920", "context_id": "90", "dataset_context": "For Devanagari, we present the additional results on the IIIT-ILST dataset by fine-tuning our best model on the MLT-19 dataset.", "mention_start": 52, "mention_end": 74, "dataset_mention": "the IIIT-ILST dataset"}, {"mentioned_in_paper": "920", "context_id": "90", "dataset_context": "For Devanagari, we present the additional results on the IIIT-ILST dataset by fine-tuning our best model on the MLT-19 dataset.", "mention_start": 107, "mention_end": 126, "dataset_mention": "the MLT-19 dataset"}, {"mentioned_in_paper": "920", "context_id": "93", "dataset_context": "The additional layer corrects the model's bias towards the synthetic datasets, and hence we call it correction LSTM.", "mention_start": 0, "mention_end": 77, "dataset_mention": "The additional layer corrects the model's bias towards the synthetic datasets"}, {"mentioned_in_paper": "920", "context_id": "97", "dataset_context": "For the Arabic MLT-17 dataset and Devanagari IIIT-ILST dataset, we achieve recognition rates better than Bu\u0161ta et al. [1] and Mathew et al. [12].", "mention_start": 4, "mention_end": 29, "dataset_mention": "the Arabic MLT-17 dataset"}, {"mentioned_in_paper": "920", "context_id": "97", "dataset_context": "For the Arabic MLT-17 dataset and Devanagari IIIT-ILST dataset, we achieve recognition rates better than Bu\u0161ta et al. [1] and Mathew et al. [12].", "mention_start": 4, "mention_end": 62, "dataset_mention": "the Arabic MLT-17 dataset and Devanagari IIIT-ILST dataset"}, {"mentioned_in_paper": "920", "context_id": "100", "dataset_context": "By fine-tuning the Devanagari model on the MLT-19 dataset, the CRR and WRR gains raise to 3.85% and 7.12%.", "mention_start": 39, "mention_end": 57, "dataset_mention": "the MLT-19 dataset"}, {"mentioned_in_paper": "920", "context_id": "103", "dataset_context": "As shown in Table 3, for the MLT-19 Arabic dataset, the model trained on 5M samples generated using 85 fonts achieve the CRR of 71.15% and WRR of 40.05%.", "mention_start": 24, "mention_end": 50, "dataset_mention": "the MLT-19 Arabic dataset"}, {"mentioned_in_paper": "920", "context_id": "105", "dataset_context": "For the MLT-19 Devanagari dataset, the model trained on 5M samples generated using 97 fonts achieves the CRR of 84.60% and WRR of 60.83%.", "mention_start": 4, "mention_end": 33, "dataset_mention": "the MLT-19 Devanagari dataset"}, {"mentioned_in_paper": "920", "context_id": "107", "dataset_context": "It is also interesting to note that the WRR of our models on MLT-17 Arabic and MLT-19 Devanagari datasets are very close Fig. 4 : Histogram of correct words (x = 0) and words with x errors (x > 0).", "mention_start": 61, "mention_end": 105, "dataset_mention": "MLT-17 Arabic and MLT-19 Devanagari datasets"}, {"mentioned_in_paper": "920", "context_id": "116", "dataset_context": "We observe few exceptions in each histogram where the frequency of incorrect words is higher for the best model than others, e.g., at edit distance of 2 for the Arabic MLT-17 dataset.", "mention_start": 156, "mention_end": 182, "dataset_mention": "the Arabic MLT-17 dataset"}, {"mentioned_in_paper": "920", "context_id": "124", "dataset_context": "For the IIIT-ILST dataset, the model, trained on 194 fonts, performs poorly on the long words (x > 8 in the top-right plot of Fig. 5), and the correction LSTM further enhances this effect.", "mention_start": 4, "mention_end": 25, "dataset_mention": "the IIIT-ILST dataset"}, {"mentioned_in_paper": "920", "context_id": "125", "dataset_context": "On the contrary, we observe that the Correction LSTM reduces WA-ECR for the MLT-17 Arabic dataset for word lengths in the range [6, 11] (compare green and blue curves in the top-left plot).", "mention_start": 71, "mention_end": 97, "dataset_mention": "the MLT-17 Arabic dataset"}, {"mentioned_in_paper": "920", "context_id": "126", "dataset_context": "Interestingly, the WA-ECR of some of our models drops after word-length of 10 and 14 for the MLT- Devanagari datasets (see blue curve in the top-left plot and the two curves in the bottom-right plot of Fig. 5).", "mention_start": 88, "mention_end": 117, "dataset_mention": "the MLT- Devanagari datasets"}, {"mentioned_in_paper": "921", "context_id": "4", "dataset_context": "We show that the convolutional autoencoder architecture doesn't have a significant effect for this task and we prove the potential of our approach on the real world dataset.", "mention_start": 150, "mention_end": 172, "dataset_mention": "the real world dataset"}, {"mentioned_in_paper": "921", "context_id": "34", "dataset_context": "For this experiment, we used the Cookie Dataset.", "mention_start": 28, "mention_end": 47, "dataset_mention": "the Cookie Dataset"}, {"mentioned_in_paper": "921", "context_id": "35", "dataset_context": "It is our internal dataset designed for the anomaly detection task, which captures Tarallini biscuits.", "mention_start": 6, "mention_end": 26, "dataset_mention": "our internal dataset"}, {"mentioned_in_paper": "921", "context_id": "41", "dataset_context": "In order to augmentate the dataset we rotated each sample by 90\u00b0for three times, so the augmented dataset contains 4900 captures in total.", "mention_start": 12, "mention_end": 34, "dataset_mention": "augmentate the dataset"}, {"mentioned_in_paper": "921", "context_id": "41", "dataset_context": "In order to augmentate the dataset we rotated each sample by 90\u00b0for three times, so the augmented dataset contains 4900 captures in total.", "mention_start": 80, "mention_end": 105, "dataset_mention": " so the augmented dataset"}, {"mentioned_in_paper": "921", "context_id": "78", "dataset_context": "We also plan to test this approach on another dataset for anomaly detection, because the Cookie Dataset shows low variance between the classes and especially the structural defects are a difficult task to distinguish.", "mention_start": 84, "mention_end": 103, "dataset_mention": "the Cookie Dataset"}, {"mentioned_in_paper": "922", "context_id": "4", "dataset_context": "Extensive results on the public point cloud databases, namely the S3DIS and ScanNet datasets, demonstrate the effectiveness of our proposed model, outperforming the state-of-the-art approaches.", "mention_start": 54, "mention_end": 92, "dataset_mention": " namely the S3DIS and ScanNet datasets"}, {"mentioned_in_paper": "922", "context_id": "140", "dataset_context": "To evaluate the performance of proposed model and compare with state-of-the-art, we conduct experiments on two public available datasets, the Stanford 3D Indoor Semantics (S3DIS) Dataset [1] and ScanNet Dataset [2].", "mention_start": 137, "mention_end": 186, "dataset_mention": " the Stanford 3D Indoor Semantics (S3DIS) Dataset"}, {"mentioned_in_paper": "922", "context_id": "140", "dataset_context": "To evaluate the performance of proposed model and compare with state-of-the-art, we conduct experiments on two public available datasets, the Stanford 3D Indoor Semantics (S3DIS) Dataset [1] and ScanNet Dataset [2].", "mention_start": 137, "mention_end": 210, "dataset_mention": " the Stanford 3D Indoor Semantics (S3DIS) Dataset [1] and ScanNet Dataset"}, {"mentioned_in_paper": "922", "context_id": "141", "dataset_context": "The S3DIS dataset comes from real scan of the indoor environment, including 3D scans of Matterport scanners from 6 areas.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The S3DIS dataset"}, {"mentioned_in_paper": "922", "context_id": "143", "dataset_context": "ScanNet is a point cloud dataset with scanned indoor scenes.", "mention_start": 11, "mention_end": 32, "dataset_mention": "a point cloud dataset"}, {"mentioned_in_paper": "922", "context_id": "153", "dataset_context": "For S3DIS dataset, we compare our method with PointNet [13], PointNet++ [14], SEGCloud [19], RSNet [6], SPGraph [8], SGPN [23], Engelmann et al. [3], A-SCN [26] and DGCNN [24].", "mention_start": 4, "mention_end": 17, "dataset_mention": "S3DIS dataset"}, {"mentioned_in_paper": "922", "context_id": "154", "dataset_context": "For ScanNet dataset, we compare with 3DCNN [2], PointNet [13], PointNet++ [14], RSNet [6] and PointCNN [10].", "mention_start": 4, "mention_end": 19, "dataset_mention": "ScanNet dataset"}, {"mentioned_in_paper": "922", "context_id": "155", "dataset_context": "We perform semantic segmentation experiments on the S3DIS dataset to evaluate our performance in indoor real-world scene scans and perform ablation experiments on this dataset.", "mention_start": 48, "mention_end": 65, "dataset_mention": "the S3DIS dataset"}, {"mentioned_in_paper": "922", "context_id": "168", "dataset_context": "In order to further demonstrate the effectiveness of our model, some qualitative examples from S3DIS dataset are provided in Fig. 4 and Fig. 5, demonstrating that our model can yield more accurate segmentation results.", "mention_start": 94, "mention_end": 108, "dataset_mention": "S3DIS dataset"}, {"mentioned_in_paper": "922", "context_id": "169", "dataset_context": "For the ScanNet dataset, the number of scenes trained and tested is 1201 and 312, same as [14, 10].", "mention_start": 4, "mention_end": 23, "dataset_mention": "the ScanNet dataset"}, {"mentioned_in_paper": "922", "context_id": "203", "dataset_context": "Extensive experiments on two public point cloud semantic segmentation datasets demonstrating the superiority of our proposed model.", "mention_start": 25, "mention_end": 78, "dataset_mention": "two public point cloud semantic segmentation datasets"}, {"mentioned_in_paper": "923", "context_id": "117", "dataset_context": "We train DCAN on two source datasets, Synthia [22] and Gta5 [23] respectively, and then evaluate the models on Cityscapes [24].", "mention_start": 17, "mention_end": 36, "dataset_mention": "two source datasets"}, {"mentioned_in_paper": "923", "context_id": "203", "dataset_context": "We conducted extensive experiments by transferring models learned on synthetic segmentation datasets to real urban scenes, and demonstrated the effectiveness of DCAN over state-of-the-art methods and its compatibility with modern segmentation networks.", "mention_start": 69, "mention_end": 100, "dataset_mention": "synthetic segmentation datasets"}, {"mentioned_in_paper": "924", "context_id": "18", "dataset_context": "Experiments on VQA 2.0 dataset shows the effectiveness of each component of our solution.", "mention_start": 15, "mention_end": 30, "dataset_mention": "VQA 2.0 dataset"}, {"mentioned_in_paper": "926", "context_id": "3", "dataset_context": "Researchers have applied distinct techniques to the UCI Machine Learning heart disease dataset.", "mention_start": 48, "mention_end": 94, "dataset_mention": "the UCI Machine Learning heart disease dataset"}, {"mentioned_in_paper": "926", "context_id": "26", "dataset_context": "To make our working procedure easier, applying PCA our proposed algorithm converted the dataset attributes into two main attributes namely PC1 and PC2.", "mention_start": 37, "mention_end": 95, "dataset_mention": " applying PCA our proposed algorithm converted the dataset"}, {"mentioned_in_paper": "926", "context_id": "66", "dataset_context": "UCI Machine Learning Repository heart disease dataset [11] has fourteen attributes; target is one of the attribute in the data set.", "mention_start": 0, "mention_end": 53, "dataset_mention": "UCI Machine Learning Repository heart disease dataset"}, {"mentioned_in_paper": "926", "context_id": "71", "dataset_context": "An example dataset shows with five data points in Fig. 3 that data points are divided into two distinct categories.", "mention_start": 0, "mention_end": 18, "dataset_mention": "An example dataset"}, {"mentioned_in_paper": "926", "context_id": "100", "dataset_context": "We have used UCI Machine Learning Repository [11] for the experiment heart disease dataset.", "mention_start": 54, "mention_end": 90, "dataset_mention": "the experiment heart disease dataset"}, {"mentioned_in_paper": "926", "context_id": "101", "dataset_context": "Total of 303 data points and 14 attributes in the heart disease dataset.", "mention_start": 46, "mention_end": 71, "dataset_mention": "the heart disease dataset"}, {"mentioned_in_paper": "926", "context_id": "107", "dataset_context": "For a Population size 2500, the clustering outcome of our proposed method has shown in TABLE I and Fig. 8 shows the final cluster applying our proposed method for the heart disease dataset.", "mention_start": 162, "mention_end": 188, "dataset_mention": "the heart disease dataset"}, {"mentioned_in_paper": "927", "context_id": "71", "dataset_context": "The Tai-chi-HD dataset, which includes brief videos of people doing Tai-chi exercises, was used for training and evaluation.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The Tai-chi-HD dataset"}, {"mentioned_in_paper": "929", "context_id": "2", "dataset_context": "We discuss key aspects of learning structured object-centric representations with self-supervision and validate our insights through several experiments on the CLEVR dataset.", "mention_start": 156, "mention_end": 173, "dataset_mention": "the CLEVR dataset"}, {"mentioned_in_paper": "929", "context_id": "128", "dataset_context": "Supported by a series of small experiments on the CLEVR dataset, we study the interplay of architecture components, namely, Cross Attention and Slot Attention, and alternative formulations for the object loss.", "mention_start": 46, "mention_end": 63, "dataset_mention": "the CLEVR dataset"}, {"mentioned_in_paper": "929", "context_id": "138", "dataset_context": "compactness and modularity (Racah & Chandar, 2020), and more extensive datasets, e.g.", "mention_start": 51, "mention_end": 79, "dataset_mention": " and more extensive datasets"}, {"mentioned_in_paper": "932", "context_id": "4", "dataset_context": "To overcome these problems, we construct a Chinese Open-Domain Conversational Search Behavior Dataset (ConvSearch) based on Wizard-of-Oz paradigm in the field study scenario.", "mention_start": 40, "mention_end": 101, "dataset_mention": "a Chinese Open-Domain Conversational Search Behavior Dataset"}, {"mentioned_in_paper": "932", "context_id": "7", "dataset_context": "The ConvSearch dataset contains 1,131 dialogues together with annotated search results and corresponding search behaviors.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The ConvSearch dataset"}, {"mentioned_in_paper": "932", "context_id": "20", "dataset_context": "Previous conversational search datasets fail to meet the research needs of user study because of the following two problems:", "mention_start": 0, "mention_end": 39, "dataset_mention": "Previous conversational search datasets"}, {"mentioned_in_paper": "932", "context_id": "25", "dataset_context": "To overcome these problems, we construct the Chinese Open-Domain Conversational Search Behavior Dataset (ConvSearch) based on Wizard-of-Oz paradigm in the field study scenario.", "mention_start": 40, "mention_end": 103, "dataset_mention": "the Chinese Open-Domain Conversational Search Behavior Dataset"}, {"mentioned_in_paper": "932", "context_id": "26", "dataset_context": "Table 1 shows the comparison between ConvSearch and existing popular ConvS datasets.", "mention_start": 37, "mention_end": 83, "dataset_mention": "ConvSearch and existing popular ConvS datasets"}, {"mentioned_in_paper": "932", "context_id": "30", "dataset_context": "The ConvSearch dataset contains 1,131 dialogues together with annotated search results and corresponding search behaviors.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The ConvSearch dataset"}, {"mentioned_in_paper": "932", "context_id": "36", "dataset_context": "behavior dataset, the first ConvS dataset to collect both dialogue contents and corresponding agent search behaviors for real information needs generated by actual users.", "mention_start": 0, "mention_end": 16, "dataset_mention": "behavior dataset"}, {"mentioned_in_paper": "932", "context_id": "36", "dataset_context": "behavior dataset, the first ConvS dataset to collect both dialogue contents and corresponding agent search behaviors for real information needs generated by actual users.", "mention_start": 17, "mention_end": 41, "dataset_mention": " the first ConvS dataset"}, {"mentioned_in_paper": "932", "context_id": "40", "dataset_context": "With the purpose of evaluating models of conversational search systems, CAsT-19/20/21 [9] datasets are designed with emphasis on their reusability and comparability.", "mention_start": 71, "mention_end": 98, "dataset_mention": " CAsT-19/20/21 [9] datasets"}, {"mentioned_in_paper": "932", "context_id": "48", "dataset_context": "The typical Wizard-of-Oz datasets contain Wizard of Wikipedia [11], MultiWOZ [5], Topical-Chat [13] and WISE [23].", "mention_start": 0, "mention_end": 33, "dataset_mention": "The typical Wizard-of-Oz datasets"}, {"mentioned_in_paper": "932", "context_id": "49", "dataset_context": "However, none 1 You can download the ConvSearch dataset via visiting https://github.com/chuzhumin98/ConvSearch-Dataset of these datasets satisfies the real-life property, and their user information needs are assigned directly to users by organizers.", "mention_start": 8, "mention_end": 55, "dataset_mention": " none 1 You can download the ConvSearch dataset"}, {"mentioned_in_paper": "932", "context_id": "49", "dataset_context": "However, none 1 You can download the ConvSearch dataset via visiting https://github.com/chuzhumin98/ConvSearch-Dataset of these datasets satisfies the real-life property, and their user information needs are assigned directly to users by organizers.", "mention_start": 75, "mention_end": 118, "dataset_mention": "//github.com/chuzhumin98/ConvSearch-Dataset"}, {"mentioned_in_paper": "932", "context_id": "53", "dataset_context": "The WISE dataset, as the most similar Wizard-of-Oz dataset to ours, also contains agent search behaviors, but their user information needs are distilled from the search logs of a commercial search engine and assigned to the users, rather than being naturally generated by themselves.", "mention_start": 0, "mention_end": 16, "dataset_mention": "The WISE dataset"}, {"mentioned_in_paper": "932", "context_id": "53", "dataset_context": "The WISE dataset, as the most similar Wizard-of-Oz dataset to ours, also contains agent search behaviors, but their user information needs are distilled from the search logs of a commercial search engine and assigned to the users, rather than being naturally generated by themselves.", "mention_start": 20, "mention_end": 58, "dataset_mention": "the most similar Wizard-of-Oz dataset"}, {"mentioned_in_paper": "932", "context_id": "54", "dataset_context": "Another direction of dataset construction is to crawl and organize the conversation search dataset from the existing conversation-like search platforms (e.g., forums, QA communities, etc.).", "mention_start": 48, "mention_end": 98, "dataset_mention": "crawl and organize the conversation search dataset"}, {"mentioned_in_paper": "932", "context_id": "55", "dataset_context": "MSDialog dataset [21] is a typical example of this type.", "mention_start": 0, "mention_end": 16, "dataset_mention": "MSDialog dataset"}, {"mentioned_in_paper": "932", "context_id": "57", "dataset_context": "Since the existing dialogue systems cannot be competent in the role of agent for high-quality dialogue datasets, we adopt Wizardof-Oz [17] paradigm in field study scenario [27].", "mention_start": 81, "mention_end": 111, "dataset_mention": "high-quality dialogue datasets"}, {"mentioned_in_paper": "932", "context_id": "101", "dataset_context": "Complete statistics of ConvSearch dataset can be found in Sec 4.", "mention_start": 23, "mention_end": 41, "dataset_mention": "ConvSearch dataset"}, {"mentioned_in_paper": "932", "context_id": "138", "dataset_context": "Figure 3 shows a dialogue example (id: 264) of metadata in Con-vSearch dataset.", "mention_start": 58, "mention_end": 78, "dataset_mention": "Con-vSearch dataset"}, {"mentioned_in_paper": "932", "context_id": "142", "dataset_context": "To intuitively present our ConvSearch dataset, we conduct a series of statistical analyses in this section.", "mention_start": 23, "mention_end": 45, "dataset_mention": "our ConvSearch dataset"}, {"mentioned_in_paper": "932", "context_id": "143", "dataset_context": "Table 6 shows the basic statistics of ConvSearch dataset, where # total turns denotes the number of turns in the original dialogue data, while # merged turns denotes the number of turns after we merge multiple consecutive responses of user or agent into a single turn.", "mention_start": 38, "mention_end": 56, "dataset_mention": "ConvSearch dataset"}, {"mentioned_in_paper": "932", "context_id": "144", "dataset_context": "Next, we will present more detailed analysis of the ConvSearch dataset.", "mention_start": 47, "mention_end": 70, "dataset_mention": "the ConvSearch dataset"}, {"mentioned_in_paper": "932", "context_id": "219", "dataset_context": "As a fundamental and essential task, the construction of conversational search datasets is still encountering some challenges.", "mention_start": 56, "mention_end": 87, "dataset_mention": "conversational search datasets"}, {"mentioned_in_paper": "932", "context_id": "223", "dataset_context": "To overcome these problems, we construct the Chinese Open-Domain Conversational Search Behavior Dataset (ConvSearch) based on Wizard-of-Oz paradigm in the field study scenario.", "mention_start": 40, "mention_end": 103, "dataset_mention": "the Chinese Open-Domain Conversational Search Behavior Dataset"}, {"mentioned_in_paper": "932", "context_id": "228", "dataset_context": "In this paper, we show the detailed dataset construction process, as well as the statistical analysis of the dataset.", "mention_start": 22, "mention_end": 43, "dataset_mention": "the detailed dataset"}, {"mentioned_in_paper": "932", "context_id": "245", "dataset_context": "Our ConvSearch dataset is already accessible to public for academic usage.", "mention_start": 0, "mention_end": 22, "dataset_mention": "Our ConvSearch dataset"}, {"mentioned_in_paper": "932", "context_id": "246", "dataset_context": "We wish that the ConvSearch dataset will be of assistance in follow-up studies on conversational search.", "mention_start": 0, "mention_end": 35, "dataset_mention": "We wish that the ConvSearch dataset"}, {"mentioned_in_paper": "933", "context_id": "55", "dataset_context": "Mathematically, we may model such a dataset as a set of points in a space whose dimensions are the individual features (Salton et al., 1975).", "mention_start": 15, "mention_end": 43, "dataset_mention": " we may model such a dataset"}, {"mentioned_in_paper": "935", "context_id": "4", "dataset_context": "To explore the relevance of ORCID to study data sharing practices we obtained all ORCID profiles reporting at least one dataset in their \"works\" list, together with information related to the individual researchers producing the datasets.", "mention_start": 187, "mention_end": 237, "dataset_mention": "the individual researchers producing the datasets"}, {"mentioned_in_paper": "935", "context_id": "8", "dataset_context": "The analysis of the distribution of researchers producing datasets shows that the top six countries with more data producers, also have a relatively higher percentage of people who have produced datasets out of total researchers with datasets than researchers in the total ORCID.", "mention_start": 36, "mention_end": 66, "dataset_mention": "researchers producing datasets"}, {"mentioned_in_paper": "935", "context_id": "8", "dataset_context": "The analysis of the distribution of researchers producing datasets shows that the top six countries with more data producers, also have a relatively higher percentage of people who have produced datasets out of total researchers with datasets than researchers in the total ORCID.", "mention_start": 185, "mention_end": 203, "dataset_mention": "produced datasets"}, {"mentioned_in_paper": "935", "context_id": "9", "dataset_context": "By disciplines, researchers that belong to the areas of Natural Sciences and Medicine and Life Sciences are those with the largest amount of reported datasets.", "mention_start": 140, "mention_end": 158, "dataset_mention": "reported datasets"}, {"mentioned_in_paper": "935", "context_id": "10", "dataset_context": "Finally, we observed that researchers who have started their PhD around 2015 published their first dataset earlier that those researchers that started their PhD before.", "mention_start": 46, "mention_end": 106, "dataset_mention": "started their PhD around 2015 published their first dataset"}, {"mentioned_in_paper": "935", "context_id": "13", "dataset_context": "The main rationale behind data sharing is that it contributes to saving time, money, and efforts, and allows researchers to validate their results through the reanalysis of the shared datasets (Popkin, 2019).", "mention_start": 172, "mention_end": 192, "dataset_mention": "the shared datasets"}, {"mentioned_in_paper": "935", "context_id": "17", "dataset_context": "There are four main different ways in which research data is shared in the scientific context: 1) by adding a dataset as supplementary material to a publication, 2) by sharing it upon request to other researchers or making it available in a personal or other website 1, 3) by uploading the dataset to a data repository, or 4) by publishing on a data journal (Federer et al., 2018; Costas et al., 2013).", "mention_start": 100, "mention_end": 117, "dataset_mention": "adding a dataset"}, {"mentioned_in_paper": "935", "context_id": "17", "dataset_context": "There are four main different ways in which research data is shared in the scientific context: 1) by adding a dataset as supplementary material to a publication, 2) by sharing it upon request to other researchers or making it available in a personal or other website 1, 3) by uploading the dataset to a data repository, or 4) by publishing on a data journal (Federer et al., 2018; Costas et al., 2013).", "mention_start": 275, "mention_end": 297, "dataset_mention": "uploading the dataset"}, {"mentioned_in_paper": "935", "context_id": "18", "dataset_context": "According to some studies, uploading datasets to data repositories is, in terms of preservation, openness and authorship recognition, the most appropriate way to share data (Hern\u00e1ndez-P\u00e9rez & Garc\u00eda-Moreno, 2013; Kim & Burns, 2016; Mannheimer et al., 2019).", "mention_start": 26, "mention_end": 45, "dataset_mention": " uploading datasets"}, {"mentioned_in_paper": "935", "context_id": "26", "dataset_context": "Although journal articles and conference proceedings are among the most common outputs of scholarly activities, researchers registered in ORCID can also include more output types, for instance datasets (Fenner et al., 2015; Jefferies et al., 2019).", "mention_start": 183, "mention_end": 201, "dataset_mention": "instance datasets"}, {"mentioned_in_paper": "935", "context_id": "32", "dataset_context": "An element that reinforces the relevance of ORCID is the fact that ORCID interlinks some of the most important data repositories to connect datasets with their researchers using their ORCID identifiers (Hern\u00e1ndez-P\u00e9rez & Garc\u00eda-Moreno, 2013; Kim & Burns, 2016; Mannheimer et al., 2019).", "mention_start": 132, "mention_end": 148, "dataset_mention": "connect datasets"}, {"mentioned_in_paper": "935", "context_id": "34", "dataset_context": "Therefore, we can assume that the act of registering datasets as research outputs in ORCID is a relatively active form 2 of recognizing and claiming ownership of their research outputs.", "mention_start": 40, "mention_end": 61, "dataset_mention": "registering datasets"}, {"mentioned_in_paper": "935", "context_id": "38", "dataset_context": "In this study, we analyse researchers' dataset outputs as reported in the ORCID platform.", "mention_start": 14, "mention_end": 46, "dataset_mention": " we analyse researchers' dataset"}, {"mentioned_in_paper": "935", "context_id": "51", "dataset_context": "Two types of information were extracted from ORCID: 1) all ORCID profiles reporting at least one dataset in their list of \"works\", and 2) information related with the individual researcher producing the dataset.", "mention_start": 162, "mention_end": 210, "dataset_mention": "the individual researcher producing the dataset"}, {"mentioned_in_paper": "935", "context_id": "54", "dataset_context": "We specifically searched for the label 'datasets' in the 'work type' field in order to unequivocally identify works records related to datasets.", "mention_start": 39, "mention_end": 48, "dataset_mention": "'datasets"}, {"mentioned_in_paper": "935", "context_id": "55", "dataset_context": "A total of 80,555 records (datasets) linked to 12,686 ORCID profiles were retrieved.", "mention_start": 14, "mention_end": 35, "dataset_mention": "555 records (datasets"}, {"mentioned_in_paper": "935", "context_id": "68", "dataset_context": "There are three possible combinations: a) identifying a dataset deposited in one of the selected repositories but being fed to ORCID via DataCite, b) a dataset deposited in one of the selected repositories which feeds from other source, and c) a dataset deposited from one of the selected repositories, but which metadata comes from a third-party.", "mention_start": 38, "mention_end": 63, "dataset_mention": " a) identifying a dataset"}, {"mentioned_in_paper": "935", "context_id": "68", "dataset_context": "There are three possible combinations: a) identifying a dataset deposited in one of the selected repositories but being fed to ORCID via DataCite, b) a dataset deposited in one of the selected repositories which feeds from other source, and c) a dataset deposited from one of the selected repositories, but which metadata comes from a third-party.", "mention_start": 146, "mention_end": 159, "dataset_mention": " b) a dataset"}, {"mentioned_in_paper": "935", "context_id": "68", "dataset_context": "There are three possible combinations: a) identifying a dataset deposited in one of the selected repositories but being fed to ORCID via DataCite, b) a dataset deposited in one of the selected repositories which feeds from other source, and c) a dataset deposited from one of the selected repositories, but which metadata comes from a third-party.", "mention_start": 236, "mention_end": 253, "dataset_mention": " and c) a dataset"}, {"mentioned_in_paper": "935", "context_id": "69", "dataset_context": "Once we applied these three types of combinations on the total datasets in ORCID (80,555) we obtained a total of 41,667 datasets belonging to 4,284 different researchers.", "mention_start": 116, "mention_end": 128, "dataset_mention": "667 datasets"}, {"mentioned_in_paper": "935", "context_id": "88", "dataset_context": "More specifically, we were interested in the PhD starting date of the identified researchers and used it as a fixed point of reference in the academic career of the researchers in order to assess when they published their first dataset or journal article.", "mention_start": 200, "mention_end": 235, "dataset_mention": "they published their first dataset"}, {"mentioned_in_paper": "935", "context_id": "89", "dataset_context": "We used a 5-year window of dataset publication to allow ORCID researchers to publish datasets or journal articles.", "mention_start": 77, "mention_end": 93, "dataset_mention": "publish datasets"}, {"mentioned_in_paper": "935", "context_id": "94", "dataset_context": "We observed that most datasets were concentrated within the cohort of researchers starting their PhD between 2010 and 2015.", "mention_start": 0, "mention_end": 30, "dataset_mention": "We observed that most datasets"}, {"mentioned_in_paper": "935", "context_id": "96", "dataset_context": "Therefore, we decided to consider the period between 2010-2015 for a more detailed analysis, in which we find the larger set of individuals with at least one shared dataset (69.4%).", "mention_start": 147, "mention_end": 172, "dataset_mention": "least one shared dataset"}, {"mentioned_in_paper": "935", "context_id": "103", "dataset_context": "Considering these criteria, we found a total of 41,667 datasets connected to a total of 4,284 different researchers.", "mention_start": 51, "mention_end": 63, "dataset_mention": "667 datasets"}, {"mentioned_in_paper": "935", "context_id": "107", "dataset_context": "Second, we focus on the information related with the individual researcher producing the dataset.", "mention_start": 48, "mention_end": 96, "dataset_mention": "the individual researcher producing the dataset"}, {"mentioned_in_paper": "935", "context_id": "112", "dataset_context": "Figure 5 shows the top 15 countries with researchers that have reported datasets in ORCID.", "mention_start": 63, "mention_end": 80, "dataset_mention": "reported datasets"}, {"mentioned_in_paper": "935", "context_id": "116", "dataset_context": "These percentages show us to which scientific area the journal articles published by researchers with ORCID datasets belong.", "mention_start": 102, "mention_end": 116, "dataset_mention": "ORCID datasets"}, {"mentioned_in_paper": "935", "context_id": "119", "dataset_context": "Fig. 7 Average of years that researchers take to publish their first dataset and journal article for the 2010-2015 period.", "mention_start": 49, "mention_end": 76, "dataset_mention": "publish their first dataset"}, {"mentioned_in_paper": "935", "context_id": "127", "dataset_context": "One type is represented by DataCite, which allows to find and download datasets but is not a data repository itself, but rather a DOI registering organization for datasets.", "mention_start": 52, "mention_end": 79, "dataset_mention": "find and download datasets"}, {"mentioned_in_paper": "935", "context_id": "128", "dataset_context": "The second type is represented by Zenodo, Dryad and Figshare, repositories which allow researchers to upload and preserve their datasets but do not collect information from other repositories (Enis, 2013; He & Han, 2017; Neumann & Brase, 2014; Sicilia et al., 2017).", "mention_start": 101, "mention_end": 136, "dataset_mention": "upload and preserve their datasets"}, {"mentioned_in_paper": "935", "context_id": "139", "dataset_context": "The results of our study confirm that DataCite is a central tool for locating and identifying datasets.", "mention_start": 69, "mention_end": 102, "dataset_mention": "locating and identifying datasets"}, {"mentioned_in_paper": "935", "context_id": "142", "dataset_context": "That means that DataCite offers the user the opportunity of finding datasets from different data repositories while the other three ones analyzed (Figshare, Zenodo and Dryad) do not record information of data from other resources.", "mention_start": 60, "mention_end": 76, "dataset_mention": "finding datasets"}, {"mentioned_in_paper": "935", "context_id": "144", "dataset_context": "We are aware that other strategies could have been considered, such as trying to relate the datasets to the country of the researcher at the time of publication.", "mention_start": 80, "mention_end": 100, "dataset_mention": "relate the datasets"}, {"mentioned_in_paper": "935", "context_id": "161", "dataset_context": "For example, disciplines such as Meteorology and Physics data sharing practices have been common for decades, as well as in other areas like genetics and genomics large scale datasets were crucial to enable new and important discoveries in those disciplines.", "mention_start": 123, "mention_end": 183, "dataset_mention": "other areas like genetics and genomics large scale datasets"}, {"mentioned_in_paper": "935", "context_id": "165", "dataset_context": "Regarding the analysis of the relationship between the academic experience in the research career regarding the publication of datasets, our study shows that there is a relationship between the year of beginning of the PhD and the year of publication of the first set of data: the more recently the researchers started their PhD, the more possibilities there are to publish the first dataset in the following 5 years, which reinforces the idea that new generations of researchers are embracing more actively data sharing activities, compared to previous generations.", "mention_start": 365, "mention_end": 391, "dataset_mention": "publish the first dataset"}, {"mentioned_in_paper": "937", "context_id": "9", "dataset_context": "However, collecting annotations to train these models can be expensive, especially when they require domain expertise [28] or are constantly evolving like e-commerce datasets.", "mention_start": 129, "mention_end": 174, "dataset_mention": "constantly evolving like e-commerce datasets"}, {"mentioned_in_paper": "937", "context_id": "35", "dataset_context": "Specifically, we train on Polyvore Outfits and test on Capsule Wardrobes [16], and train on the Fashion-Gen dataset [30] and test on Polyvore Outfits, reporting a 6-8% gain over prior work.", "mention_start": 91, "mention_end": 115, "dataset_mention": "the Fashion-Gen dataset"}, {"mentioned_in_paper": "937", "context_id": "141", "dataset_context": "We train on the Polyvore Outfits dataset when evaluating on Capsule Wardrobe.", "mention_start": 12, "mention_end": 40, "dataset_mention": "the Polyvore Outfits dataset"}, {"mentioned_in_paper": "937", "context_id": "160", "dataset_context": "In addition to outperforming the SSL baselines, our full model without any labels outperforms simple the Simaese Network trained with compatibility labels, 1 : Comparison of (a) supervised models with compatibility or attribute labels and (b,c) unsupervised models on the Polyvore Outfits [34] and Capsule [16] datasets.", "mention_start": 268, "mention_end": 319, "dataset_mention": "the Polyvore Outfits [34] and Capsule [16] datasets"}, {"mentioned_in_paper": "937", "context_id": "166", "dataset_context": "Polyvore-D and Cross Dataset Evaluation.", "mention_start": 0, "mention_end": 28, "dataset_mention": "Polyvore-D and Cross Dataset"}, {"mentioned_in_paper": "937", "context_id": "168", "dataset_context": "Table 3 explores a cross dataset evaluation scenario, where a model is trained on Fashion-Gen but evaluated on Polyvore Outfits.", "mention_start": 0, "mention_end": 32, "dataset_mention": "Table 3 explores a cross dataset"}, {"mentioned_in_paper": "937", "context_id": "176", "dataset_context": "We train a model on the Fashion-Gen dataset and test it on the Polyvore dataset.", "mention_start": 20, "mention_end": 43, "dataset_mention": "the Fashion-Gen dataset"}, {"mentioned_in_paper": "937", "context_id": "176", "dataset_context": "We train a model on the Fashion-Gen dataset and test it on the Polyvore dataset.", "mention_start": 59, "mention_end": 79, "dataset_mention": "the Polyvore dataset"}, {"mentioned_in_paper": "938", "context_id": "21", "dataset_context": "Traditional AD (and other ML) approaches often face difficulties in handling high-dimensional datasets due to the curse of dimensionality [5, 6].", "mention_start": 77, "mention_end": 102, "dataset_mention": "high-dimensional datasets"}, {"mentioned_in_paper": "938", "context_id": "63", "dataset_context": "Sometimes a small labeled dataset is available which can be used for choosing an optimal threshold parameter.", "mention_start": 0, "mention_end": 33, "dataset_mention": "Sometimes a small labeled dataset"}, {"mentioned_in_paper": "938", "context_id": "145", "dataset_context": "A historical dataset contains 50502 observations of these signals during normal operation of the system: the object is transported from the bottom right to the top left position and back for 232 such cycles.", "mention_start": 0, "mention_end": 20, "dataset_mention": "A historical dataset"}, {"mentioned_in_paper": "938", "context_id": "149", "dataset_context": "HRSS dataset contains no anomalies so we can not evaluate the performance of anomaly detection approaches.", "mention_start": 0, "mention_end": 12, "dataset_mention": "HRSS dataset"}, {"mentioned_in_paper": "938", "context_id": "199", "dataset_context": "The historical dataset is divided into a training set with 232749 observations and a test set with 11544 observations.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The historical dataset"}, {"mentioned_in_paper": "940", "context_id": "42", "dataset_context": "So while the OD discovery algorithm in [9, 10] is shown to scale to datasets with millions of tuples, it is infeasible to run their adapted AOC discovery algorithm over even moderately sized datasets.", "mention_start": 173, "mention_end": 199, "dataset_mention": "moderately sized datasets"}, {"mentioned_in_paper": "940", "context_id": "194", "dataset_context": "As shown in Figure 2, while when using our algorithm, the framework can discover AOCs in datasets with up to millions of tuples, when using the iterative algorithm, it does not terminate within 24 hours on 400K and 1M tuples of the flight and ncvoter datasets, respectively (the running times for the flight dataset have been projected for better comparison).", "mention_start": 227, "mention_end": 259, "dataset_mention": "the flight and ncvoter datasets"}, {"mentioned_in_paper": "940", "context_id": "194", "dataset_context": "As shown in Figure 2, while when using our algorithm, the framework can discover AOCs in datasets with up to millions of tuples, when using the iterative algorithm, it does not terminate within 24 hours on 400K and 1M tuples of the flight and ncvoter datasets, respectively (the running times for the flight dataset have been projected for better comparison).", "mention_start": 296, "mention_end": 315, "dataset_mention": "the flight dataset"}, {"mentioned_in_paper": "940", "context_id": "212", "dataset_context": "For instance, in the flight dataset, the AOC of arrivalDelay \u223c lateAircraftDelay holds with an approximation factor of 9.5%.", "mention_start": 16, "mention_end": 35, "dataset_mention": "the flight dataset"}, {"mentioned_in_paper": "940", "context_id": "219", "dataset_context": "For instance, in the ncvoter dataset with 5M tuples and with the approximation threshold set to 20%, the AOC of municipalityAbbrv \u223c municipalityDesc is discovered, which points to exceptions in creating abbreviations for municipalities; e.g., \"Raleigh\" is abbreviated as \"RAL\", while \"Charlotte\" is abbreviated as \"CLT\".", "mention_start": 16, "mention_end": 36, "dataset_mention": "the ncvoter dataset"}, {"mentioned_in_paper": "940", "context_id": "226", "dataset_context": "Figure 5 shows the number of OCs or AOCs found at each level of the lattice, when using 5M tuples and 10 attributes of the ncvoter dataset.", "mention_start": 118, "mention_end": 138, "dataset_mention": "the ncvoter dataset"}, {"mentioned_in_paper": "940", "context_id": "239", "dataset_context": "Other than the AOCs discussed in Exp-4, in the flight dataset, we discover the AOC originAirport \u223c IATACode with an 8% approximation factor.", "mention_start": 42, "mention_end": 61, "dataset_mention": "the flight dataset"}, {"mentioned_in_paper": "940", "context_id": "241", "dataset_context": "Furthermore, the AOC streetAddress \u223c mailAddress holds in the ncvoter dataset with an approximation factor of 18%.", "mention_start": 57, "mention_end": 77, "dataset_mention": "the ncvoter dataset"}, {"mentioned_in_paper": "940", "context_id": "244", "dataset_context": "Even if there are fewer AOCs than OCs (e.g., the flight dataset in Exp-2), the discovered dependencies are on lower levels of the lattice, as shown in Exp-5, which makes them more interesting [9, 10].", "mention_start": 44, "mention_end": 63, "dataset_mention": " the flight dataset"}, {"mentioned_in_paper": "941", "context_id": "2", "dataset_context": "We study the problem of HoW-3D by proposing an ABC-HoW benchmark, which is created on top of CAD models sourced from the ABC-dataset with 12k single-view images and the corresponding holistic 3D wireframe models.", "mention_start": 116, "mention_end": 132, "dataset_mention": "the ABC-dataset"}, {"mentioned_in_paper": "941", "context_id": "31", "dataset_context": "To train and benchmark the proposed HoW-3D task, we build the ABC-HoW dataset based on the large-scale ABC dataset [14] for geometric deep learning with CAD models.", "mention_start": 57, "mention_end": 77, "dataset_mention": "the ABC-HoW dataset"}, {"mentioned_in_paper": "941", "context_id": "31", "dataset_context": "To train and benchmark the proposed HoW-3D task, we build the ABC-HoW dataset based on the large-scale ABC dataset [14] for geometric deep learning with CAD models.", "mention_start": 86, "mention_end": 114, "dataset_mention": "the large-scale ABC dataset"}, {"mentioned_in_paper": "941", "context_id": "33", "dataset_context": "Statistically, our ABC-HoW dataset contains 11179 images of 900 objects and 1079 images of 91 objects for training and testing, respectively.", "mention_start": 14, "mention_end": 34, "dataset_mention": " our ABC-HoW dataset"}, {"mentioned_in_paper": "941", "context_id": "36", "dataset_context": "-It presents a dataset ABC-HoW for a new and challenging task of HoW-3D, which enables studying and benchmarking single-view 3D reconstruction methods from recovering only the visible wireframes to the holistic one.", "mention_start": 0, "mention_end": 22, "dataset_mention": "-It presents a dataset"}, {"mentioned_in_paper": "941", "context_id": "48", "dataset_context": "Benefiting from the large-scale CAD dataset [14], recent works [37, 18, 10] achieve promising 3D wireframe reconstruction results based on the high fidelity 3D point clouds in a supervised manner.", "mention_start": 16, "mention_end": 43, "dataset_mention": "the large-scale CAD dataset"}, {"mentioned_in_paper": "941", "context_id": "53", "dataset_context": "And recent work [35] approaches this problem in a data-driven way by projecting the CAD models in ABC dataset [14] into line drawings.", "mention_start": 98, "mention_end": 109, "dataset_mention": "ABC dataset"}, {"mentioned_in_paper": "941", "context_id": "56", "dataset_context": "3D Shape Datasets.", "mention_start": 0, "mention_end": 17, "dataset_mention": "3D Shape Datasets"}, {"mentioned_in_paper": "941", "context_id": "57", "dataset_context": "There have been many large-scale 3D shape datasets [4, 40, 49, 28] that boosted the research of 3D object reconstruction and understanding by using meshes or point clouds.", "mention_start": 11, "mention_end": 50, "dataset_mention": "been many large-scale 3D shape datasets"}, {"mentioned_in_paper": "941", "context_id": "58", "dataset_context": "Most recently, the ABC [14] dataset provided the analytic boundary representation of surfaces and curves for the 3D models, which motivates recent works [37, 18, 35, 10] to use the selected objects from the ABC dataset for the 3D parametric boundary perception from the sampled point clouds or line drawings.", "mention_start": 14, "mention_end": 35, "dataset_mention": " the ABC [14] dataset"}, {"mentioned_in_paper": "941", "context_id": "58", "dataset_context": "Most recently, the ABC [14] dataset provided the analytic boundary representation of surfaces and curves for the 3D models, which motivates recent works [37, 18, 35, 10] to use the selected objects from the ABC dataset for the 3D parametric boundary perception from the sampled point clouds or line drawings.", "mention_start": 202, "mention_end": 218, "dataset_mention": "the ABC dataset"}, {"mentioned_in_paper": "941", "context_id": "59", "dataset_context": "Besides the sampled ABC dataset, PC2WF [18] also prepared a small-scale Furniture dataset that consists of 250 objects from Google 3D Warehouse with only 5 categories, which encodes strong class priors for 3D wireframe perception.", "mention_start": 8, "mention_end": 31, "dataset_mention": "the sampled ABC dataset"}, {"mentioned_in_paper": "941", "context_id": "59", "dataset_context": "Besides the sampled ABC dataset, PC2WF [18] also prepared a small-scale Furniture dataset that consists of 250 objects from Google 3D Warehouse with only 5 categories, which encodes strong class priors for 3D wireframe perception.", "mention_start": 57, "mention_end": 89, "dataset_mention": "a small-scale Furniture dataset"}, {"mentioned_in_paper": "941", "context_id": "60", "dataset_context": "For the task of single-view HoW-3D without incurring the semantic labels, to the best of our knowledge, there is no off-the-shelf dataset for either training a DNN or evaluating the reconstruction results.", "mention_start": 112, "mention_end": 137, "dataset_mention": "no off-the-shelf dataset"}, {"mentioned_in_paper": "941", "context_id": "61", "dataset_context": "Accordingly, we present our ABC-HoW benchmark based on the original ABC dataset  The rendering process for ABC-HoW.", "mention_start": 54, "mention_end": 79, "dataset_mention": "the original ABC dataset"}, {"mentioned_in_paper": "941", "context_id": "62", "dataset_context": "Figure 2: The statistics (a) and illustration (b) of our ABC-HoW dataset.", "mention_start": 52, "mention_end": 72, "dataset_mention": "our ABC-HoW dataset"}, {"mentioned_in_paper": "941", "context_id": "63", "dataset_context": "For each selected object from the ABC dataset [14], we render the images and 3D wireframe models for the randomly-generated view candidates, and then manually select the meaningful viewpoints (marked in blue) for the HoW-3D task.", "mention_start": 30, "mention_end": 45, "dataset_mention": "the ABC dataset"}, {"mentioned_in_paper": "941", "context_id": "70", "dataset_context": "In this section, we describe the detail of the proposed ABC-HoW dataset for holistic 3D wireframe perception.", "mention_start": 42, "mention_end": 71, "dataset_mention": "the proposed ABC-HoW dataset"}, {"mentioned_in_paper": "941", "context_id": "71", "dataset_context": "We create our ABC-HoW dataset based on the large-scale ABC dataset [14].", "mention_start": 10, "mention_end": 29, "dataset_mention": "our ABC-HoW dataset"}, {"mentioned_in_paper": "941", "context_id": "71", "dataset_context": "We create our ABC-HoW dataset based on the large-scale ABC dataset [14].", "mention_start": 39, "mention_end": 66, "dataset_mention": "the large-scale ABC dataset"}, {"mentioned_in_paper": "941", "context_id": "76", "dataset_context": "We select a subset of CAD models that are dominated by straight line segments as the main physical object boundaries from the first 10 chunks of the ABC dataset [14], which yields a total of 3000 CAD models for the HoW-3D task.", "mention_start": 145, "mention_end": 160, "dataset_mention": "the ABC dataset"}, {"mentioned_in_paper": "941", "context_id": "90", "dataset_context": "In summary, for each data sample in the ABC-HoW dataset,c it contains a 2D image I \u2208 R H\u00d7W \u00d73 , the 3D junction set J 3d , the line index set L for the line segments, and the visibility label V \u2208 {0, 1} |J | to represent the holistic 3D wireframe graph W 3d = (J 3d , L, V).", "mention_start": 35, "mention_end": 55, "dataset_mention": "the ABC-HoW dataset"}, {"mentioned_in_paper": "941", "context_id": "133", "dataset_context": "We train our DSG model in a supervised manner on the proposed ABC-HoW dataset.", "mention_start": 49, "mention_end": 77, "dataset_mention": "the proposed ABC-HoW dataset"}, {"mentioned_in_paper": "941", "context_id": "166", "dataset_context": "For the first possible baseline configuration, we train Pixel2mesh [36] on our ABC-HoW dataset to reconstruct the 3D mesh of the object from single-view images.", "mention_start": 74, "mention_end": 94, "dataset_mention": "our ABC-HoW dataset"}, {"mentioned_in_paper": "941", "context_id": "168", "dataset_context": "As shown in Tab. 1 and Fig. 4, the mesh reconstruction performance of Pixel2Mesh is dramatically dropped, especially for the invisible parts on our dataset as the objects in our ABC-HoW dataset do not have any semantic priors.", "mention_start": 173, "mention_end": 193, "dataset_mention": "our ABC-HoW dataset"}, {"mentioned_in_paper": "941", "context_id": "197", "dataset_context": "This paper studies a new task of HoW-3D for single-view images by proposing an ABC-HoW dataset and a novel Deep Spatial Gestalt (DSG) model, making the first step of perceiving holistic 3D wireframes from single-view images.", "mention_start": 66, "mention_end": 94, "dataset_mention": "proposing an ABC-HoW dataset"}, {"mentioned_in_paper": "942", "context_id": "137", "dataset_context": "Thus, in the CLAP2015 dataset [14], the entire range [3, 85] is divided into five groups [3, 18], [10, 29], [19, 44], [30, 62], and [45, 85].", "mention_start": 8, "mention_end": 29, "dataset_mention": "the CLAP2015 dataset"}, {"mentioned_in_paper": "942", "context_id": "162", "dataset_context": "For the Adience dataset, we use aligned images in [23].", "mention_start": 4, "mention_end": 23, "dataset_mention": "the Adience dataset"}, {"mentioned_in_paper": "942", "context_id": "163", "dataset_context": "Unless specified otherwise, we pre-train the \u03c1-regressors on the IMDB-WIKI dataset, as done in [22, 26, 27, 38, 40, 44, 45].", "mention_start": 60, "mention_end": 82, "dataset_mention": "the IMDB-WIKI dataset"}, {"mentioned_in_paper": "942", "context_id": "198", "dataset_context": "Table 3 lists the performances on the UTK dataset [49].", "mention_start": 34, "mention_end": 49, "dataset_mention": "the UTK dataset"}, {"mentioned_in_paper": "942", "context_id": "213", "dataset_context": "Table 4 also compares the results on the HCI dataset.", "mention_start": 37, "mention_end": 52, "dataset_mention": "the HCI dataset"}, {"mentioned_in_paper": "942", "context_id": "220", "dataset_context": "Next, we analyze the proposed MWR algorithm using facial age estimation datasets.", "mention_start": 49, "mention_end": 80, "dataset_mention": "facial age estimation datasets"}, {"mentioned_in_paper": "943", "context_id": "12", "dataset_context": "Examples of some fallacious detection results for car category in the KITTI dataset by the 2D detector used in our paper.", "mention_start": 66, "mention_end": 83, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "943", "context_id": "37", "dataset_context": "Experiments on KITTI dataset demonstrate the improvement of our work on orientation estimation and overall detection precision compared with current state-of-the-art methods only using a single RGB image.", "mention_start": 15, "mention_end": 28, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "943", "context_id": "41", "dataset_context": "And plentiful of methods have been proposed to address the problem of 3D object detection by the data collected from autonomous driving scenarios, such as KITTI dataset.", "mention_start": 154, "mention_end": 168, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "943", "context_id": "68", "dataset_context": "And then the cropped image patches according to 2D detections are fed into a multi-branches CNN to respectively infer: 1) dimensions 2) orientation, i.e. the local angle is regressed to calculate the global angle that only the yaw angle be specified in autonomous driving scenarios in KITTI dataset 3) viewpoint 4) the center projection of the bottom face.", "mention_start": 284, "mention_end": 298, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "943", "context_id": "126", "dataset_context": "And the RGB images in the KITTI dataset used in this paper are collected by the VW Passat station wagon which is equipped with four video cameras.", "mention_start": 22, "mention_end": 39, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "943", "context_id": "151", "dataset_context": "Our 3D properties estimation network was trained and tested on KITTI object detection dataset by the split used in [6].", "mention_start": 63, "mention_end": 93, "dataset_mention": "KITTI object detection dataset"}, {"mentioned_in_paper": "943", "context_id": "161", "dataset_context": "Since most works only released their result on cars, thus we made evaluation of our model on KITTI dataset focused on for car category like most previous works did.", "mention_start": 92, "mention_end": 106, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "943", "context_id": "168", "dataset_context": "By adding two appearance-related tasks, we got competitive performance with [8] Average Orientation Similarity (AOS) is the official 3D orientation metric of the KITTI dataset which is described in [1] and multiplies the average precision (AP) of the 2D detector with the average cosine distance similarity for azimuth orientation is calculated to evaluate the performance of orientation estimation.", "mention_start": 157, "mention_end": 175, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "943", "context_id": "170", "dataset_context": "The AOS is first published in [8] as assessment criteria and our method is first among all non-anonymous methods for car examples on the KITTI dataset.", "mention_start": 133, "mention_end": 150, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "943", "context_id": "173", "dataset_context": "On the KITTI detection dataset, 2 bins was achieved better performance than 8 bins in our work as it decreased the training data amount for each bin.", "mention_start": 3, "mention_end": 30, "dataset_mention": "the KITTI detection dataset"}, {"mentioned_in_paper": "943", "context_id": "199", "dataset_context": "Fig. 7 was the examples of qualitative detection results by cascaded geometric constraints method without filtering wrong 2D detections on the scenes of KITTI dataset.", "mention_start": 153, "mention_end": 166, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "945", "context_id": "27", "dataset_context": "However, it requires paired dataset for learning a random variable which contains the variation in the learned one-to-many mapping.", "mention_start": 20, "mention_end": 35, "dataset_mention": "paired dataset"}, {"mentioned_in_paper": "945", "context_id": "52", "dataset_context": "We extensively evaluate the proposed model trained on edge \u2194 photo, male \u2194 female and face \u2194 Emoji datasets.", "mention_start": 67, "mention_end": 107, "dataset_mention": " male \u2194 female and face \u2194 Emoji datasets"}, {"mentioned_in_paper": "945", "context_id": "78", "dataset_context": "To enhance the diversity, the BicycleGAN [40] succeeded in generating diverse samples in the target domain after a supervised training on paired dataset.", "mention_start": 137, "mention_end": 152, "dataset_mention": "paired dataset"}, {"mentioned_in_paper": "945", "context_id": "180", "dataset_context": "The CelebA [19] dataset is for attribute-based translation of face images.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The CelebA [19] dataset"}, {"mentioned_in_paper": "945", "context_id": "185", "dataset_context": "One domain is composed of realworld frontal-face images from the CelebA dataset.", "mention_start": 61, "mention_end": 79, "dataset_mention": "the CelebA dataset"}, {"mentioned_in_paper": "945", "context_id": "212", "dataset_context": "In this subsection, we quantitatively compare the performances against baseline methods on the Edge \u2194 photo and Female \u2194 Male datasets.", "mention_start": 90, "mention_end": 134, "dataset_mention": "the Edge \u2194 photo and Female \u2194 Male datasets"}, {"mentioned_in_paper": "947", "context_id": "29", "dataset_context": "Nogueira et al. [19] found that AlexNet [20] performs better than VGG16 [21] on the UCMerced Land-Use dataset [22] but the result is opposite on the Brazilian Coffee Scenes Dataset [23].", "mention_start": 80, "mention_end": 109, "dataset_mention": "the UCMerced Land-Use dataset"}, {"mentioned_in_paper": "947", "context_id": "29", "dataset_context": "Nogueira et al. [19] found that AlexNet [20] performs better than VGG16 [21] on the UCMerced Land-Use dataset [22] but the result is opposite on the Brazilian Coffee Scenes Dataset [23].", "mention_start": 145, "mention_end": 180, "dataset_mention": "the Brazilian Coffee Scenes Dataset"}, {"mentioned_in_paper": "947", "context_id": "41", "dataset_context": "3) Our experiments show that the best architectures outperform several classical hand-designing architectures in three remote sensing scene classification data sets.", "mention_start": 113, "mention_end": 164, "dataset_mention": "three remote sensing scene classification data sets"}, {"mentioned_in_paper": "947", "context_id": "107", "dataset_context": "To confirm the effectiveness of our proposed architecture learning procedure, we searched optimal architecture on three remote sensing scene classification data sets respectively and validated their performances.", "mention_start": 113, "mention_end": 165, "dataset_mention": "three remote sensing scene classification data sets"}, {"mentioned_in_paper": "947", "context_id": "110", "dataset_context": "In this section, we will choose three widely used remote sensing scene classification data sets (UC Merced Land-Use [22], AID [36], and NWPU-RESISC45 [3]) to test the robustness and effectiveness of our proposed method.", "mention_start": 49, "mention_end": 95, "dataset_mention": "remote sensing scene classification data sets"}, {"mentioned_in_paper": "947", "context_id": "111", "dataset_context": "1) UC Merced Land-Use Data Set: The UC Merced Land-Use dataset is composed of 2100 aerial scene images divided into 21 land use scene classes.", "mention_start": 0, "mention_end": 30, "dataset_mention": "1) UC Merced Land-Use Data Set"}, {"mentioned_in_paper": "947", "context_id": "111", "dataset_context": "1) UC Merced Land-Use Data Set: The UC Merced Land-Use dataset is composed of 2100 aerial scene images divided into 21 land use scene classes.", "mention_start": 31, "mention_end": 62, "dataset_mention": " The UC Merced Land-Use dataset"}, {"mentioned_in_paper": "947", "context_id": "115", "dataset_context": "Some highly overlapped classes such as dense residential, medium residential and sparse residential are included in this dataset, which are mainly different in the density of structures and makes the dataset more difficult to classify.", "mention_start": 174, "mention_end": 207, "dataset_mention": "structures and makes the dataset"}, {"mentioned_in_paper": "947", "context_id": "117", "dataset_context": "2) Aerial Image Data Set: AID is large-scale aerial image dataset, which was collected from Google Earth imagery and is a more challenging dataset compared with The UC Merced Land-Use dataset because of The following reasons.", "mention_start": 0, "mention_end": 24, "dataset_mention": "2) Aerial Image Data Set"}, {"mentioned_in_paper": "947", "context_id": "117", "dataset_context": "2) Aerial Image Data Set: AID is large-scale aerial image dataset, which was collected from Google Earth imagery and is a more challenging dataset compared with The UC Merced Land-Use dataset because of The following reasons.", "mention_start": 32, "mention_end": 65, "dataset_mention": "large-scale aerial image dataset"}, {"mentioned_in_paper": "947", "context_id": "117", "dataset_context": "2) Aerial Image Data Set: AID is large-scale aerial image dataset, which was collected from Google Earth imagery and is a more challenging dataset compared with The UC Merced Land-Use dataset because of The following reasons.", "mention_start": 160, "mention_end": 191, "dataset_mention": "The UC Merced Land-Use dataset"}, {"mentioned_in_paper": "947", "context_id": "118", "dataset_context": "First, the AID dataset contains more scene types and images.", "mention_start": 6, "mention_end": 22, "dataset_mention": " the AID dataset"}, {"mentioned_in_paper": "947", "context_id": "123", "dataset_context": "3) NWPU-RESISC45 Data Set: NWPU-RESISC45 dataset is more complex than UC Merced Land-Use and AID datasets and consists of a total of 31,500 remote sensing images divided into 45 scene classes.", "mention_start": 0, "mention_end": 25, "dataset_mention": "3) NWPU-RESISC45 Data Set"}, {"mentioned_in_paper": "947", "context_id": "123", "dataset_context": "3) NWPU-RESISC45 Data Set: NWPU-RESISC45 dataset is more complex than UC Merced Land-Use and AID datasets and consists of a total of 31,500 remote sensing images divided into 45 scene classes.", "mention_start": 26, "mention_end": 48, "dataset_mention": " NWPU-RESISC45 dataset"}, {"mentioned_in_paper": "947", "context_id": "123", "dataset_context": "3) NWPU-RESISC45 Data Set: NWPU-RESISC45 dataset is more complex than UC Merced Land-Use and AID datasets and consists of a total of 31,500 remote sensing images divided into 45 scene classes.", "mention_start": 69, "mention_end": 105, "dataset_mention": "UC Merced Land-Use and AID datasets"}, {"mentioned_in_paper": "947", "context_id": "145", "dataset_context": "We tried multiple times to obtain the best number of cells for every dataset, i.e., the number of cells is set to 6 for UC Merced Land-Use data set, 7 for AID and 10 for NWPU-RESISC45.", "mention_start": 119, "mention_end": 147, "dataset_mention": "UC Merced Land-Use data set"}, {"mentioned_in_paper": "947", "context_id": "173", "dataset_context": "It is easy to observe that the Fig. 5 : The train snd valid Accuracy during architecture learning procedure Fig. 6 : The optimal cell on the UCM data set Fig. 7 : The optimal cell on the AID architectures we found have smaller numbers of parameters than the calssical ones.", "mention_start": 136, "mention_end": 153, "dataset_mention": "the UCM data set"}, {"mentioned_in_paper": "947", "context_id": "176", "dataset_context": "For UC Merced Land-Use data set, we stack the learned optimal cells in 6 times, and train the architecture from scratch for 150 epochs.", "mention_start": 4, "mention_end": 31, "dataset_mention": "UC Merced Land-Use data set"}, {"mentioned_in_paper": "948", "context_id": "16", "dataset_context": "An interesting yet challenging research question then arises: Can we learn a multimodal model from an incomplete dataset while its performance should as close as possible to the one that learns from a full-modality dataset?", "mention_start": 98, "mention_end": 120, "dataset_mention": "an incomplete dataset"}, {"mentioned_in_paper": "948", "context_id": "16", "dataset_context": "An interesting yet challenging research question then arises: Can we learn a multimodal model from an incomplete dataset while its performance should as close as possible to the one that learns from a full-modality dataset?", "mention_start": 198, "mention_end": 222, "dataset_mention": "a full-modality dataset"}, {"mentioned_in_paper": "948", "context_id": "57", "dataset_context": "Formally, we let D = {D f , D m } denote a multimodal dataset; D f = {x 1 i , x 2 i , y i } i is a modality-complete dataset, where x 1 i and x 2 i represent two different modalities of i-th sample and y i is the corresponding class label; D m = {x 1 j , y j } j is a modality-incomplete dataset, where one modality is missing.", "mention_start": 96, "mention_end": 124, "dataset_mention": "a modality-complete dataset"}, {"mentioned_in_paper": "948", "context_id": "57", "dataset_context": "Formally, we let D = {D f , D m } denote a multimodal dataset; D f = {x 1 i , x 2 i , y i } i is a modality-complete dataset, where x 1 i and x 2 i represent two different modalities of i-th sample and y i is the corresponding class label; D m = {x 1 j , y j } j is a modality-incomplete dataset, where one modality is missing.", "mention_start": 265, "mention_end": 295, "dataset_mention": "a modality-incomplete dataset"}, {"mentioned_in_paper": "948", "context_id": "69", "dataset_context": "We intend to train a model on the modality severely missing dataset to achieve comparable performance as the model trained on a full-modality dataset.", "mention_start": 30, "mention_end": 67, "dataset_mention": "the modality severely missing dataset"}, {"mentioned_in_paper": "948", "context_id": "69", "dataset_context": "We intend to train a model on the modality severely missing dataset to achieve comparable performance as the model trained on a full-modality dataset.", "mention_start": 126, "mention_end": 149, "dataset_mention": "a full-modality dataset"}, {"mentioned_in_paper": "948", "context_id": "84", "dataset_context": "Inspired by (Kuo et al. 2019), we approximate the missing modality using a weighted sum of modality priors learned from the modality-complete dataset.", "mention_start": 119, "mention_end": 149, "dataset_mention": "the modality-complete dataset"}, {"mentioned_in_paper": "948", "context_id": "127", "dataset_context": "Totally three datasets are used in the experiment: \u2022 The Multimodal IMDb (MM-IMDb) (Arevalo et al. 2017) contains two modalities: image and text.", "mention_start": 0, "mention_end": 22, "dataset_mention": "Totally three datasets"}, {"mentioned_in_paper": "948", "context_id": "131", "dataset_context": "The images, which are digits from 0 to 9, are collected from the MNIST dataset (LeCun et al. 1998) with a size of 28 \u00d7 28, and the audio modality is collected from Free Spoken Digits Dataset 1 containing raw 1, 500 audios.", "mention_start": 60, "mention_end": 78, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "948", "context_id": "131", "dataset_context": "The images, which are digits from 0 to 9, are collected from the MNIST dataset (LeCun et al. 1998) with a size of 28 \u00d7 28, and the audio modality is collected from Free Spoken Digits Dataset 1 containing raw 1, 500 audios.", "mention_start": 163, "mention_end": 190, "dataset_mention": "Free Spoken Digits Dataset"}, {"mentioned_in_paper": "948", "context_id": "137", "dataset_context": "For MM-IMDb dataset, we follow previous works (Arevalo et al. 2017; Vielzeuf et al. 2018) by adopting the F1 Samples and F1 Micro to evaluate multilabel classification.", "mention_start": 4, "mention_end": 19, "dataset_mention": "MM-IMDb dataset"}, {"mentioned_in_paper": "948", "context_id": "139", "dataset_context": "For avMNIST dataset, we compute accuracy to measure the performance.", "mention_start": 4, "mention_end": 19, "dataset_mention": "avMNIST dataset"}, {"mentioned_in_paper": "948", "context_id": "212", "dataset_context": "For the avMNIST dataset, the missing modality problem only happens to audio modality.", "mention_start": 4, "mention_end": 23, "dataset_mention": "the avMNIST dataset"}, {"mentioned_in_paper": "948", "context_id": "228", "dataset_context": "We conduct the ablation analysis on the MM-IMDb dataset to evaluate the effectiveness of the missing modality reconstruction, feature regularization, and Bayesian Inference.", "mention_start": 36, "mention_end": 55, "dataset_mention": "the MM-IMDb dataset"}, {"mentioned_in_paper": "950", "context_id": "26", "dataset_context": "al. [2] compare PCA, Kernel PCA (KPCA), and Independent Component Analysis (ICA) as applied to Support Vector Machine (SVM) for feature extraction to three data sets (sunspot data, Satan Fe data set A, and five real futures contracts).", "mention_start": 180, "mention_end": 198, "dataset_mention": " Satan Fe data set"}, {"mentioned_in_paper": "950", "context_id": "27", "dataset_context": "SVM by feature extraction using PCA, KPCA or ICA can achieve better generalization performance than that without feature extraction, and that KPCA and ICA perform better than PCA on all the studied data sets, with the best performance in KPCA.", "mention_start": 181, "mention_end": 207, "dataset_mention": "all the studied data sets"}, {"mentioned_in_paper": "950", "context_id": "56", "dataset_context": "PCA is a powerful technique for extracting structure from high-dimensional data sets.", "mention_start": 58, "mention_end": 84, "dataset_mention": "high-dimensional data sets"}, {"mentioned_in_paper": "951", "context_id": "4", "dataset_context": "Our proposed method is evaluated on FERET data set and compared with state-of-the-art competing methods.", "mention_start": 36, "mention_end": 50, "dataset_mention": "FERET data set"}, {"mentioned_in_paper": "951", "context_id": "148", "dataset_context": "From FERET dataset [40], we used 10585 images in training and the rest (3541 images) in the evaluation phase.", "mention_start": 5, "mention_end": 18, "dataset_mention": "FERET dataset"}, {"mentioned_in_paper": "951", "context_id": "150", "dataset_context": "The FERET face data set contains 14126 face images from 1199 individuals.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The FERET face data set"}, {"mentioned_in_paper": "951", "context_id": "162", "dataset_context": "In this experiment, we evaluated our proposed method on the four categories of FERET evaluation datasets described in Section IV-A.", "mention_start": 78, "mention_end": 104, "dataset_mention": "FERET evaluation datasets"}, {"mentioned_in_paper": "951", "context_id": "169", "dataset_context": "Fig. 4.a depicts the cumulative match curves on the F B dataset.", "mention_start": 48, "mention_end": 63, "dataset_mention": "the F B dataset"}, {"mentioned_in_paper": "951", "context_id": "173", "dataset_context": "Fig. 4.b depicts the CMC results on f c dataset.", "mention_start": 36, "mention_end": 47, "dataset_mention": "f c dataset"}, {"mentioned_in_paper": "951", "context_id": "177", "dataset_context": "Cumulative match curves on (a) FB, (b) fc, (c) duplicate I, and (d) duplicate II datasets.", "mention_start": 59, "mention_end": 89, "dataset_mention": " and (d) duplicate II datasets"}, {"mentioned_in_paper": "951", "context_id": "179", "dataset_context": "In rank 1, our method demonstrates an increase of 5.6% compared to the best competing method on f c dataset.", "mention_start": 95, "mention_end": 107, "dataset_mention": "f c dataset"}, {"mentioned_in_paper": "951", "context_id": "181", "dataset_context": "While the performance of our method is robust against the changes in illumination, the other competing methods performance drops significantly on f c dataset compared to F B. DuplicateI includes images in similar condition as the gallery, but with slightly expression variation.", "mention_start": 145, "mention_end": 157, "dataset_mention": "f c dataset"}, {"mentioned_in_paper": "951", "context_id": "186", "dataset_context": "Taken together, our proposed method shows the best performance on F B, f c, and duplicateII probe images and close to the best on duplicateI dataset.", "mention_start": 129, "mention_end": 148, "dataset_mention": "duplicateI dataset"}, {"mentioned_in_paper": "951", "context_id": "189", "dataset_context": "In this experiment, we compared the performance of our method with state-of-theart methods on F B probe dataset which all probe faces of this dataset are similar to gallery faces, but with slighly variation in expression.", "mention_start": 93, "mention_end": 111, "dataset_mention": "F B probe dataset"}, {"mentioned_in_paper": "953", "context_id": "39", "dataset_context": "We conduct extensive experiments on four real-world public recommendation datasets.", "mention_start": 36, "mention_end": 82, "dataset_mention": "four real-world public recommendation datasets"}, {"mentioned_in_paper": "953", "context_id": "193", "dataset_context": "In this work, we follow the settings in [50] and adopt two categories, \"Beauty\" and \"Sports and Outdoors\" Another dataset is collected by Yelp 1 , which is a famous business recommendation platform for restaurants, bars, beauty salons, etc.", "mention_start": 70, "mention_end": 121, "dataset_mention": " \"Beauty\" and \"Sports and Outdoors\" Another dataset"}, {"mentioned_in_paper": "953", "context_id": "195", "dataset_context": "The last dataset is MovieLens-1M 2 (ML-1M) [14] dataset, which is widely used for evaluating recommendation algorithms.", "mention_start": 20, "mention_end": 55, "dataset_mention": "MovieLens-1M 2 (ML-1M) [14] dataset"}, {"mentioned_in_paper": "953", "context_id": "199", "dataset_context": "It is worth mentioning that to guarantee each user/item with enough interactions, we follow the preprocessing procedure in [39, 50], which only keeps the \"5-core\" datasets.", "mention_start": 143, "mention_end": 171, "dataset_mention": "keeps the \"5-core\" datasets"}, {"mentioned_in_paper": "953", "context_id": "242", "dataset_context": "For a fair comparison, following the settings in [50], the maximum sequence length of  is set to 20 and 50 for ML-1M and other datasets, respectively 4.", "mention_start": 109, "mention_end": 135, "dataset_mention": "ML-1M and other datasets"}, {"mentioned_in_paper": "953", "context_id": "258", "dataset_context": "It achieves improvements on both sparse (e.g., Sports and Yelp) and dense datasets (e.g., ML-1m), especially on the sparse datasets.", "mention_start": 46, "mention_end": 82, "dataset_mention": " Sports and Yelp) and dense datasets"}, {"mentioned_in_paper": "953", "context_id": "258", "dataset_context": "It achieves improvements on both sparse (e.g., Sports and Yelp) and dense datasets (e.g., ML-1m), especially on the sparse datasets.", "mention_start": 111, "mention_end": 131, "dataset_mention": "the sparse datasets"}, {"mentioned_in_paper": "953", "context_id": "265", "dataset_context": "We report HR@20 and NDCG@20 as metrics for the experiments on the Sports and Yelp datasets due to the space limitation.", "mention_start": 62, "mention_end": 90, "dataset_mention": "the Sports and Yelp datasets"}, {"mentioned_in_paper": "953", "context_id": "271", "dataset_context": "For example, item mask operator peaks at the proportion rate of 0.5 on the Yelp dataset.", "mention_start": 70, "mention_end": 87, "dataset_mention": "the Yelp dataset"}, {"mentioned_in_paper": "953", "context_id": "291", "dataset_context": "We utilize the friend relations provided by Yelp dataset to explore whether users are closer to each other in the latent space if they are friends.", "mention_start": 44, "mention_end": 56, "dataset_mention": "Yelp dataset"}, {"mentioned_in_paper": "954", "context_id": "8", "dataset_context": "Comprehensive results on two large Yelp review datasets demonstrate that the proposed model can be trained to treat the subgroups more fairly.", "mention_start": 25, "mention_end": 55, "dataset_mention": "two large Yelp review datasets"}, {"mentioned_in_paper": "954", "context_id": "188", "dataset_context": "We use two commonly used Yelp review datasets (see Table 3) in previous spam detection [5, 14, 15].", "mention_start": 25, "mention_end": 45, "dataset_mention": "Yelp review datasets"}, {"mentioned_in_paper": "954", "context_id": "230", "dataset_context": "In Figure 3, we demonstrate the training and the test AFRR of the subgroups of \"pure\" and \"mixed\" with four methods, averaged over 9 training-validation-test splits of the YelpZip dataset.", "mention_start": 167, "mention_end": 187, "dataset_mention": "the YelpZip dataset"}, {"mentioned_in_paper": "956", "context_id": "130", "dataset_context": "To overcome this problem, [55, 56] trained a scene-centric CNN by constructing large scale scene dataset, called Places, resulting a significant performance improvement.", "mention_start": 78, "mention_end": 104, "dataset_mention": "large scale scene dataset"}, {"mentioned_in_paper": "956", "context_id": "225", "dataset_context": "The cross-spectral template matching was applied on 100 RGB-NIR image pairs randomly selected from RGB-NIR Scene Dataset [17].", "mention_start": 99, "mention_end": 120, "dataset_mention": "RGB-NIR Scene Dataset"}, {"mentioned_in_paper": "956", "context_id": "258", "dataset_context": "For the evaluation and comparison of the performance of LAT with others, middlebury stereo data sets [70] including Aloe, Baby1, Baby3, Bowling2, Cloth2, Cloth3, Lampshade1, and Monopoly were used.", "mention_start": 72, "mention_end": 100, "dataset_mention": " middlebury stereo data sets"}, {"mentioned_in_paper": "957", "context_id": "47", "dataset_context": "It achieves state-of-the-art performances on the DOTA and HRSC2016 datasets.", "mention_start": 45, "mention_end": 75, "dataset_mention": "the DOTA and HRSC2016 datasets"}, {"mentioned_in_paper": "957", "context_id": "147", "dataset_context": "We evaluate our method on two public aerial image datasets: DOTA [22] and HRSC2016 [13].", "mention_start": 26, "mention_end": 58, "dataset_mention": "two public aerial image datasets"}, {"mentioned_in_paper": "957", "context_id": "149", "dataset_context": "We use DOTA-v1.0 [22] dataset for the oriented object detection.", "mention_start": 7, "mention_end": 29, "dataset_mention": "DOTA-v1.0 [22] dataset"}, {"mentioned_in_paper": "957", "context_id": "167", "dataset_context": "The HRSC2016 [13] is a ship dataset collected from Google Earth, which contains 1,061 images with ships in various appearances.", "mention_start": 21, "mention_end": 35, "dataset_mention": "a ship dataset"}, {"mentioned_in_paper": "957", "context_id": "174", "dataset_context": "The backbone weights are pre-trained on the ImageNet dataset.", "mention_start": 40, "mention_end": 60, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "957", "context_id": "179", "dataset_context": "We train the network for about 40 epochs on the DOTA dataset and 100 epochs on the HRSC2016 dataset.", "mention_start": 44, "mention_end": 60, "dataset_mention": "the DOTA dataset"}, {"mentioned_in_paper": "957", "context_id": "179", "dataset_context": "We train the network for about 40 epochs on the DOTA dataset and 100 epochs on the HRSC2016 dataset.", "mention_start": 79, "mention_end": 99, "dataset_mention": "the HRSC2016 dataset"}, {"mentioned_in_paper": "957", "context_id": "181", "dataset_context": "The speed of the proposed network is measured on a single NVIDIA TITAN X GPU on the HRSC2016 dataset.", "mention_start": 80, "mention_end": 100, "dataset_mention": "the HRSC2016 dataset"}, {"mentioned_in_paper": "957", "context_id": "192", "dataset_context": "We compare the performance of the proposed method with the state-of-the-art algorithms on the DOTA and HRSC2016 datasets.", "mention_start": 90, "mention_end": 120, "dataset_mention": "the DOTA and HRSC2016 datasets"}, {"mentioned_in_paper": "957", "context_id": "197", "dataset_context": "The detection results on the DOTA dataset are illustrated in Table 1.", "mention_start": 25, "mention_end": 41, "dataset_mention": "the DOTA dataset"}, {"mentioned_in_paper": "957", "context_id": "210", "dataset_context": "The visualization of the detection results of BBAVectors+rh on DOTA dataset is illustrated in Fig. 3.", "mention_start": 63, "mention_end": 75, "dataset_mention": "DOTA dataset"}, {"mentioned_in_paper": "957", "context_id": "214", "dataset_context": "The performance comparison results between the proposed method and the state-of-the-arts on HRSC2016 dataset is illustrated in Table 2. Detection results on the testing dataset of HRSC2016.", "mention_start": 92, "mention_end": 108, "dataset_mention": "HRSC2016 dataset"}, {"mentioned_in_paper": "957", "context_id": "226", "dataset_context": "On the DOTA dataset (see Table 1), the BBAVectors+rh improves 0.71% over BBAVectors+r.", "mention_start": 3, "mention_end": 19, "dataset_mention": "the DOTA dataset"}, {"mentioned_in_paper": "957", "context_id": "227", "dataset_context": "On the HRSC2016 dataset (see Table 2), BBAVectors+rh achieves 0.4% improvement over BBAVectors+r.", "mention_start": 3, "mention_end": 23, "dataset_mention": "the HRSC2016 dataset"}, {"mentioned_in_paper": "957", "context_id": "232", "dataset_context": "From Table 3, we can see that the proposed method performs 4.82% and 2.74% better than Center+wh+\u03b8 on HRSC2016 and DOTA datasets, respectively.", "mention_start": 101, "mention_end": 128, "dataset_mention": "HRSC2016 and DOTA datasets"}, {"mentioned_in_paper": "957", "context_id": "239", "dataset_context": "The results on the HRSC2016 and DOTA datasets demonstrate the superiority of the proposed method over the state-of-thearts.", "mention_start": 15, "mention_end": 45, "dataset_mention": "the HRSC2016 and DOTA datasets"}, {"mentioned_in_paper": "958", "context_id": "163", "dataset_context": "ImageNet-100 is the subset of ImageNet dataset 3 with 100 randomly sampled classes.", "mention_start": 30, "mention_end": 46, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "958", "context_id": "164", "dataset_context": "Nuswide dataset contains about 220K available images associating with 81 ground truth concept labels.", "mention_start": 0, "mention_end": 15, "dataset_mention": "Nuswide dataset"}, {"mentioned_in_paper": "958", "context_id": "167", "dataset_context": "In CIFAR-10 dataset, we randomly select 1,000 images (100 images per class) as query set, the rest 59,000 images as retrieval database, and we random select 5,000 images from the database as the training data.", "mention_start": 3, "mention_end": 19, "dataset_mention": "CIFAR-10 dataset"}, {"mentioned_in_paper": "958", "context_id": "168", "dataset_context": "In Nuswide dataset, we randomly select 2,100 images (100 images per class) as the query set and 10,500 images as the training set.", "mention_start": 3, "mention_end": 18, "dataset_mention": "Nuswide dataset"}, {"mentioned_in_paper": "958", "context_id": "169", "dataset_context": "In ImageNet-100 dataset, we use the same data split as Hash-Net [4] such that 130 images per class(totally 13K images) for training, and all images in the selected classes from the validation set are used as queries.", "mention_start": 3, "mention_end": 23, "dataset_mention": "ImageNet-100 dataset"}, {"mentioned_in_paper": "958", "context_id": "173", "dataset_context": "For Nuswide dataset, similar images share at least one semantic label.", "mention_start": 4, "mention_end": 19, "dataset_mention": "Nuswide dataset"}, {"mentioned_in_paper": "958", "context_id": "190", "dataset_context": "Similar with [30, 4], for each retrieval dataset, we report the compared results in terms of mean average precision(MAP), precision at Hamming distance within 2, pre- cision of top returned candidates.", "mention_start": 25, "mention_end": 48, "dataset_mention": "each retrieval dataset"}, {"mentioned_in_paper": "958", "context_id": "213", "dataset_context": "It should be noticed that there are little performance gain on MAP with the quantized similarity loss for Nuswide dataset, as the distribution of similar pairs underlying the dataset is a little complicated.", "mention_start": 106, "mention_end": 121, "dataset_mention": "Nuswide dataset"}, {"mentioned_in_paper": "958", "context_id": "213", "dataset_context": "It should be noticed that there are little performance gain on MAP with the quantized similarity loss for Nuswide dataset, as the distribution of similar pairs underlying the dataset is a little complicated.", "mention_start": 145, "mention_end": 182, "dataset_mention": "similar pairs underlying the dataset"}, {"mentioned_in_paper": "959", "context_id": "7", "dataset_context": "We evaluate our model on SUN-RGBD and NYUDv2 datasets, and prove that our model outperforms state-of-the-art methods.", "mention_start": 25, "mention_end": 53, "dataset_mention": "SUN-RGBD and NYUDv2 datasets"}, {"mentioned_in_paper": "959", "context_id": "62", "dataset_context": "NYUDv2 [13] : The NYU-Depth V2 data set (NYUDv2) contains 1,449 RGBD images with dense pixel-wise annotation.", "mention_start": 13, "mention_end": 39, "dataset_mention": " The NYU-Depth V2 data set"}, {"mentioned_in_paper": "959", "context_id": "114", "dataset_context": "We evaluate our model on NYUDv2 and SUN-RGBD datasets, and the experiments show that our model can outperform state-of-the-art methods.", "mention_start": 25, "mention_end": 53, "dataset_mention": "NYUDv2 and SUN-RGBD datasets"}, {"mentioned_in_paper": "960", "context_id": "136", "dataset_context": "We also used a much larger dataset called Open Images, a huge collection containing 16M bounding boxes across 1.9M images, making it the largest object detection dataset.", "mention_start": 13, "mention_end": 34, "dataset_mention": "a much larger dataset"}, {"mentioned_in_paper": "960", "context_id": "136", "dataset_context": "We also used a much larger dataset called Open Images, a huge collection containing 16M bounding boxes across 1.9M images, making it the largest object detection dataset.", "mention_start": 129, "mention_end": 169, "dataset_mention": "it the largest object detection dataset"}, {"mentioned_in_paper": "960", "context_id": "154", "dataset_context": "We trained and evaluated on the most popular \"Karpathy\" split built on MS-COCO dataset, where 5K images for validation, 5K for testing, and the rest for training.", "mention_start": 71, "mention_end": 86, "dataset_mention": "MS-COCO dataset"}, {"mentioned_in_paper": "960", "context_id": "167", "dataset_context": "The VCR dataset [75] contains over 212K (training), 26K (validation) and 25K (testing) derived from 110K movie scenes.", "mention_start": 0, "mention_end": 15, "dataset_mention": "The VCR dataset"}, {"mentioned_in_paper": "961", "context_id": "191", "dataset_context": "All the three methods received as input the 25 bands and used the same parameters configuration and training procedure reported in Sothe et al. (2020) for the VNIR dataset.", "mention_start": 155, "mention_end": 171, "dataset_mention": "the VNIR dataset"}, {"mentioned_in_paper": "962", "context_id": "9", "dataset_context": "FERET [31] is an example of an easy data set.", "mention_start": 28, "mention_end": 44, "dataset_mention": "an easy data set"}, {"mentioned_in_paper": "962", "context_id": "10", "dataset_context": "For easy data sets face detection is a solved problem, with the Viola-Jones algorithm [42] being the most widely used solution.", "mention_start": 4, "mention_end": 18, "dataset_mention": "easy data sets"}, {"mentioned_in_paper": "962", "context_id": "31", "dataset_context": "3) CorrFaD is the first algorithm specifically tested on faces in a newly released dataset, the Point and Shoot Face Recognition Challenge (PaSC) [1].", "mention_start": 66, "mention_end": 90, "dataset_mention": "a newly released dataset"}, {"mentioned_in_paper": "962", "context_id": "165", "dataset_context": "To measure the sensitivity of MOSSE face filters to changes in scale, we first conducted a set of experiments on images from the FERET [31] face data set.", "mention_start": 124, "mention_end": 153, "dataset_mention": "the FERET [31] face data set"}, {"mentioned_in_paper": "962", "context_id": "166", "dataset_context": "As mentioned in the introduction, FERET is an easy data set.", "mention_start": 42, "mention_end": 59, "dataset_mention": "an easy data set"}, {"mentioned_in_paper": "962", "context_id": "186", "dataset_context": "In general, however, MOSSE filters localize faces over 90% of the time in easy images from the FERET data set when there are no differences in scale or orientation.", "mention_start": 90, "mention_end": 109, "dataset_mention": "the FERET data set"}, {"mentioned_in_paper": "962", "context_id": "227", "dataset_context": "One observation from this table is that most of the faces in the FERET data set have between 64 and 80 pixels between the eyes.", "mention_start": 61, "mention_end": 79, "dataset_mention": "the FERET data set"}, {"mentioned_in_paper": "962", "context_id": "234", "dataset_context": "As mentioned in the introduction, face detection in easy data sets like FERET is a solved problem.", "mention_start": 51, "mention_end": 66, "dataset_mention": "easy data sets"}, {"mentioned_in_paper": "962", "context_id": "241", "dataset_context": "The Point-and-Shoot Challenge (PaSC) data set was introduced for the handheld video face and person recognition competition at International Joint Conference on Biometrics (IJCB) in 2014 [16].", "mention_start": 0, "mention_end": 45, "dataset_mention": "The Point-and-Shoot Challenge (PaSC) data set"}, {"mentioned_in_paper": "962", "context_id": "248", "dataset_context": "The PaSC data set is, in fact, too difficult for the linear filters advocated here.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The PaSC data set"}, {"mentioned_in_paper": "962", "context_id": "249", "dataset_context": "As shown below, they are not able to reliably detect faces in the PaSC data set.", "mention_start": 61, "mention_end": 79, "dataset_mention": "the PaSC data set"}, {"mentioned_in_paper": "962", "context_id": "250", "dataset_context": "Unlike JANUS, however, the PaSC data set was collected at a small set of indoor and outdoor locations.", "mention_start": 22, "mention_end": 40, "dataset_mention": " the PaSC data set"}, {"mentioned_in_paper": "962", "context_id": "251", "dataset_context": "As a result, it is possible to create a set of smaller, restricted data sets that we will collectively call R-PaSC (Restricted-PaSC).", "mention_start": 55, "mention_end": 76, "dataset_mention": " restricted data sets"}, {"mentioned_in_paper": "962", "context_id": "257", "dataset_context": "To be specific, the unrestricted PaSC data set contains 2,802 videos of 265 people carrying out simple tasks.", "mention_start": 15, "mention_end": 46, "dataset_mention": " the unrestricted PaSC data set"}, {"mentioned_in_paper": "962", "context_id": "270", "dataset_context": "This was determined based on experiments with the FERET data set, so that confounding factors such as changes in illumination or expression would not alter the result.", "mention_start": 46, "mention_end": 64, "dataset_mention": "the FERET data set"}, {"mentioned_in_paper": "962", "context_id": "271", "dataset_context": "Real applications include these other factors, however, so we decided to confirm whether 3 to 5 filters per octave was still sufficient for this new, harder data set.", "mention_start": 149, "mention_end": 165, "dataset_mention": " harder data set"}, {"mentioned_in_paper": "962", "context_id": "275", "dataset_context": "The resampling was done on test images from the PaSC data set that have an interocular width greater than or equal to 64 pixels to avoid \"up sampling\".", "mention_start": 44, "mention_end": 61, "dataset_mention": "the PaSC data set"}, {"mentioned_in_paper": "962", "context_id": "302", "dataset_context": "In this section, we present a case for using a bank of filters approach in the spatial domain instead of the bank of filters approach in the Fourier domain because the former approach is more effective on the PaSC dataset than the latter one.", "mention_start": 204, "mention_end": 221, "dataset_mention": "the PaSC dataset"}, {"mentioned_in_paper": "962", "context_id": "318", "dataset_context": "Specifically, the testing of PaSC datasets was carried out by using OpenCV template matching [4].", "mention_start": 28, "mention_end": 42, "dataset_mention": "PaSC datasets"}, {"mentioned_in_paper": "962", "context_id": "321", "dataset_context": "For our experiments on the PaSC dataset, the templates to match were created by cropping out the faces from the trained filters.", "mention_start": 23, "mention_end": 39, "dataset_mention": "the PaSC dataset"}, {"mentioned_in_paper": "962", "context_id": "332", "dataset_context": "In this section we present our results on a customized data set, obtained from the PaSC data set to maintain a uniform background.", "mention_start": 42, "mention_end": 63, "dataset_mention": "a customized data set"}, {"mentioned_in_paper": "962", "context_id": "332", "dataset_context": "In this section we present our results on a customized data set, obtained from the PaSC data set to maintain a uniform background.", "mention_start": 78, "mention_end": 96, "dataset_mention": "the PaSC data set"}, {"mentioned_in_paper": "962", "context_id": "336", "dataset_context": "We have controlled for location and as a result created a customized dataset of video frames.", "mention_start": 56, "mention_end": 76, "dataset_mention": "a customized dataset"}, {"mentioned_in_paper": "962", "context_id": "386", "dataset_context": "To be precise it is 70.54% accuracy for our approach versus 87.31% accuracy using Viola and Jones face detector, on the still image data set.", "mention_start": 115, "mention_end": 140, "dataset_mention": "the still image data set"}, {"mentioned_in_paper": "962", "context_id": "394", "dataset_context": "It was applied to FERET, and PaSC image and video data sets.", "mention_start": 24, "mention_end": 59, "dataset_mention": " and PaSC image and video data sets"}, {"mentioned_in_paper": "963", "context_id": "61", "dataset_context": "3. We evaluate our method on adaptation for digits and objects datasets and demonstrate its effectiveness.", "mention_start": 44, "mention_end": 71, "dataset_mention": "digits and objects datasets"}, {"mentioned_in_paper": "963", "context_id": "169", "dataset_context": "We conduct experiments on Office [3], VisDA [28] and digits datasets.", "mention_start": 37, "mention_end": 68, "dataset_mention": " VisDA [28] and digits datasets"}, {"mentioned_in_paper": "963", "context_id": "171", "dataset_context": "In the experiments on both Office and VisDA dataset, we did not update the parameters of the networks.", "mention_start": 22, "mention_end": 51, "dataset_mention": "both Office and VisDA dataset"}, {"mentioned_in_paper": "963", "context_id": "197", "dataset_context": "The classes are also common in the Caltech dataset [8].", "mention_start": 31, "mention_end": 50, "dataset_mention": "the Caltech dataset"}, {"mentioned_in_paper": "963", "context_id": "228", "dataset_context": "Probability for Unknown Class In Fig. 5(c)(d), frequency diagram of the probability for an unknown class is shown in the adaptation from Webcam to DSLR dataset.", "mention_start": 146, "mention_end": 159, "dataset_mention": "DSLR dataset"}, {"mentioned_in_paper": "963", "context_id": "246", "dataset_context": "VisDA dataset [28] consists of 12 categories in total.", "mention_start": 0, "mention_end": 13, "dataset_mention": "VisDA dataset"}, {"mentioned_in_paper": "963", "context_id": "264", "dataset_context": "We also evaluate our method on digits dataset.", "mention_start": 31, "mention_end": 45, "dataset_mention": "digits dataset"}, {"mentioned_in_paper": "964", "context_id": "37", "dataset_context": "The KITTI dataset is a large, high quality dataset to improve visual-based localization methods for autonomous cars (22), and the top performing algorithms use variations of CNNs to process the visual information.", "mention_start": 0, "mention_end": 17, "dataset_mention": "The KITTI dataset"}, {"mentioned_in_paper": "964", "context_id": "37", "dataset_context": "The KITTI dataset is a large, high quality dataset to improve visual-based localization methods for autonomous cars (22), and the top performing algorithms use variations of CNNs to process the visual information.", "mention_start": 29, "mention_end": 50, "dataset_mention": " high quality dataset"}, {"mentioned_in_paper": "964", "context_id": "45", "dataset_context": "Despite the algorithms only inconsistently localizing on cadaver lung sequences, we demonstrate the proof of concept for our algorithms in a challenging, clinicallyrelevant dataset.", "mention_start": 153, "mention_end": 180, "dataset_mention": " clinicallyrelevant dataset"}, {"mentioned_in_paper": "964", "context_id": "54", "dataset_context": "Lung Phantom Dataset -13 test sequences that cover several generations in the left and right lung were chosen to evaluate the performance of BifurcationNet and AirwayNet.", "mention_start": 0, "mention_end": 20, "dataset_mention": "Lung Phantom Dataset"}, {"mentioned_in_paper": "964", "context_id": "67", "dataset_context": "In the lung phantom dataset and the cadaver lung dataset, AirwayNet outperforms BifurcationNet (p < 0.05).", "mention_start": 3, "mention_end": 27, "dataset_mention": "the lung phantom dataset"}, {"mentioned_in_paper": "964", "context_id": "67", "dataset_context": "In the lung phantom dataset and the cadaver lung dataset, AirwayNet outperforms BifurcationNet (p < 0.05).", "mention_start": 3, "mention_end": 56, "dataset_mention": "the lung phantom dataset and the cadaver lung dataset"}, {"mentioned_in_paper": "966", "context_id": "4", "dataset_context": "To overcome the hypothesized supervision challenge, a novel information converter unit is introduced, whose effectiveness has been extensively evaluated on SBD and Cityscapes datasets.", "mention_start": 155, "mention_end": 183, "dataset_mention": "SBD and Cityscapes datasets"}, {"mentioned_in_paper": "966", "context_id": "35", "dataset_context": "We extensively evaluate DDS on SBD (Hariharan et al. 2011) and Cityscapes (Cordts et al. 2016) datasets.", "mention_start": 31, "mention_end": 103, "dataset_mention": "SBD (Hariharan et al. 2011) and Cityscapes (Cordts et al. 2016) datasets"}, {"mentioned_in_paper": "966", "context_id": "57", "dataset_context": " Liu et al. (2019; 2017) introduced the first real-time edge detector, which achieves higher F-measure scores than average human annotators on the popular BSDS500 dataset (Arbel\u00e1ez et al. 2011).", "mention_start": 142, "mention_end": 170, "dataset_mention": "the popular BSDS500 dataset"}, {"mentioned_in_paper": "966", "context_id": "70", "dataset_context": "For example, it needs over 16 days to train CASENet on the SBD dataset (Hariharan et al. 2011), despite that we have used a powerful CPU (Intel Xeon(R) CPU E5-2683 v3 @ 2.00GHz \u00d7 56).", "mention_start": 54, "mention_end": 70, "dataset_mention": "the SBD dataset"}, {"mentioned_in_paper": "966", "context_id": "178", "dataset_context": "Due to the heavy computational load on the CPU, their approach was very time-consuming (over 16 days for SBD dataset (Hariharan et al. 2011) with 28 CPU kernels and an NVIDIA TITAN Xp GPU) to train a network.", "mention_start": 104, "mention_end": 116, "dataset_mention": "SBD dataset"}, {"mentioned_in_paper": "966", "context_id": "189", "dataset_context": "Specifically, from the second convolution block to the fourth, we use dilated convolutions with a dilation rate of 2; for the fifth block, we use a dilation rate of 4. We also follow CASENet to pre-train the convolution blocks on the COCO dataset (Lin et al. 2014).", "mention_start": 229, "mention_end": 246, "dataset_mention": "the COCO dataset"}, {"mentioned_in_paper": "966", "context_id": "202", "dataset_context": "We evaluate our method on the SBD (Hariharan et al. 2011) and Cityscapes (Cordts et al. 2016) datasets.", "mention_start": 26, "mention_end": 102, "dataset_mention": "the SBD (Hariharan et al. 2011) and Cityscapes (Cordts et al. 2016) datasets"}, {"mentioned_in_paper": "966", "context_id": "206", "dataset_context": "The Cityscapes dataset (Cordts et al. 2016) is a large-scale semantic segmentation dataset with stereo video sequences recorded in street scenarios from 50 different cities.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The Cityscapes dataset"}, {"mentioned_in_paper": "966", "context_id": "206", "dataset_context": "The Cityscapes dataset (Cordts et al. 2016) is a large-scale semantic segmentation dataset with stereo video sequences recorded in street scenarios from 50 different cities.", "mention_start": 47, "mention_end": 90, "dataset_mention": "a large-scale semantic segmentation dataset"}, {"mentioned_in_paper": "966", "context_id": "214", "dataset_context": "The maximum F-measure at the optimal dataset scale (ODS) for each class and mean maximum F-measure across all classes are reported.", "mention_start": 25, "mention_end": 44, "dataset_mention": "the optimal dataset"}, {"mentioned_in_paper": "966", "context_id": "222", "dataset_context": "We follow (Yu et al. 2018) to set the matching distance tolerance of 0.02 for the original SBD dataset (Hariharan et al. 2011), 0.0075 for the re-annotated SBD dataset (Yu et al. 2018), and 0.0035 for the Cityscapes dataset (Cordts et al. 2016).", "mention_start": 78, "mention_end": 102, "dataset_mention": "the original SBD dataset"}, {"mentioned_in_paper": "966", "context_id": "222", "dataset_context": "We follow (Yu et al. 2018) to set the matching distance tolerance of 0.02 for the original SBD dataset (Hariharan et al. 2011), 0.0075 for the re-annotated SBD dataset (Yu et al. 2018), and 0.0035 for the Cityscapes dataset (Cordts et al. 2016).", "mention_start": 138, "mention_end": 167, "dataset_mention": "the re-annotated SBD dataset"}, {"mentioned_in_paper": "966", "context_id": "222", "dataset_context": "We follow (Yu et al. 2018) to set the matching distance tolerance of 0.02 for the original SBD dataset (Hariharan et al. 2011), 0.0075 for the re-annotated SBD dataset (Yu et al. 2018), and 0.0035 for the Cityscapes dataset (Cordts et al. 2016).", "mention_start": 200, "mention_end": 223, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "966", "context_id": "223", "dataset_context": "The image borders of 5-pixels width are ignored for the SBD dataset, while not for the Cityscapes dataset.", "mention_start": 52, "mention_end": 67, "dataset_mention": "the SBD dataset"}, {"mentioned_in_paper": "966", "context_id": "223", "dataset_context": "The image borders of 5-pixels width are ignored for the SBD dataset, while not for the Cityscapes dataset.", "mention_start": 82, "mention_end": 105, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "966", "context_id": "226", "dataset_context": "We downsample the ground truths and predicted edge maps of Cityscapes dataset to half the original dimensions to speed up evaluation as in previous works (Acuna et al. 2019; Hu et al. 2019; Yu et al. 2017 Yu et al. , 2018)).", "mention_start": 59, "mention_end": 77, "dataset_mention": "Cityscapes dataset"}, {"mentioned_in_paper": "966", "context_id": "228", "dataset_context": "We first perform ablation studies on the SBD dataset (Yu et al. 2018) to investigate various aspects of the proposed DDS before comparing it with existing state-of-the-art methods.", "mention_start": 37, "mention_end": 52, "dataset_mention": "the SBD dataset"}, {"mentioned_in_paper": "966", "context_id": "233", "dataset_context": "-DSN, which directly applies the deeply supervised network architecture, in which each side of the backbone network is connected to a 1 \u00d7 1 conv layer with Table 3 Class-agnostic evaluation results on the SBD dataset (Hariharan et al. 2011).", "mention_start": 200, "mention_end": 216, "dataset_mention": "the SBD dataset"}, {"mentioned_in_paper": "966", "context_id": "241", "dataset_context": "All these variants are trained using the reweighted loss function Eq. ( 6) (except Softmax ) and the original SBD dataset for a fair comparison.", "mention_start": 37, "mention_end": 121, "dataset_mention": "the reweighted loss function Eq. ( 6) (except Softmax ) and the original SBD dataset"}, {"mentioned_in_paper": "966", "context_id": "242", "dataset_context": "We evaluate these variants and the original DDS and CASENet (Yu et al. 2017) on the SBD dataset using the original benchmark protocol in (Hariharan et al. 2011).", "mention_start": 80, "mention_end": 95, "dataset_mention": "the SBD dataset"}, {"mentioned_in_paper": "966", "context_id": "295", "dataset_context": "In this part, we compare DDS-R/DDS-U on the SBD dataset (Hariharan et al. 2011) with previous state-of-the- Results are summarized in Table 4. DDS-U achieves the state-of-the-art performance across all competitors.", "mention_start": 39, "mention_end": 55, "dataset_mention": "the SBD dataset"}, {"mentioned_in_paper": "966", "context_id": "319", "dataset_context": "The Cityscapes dataset (Cordts et al. 2016) is more challenging than SBD (Hariharan et al. 2011).", "mention_start": 0, "mention_end": 22, "dataset_mention": "The Cityscapes dataset"}, {"mentioned_in_paper": "966", "context_id": "339", "dataset_context": "The proposed DDS achieves state-of-the-art performance on the popular SBD (Hariharan et al. 2011) and Cityscapes (Cordts et al. 2016) datasets.", "mention_start": 58, "mention_end": 142, "dataset_mention": "the popular SBD (Hariharan et al. 2011) and Cityscapes (Cordts et al. 2016) datasets"}, {"mentioned_in_paper": "968", "context_id": "171", "dataset_context": "Overall, EGC obtains the best performance on 4 out of the 5 main datasets; on the remaining dataset (MolHIV), EGC is the second best architecture.", "mention_start": 53, "mention_end": 73, "dataset_mention": "the 5 main datasets"}, {"mentioned_in_paper": "968", "context_id": "187", "dataset_context": "Applying EGC to Large-Scale Heterogeneous Graphs We evaluated EGC on the heterogeneous OGB-MAG dataset, containing 2M nodes and 21M edges.", "mention_start": 69, "mention_end": 102, "dataset_mention": "the heterogeneous OGB-MAG dataset"}, {"mentioned_in_paper": "968", "context_id": "205", "dataset_context": "We believe that there are a variety of reasons, but the most important is that most real world datasets do not require the theoretical power these more expressive models provide to achieve good results.", "mention_start": 73, "mention_end": 103, "dataset_mention": "that most real world datasets"}, {"mentioned_in_paper": "968", "context_id": "206", "dataset_context": "In particular, many real world datasets are homophilous: therefore simplistic approaches, such as EGC, can achieve high performance.", "mention_start": 14, "mention_end": 39, "dataset_mention": " many real world datasets"}, {"mentioned_in_paper": "968", "context_id": "207", "dataset_context": "This implies that the community should consider adding more difficult datasets to standard benchmarks, such as those presented in Lim et al. (2021) and Veli\u010dkovi\u0107 et al. (2021).", "mention_start": 48, "mention_end": 78, "dataset_mention": "adding more difficult datasets"}, {"mentioned_in_paper": "968", "context_id": "286", "dataset_context": "Our benchmarks were conducted on a batch of 10k graphs from the ZINC and Code datasets, Arxiv, and the popular Reddit dataset (Hamilton et al., 2017), which is one of the largest graph datasets commonly evaluated on in the GNN literature.", "mention_start": 60, "mention_end": 86, "dataset_mention": "the ZINC and Code datasets"}, {"mentioned_in_paper": "968", "context_id": "286", "dataset_context": "Our benchmarks were conducted on a batch of 10k graphs from the ZINC and Code datasets, Arxiv, and the popular Reddit dataset (Hamilton et al., 2017), which is one of the largest graph datasets commonly evaluated on in the GNN literature.", "mention_start": 94, "mention_end": 125, "dataset_mention": " and the popular Reddit dataset"}, {"mentioned_in_paper": "969", "context_id": "9", "dataset_context": "Most importantly, InstanceFormer surpasses offline approaches for challenging and long datasets such as YouTube-VIS-2021 and OVIS.", "mention_start": 77, "mention_end": 95, "dataset_mention": "and long datasets"}, {"mentioned_in_paper": "969", "context_id": "140", "dataset_context": "The recently proposed OVIS data set is more complex than the YTVIS data sets as it primarily contains long video sequences with high percentages of occlusion and a large number of objects For YTVIS-22, we report the same set of metrics solely for long videos (e.g., AP L and AR L ) as proposed in the official challenge.", "mention_start": 13, "mention_end": 35, "dataset_mention": "proposed OVIS data set"}, {"mentioned_in_paper": "969", "context_id": "140", "dataset_context": "The recently proposed OVIS data set is more complex than the YTVIS data sets as it primarily contains long video sequences with high percentages of occlusion and a large number of objects For YTVIS-22, we report the same set of metrics solely for long videos (e.g., AP L and AR L ) as proposed in the official challenge.", "mention_start": 57, "mention_end": 76, "dataset_mention": "the YTVIS data sets"}, {"mentioned_in_paper": "969", "context_id": "181", "dataset_context": "All experiments are conducted on the OVIS dataset with a ResNet-50 backbone.", "mention_start": 33, "mention_end": 49, "dataset_mention": "the OVIS dataset"}, {"mentioned_in_paper": "969", "context_id": "195", "dataset_context": "We took one of the challenging videos from OVIS (please refer to the 3rd video of OVIS from Fig. 8 Table 5. Ablation on the number of memory tokens and length of temporal memory frame in the OVIS dataset.", "mention_start": 187, "mention_end": 203, "dataset_mention": "the OVIS dataset"}, {"mentioned_in_paper": "969", "context_id": "204", "dataset_context": "Our proposed approach not only outperforms all existing online methods but also establishes a new state-ofthe-art performance on challenging YTVIS-21 and OVIS datasets by outperforming existing offline and online methods.", "mention_start": 141, "mention_end": 167, "dataset_mention": "YTVIS-21 and OVIS datasets"}, {"mentioned_in_paper": "970", "context_id": "25", "dataset_context": "For example, consider the query consisting of just two keywords, \"Carter\" and \"Depp\" on the IMDB dataset.", "mention_start": 87, "mention_end": 104, "dataset_mention": "the IMDB dataset"}, {"mentioned_in_paper": "971", "context_id": "73", "dataset_context": "In future works we plan to investigate further into this topic, extending our data set, researching for other factors, such as time, scoring, revenue, and others.", "mention_start": 63, "mention_end": 86, "dataset_mention": " extending our data set"}, {"mentioned_in_paper": "972", "context_id": "8", "dataset_context": "We apply our method to the famous M3D-RPN detector and CaDDN detector, conducting extensive experiments on KITTI and Waymo Open datasets.", "mention_start": 106, "mention_end": 136, "dataset_mention": "KITTI and Waymo Open datasets"}, {"mentioned_in_paper": "972", "context_id": "38", "dataset_context": "Extensive experiments are conduct on KITTI, the currently most widely used benchmark, and Waymo Open dataset, the currently most large scale dataset.", "mention_start": 85, "mention_end": 108, "dataset_mention": " and Waymo Open dataset"}, {"mentioned_in_paper": "972", "context_id": "38", "dataset_context": "Extensive experiments are conduct on KITTI, the currently most widely used benchmark, and Waymo Open dataset, the currently most large scale dataset.", "mention_start": 109, "mention_end": 148, "dataset_mention": " the currently most large scale dataset"}, {"mentioned_in_paper": "972", "context_id": "44", "dataset_context": "\u2022 We conduct extensive experiments on the most widely used KITTI benchmark and the most large scale Waymo Open dataset to verify the effectiveness of our method.", "mention_start": 59, "mention_end": 118, "dataset_mention": "KITTI benchmark and the most large scale Waymo Open dataset"}, {"mentioned_in_paper": "972", "context_id": "132", "dataset_context": "To verify the effectiveness of our methods, we conduct experiments on the KITTI dataset [12] and Waymo Open dataset [43].", "mention_start": 69, "mention_end": 87, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "972", "context_id": "132", "dataset_context": "To verify the effectiveness of our methods, we conduct experiments on the KITTI dataset [12] and Waymo Open dataset [43].", "mention_start": 69, "mention_end": 115, "dataset_mention": "the KITTI dataset [12] and Waymo Open dataset"}, {"mentioned_in_paper": "972", "context_id": "133", "dataset_context": "KITTI dataset is one of the most widely used 3D detection datasets.", "mention_start": 0, "mention_end": 13, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "972", "context_id": "137", "dataset_context": "Following previous methods, we only consider the \"Car\" category in KITTI dataset.", "mention_start": 66, "mention_end": 80, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "972", "context_id": "138", "dataset_context": "Waymo Open dataset is the recently released large-scale autonomous driving 3D detection dataset, which consists of 798 training sequences, 202 validation sequences and 150 test sequences [43].", "mention_start": 0, "mention_end": 18, "dataset_mention": "Waymo Open dataset"}, {"mentioned_in_paper": "972", "context_id": "138", "dataset_context": "Waymo Open dataset is the recently released large-scale autonomous driving 3D detection dataset, which consists of 798 training sequences, 202 validation sequences and 150 test sequences [43].", "mention_start": 35, "mention_end": 95, "dataset_mention": "released large-scale autonomous driving 3D detection dataset"}, {"mentioned_in_paper": "972", "context_id": "142", "dataset_context": "We use M3D-RPN [1] to conduct experiments on both Waymo Open dataset [43] and KITTI dataset [12] and use CaDDN [36] to conduct experiments on KITTI dataset.", "mention_start": 45, "mention_end": 68, "dataset_mention": "both Waymo Open dataset"}, {"mentioned_in_paper": "972", "context_id": "142", "dataset_context": "We use M3D-RPN [1] to conduct experiments on both Waymo Open dataset [43] and KITTI dataset [12] and use CaDDN [36] to conduct experiments on KITTI dataset.", "mention_start": 45, "mention_end": 91, "dataset_mention": "both Waymo Open dataset [43] and KITTI dataset"}, {"mentioned_in_paper": "972", "context_id": "142", "dataset_context": "We use M3D-RPN [1] to conduct experiments on both Waymo Open dataset [43] and KITTI dataset [12] and use CaDDN [36] to conduct experiments on KITTI dataset.", "mention_start": 142, "mention_end": 155, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "972", "context_id": "144", "dataset_context": "For training CaDDN on the KITTI dataset, we adopt the Adam optimizer with a batch size 2. The learning rate is 0.0002.", "mention_start": 22, "mention_end": 39, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "972", "context_id": "159", "dataset_context": "Finally, Tab. 1 demonstrates that our method achieves performance close to that of state-of-the-art methods on the KITTI dataset.", "mention_start": 110, "mention_end": 128, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "972", "context_id": "160", "dataset_context": "Tab. 2 shows the performance of MonoSIM on Waymo Open dataset.", "mention_start": 43, "mention_end": 61, "dataset_mention": "Waymo Open dataset"}, {"mentioned_in_paper": "972", "context_id": "202", "dataset_context": "Experiments on KITTI dataset and Waymo Open dataset have demonstrated the effectiveness of our method.", "mention_start": 15, "mention_end": 28, "dataset_mention": "KITTI dataset"}, {"mentioned_in_paper": "972", "context_id": "202", "dataset_context": "Experiments on KITTI dataset and Waymo Open dataset have demonstrated the effectiveness of our method.", "mention_start": 15, "mention_end": 51, "dataset_mention": "KITTI dataset and Waymo Open dataset"}, {"mentioned_in_paper": "972", "context_id": "207", "dataset_context": "On the KITTI dataset, P ps \u2208 R 640\u00d72048 .", "mention_start": 3, "mention_end": 20, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "972", "context_id": "208", "dataset_context": "On the Waymo Open dataset, P ps \u2208 R 544\u00d74096 .", "mention_start": 3, "mention_end": 25, "dataset_mention": "the Waymo Open dataset"}, {"mentioned_in_paper": "972", "context_id": "211", "dataset_context": "On both of KITTI and Waymo Open datasets, P pr \u2208 R 256\u00d721600 .", "mention_start": 11, "mention_end": 40, "dataset_mention": "KITTI and Waymo Open datasets"}, {"mentioned_in_paper": "972", "context_id": "213", "dataset_context": "We use CaDDN to conduct experiments on the KITTI dataset.", "mention_start": 39, "mention_end": 56, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "972", "context_id": "234", "dataset_context": "On the KITTI dataset, F ms glo \u2208 R 512\u00d732\u00d7110 , F mr glo \u2208 R 512\u00d732\u00d7110 , F ms loc \u2208 R 512\u00d732\u00d7110 and F mr loc \u2208 R 512\u00d732\u00d7110 .", "mention_start": 3, "mention_end": 20, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "972", "context_id": "235", "dataset_context": "On the Waymo Open dataset, F ms glo \u2208 R 512\u00d732\u00d748 , F mr glo \u2208 R 512\u00d732\u00d748 , F ms loc \u2208 R 512\u00d732\u00d748 and F mr loc \u2208 R 512\u00d732\u00d748 .", "mention_start": 3, "mention_end": 25, "dataset_mention": "the Waymo Open dataset"}, {"mentioned_in_paper": "972", "context_id": "252", "dataset_context": "On the KITTI dataset, Average Precision (AP | R ) is adopted as the metric for 3D detection which can be expressed as:", "mention_start": 3, "mention_end": 20, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "974", "context_id": "38", "dataset_context": "Section 3 explains the dataset and definition of engagement.", "mention_start": 0, "mention_end": 30, "dataset_mention": "Section 3 explains the dataset"}, {"mentioned_in_paper": "974", "context_id": "98", "dataset_context": "From the available data, a balanced training set of 60,000 images (30,000 from each category) and a balanced testing set of 20,000 images (10,000 from each category) was sampled from the main dataset.", "mention_start": 183, "mention_end": 199, "dataset_mention": "the main dataset"}, {"mentioned_in_paper": "975", "context_id": "142", "dataset_context": "As shown previously, the mainland China patent dataset already had almost 36 million patents.", "mention_start": 20, "mention_end": 54, "dataset_mention": " the mainland China patent dataset"}, {"mentioned_in_paper": "976", "context_id": "10", "dataset_context": "Extensive experiments are conducted on Office and VisDA datasets for both open-set and closed-set domain adaptation, and superior results are reported when comparing to the state-of-the-art approaches.", "mention_start": 39, "mention_end": 64, "dataset_mention": "Office and VisDA datasets"}, {"mentioned_in_paper": "976", "context_id": "135", "dataset_context": "We empirically verify the merit of our SE-CC by conducting experiments on Office [27] and VisDA [23] datasets for both open-set and closed-set domain adaptation.", "mention_start": 74, "mention_end": 109, "dataset_mention": "Office [27] and VisDA [23] datasets"}, {"mentioned_in_paper": "976", "context_id": "178", "dataset_context": "Tables 4 and 3 show the performance comparisons on Office and VisDA datasets for closed-set domain adaptation.", "mention_start": 51, "mention_end": 76, "dataset_mention": "Office and VisDA datasets"}, {"mentioned_in_paper": "977", "context_id": "32", "dataset_context": "To demonstrate the interface functionality, we implemented a representative TDA pipeline analyzing the famous MNIST data set, a collection of 1000 handwritten digits (100 images per digit).", "mention_start": 43, "mention_end": 124, "dataset_mention": " we implemented a representative TDA pipeline analyzing the famous MNIST data set"}, {"mentioned_in_paper": "977", "context_id": "45", "dataset_context": "The idea of the third stage is to interpret the input dataset as a higher dimensional point cloud.", "mention_start": 34, "mention_end": 61, "dataset_mention": "interpret the input dataset"}, {"mentioned_in_paper": "977", "context_id": "82", "dataset_context": "As an example, Figure 4 illustrates a possible use case specific to the MNIST dataset.", "mention_start": 67, "mention_end": 85, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "977", "context_id": "84", "dataset_context": "Figure 4 : While the digit 8 highlighted in blue belongs to the correct branch where most digits 8 are, a few digits in the MNIST dataset are being classified as belonging to the wrong branch of the Isomap embedding.", "mention_start": 119, "mention_end": 137, "dataset_mention": "the MNIST dataset"}, {"mentioned_in_paper": "978", "context_id": "10", "dataset_context": "We conduct extensive experiments using three active authentication benchmark datasets (MOBIO, UMDAA-01, UMDAA-02) and show that such approach performs better than stateof-the-art one-class based FAA methods and is also able to outperform traditional FL/SL methods.", "mention_start": 39, "mention_end": 85, "dataset_mention": "three active authentication benchmark datasets"}, {"mentioned_in_paper": "978", "context_id": "43", "dataset_context": "In the proposed method, we first train the full model on an exiting publicly available face recognition dataset.", "mention_start": 56, "mention_end": 111, "dataset_mention": "an exiting publicly available face recognition dataset"}, {"mentioned_in_paper": "978", "context_id": "49", "dataset_context": "This process is illustrated in Fig. 2. We evaluate the proposed method on three active authentication datasets -MOBIO [7], UMDAA-01 [4] and UMDAA-02 [11].", "mention_start": 74, "mention_end": 110, "dataset_mention": "three active authentication datasets"}, {"mentioned_in_paper": "978", "context_id": "95", "dataset_context": "Let K i be the number of users contained in the i th device dataset having sufficient number of data samples.", "mention_start": 44, "mention_end": 67, "dataset_mention": "the i th device dataset"}, {"mentioned_in_paper": "978", "context_id": "102", "dataset_context": "To show how the performance of FedAvg algorithm changes when the IID assumption is violated, we perform identification experiments using the UMDAA-01 dataset by changing the qIID value from one to zero.", "mention_start": 136, "mention_end": 157, "dataset_mention": "the UMDAA-01 dataset"}, {"mentioned_in_paper": "978", "context_id": "148", "dataset_context": "For all experiments, we utilize the VGG16 [23] trained on the VGGFace dataset [15].", "mention_start": 57, "mention_end": 77, "dataset_mention": "the VGGFace dataset"}, {"mentioned_in_paper": "978", "context_id": "154", "dataset_context": "The MOBIO [7] dataset contains face images and voice data from 150 individuals collected in six different sessions at six different locations.", "mention_start": 0, "mention_end": 21, "dataset_mention": "The MOBIO [7] dataset"}, {"mentioned_in_paper": "978", "context_id": "167", "dataset_context": "The UMDAA-02 dataset [11] contains information from 18 different sensors such as keystrokes, touch pattern, face images, accelerometer readings from 44 individuals collected using Nexus5 across two months.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The UMDAA-02 dataset"}, {"mentioned_in_paper": "978", "context_id": "189", "dataset_context": "In com-  parison, the proposed approach is able to achieve 99.8% average detection accuracy, resulting in nearly 38% and 7% improvement on the MOBIO dataset, respectively.", "mention_start": 138, "mention_end": 156, "dataset_mention": "the MOBIO dataset"}, {"mentioned_in_paper": "978", "context_id": "190", "dataset_context": "For the slightly challenging UMDAA-01 dataset, when the authentication model is trained using FedAvg and SLA, the model achieves the performance of 52.4% and 89%, respectively.", "mention_start": 29, "mention_end": 45, "dataset_mention": "UMDAA-01 dataset"}, {"mentioned_in_paper": "978", "context_id": "192", "dataset_context": "Similarly, FedAvg and SLA perform about 51% and 62%, respectively on the most challenging UMDAA-02 dataset.", "mention_start": 89, "mention_end": 106, "dataset_mention": "UMDAA-02 dataset"}, {"mentioned_in_paper": "978", "context_id": "199", "dataset_context": "FedAvg Split Learning Proposed Figure 7. Comparing the performance between FedAvg, Split Learning [8] and the proposed method on the MOBIO, UMDAA-01 and UMDAA-02 datasets.", "mention_start": 139, "mention_end": 170, "dataset_mention": " UMDAA-01 and UMDAA-02 datasets"}, {"mentioned_in_paper": "979", "context_id": "54", "dataset_context": "For interactive applications, our solution achieves 10 FPS with 91% top-1 accuracy on Speech Commands dataset.", "mention_start": 85, "mention_end": 109, "dataset_mention": "Speech Commands dataset"}, {"mentioned_in_paper": "979", "context_id": "156", "dataset_context": "VWW and Speech Commands represent popular microcontroller use-cases: VWW is a vision based dataset identifying whether a person is present in the image or not; Speech Commands is an audio dataset for keyword spotting (e.g., \"Hey Siri\"), requiring to classify a spoken word from a vocabulary of size 35.", "mention_start": 75, "mention_end": 98, "dataset_mention": "a vision based dataset"}, {"mentioned_in_paper": "979", "context_id": "156", "dataset_context": "VWW and Speech Commands represent popular microcontroller use-cases: VWW is a vision based dataset identifying whether a person is present in the image or not; Speech Commands is an audio dataset for keyword spotting (e.g., \"Hey Siri\"), requiring to classify a spoken word from a vocabulary of size 35.", "mention_start": 178, "mention_end": 195, "dataset_mention": "an audio dataset"}, {"mentioned_in_paper": "979", "context_id": "194", "dataset_context": "We benchmarked the performance on two wake words datasets: Visual Wake Words [12] (VWW) and Google Speech Commands (denoted as GSC) to compare the accuracy-latency and accuracy-peak memory trade-off.", "mention_start": 34, "mention_end": 57, "dataset_mention": "two wake words datasets"}, {"mentioned_in_paper": "979", "context_id": "197", "dataset_context": "On VWW dataset, we can achieve higher accuracy at 2.4-3.4\u00d7", "mention_start": 3, "mention_end": 14, "dataset_mention": "VWW dataset"}, {"mentioned_in_paper": "979", "context_id": "204", "dataset_context": "On the Speech Commands dataset, MCUNet achieves a higher accuracy at 2.8\u00d7 faster inference speed and 4.1\u00d7 smaller peak memory.", "mention_start": 3, "mention_end": 30, "dataset_mention": "the Speech Commands dataset"}, {"mentioned_in_paper": "979", "context_id": "211", "dataset_context": "We benchmark the object detection performance of our MCUNet and scaled MobileNetV2+CMSIS-NN on on Pascal VOC [15] dataset.", "mention_start": 98, "mention_end": 121, "dataset_mention": "Pascal VOC [15] dataset"}, {"mentioned_in_paper": "979", "context_id": "262", "dataset_context": "We release a demo video of MCUNet running the visual wake words dataset [12] in this link.", "mention_start": 27, "mention_end": 71, "dataset_mention": "MCUNet running the visual wake words dataset"}, {"mentioned_in_paper": "979", "context_id": "268", "dataset_context": "SmallCifar is a small network for CIFAR [27] dataset used in the MicroTVM/\u00b5TVM post \u2021 .", "mention_start": 34, "mention_end": 52, "dataset_mention": "CIFAR [27] dataset"}, {"mentioned_in_paper": "980", "context_id": "0", "dataset_context": "Dimensionality reduction methods are an essential tool for multidimensional data analysis, and many interesting processes can be studied as time-dependent multivariate datasets.", "mention_start": 139, "mention_end": 176, "dataset_mention": "time-dependent multivariate datasets"}, {"mentioned_in_paper": "980", "context_id": "24", "dataset_context": "Finally, our creation of an open benchmark for assessing dynamic projections (containing datasets, techniques, metrics, visualizations, and associated workflows) should benefit both user types by providing a basis via which such techniques can be transparently compared.", "mention_start": 46, "mention_end": 97, "dataset_mention": "assessing dynamic projections (containing datasets"}, {"mentioned_in_paper": "980", "context_id": "35", "dataset_context": "A dynamic dataset D is a list of T revisions D = R t , 1 \u2264 t \u2264 T .", "mention_start": 0, "mention_end": 17, "dataset_mention": "A dynamic dataset"}, {"mentioned_in_paper": "980", "context_id": "62", "dataset_context": "However, time-dependent datasets were not considered.", "mention_start": 8, "mention_end": 32, "dataset_mention": " time-dependent datasets"}, {"mentioned_in_paper": "980", "context_id": "83", "dataset_context": "Thirdly, we evaluate both spatial quality and stability metrics on all combinations of techniques and datasets; in this step, we also propose novel metrics to gauge stability.", "mention_start": 86, "mention_end": 110, "dataset_mention": "techniques and datasets"}, {"mentioned_in_paper": "980", "context_id": "113", "dataset_context": "We followed here a similar approach, i.e. collecting a set of 10 highdimensional and dynamic datasets that exhibit significant variations in terms of provenance, number of samples N, number of timesteps T , dimensionality n, intrinsic dimensionality \u03c1n (percentage of n dimensions that explain 95% of the data variance), and sparsity ratio \u03c3n (percentage of zeros in the data).", "mention_start": 61, "mention_end": 101, "dataset_mention": "10 highdimensional and dynamic datasets"}, {"mentioned_in_paper": "980", "context_id": "159", "dataset_context": "3.3 on all (dataset, method) pairs formed by the selected 9 DR methods and 10 datasets, and analyze next the results.", "mention_start": 7, "mention_end": 19, "dataset_mention": "all (dataset"}, {"mentioned_in_paper": "980", "context_id": "159", "dataset_context": "3.3 on all (dataset, method) pairs formed by the selected 9 DR methods and 10 datasets, and analyze next the results.", "mention_start": 57, "mention_end": 86, "dataset_mention": "9 DR methods and 10 datasets"}, {"mentioned_in_paper": "980", "context_id": "200", "dataset_context": "In contrast to the good results qualitatively observed on the single gaussians dataset showed in [RFT16], dt-SNE performs less well in both spatial quality and stability for several other of the considered datasets, being quality-wise somewhere between TF-t-SNE and G-t-SNE for all considered metrics.", "mention_start": 58, "mention_end": 86, "dataset_mention": "the single gaussians dataset"}, {"mentioned_in_paper": "980", "context_id": "204", "dataset_context": "To study how dataset characteristics influence quality, we compute the correlation of the distance-preservation, neighborhood, and temporal stability metrics (measured over all techniques) with the six traits that we used to characterize our datasets (Tab.", "mention_start": 9, "mention_end": 20, "dataset_mention": "how dataset"}, {"mentioned_in_paper": "980", "context_id": "204", "dataset_context": "To study how dataset characteristics influence quality, we compute the correlation of the distance-preservation, neighborhood, and temporal stability metrics (measured over all techniques) with the six traits that we used to characterize our datasets (Tab.", "mention_start": 224, "mention_end": 250, "dataset_mention": "characterize our datasets"}, {"mentioned_in_paper": "980", "context_id": "212", "dataset_context": "While Fig. 2 shows all computed metrics for each (dataset, method) combination, metric values are still aggregated to a single scalar per combination.", "mention_start": 44, "mention_end": 57, "dataset_mention": "each (dataset"}, {"mentioned_in_paper": "980", "context_id": "244", "dataset_context": "The coarse-grained and fine-grained analyses presented so far highlighted that there are significant differences in the behavior of dynamic DR methods that depend on both the method and the dataset.", "mention_start": 166, "mention_end": 197, "dataset_mention": "both the method and the dataset"}, {"mentioned_in_paper": "980", "context_id": "256", "dataset_context": "From timesteps 1 to 2 of the TF-t-SNE run of the fashion dataset, even though the local structure remains the same, the absolute position of the points and clusters changes drastically.", "mention_start": 45, "mention_end": 64, "dataset_mention": "the fashion dataset"}, {"mentioned_in_paper": "980", "context_id": "263", "dataset_context": "Specifically, if there is a change in rank of the top two eigenvectors from timestep t to the next one, i.e., one of the associated eigenvalues becomes larger than the other, the projection exhibits an artifact that resembles a reflection -see the quickdraw dataset in the two timesteps in Fig. 4b,c.", "mention_start": 174, "mention_end": 265, "dataset_mention": " the projection exhibits an artifact that resembles a reflection -see the quickdraw dataset"}, {"mentioned_in_paper": "980", "context_id": "276", "dataset_context": "This is arguably important for practitioners interested in choosing a technique in a given context (dataset type and metrics to maximize).", "mention_start": 91, "mention_end": 107, "dataset_mention": "context (dataset"}, {"mentioned_in_paper": "980", "context_id": "281", "dataset_context": "Image (a) shows the techniques and datasets, coded by glyph, respectively categorical colors.", "mention_start": 16, "mention_end": 43, "dataset_mention": "the techniques and datasets"}, {"mentioned_in_paper": "980", "context_id": "283", "dataset_context": "For instance, we see the sorts dataset well-separated as the purple cluster bottom-left in Fig. 5a colored by stability, distance preservation, and neighborhood preservation, respectively.", "mention_start": 20, "mention_end": 38, "dataset_mention": "the sorts dataset"}, {"mentioned_in_paper": "980", "context_id": "289", "dataset_context": "The plots in Fig. 5 can guide choosing a DR technique to project dynamic data: Given a dataset D to project, (1) find the most similar dataset D in the benchmark, i.e., that contains data of similar nature (e.g., natural images, sounds) and is obtained via a similar acquisition process; (2) decide what is important for the dynamic projection of D -stability, distance preservation, neighborhood preservation, or a mix of them; (3) find the projection techniques P in the respective quality plots that have the desired qualities on D , and possibly also consider other projection techniques that behave similarly (close points in the plots).", "mention_start": 108, "mention_end": 142, "dataset_mention": " (1) find the most similar dataset"}, {"mentioned_in_paper": "981", "context_id": "5", "dataset_context": "We argue that it is effectiveness because of the extra information that we can consider together with labeled data to curate a less biased dataset.", "mention_start": 125, "mention_end": 146, "dataset_mention": "a less biased dataset"}, {"mentioned_in_paper": "981", "context_id": "54", "dataset_context": "Therefore, we can use this information of crowd workers bias in conjunction with the crowd worker responses to collect fairer labels and achieve a better dataset.", "mention_start": 118, "mention_end": 161, "dataset_mention": "fairer labels and achieve a better dataset"}, {"mentioned_in_paper": "982", "context_id": "114", "dataset_context": "We built the YOLOv3 model based on Keras (Gulli, Pal, 2017) to train the above data set.", "mention_start": 68, "mention_end": 87, "dataset_mention": "the above data set"}, {"mentioned_in_paper": "982", "context_id": "120", "dataset_context": "Both methods are trained on the CMP dataset.", "mention_start": 28, "mention_end": 43, "dataset_mention": "the CMP dataset"}, {"mentioned_in_paper": "983", "context_id": "22", "dataset_context": "This will be accomplished by releasing high quality labeled datasets, and running targeted public challenges to encourage the development of algorithms designed to solve increasingly complex geospatial problems.", "mention_start": 29, "mention_end": 68, "dataset_mention": "releasing high quality labeled datasets"}, {"mentioned_in_paper": "983", "context_id": "25", "dataset_context": "Existing publicly available labeled overhead or satellite imagery datasets tend to be relatively small, or labeled with lower fidelity than desired for foundational mapping.", "mention_start": 48, "mention_end": 74, "dataset_mention": "satellite imagery datasets"}, {"mentioned_in_paper": "983", "context_id": "26", "dataset_context": "For example, the ISPRS semantic labeling benchmark [7] dataset contains high quality 2D semantic labels over two cities in Germany and covers a compact area of 4.8 km 2 ; imagery is obtained via an aerial platform and is 3 or 4 channel and 5-10cm in resolution.", "mention_start": 12, "mention_end": 62, "dataset_mention": " the ISPRS semantic labeling benchmark [7] dataset"}, {"mentioned_in_paper": "983", "context_id": "27", "dataset_context": "The TorontoCity Dataset [8] contains high resolution 5-10cm aerial 4-channel imagery, and \u223c 700 km 2 of coverage; building and roads are labeled at high fidelity (among other items), but the data has yet to be publicly released.", "mention_start": 0, "mention_end": 23, "dataset_mention": "The TorontoCity Dataset"}, {"mentioned_in_paper": "983", "context_id": "28", "dataset_context": "The Massachusetts Roads Dataset [9] contains 3-channel imagery at 1 meter resolution, and 2600 km 2 of coverage; the imagery and labels are publicly available, though labels are scraped from OpenStreetMap and not independently collected or validated.", "mention_start": 0, "mention_end": 31, "dataset_mention": "The Massachusetts Roads Dataset"}, {"mentioned_in_paper": "983", "context_id": "29", "dataset_context": "Another useful overhead dataset (though not so relevant to foundational mapping) is the COWC dataset [10] of cars, with 15cm aerial imagery collected over six different geographic regions; labels consist of a point at the centroid of each car.", "mention_start": 0, "mention_end": 31, "dataset_mention": "Another useful overhead dataset"}, {"mentioned_in_paper": "983", "context_id": "29", "dataset_context": "Another useful overhead dataset (though not so relevant to foundational mapping) is the COWC dataset [10] of cars, with 15cm aerial imagery collected over six different geographic regions; labels consist of a point at the centroid of each car.", "mention_start": 84, "mention_end": 100, "dataset_mention": "the COWC dataset"}, {"mentioned_in_paper": "983", "context_id": "30", "dataset_context": "A recently released satellite imagery dataset is xVIEW [11], which contains 1400 km 2 of 30 cm satellite imagery; labels, however, consist of bounding boxes that are not ideal for foundational mapping of buildings or roads.", "mention_start": 11, "mention_end": 45, "dataset_mention": "released satellite imagery dataset"}, {"mentioned_in_paper": "983", "context_id": "34", "dataset_context": "The IARPA Functional Map of the World [13] challenge and dataset aimed to classify region proposals based on building type.", "mention_start": 28, "mention_end": 64, "dataset_mention": "the World [13] challenge and dataset"}, {"mentioned_in_paper": "983", "context_id": "35", "dataset_context": "The DIUx xView Detection Challenge [14] subsequently released a large dataset of 60 bounding box object classes and approximately 1 million instances.", "mention_start": 0, "mention_end": 77, "dataset_mention": "The DIUx xView Detection Challenge [14] subsequently released a large dataset"}, {"mentioned_in_paper": "983", "context_id": "65", "dataset_context": "In addition to the data detailed above, SpaceNet also hosts data from previous competitions such as the IARPA Functional Map of the World Competition [13] and the Urban 3D Challenge Dataset [18].", "mention_start": 127, "mention_end": 189, "dataset_mention": "the World Competition [13] and the Urban 3D Challenge Dataset"}, {"mentioned_in_paper": "983", "context_id": "157", "dataset_context": "The SpaceNet dataset provides a large corpus of high resolution multi-band imagery, with attendant validated building footprint and road network labels.", "mention_start": 0, "mention_end": 20, "dataset_mention": "The SpaceNet dataset"}, {"mentioned_in_paper": "984", "context_id": "7", "dataset_context": "Experimental evaluations on two ancient Chinese architecture datasets demonstrate the effectiveness of our proposed complete scene reconstruction pipeline.", "mention_start": 28, "mention_end": 69, "dataset_mention": "two ancient Chinese architecture datasets"}, {"mentioned_in_paper": "984", "context_id": "225", "dataset_context": "We perform experiments on two ancient Chinese architecture datasets, Nan-chan Temple (NCT) and Fo-guang Temple (FGT).", "mention_start": 26, "mention_end": 67, "dataset_mention": "two ancient Chinese architecture datasets"}, {"mentioned_in_paper": "984", "context_id": "308", "dataset_context": "As a result, for our NCT and FGT datasets, ICP would not achieve a highly accurate registration of the laser scans to help to significantly improve the image and laser scan merging accuracy.", "mention_start": 16, "mention_end": 41, "dataset_mention": "our NCT and FGT datasets"}, {"mentioned_in_paper": "984", "context_id": "339", "dataset_context": "Experimental results on our two ancient Chinese architecture datasets demonstrate the effectiveness of each main step of our proposed pipeline.", "mention_start": 24, "mention_end": 69, "dataset_mention": "our two ancient Chinese architecture datasets"}, {"mentioned_in_paper": "986", "context_id": "111", "dataset_context": "Synthetic Datasets Our synthetic datasets are generated from the ModelNet40 dataset [44] and the Axyz-pose human dataset [1].", "mention_start": 61, "mention_end": 83, "dataset_mention": "the ModelNet40 dataset"}, {"mentioned_in_paper": "986", "context_id": "111", "dataset_context": "Synthetic Datasets Our synthetic datasets are generated from the ModelNet40 dataset [44] and the Axyz-pose human dataset [1].", "mention_start": 61, "mention_end": 120, "dataset_mention": "the ModelNet40 dataset [44] and the Axyz-pose human dataset"}, {"mentioned_in_paper": "986", "context_id": "112", "dataset_context": "The ModelNet40 dataset contains CAD models from 40 artificial object categories.", "mention_start": 0, "mention_end": 22, "dataset_mention": "The ModelNet40 dataset"}, {"mentioned_in_paper": "986", "context_id": "113", "dataset_context": "We select 625 cases from the Airplane category to construct an Airline dataset, randomly sampling 500 cases for training and using the remaining 125 for testing.", "mention_start": 60, "mention_end": 78, "dataset_mention": "an Airline dataset"}, {"mentioned_in_paper": "986", "context_id": "114", "dataset_context": "The Axyz-pose human dataset contains 110 clothed human mesh models.", "mention_start": 0, "mention_end": 27, "dataset_mention": "The Axyz-pose human dataset"}, {"mentioned_in_paper": "986", "context_id": "115", "dataset_context": "We randomly choose 110 models to construct a Human dataset, using 100 models for training set and the remaining 10 for testing.", "mention_start": 43, "mention_end": 58, "dataset_mention": "a Human dataset"}, {"mentioned_in_paper": "986", "context_id": "119", "dataset_context": "Specifically, we choose a certain axis and rotate an imaginary camera around it to derive N camera locations with rotation angles at regular intervals (we set N to 50 for the Human dataset and 18 for the Airplane dataset).", "mention_start": 170, "mention_end": 188, "dataset_mention": "the Human dataset"}, {"mentioned_in_paper": "986", "context_id": "119", "dataset_context": "Specifically, we choose a certain axis and rotate an imaginary camera around it to derive N camera locations with rotation angles at regular intervals (we set N to 50 for the Human dataset and 18 for the Airplane dataset).", "mention_start": 199, "mention_end": 220, "dataset_mention": "the Airplane dataset"}, {"mentioned_in_paper": "986", "context_id": "121", "dataset_context": "For each source point cloud of the Airplane dataset, we rotate the camera by a random angle in the range [\u221275 \u2022 , 75 \u2022 ] with respective to a random axis, and save the visible part as the corresponding target point cloud.", "mention_start": 31, "mention_end": 51, "dataset_mention": "the Airplane dataset"}, {"mentioned_in_paper": "986", "context_id": "122", "dataset_context": "For the Human dataset, we use the whole model to construct the target point cloud.", "mention_start": 4, "mention_end": 21, "dataset_mention": "the Human dataset"}, {"mentioned_in_paper": "986", "context_id": "128", "dataset_context": "Real Dataset To test our metric on unlabeled data, we also construct a real dataset based on the 3D-Match dataset [47], the 7scenes dataset [37] and the RGB-D SLAM dataset [39].", "mention_start": 92, "mention_end": 113, "dataset_mention": "the 3D-Match dataset"}, {"mentioned_in_paper": "986", "context_id": "128", "dataset_context": "Real Dataset To test our metric on unlabeled data, we also construct a real dataset based on the 3D-Match dataset [47], the 7scenes dataset [37] and the RGB-D SLAM dataset [39].", "mention_start": 119, "mention_end": 139, "dataset_mention": " the 7scenes dataset"}, {"mentioned_in_paper": "986", "context_id": "128", "dataset_context": "Real Dataset To test our metric on unlabeled data, we also construct a real dataset based on the 3D-Match dataset [47], the 7scenes dataset [37] and the RGB-D SLAM dataset [39].", "mention_start": 119, "mention_end": 171, "dataset_mention": " the 7scenes dataset [37] and the RGB-D SLAM dataset"}, {"mentioned_in_paper": "986", "context_id": "131", "dataset_context": "Therefore, we select point cloud pairs separated by 20 frames from the RGB-D SLAM dataset and the 7scenes dataset, respectively.", "mention_start": 66, "mention_end": 89, "dataset_mention": "the RGB-D SLAM dataset"}, {"mentioned_in_paper": "986", "context_id": "131", "dataset_context": "Therefore, we select point cloud pairs separated by 20 frames from the RGB-D SLAM dataset and the 7scenes dataset, respectively.", "mention_start": 66, "mention_end": 113, "dataset_mention": "the RGB-D SLAM dataset and the 7scenes dataset"}, {"mentioned_in_paper": "986", "context_id": "132", "dataset_context": "For the 3D-Match dataset, we collect the pre-processed data pairs from [9] 1 where the overlap ratio is greater than 70%.", "mention_start": 4, "mention_end": 24, "dataset_mention": "the 3D-Match dataset"}, {"mentioned_in_paper": "986", "context_id": "139", "dataset_context": "Using the Human test dataset as the benchmark, we compare our results with other optimization-based methods, including ICP [5], FRICP [48] and FGR [50] with their open-source implementations 23 .", "mention_start": 6, "mention_end": 28, "dataset_mention": "the Human test dataset"}, {"mentioned_in_paper": "986", "context_id": "141", "dataset_context": "Tab. 1 shows the performance of different methods on the Human test dataset.", "mention_start": 53, "mention_end": 75, "dataset_mention": "the Human test dataset"}, {"mentioned_in_paper": "986", "context_id": "151", "dataset_context": "Table 2. Optimization using our metric with different settings on the Human dataset [1], including an alternative line intersection method (Insec1), two alternative line sample methods (Sample1 and Sample2), and different values of \u03bd0.", "mention_start": 66, "mention_end": 83, "dataset_mention": "the Human dataset"}, {"mentioned_in_paper": "986", "context_id": "172", "dataset_context": "During preprocessing, we first generate an easier dataset with 100 data pairs and smaller pose differences between the source and target point clouds, and train the model on them for 500 epochs to obtain an overfit model.", "mention_start": 39, "mention_end": 57, "dataset_mention": "an easier dataset"}, {"mentioned_in_paper": "986", "context_id": "175", "dataset_context": "For the real dataset, we generate an easier dataset consisting of pairs that are separated by a smaller number of frames during preprocessing, while the remaining training process is the same as the synthetic datasets.", "mention_start": 33, "mention_end": 51, "dataset_mention": "an easier dataset"}, {"mentioned_in_paper": "986", "context_id": "176", "dataset_context": "Tab. 3 and Tab. 4 show the performance results on the Airplane dataset and the Human dataset, respectively.", "mention_start": 50, "mention_end": 70, "dataset_mention": "the Airplane dataset"}, {"mentioned_in_paper": "986", "context_id": "176", "dataset_context": "Tab. 3 and Tab. 4 show the performance results on the Airplane dataset and the Human dataset, respectively.", "mention_start": 50, "mention_end": 92, "dataset_mention": "the Airplane dataset and the Human dataset"}, {"mentioned_in_paper": "986", "context_id": "179", "dataset_context": "Figs. 6  and 7 show registration results using different methods on two problems from the Human dataset and the Airplane dataset respectively.", "mention_start": 85, "mention_end": 103, "dataset_mention": "the Human dataset"}, {"mentioned_in_paper": "986", "context_id": "179", "dataset_context": "Figs. 6  and 7 show registration results using different methods on two problems from the Human dataset and the Airplane dataset respectively.", "mention_start": 85, "mention_end": 128, "dataset_mention": "the Human dataset and the Airplane dataset"}, {"mentioned_in_paper": "986", "context_id": "189", "dataset_context": "Second, our proposed metric can turn various supervised learning frameworks into unsupervised and has the ability to train on massive real unlabeled suitable data sets.", "mention_start": 125, "mention_end": 167, "dataset_mention": "massive real unlabeled suitable data sets"}, {"mentioned_in_paper": "986", "context_id": "198", "dataset_context": "We provide more visualized results compared with traditional methods, the Fig. 12 and Fig. 13 are the visualized results of other traditional methods and our metric on the Human dataset and the 3D-Match dataset, respectively.", "mention_start": 167, "mention_end": 185, "dataset_mention": "the Human dataset"}, {"mentioned_in_paper": "986", "context_id": "198", "dataset_context": "We provide more visualized results compared with traditional methods, the Fig. 12 and Fig. 13 are the visualized results of other traditional methods and our metric on the Human dataset and the 3D-Match dataset, respectively.", "mention_start": 167, "mention_end": 210, "dataset_mention": "the Human dataset and the 3D-Match dataset"}, {"mentioned_in_paper": "986", "context_id": "202", "dataset_context": "Comparison with variants of chamfer distance by directly optimizing a Lie algebra on Axyz-pose human dataset [1].", "mention_start": 85, "mention_end": 108, "dataset_mention": "Axyz-pose human dataset"}, {"mentioned_in_paper": "988", "context_id": "4", "dataset_context": "Experimental results show that with the help of NMS-Loss, our detector, namely NMS-Ped, achieves impressive results with Miss Rate of 5.92% on Caltech dataset and 10.08% on CityPersons dataset, which are both better than state-of-the-art competitors.", "mention_start": 142, "mention_end": 158, "dataset_mention": "Caltech dataset"}, {"mentioned_in_paper": "988", "context_id": "4", "dataset_context": "Experimental results show that with the help of NMS-Loss, our detector, namely NMS-Ped, achieves impressive results with Miss Rate of 5.92% on Caltech dataset and 10.08% on CityPersons dataset, which are both better than state-of-the-art competitors.", "mention_start": 172, "mention_end": 192, "dataset_mention": "CityPersons dataset"}, {"mentioned_in_paper": "988", "context_id": "28", "dataset_context": "\u2022 With the help of NMS-Loss, in pedestrian detection, our proposed NMS-Ped outperforms SOTA methods on the widely used Caltech and CityPersons datasets.", "mention_start": 118, "mention_end": 151, "dataset_mention": "Caltech and CityPersons datasets"}, {"mentioned_in_paper": "988", "context_id": "61", "dataset_context": "For crowded scenes, especially in the CityPersons dataset, the ground truths of bounding boxes are overlapped with each other.", "mention_start": 33, "mention_end": 57, "dataset_mention": "the CityPersons dataset"}, {"mentioned_in_paper": "988", "context_id": "69", "dataset_context": "We evaluate our method on two challenging pedestrian datasets: Caltech [12, 13] and CityPersons [36].", "mention_start": 42, "mention_end": 61, "dataset_mention": "pedestrian datasets"}, {"mentioned_in_paper": "988", "context_id": "119", "dataset_context": "Tab. 3 presents the performance of NMS-Ped and SOTA methods on the CityPersons dataset.", "mention_start": 63, "mention_end": 86, "dataset_mention": "the CityPersons dataset"}, {"mentioned_in_paper": "989", "context_id": "33", "dataset_context": "This paper presents a video dataset for OR, the first of its kind, with 30, 000 objects in 5, 000 stereo video sequences.", "mention_start": 0, "mention_end": 35, "dataset_mention": "This paper presents a video dataset"}, {"mentioned_in_paper": "989", "context_id": "41", "dataset_context": "Our main contributions are: 1) presenting a new video dataset for object referring, featuring bounding-boxes, language descriptions and human gazes; 2) developing a novel OR approach to detect objects in videos by learning from appearance, motion, gaze, and temporal-spatial context.", "mention_start": 27, "mention_end": 61, "dataset_mention": " 1) presenting a new video dataset"}, {"mentioned_in_paper": "989", "context_id": "58", "dataset_context": "Object Referring Datasets.", "mention_start": 0, "mention_end": 25, "dataset_mention": "Object Referring Datasets"}, {"mentioned_in_paper": "989", "context_id": "60", "dataset_context": "The Google Refexp dataset, which was collected by Mao et al. [34], contains 104, 560 referring expressions annotated for 54, 822 objects from 26, 711 images from the MSCOCO dataset [32].", "mention_start": 0, "mention_end": 25, "dataset_mention": "The Google Refexp dataset"}, {"mentioned_in_paper": "989", "context_id": "60", "dataset_context": "The Google Refexp dataset, which was collected by Mao et al. [34], contains 104, 560 referring expressions annotated for 54, 822 objects from 26, 711 images from the MSCOCO dataset [32].", "mention_start": 161, "mention_end": 180, "dataset_mention": "the MSCOCO dataset"}, {"mentioned_in_paper": "989", "context_id": "64", "dataset_context": "We build on the success of these datasets and present a new object referring dataset for stereo videos.", "mention_start": 54, "mention_end": 84, "dataset_mention": "a new object referring dataset"}, {"mentioned_in_paper": "989", "context_id": "137", "dataset_context": "In this work, we choose to use the existing stereo videos from Cityscapes dataset [11], and annotate language expressions, object bounding boxes, and gaze recordings via crowd sourcing.", "mention_start": 62, "mention_end": 81, "dataset_mention": "Cityscapes dataset"}, {"mentioned_in_paper": "989", "context_id": "143", "dataset_context": "The videos in the Cityscapes dataset are all 2 seconds long, comprising 30 frames.", "mention_start": 14, "mention_end": 36, "dataset_mention": "the Cityscapes dataset"}, {"mentioned_in_paper": "989", "context_id": "150", "dataset_context": "We qualified 20 workers based on their annotation of bounding boxes and natural language descriptions who further an-notated the entire dataset.", "mention_start": 53, "mention_end": 143, "dataset_mention": "bounding boxes and natural language descriptions who further an-notated the entire dataset"}, {"mentioned_in_paper": "989", "context_id": "156", "dataset_context": "The average length of referring expressions of the objects is 15.59 words compared to 8.43 in Google Refexp and 3.61 in the UNC Refexp dataset, which are popular referring expression datasets.", "mention_start": 120, "mention_end": 142, "dataset_mention": "the UNC Refexp dataset"}, {"mentioned_in_paper": "989", "context_id": "156", "dataset_context": "The average length of referring expressions of the objects is 15.59 words compared to 8.43 in Google Refexp and 3.61 in the UNC Refexp dataset, which are popular referring expression datasets.", "mention_start": 153, "mention_end": 191, "dataset_mention": "popular referring expression datasets"}, {"mentioned_in_paper": "989", "context_id": "162", "dataset_context": "This is inspired from Krafke et al. [29] where eye tracking dataset is created via crowdsourcing by asking workers to gaze at particular points on the device screen with their face being recorded.", "mention_start": 47, "mention_end": 67, "dataset_mention": "eye tracking dataset"}, {"mentioned_in_paper": "989", "context_id": "163", "dataset_context": "Here, we collect the gaze recording on objects annotated in Cityscapes dataset as mentioned beforehand in this section.", "mention_start": 59, "mention_end": 78, "dataset_mention": "Cityscapes dataset"}, {"mentioned_in_paper": "989", "context_id": "184", "dataset_context": "The main advantage of Cityscapes referring expression annotations over other referring expressions datasets like GoogleRef, UNC Refexp and ReferIt is that the Cityscapes consists of short video snippets and the corresponding depth maps, suited for our task.", "mention_start": 71, "mention_end": 107, "dataset_mention": "other referring expressions datasets"}, {"mentioned_in_paper": "989", "context_id": "208", "dataset_context": "Out of the 30, 000 annotated objects in our Cityscape dataset [11], we use 80% of videos for the training and 20% for the evaluation of our model on the task of OR in videos.", "mention_start": 39, "mention_end": 61, "dataset_mention": "our Cityscape dataset"}, {"mentioned_in_paper": "990", "context_id": "160", "dataset_context": "For faster experiments, we report results on the ImageNet-100 dataset [61] (Fig. 6).", "mention_start": 44, "mention_end": 69, "dataset_mention": "the ImageNet-100 dataset"}, {"mentioned_in_paper": "990", "context_id": "167", "dataset_context": "Based on the WordNet hierarchy, we merge each category in the ImageNet dataset to its parent class.", "mention_start": 57, "mention_end": 78, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "990", "context_id": "171", "dataset_context": "Mean Transfer Accuracy is the average over 10 transfer datasets.", "mention_start": 43, "mention_end": 63, "dataset_mention": "10 transfer datasets"}, {"mentioned_in_paper": "990", "context_id": "201", "dataset_context": "Additional ablations and results on ImageNet-100 dataset are in supplementary.", "mention_start": 36, "mention_end": 56, "dataset_mention": "ImageNet-100 dataset"}, {"mentioned_in_paper": "990", "context_id": "276", "dataset_context": "Unless specified, we use the ImageNet100 dataset for all the ablations on the semi-supervised setting for faster experimentation.", "mention_start": 24, "mention_end": 48, "dataset_mention": "the ImageNet100 dataset"}, {"mentioned_in_paper": "990", "context_id": "307", "dataset_context": "We set the value of t to 0.9 on the ImageNet100 dataset and to 0.85 on the more diverse (1000 classes) ImageNet-1k dataset.", "mention_start": 32, "mention_end": 55, "dataset_mention": "the ImageNet100 dataset"}, {"mentioned_in_paper": "990", "context_id": "307", "dataset_context": "We set the value of t to 0.9 on the ImageNet100 dataset and to 0.85 on the more diverse (1000 classes) ImageNet-1k dataset.", "mention_start": 71, "mention_end": 122, "dataset_mention": "the more diverse (1000 classes) ImageNet-1k dataset"}, {"mentioned_in_paper": "990", "context_id": "310", "dataset_context": "Results on ImageNet100 dataset are shown in table A7. k-NN classifier has lower pseudo-labeling accuracy and thus results in poorer performance.", "mention_start": 11, "mention_end": 30, "dataset_mention": "ImageNet100 dataset"}, {"mentioned_in_paper": "990", "context_id": "320", "dataset_context": "ImageNet dataset was constructed using the WordNet hierarchy.", "mention_start": 0, "mention_end": 16, "dataset_mention": "ImageNet dataset"}, {"mentioned_in_paper": "992", "context_id": "36", "dataset_context": "All of the aforementioned studies investigate recognition/classification of fully observed action or activity, e.g., jumping, walking, running, drinking, etc. (i.e., activities of daily living), using well-curated datasets.", "mention_start": 200, "mention_end": 222, "dataset_mention": "well-curated datasets"}, {"mentioned_in_paper": "992", "context_id": "117", "dataset_context": "We selected four models for evaluating the performance of RAHAR against the performance of an expert-based HAR using a tool on the described actigraphy dataset: logistic regression, support vector machines with radial basis function kernel, random forest, and adaboost.", "mention_start": 127, "mention_end": 159, "dataset_mention": "the described actigraphy dataset"}, {"mentioned_in_paper": "992", "context_id": "132", "dataset_context": "With an AU-ROC score of 0.5884 for SE+AL approach, the logistic regression model was, however, unable to stratify the dataset, and so predicted all cases to be in a single class.", "mention_start": 104, "mention_end": 125, "dataset_mention": "stratify the dataset"}, {"mentioned_in_paper": "993", "context_id": "6", "dataset_context": "Experiments on the KITTI dataset show that our results significantly outperform other state-ofthe-art algorithms.", "mention_start": 15, "mention_end": 32, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "993", "context_id": "105", "dataset_context": "We evaluate our methods on the KITTI dataset, and compare our results to existing supervised and unsupervised methods on the tasks of depth, optical flow, camera motion, scene flow and motion segmentation.", "mention_start": 27, "mention_end": 44, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "993", "context_id": "114", "dataset_context": "Notice that KITTI 2012 dataset only contains static scenes.", "mention_start": 0, "mention_end": 30, "dataset_mention": "Notice that KITTI 2012 dataset"}, {"mentioned_in_paper": "993", "context_id": "159", "dataset_context": "Motion Segmentation Evaluation The motion segmentation task is evaluated using the object map provided by the KITTI 2015 dataset.", "mention_start": 106, "mention_end": 128, "dataset_mention": "the KITTI 2015 dataset"}, {"mentioned_in_paper": "993", "context_id": "165", "dataset_context": "In summary, by mutually leveraging stereo and temporal information, and treating the learning of depth and optical flow as a whole, our proposed method shows substantial improvements on unsupervised learning of depth, optical flow, and motion segmentation on the KITTI dataset.", "mention_start": 258, "mention_end": 276, "dataset_mention": "the KITTI dataset"}, {"mentioned_in_paper": "994", "context_id": "47", "dataset_context": "Following related work on facial emotion recognition, these experiments are conducted on the two benchmark public face emotion datasets: Ck+ dataset [12], and Fer2013 dataset [8].", "mention_start": 88, "mention_end": 135, "dataset_mention": "the two benchmark public face emotion datasets"}, {"mentioned_in_paper": "994", "context_id": "47", "dataset_context": "Following related work on facial emotion recognition, these experiments are conducted on the two benchmark public face emotion datasets: Ck+ dataset [12], and Fer2013 dataset [8].", "mention_start": 136, "mention_end": 148, "dataset_mention": " Ck+ dataset"}, {"mentioned_in_paper": "994", "context_id": "47", "dataset_context": "Following related work on facial emotion recognition, these experiments are conducted on the two benchmark public face emotion datasets: Ck+ dataset [12], and Fer2013 dataset [8].", "mention_start": 154, "mention_end": 174, "dataset_mention": " and Fer2013 dataset"}, {"mentioned_in_paper": "994", "context_id": "48", "dataset_context": "Ck+: The most widely utilized laboratory-controlled dataset for facial expression recognition is the Extended Cohn-Kanade (Ck+) Dataset [12] (some samples are shown in Figure 3).", "mention_start": 29, "mention_end": 59, "dataset_mention": "laboratory-controlled dataset"}, {"mentioned_in_paper": "994", "context_id": "48", "dataset_context": "Ck+: The most widely utilized laboratory-controlled dataset for facial expression recognition is the Extended Cohn-Kanade (Ck+) Dataset [12] (some samples are shown in Figure 3).", "mention_start": 96, "mention_end": 135, "dataset_mention": "the Extended Cohn-Kanade (Ck+) Dataset"}, {"mentioned_in_paper": "994", "context_id": "49", "dataset_context": "Sequences that change from neutral to peak expression are included in the Ck+ dataset.", "mention_start": 70, "mention_end": 85, "dataset_mention": "the Ck+ dataset"}, {"mentioned_in_paper": "994", "context_id": "62", "dataset_context": "The results of the Ck+ dataset 6 indicate the accuracies of the Ck+ dataset and those of the HDACK+ dataset.", "mention_start": 15, "mention_end": 30, "dataset_mention": "the Ck+ dataset"}, {"mentioned_in_paper": "994", "context_id": "62", "dataset_context": "The results of the Ck+ dataset 6 indicate the accuracies of the Ck+ dataset and those of the HDACK+ dataset.", "mention_start": 60, "mention_end": 75, "dataset_mention": "the Ck+ dataset"}, {"mentioned_in_paper": "994", "context_id": "62", "dataset_context": "The results of the Ck+ dataset 6 indicate the accuracies of the Ck+ dataset and those of the HDACK+ dataset.", "mention_start": 89, "mention_end": 107, "dataset_mention": "the HDACK+ dataset"}, {"mentioned_in_paper": "994", "context_id": "63", "dataset_context": "The dataset with data augmentation always has a higher performance in most models, and the ResNet18 model achieved 100% testing accuracy in the HDACK+ dataset.", "mention_start": 139, "mention_end": 158, "dataset_mention": "the HDACK+ dataset"}, {"mentioned_in_paper": "994", "context_id": "68", "dataset_context": "For the left one of each column, the Ck+ dataset is used.", "mention_start": 32, "mention_end": 48, "dataset_mention": " the Ck+ dataset"}, {"mentioned_in_paper": "994", "context_id": "71", "dataset_context": "However, when comes to the right one of each column, the HDACK+ dataset is used.", "mention_start": 52, "mention_end": 71, "dataset_mention": " the HDACK+ dataset"}, {"mentioned_in_paper": "994", "context_id": "74", "dataset_context": "There are 28,709, 3,589 and 3,589 images respectively for training, validation, and testing with seven expression labels in the Fer2013 dataset (shown in Figure 7).", "mention_start": 123, "mention_end": 143, "dataset_mention": "the Fer2013 dataset"}, {"mentioned_in_paper": "994", "context_id": "75", "dataset_context": "The results of the Fer2013 dataset and the HDAFer2013 dataset are presented in Table 3, where both the Fer2013 dataset and the HDAFer2013 dataset are divided in the ratio of 8:1:1 for training, validating, and testing respectively.", "mention_start": 15, "mention_end": 34, "dataset_mention": "the Fer2013 dataset"}, {"mentioned_in_paper": "994", "context_id": "75", "dataset_context": "The results of the Fer2013 dataset and the HDAFer2013 dataset are presented in Table 3, where both the Fer2013 dataset and the HDAFer2013 dataset are divided in the ratio of 8:1:1 for training, validating, and testing respectively.", "mention_start": 15, "mention_end": 61, "dataset_mention": "the Fer2013 dataset and the HDAFer2013 dataset"}, {"mentioned_in_paper": "994", "context_id": "75", "dataset_context": "The results of the Fer2013 dataset and the HDAFer2013 dataset are presented in Table 3, where both the Fer2013 dataset and the HDAFer2013 dataset are divided in the ratio of 8:1:1 for training, validating, and testing respectively.", "mention_start": 93, "mention_end": 118, "dataset_mention": "both the Fer2013 dataset"}, {"mentioned_in_paper": "994", "context_id": "75", "dataset_context": "The results of the Fer2013 dataset and the HDAFer2013 dataset are presented in Table 3, where both the Fer2013 dataset and the HDAFer2013 dataset are divided in the ratio of 8:1:1 for training, validating, and testing respectively.", "mention_start": 93, "mention_end": 145, "dataset_mention": "both the Fer2013 dataset and the HDAFer2013 dataset"}, {"mentioned_in_paper": "994", "context_id": "77", "dataset_context": " 8 indicate the accuracies of the Fer2013 dataset and those of the HDAFer2013 dataset.", "mention_start": 29, "mention_end": 49, "dataset_mention": "the Fer2013 dataset"}, {"mentioned_in_paper": "994", "context_id": "77", "dataset_context": " 8 indicate the accuracies of the Fer2013 dataset and those of the HDAFer2013 dataset.", "mention_start": 62, "mention_end": 85, "dataset_mention": "the HDAFer2013 dataset"}, {"mentioned_in_paper": "994", "context_id": "83", "dataset_context": "For the left one of each column, the Fer2013 dataset is used.", "mention_start": 32, "mention_end": 52, "dataset_mention": " the Fer2013 dataset"}, {"mentioned_in_paper": "994", "context_id": "85", "dataset_context": "However, when comes to the right one of each column, the HDAFer2013 dataset is used.", "mention_start": 52, "mention_end": 75, "dataset_mention": " the HDAFer2013 dataset"}, {"mentioned_in_paper": "996", "context_id": "30", "dataset_context": "In our experiments, we demonstrate that both our proposed AE and the convolution and (un)pooling operators exceed SOTA performance on the D-FAUST [4] dynamic 3D human body dataset which contains large variations in both pose and local details.", "mention_start": 133, "mention_end": 179, "dataset_mention": "the D-FAUST [4] dynamic 3D human body dataset"}, {"mentioned_in_paper": "996", "context_id": "144", "dataset_context": "We first experimented on the 2D-manifold D-FAUST human body dataset [4].", "mention_start": 25, "mention_end": 67, "dataset_mention": "the 2D-manifold D-FAUST human body dataset"}, {"mentioned_in_paper": "996", "context_id": "156", "dataset_context": "Therefore, we experimented our network on a high-resolution human dataset that contains 24,628 fully aligned meshes, each with 154k vertices and 308k triangles.", "mention_start": 41, "mention_end": 73, "dataset_mention": "a high-resolution human dataset"}, {"mentioned_in_paper": "996", "context_id": "172", "dataset_context": "For comparison, we choose to test on the D-FAUST dataset as it captures both high-frequency variance in poses and low-frequency variance in local details and is widely used for estimation in previous works.", "mention_start": 36, "mention_end": 56, "dataset_mention": "the D-FAUST dataset"}, {"mentioned_in_paper": "996", "context_id": "178", "dataset_context": "We also compare with MeshCNN [14], but found that MeshCNN is infeasible for the full resolution D-FAUST dataset due to memory and speed constraints.", "mention_start": 75, "mention_end": 111, "dataset_mention": "the full resolution D-FAUST dataset"}, {"mentioned_in_paper": "996", "context_id": "186", "dataset_context": "All experiments were trained on the D-FAUST dataset with the same setting.", "mention_start": 32, "mention_end": 51, "dataset_mention": "the D-FAUST dataset"}, {"mentioned_in_paper": "996", "context_id": "246", "dataset_context": "For the high resolution human dataset, Our auto-encoder has 6 down-sampling residual blocks and 6 up-sampling residual blocks, with s = 2,r = 2 and M = 17 for all blocks.", "mention_start": 4, "mention_end": 37, "dataset_mention": "the high resolution human dataset"}, {"mentioned_in_paper": "996", "context_id": "249", "dataset_context": "The hand dataset contains fully aligned hand meshes reconstructed from performance captures of two people with roughly 200 seconds of 90 poses per person.", "mention_start": 0, "mention_end": 16, "dataset_mention": "The hand dataset"}, {"mentioned_in_paper": "997", "context_id": "186", "dataset_context": "The proposed method was quantitatively evaluated using the publicly available AVM image dataset called Tongji Parking Slot Dataset 2.0 (PS2.0)", "mention_start": 55, "mention_end": 95, "dataset_mention": "the publicly available AVM image dataset"}, {"mentioned_in_paper": "997", "context_id": "186", "dataset_context": "The proposed method was quantitatively evaluated using the publicly available AVM image dataset called Tongji Parking Slot Dataset 2.0 (PS2.0)", "mention_start": 55, "mention_end": 130, "dataset_mention": "the publicly available AVM image dataset called Tongji Parking Slot Dataset"}, {"mentioned_in_paper": "998", "context_id": "160", "dataset_context": "As for our fine-grained correspondence network, we use YouTube-VOS [79] as our pre-training dataset for direct comparison with previous works [35].", "mention_start": 74, "mention_end": 99, "dataset_mention": "our pre-training dataset"}, {"mentioned_in_paper": "998", "context_id": "162", "dataset_context": "Although Youtube-VOS is a video dataset, we treat it as a conventional image dataset and randomly sample individual frames during training (equivalent to 95k images).", "mention_start": 55, "mention_end": 84, "dataset_mention": "a conventional image dataset"}, {"mentioned_in_paper": "998", "context_id": "192", "dataset_context": "We show the results on DAVIS-2017 of FC using different pre-training datasets in Appendix B.1.", "mention_start": 46, "mention_end": 77, "dataset_mention": "different pre-training datasets"}, {"mentioned_in_paper": "998", "context_id": "196", "dataset_context": "This indicates that the performance gain of SFC doesn't come from the extra YouTube-VOS dataset.", "mention_start": 66, "mention_end": 95, "dataset_mention": "the extra YouTube-VOS dataset"}, {"mentioned_in_paper": "998", "context_id": "225", "dataset_context": "On the DAVIS dataset, we show that a large (smooth) or small (sharp) r is demonstrably harmful to performance.", "mention_start": 3, "mention_end": 20, "dataset_mention": "the DAVIS dataset"}, {"mentioned_in_paper": "998", "context_id": "274", "dataset_context": "When pretrained on non object-centric dataset (e.g.", "mention_start": 19, "mention_end": 45, "dataset_mention": "non object-centric dataset"}, {"mentioned_in_paper": "998", "context_id": "276", "dataset_context": "At the same time, it is largely recognized that a larger dataset usually results in stronger semantic representation for these methods.", "mention_start": 23, "mention_end": 64, "dataset_mention": "largely recognized that a larger dataset"}, {"mentioned_in_paper": "1003", "context_id": "31", "dataset_context": "1. Dataset containing 35 activity types recorded during 42 actual trauma resuscitations.", "mention_start": 0, "mention_end": 10, "dataset_mention": "1. Dataset"}, {"mentioned_in_paper": "1003", "context_id": "36", "dataset_context": "2. The Charades dataset [11] of 9,848 videos of indoors daily activities performed by a single person (another person may be in the scene).", "mention_start": 0, "mention_end": 23, "dataset_mention": "2. The Charades dataset"}, {"mentioned_in_paper": "1003", "context_id": "39", "dataset_context": "3. Olympic sports dataset with 16 sports activities performed by athletes, without concurrent activities [12].", "mention_start": 0, "mention_end": 25, "dataset_mention": "3. Olympic sports dataset"}, {"mentioned_in_paper": "1003", "context_id": "74", "dataset_context": "Our analysis of the 42 trauma resuscitations dataset showed that more than 50% of time instances had at least two concurrent activities (Fig. 1 (a)).", "mention_start": 16, "mention_end": 52, "dataset_mention": "the 42 trauma resuscitations dataset"}, {"mentioned_in_paper": "1003", "context_id": "75", "dataset_context": "Even for daily living scenarios (Charades dataset) there were more than 70% time instances with at least two concurrent activities (Fig. 3).", "mention_start": 9, "mention_end": 49, "dataset_mention": "daily living scenarios (Charades dataset"}, {"mentioned_in_paper": "1003", "context_id": "163", "dataset_context": "The fully-connected layer takes the output of the previous LSTM layer and has N neurons, where N is the total number of activities (N=35 for the trauma resuscitation dataset).", "mention_start": 140, "mention_end": 173, "dataset_mention": "the trauma resuscitation dataset"}, {"mentioned_in_paper": "1003", "context_id": "250", "dataset_context": "To evaluate our system on concurrent activity recognition using different data sources, we selected the Charades dataset, containing 157 action labels [11].", "mention_start": 99, "mention_end": 120, "dataset_mention": "the Charades dataset"}, {"mentioned_in_paper": "1003", "context_id": "254", "dataset_context": "Second, concurrent activities are common in daily living scenarios and this dataset demonstrates our system's ability to recognize concurrent activities.", "mention_start": 43, "mention_end": 83, "dataset_mention": "daily living scenarios and this dataset"}, {"mentioned_in_paper": "1003", "context_id": "265", "dataset_context": "Our system also works well with regular single-activity prediction from single sensor data and we used the Olympic sports dataset [12] as a demonstration.", "mention_start": 103, "mention_end": 129, "dataset_mention": "the Olympic sports dataset"}, {"mentioned_in_paper": "1003", "context_id": "268", "dataset_context": "Similar to the Charades dataset, we slightly modified our network structure to have only an RGB-video ConvNet branch and removed the fusion layer.", "mention_start": 11, "mention_end": 31, "dataset_mention": "the Charades dataset"}, {"mentioned_in_paper": "1003", "context_id": "278", "dataset_context": "There were 35 activity types labeled in the trauma resuscitation dataset and 157 in the Charades dataset.", "mention_start": 40, "mention_end": 72, "dataset_mention": "the trauma resuscitation dataset"}, {"mentioned_in_paper": "1003", "context_id": "278", "dataset_context": "There were 35 activity types labeled in the trauma resuscitation dataset and 157 in the Charades dataset.", "mention_start": 84, "mention_end": 104, "dataset_mention": "the Charades dataset"}, {"mentioned_in_paper": "1003", "context_id": "284", "dataset_context": "We applied our system to the well-known CIFAR 100 dataset, containing 100 types of images to perform concurrent image recognition.", "mention_start": 25, "mention_end": 57, "dataset_mention": "the well-known CIFAR 100 dataset"}, {"mentioned_in_paper": "1003", "context_id": "286", "dataset_context": "To simulate concurrent image recognition (recognizing multiple targets in a single image at once), we randomly selected 6 images from the CIFAR 100 dataset and combined them into a single large image (Fig. 11).", "mention_start": 133, "mention_end": 155, "dataset_mention": "the CIFAR 100 dataset"}, {"mentioned_in_paper": "1003", "context_id": "320", "dataset_context": "We performed some preliminary experiments using the famous MNIST dataset [46] and CIFAR 100 dataset [42] for multi-target image recognition.", "mention_start": 48, "mention_end": 72, "dataset_mention": "the famous MNIST dataset"}, {"mentioned_in_paper": "1003", "context_id": "320", "dataset_context": "We performed some preliminary experiments using the famous MNIST dataset [46] and CIFAR 100 dataset [42] for multi-target image recognition.", "mention_start": 48, "mention_end": 99, "dataset_mention": "the famous MNIST dataset [46] and CIFAR 100 dataset"}, {"mentioned_in_paper": "1003", "context_id": "322", "dataset_context": "The preliminary results show 94.1% accuracy and 0.96 mAP for multiple digits recognition in MNIST dataset and 91.3% accuracy with 0.16 mAP for CIFAR 100 dataset.", "mention_start": 92, "mention_end": 105, "dataset_mention": "MNIST dataset"}, {"mentioned_in_paper": "1003", "context_id": "322", "dataset_context": "The preliminary results show 94.1% accuracy and 0.96 mAP for multiple digits recognition in MNIST dataset and 91.3% accuracy with 0.16 mAP for CIFAR 100 dataset.", "mention_start": 143, "mention_end": 160, "dataset_mention": "CIFAR 100 dataset"}, {"mentioned_in_paper": "1004", "context_id": "266", "dataset_context": "Next, they form a transaction dataset that is formed by a chronological set of events (co-authoring articles).", "mention_start": 15, "mention_end": 37, "dataset_mention": "a transaction dataset"}, {"mentioned_in_paper": "1004", "context_id": "443", "dataset_context": "\u2022 BioSNAP [140] : more than 30 Bio networks data sets by Stanford Network Analysis Platform \u2022 KONECT [141] : this collection contains more than 250 network data sets of various types, including social networks, authorship networks, interaction networks, etc. \u2022 PAJEK [142] : this collection contains more than 40 data sets of various types.", "mention_start": 27, "mention_end": 53, "dataset_mention": "30 Bio networks data sets"}, {"mentioned_in_paper": "1004", "context_id": "443", "dataset_context": "\u2022 BioSNAP [140] : more than 30 Bio networks data sets by Stanford Network Analysis Platform \u2022 KONECT [141] : this collection contains more than 250 network data sets of various types, including social networks, authorship networks, interaction networks, etc. \u2022 PAJEK [142] : this collection contains more than 40 data sets of various types.", "mention_start": 143, "mention_end": 165, "dataset_mention": "250 network data sets"}, {"mentioned_in_paper": "1004", "context_id": "444", "dataset_context": "\u2022 Network Repository [143] : a huge collection of more than 5000 network data sets of various types, including social networks.", "mention_start": 59, "mention_end": 82, "dataset_mention": "5000 network data sets"}, {"mentioned_in_paper": "1004", "context_id": "445", "dataset_context": "\u2022 Uri ALON [144] : a collection of complex networks data sets by Uri Alon Lab.", "mention_start": 34, "mention_end": 61, "dataset_mention": "complex networks data sets"}, {"mentioned_in_paper": "1004", "context_id": "447", "dataset_context": "\u2022 WOSN 2009 Data Sets [146] : a collection of Facebook data provided by social computing group.", "mention_start": 0, "mention_end": 21, "dataset_mention": "\u2022 WOSN 2009 Data Sets"}, {"mentioned_in_paper": "1004", "context_id": "448", "dataset_context": "\u2022 Citation Network Data set [147] : a collection of citation network dat aset extracted from DBLP, ACM, and other sources.", "mention_start": 0, "mention_end": 27, "dataset_mention": "\u2022 Citation Network Data set"}, {"mentioned_in_paper": "1004", "context_id": "449", "dataset_context": "\u2022 Grouplens Research [148] : a movie rating network data set.", "mention_start": 28, "mention_end": 60, "dataset_mention": " a movie rating network data set"}, {"mentioned_in_paper": "1004", "context_id": "472", "dataset_context": "The resulting unbalanced dataset obstructs the handling of link prediction problems, especially with the utilization of supervised techniques.", "mention_start": 0, "mention_end": 32, "dataset_mention": "The resulting unbalanced dataset"}, {"mentioned_in_paper": "1005", "context_id": "163", "dataset_context": "Hence, we curated a balanced dataset with 500 randomly chosen images of 5 iconographies (100 per each iconography) from the publicly available WGA [25] dataset of art historic paintings.", "mention_start": 119, "mention_end": 159, "dataset_mention": "the publicly available WGA [25] dataset"}, {"mentioned_in_paper": "1005", "context_id": "192", "dataset_context": "Additionally, we evaluated ResNet50 [33] pre-trained on the ImageNet dataset and fine-tuned on the Places365 dataset [34].", "mention_start": 55, "mention_end": 76, "dataset_mention": "the ImageNet dataset"}, {"mentioned_in_paper": "1005", "context_id": "192", "dataset_context": "Additionally, we evaluated ResNet50 [33] pre-trained on the ImageNet dataset and fine-tuned on the Places365 dataset [34].", "mention_start": 94, "mention_end": 116, "dataset_mention": "the Places365 dataset"}, {"mentioned_in_paper": "1005", "context_id": "199", "dataset_context": "We re-implemented their work, because their code is not publicly available, and compared their results with ours on WGA500 dataset.", "mention_start": 115, "mention_end": 130, "dataset_mention": "WGA500 dataset"}, {"mentioned_in_paper": "1005", "context_id": "224", "dataset_context": "However for art historical datasets, the intra-class distance can vary greatly and thus affect the low-level similarity.", "mention_start": 12, "mention_end": 35, "dataset_mention": "art historical datasets"}, {"mentioned_in_paper": "1005", "context_id": "296", "dataset_context": "Therefore our method has potential use cases in linking big art historical datasets based on their compositions.", "mention_start": 56, "mention_end": 83, "dataset_mention": "big art historical datasets"}]